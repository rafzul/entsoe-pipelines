{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#buat pandasrawclient yang bentuknya dah langsung table data \n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "from google.oauth2 import service_account\n",
    "import xmltodict\n",
    "import json\n",
    "import pandas as pd \n",
    "from requests import request\n",
    "import pytz\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import Dict\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud import storage\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "import csv\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, ArrayType, FloatType, BooleanType\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType, DataType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from entsoe import EntsoeRawClient\n",
    "from entsoe import EntsoePandasClient\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load env variables\n",
    "load_dotenv('./creds/.env', verbose=True, override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(os.environ.get(\"PYSPARK_DRIVER_PYTHON\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entsoe_analytics_csv_1009\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "'''\n",
    "----------------\n",
    "INIT VARIABLES\n",
    "----------------\n",
    "'''\n",
    "\n",
    "#setting up entsoe variables\n",
    "security_token = os.environ.get(\"SECURITY_TOKEN\")\n",
    "ENTSOE_URL = 'https://transparency.entsoe.eu/api'\n",
    "\n",
    "#setting up GCP variables\n",
    "service_account_file = os.environ.get(\"SERVICE_ACCOUNT_FILE\")\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    service_account_file\n",
    ")\n",
    "gcs_bucket = os.environ.get(\"CLOUD_STORAGE_BUCKET\")\n",
    "print(gcs_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#setting up session\n",
    "entsoe_client = EntsoePandasClient(security_token)\n",
    "'''\n",
    "----------------\n",
    "SETTING UP FUNCTION CALLS \n",
    "----------------\n",
    "'''\n",
    "\n",
    "# upload data to GCS\n",
    "def upload_blob_to_gcs(bucket_name, contents, destination_blob_name):\n",
    "    # Upload file to bucket\"\"\"\n",
    "\n",
    "    # ID of GCS bucket\n",
    "    # bucket_name =\n",
    "\n",
    "    # the contents from memory to be uploaded to file\n",
    "    # contents =\n",
    "\n",
    "    # the ID of your GCS object\n",
    "    # destination_blob_name =\n",
    "\n",
    "    storage_client = storage.Client(credentials=credentials)\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_string(contents)\n",
    "\n",
    "\n",
    "'''\n",
    "----------------\n",
    "EXTRACTION\n",
    "----------------\n",
    "'''\n",
    "#for test, we'll be querying\n",
    "\n",
    "start=pd.Timestamp('202101010000', tz='Europe/Berlin')\n",
    "end=pd.Timestamp('202101010600', tz='Europe/Berlin')\n",
    "country_code= 'DE_TENNET'\n",
    "country_code_from=''\n",
    "country_code_to=''\n",
    "type_marketagreement_type=''\n",
    "contract_marketagreement_type=''\n",
    "label_data='total_generation'\n",
    "\n",
    "try:\n",
    "    entsoe_data = entsoe_client.query_generation(country_code, start=start, end=end)\n",
    "    #for those method which header is in first AND second row (place it in if block later)\n",
    "    #get header correctly if it's for query generation\n",
    "    entsoe_header_list = [f\"{x} - {y}\" for x,y in entsoe_data.columns]\n",
    "    entsoe_data = entsoe_data[1:]\n",
    "    #if header is already correct, header = True. if not, header = entsoe_header_list\n",
    "    entsoe_csv = entsoe_data.to_csv(header=entsoe_header_list)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"An exception occurred:\", e)\n",
    "\n",
    "\n",
    "'''\n",
    "----------------\n",
    "LOAD\n",
    "----------------\n",
    "'''\n",
    "#upload to GCS\n",
    "landing_filename=f\"{label_data}__{country_code}__{start}__{end}.csv\"\n",
    "upload_blob_to_gcs(bucket_name=gcs_bucket, contents=entsoe_csv, destination_blob_name=landing_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "## COBA SPARK GCS CONNECTOR ##\n",
    "\n",
    "\n",
    "# berangkat pak haji\n",
    "# setup parameternya\n",
    "gcs_bucket = gcs_bucket\n",
    "path = f\"gs://{gcs_bucket}/{landing_filename}\"\n",
    "# path =\"/home/rafzul/projects/entsoe-pipelines/sample.xml\"\n",
    "\n",
    "#coba spark gcs connector\n",
    "#setup sparksession for entry point - COBA GCS CONNECTOR\n",
    "SPARK_HOME = os.environ[\"SPARK_HOME\"]\n",
    "spark = SparkSession.builder.appName(\"gcp_playground\") \\\n",
    "    .config(\"spark.jars\", f\"{SPARK_HOME}/jars/gcs-connector-hadoop3-latest.jar, {SPARK_HOME}/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar\") \\\n",
    "    .config(\"spark.hadoop.fs.AbstractFileSystem.gs.impl\", \"google.cloud.hadoop.fs.gcs.GoogleHadoopFS\") \\\n",
    "    .config(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .config(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", service_account_file) \\\n",
    "    .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setiap jam 12 malam, ambil schema dari gcs\n",
    "# download data from GCS\n",
    "def download_blob_from_gcs(bucket_name, source_blob_name, local_source_blob_name):\n",
    "    # Upload file to bucket\"\"\"\n",
    "\n",
    "    # ID of GCS bucket\n",
    "    # bucket_name =\n",
    "\n",
    "    # the contents from memory to be uploaded to file\n",
    "    # contents =\n",
    "\n",
    "    # the ID of your GCS object\n",
    "    # destination_blob_name =\n",
    "\n",
    "    storage_client = storage.Client(credentials=credentials)\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(source_blob_name)\n",
    "\n",
    "    blob.download_to_filename(local_source_blob_name)\n",
    "    \n",
    "    \n",
    "source_schema_filename = f\"schema_source/{label_data}__schema.json\"\n",
    "local_source_schema_filename = f\"/home/rafzul/projects/entsoe-pipelines/schemas/source/{label_data}__schema.json\"\n",
    "download_blob_from_gcs(bucket_name=gcs_bucket, source_blob_name=source_schema_filename, local_source_blob_name=local_source_schema_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up schema - block\n",
    "\n",
    "with open(local_source_schema_filename, \"r\") as local_source:\n",
    "    source_schema_data = json.loads(local_source.read())\n",
    "    source_schema_version = json_schema_data[\"version\"]\n",
    "    json_enforced_source_schema = StructType.fromJson(source_schema_data[\"schema\"])\n",
    "    \n",
    "\n",
    "try: \n",
    "   df_spark = spark.read.format(\"csv\").schema(json_enforced_source_schema) \\\n",
    "      .option(\"inferSchema\",\"false\") \\\n",
    "      .option(\"header\",\"true\") \\\n",
    "      .load(path)\n",
    "except Exception as e:\n",
    "   df_spark = spark.read.format(\"csv\") \\\n",
    "      .option(\"inferSchema\",\"true\") \\\n",
    "      .option(\"header\",\"true\") \\\n",
    "      .load(path)\n",
    "   schema_version = source_schema_version + 0.1\n",
    "   new_schema = { \"version\": float(schema_version),\"schema\": df_spark.schema.jsonValue()}\n",
    "   new_schema_json = json.dumps(new_schema)\n",
    "   new_source_schema_filename_new = f\"schema_source/{label_data}__schema__{schema_version}.json\"\n",
    "   upload_blob_to_gcs(bucket_name=gcs_bucket, contents=new_schema_json, destination_blob_name=new_source_schema_filenam)\n",
    "\n",
    "\n",
    "   # create code to increment schema version\n",
    "#inferSchema true kalo ngga ada kesalahan. except exception: inferSchema false dan .schema nya di set manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_c0', 'timestamp'),\n",
       " ('Biomass - Actual Aggregated', 'double'),\n",
       " ('Fossil Brown coal/Lignite - Actual Aggregated', 'double'),\n",
       " ('Fossil Gas - Actual Aggregated', 'double'),\n",
       " ('Fossil Hard coal - Actual Aggregated', 'double'),\n",
       " ('Fossil Oil - Actual Aggregated', 'double'),\n",
       " ('Geothermal - Actual Aggregated', 'double'),\n",
       " ('Hydro Pumped Storage - Actual Aggregated', 'double'),\n",
       " ('Hydro Pumped Storage - Actual Consumption', 'double'),\n",
       " ('Hydro Run-of-river and poundage - Actual Aggregated', 'double'),\n",
       " ('Hydro Water Reservoir - Actual Aggregated', 'double'),\n",
       " ('Nuclear - Actual Aggregated', 'double'),\n",
       " ('Other - Actual Aggregated', 'double'),\n",
       " ('Other renewable - Actual Aggregated', 'double'),\n",
       " ('Solar - Actual Aggregated', 'double'),\n",
       " ('Waste - Actual Aggregated', 'double'),\n",
       " ('Wind Offshore - Actual Aggregated', 'double'),\n",
       " ('Wind Onshore - Actual Aggregated', 'double')]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_schema = { \"version\": float(schema_version),\n",
    "#               \"schema\": df_spark.schema.jsonValue()}\n",
    "# new_schema_json = json.dumps(new_schema)\n",
    "# source_schema_filename = f\"schema_source/{label_data}__schema__{schema_version}.json\"\n",
    "# upload_blob_to_gcs(bucket_name=gcs_bucket, contents=new_schema_json, destination_blob_name=source_schema_filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #create schema to be enforced in subsequent json load operation\n",
    "# with open(\"schema_raw_j\", \"w\") as schrawjson:\n",
    "#     schrawjson.write(df_spark_orig.schema.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"schema_raw2.json\", \"r\") as schrawjson:\n",
    "#     json_schema_data = schrawjson.read()\n",
    "#     json_enforced_schema = StructType.fromJson(json.loads(json_schema_data))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://entsoe_analytics_1009/entsoe_data_DE_TENNET.json\n"
     ]
    }
   ],
   "source": [
    "# #create dataframe from gcs\n",
    "# path = f\"gs://{gcs_bucket}/{landing_filename}\"\n",
    "# print(path)\n",
    "# df_spark = spark.read.format(\"json\").schema(json_enforced_schema) \\\n",
    "#    .option(\"header\",\"true\") \\\n",
    "#    .option(\"multiLine\",\"true\") \\\n",
    "#    .load(path) \\\n",
    "#    .select(\"GL_MarketDocument.*\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean non timeseries column name. change dot to underscores\n",
    "df_spark = df_spark.toDF(*(c_name.replace(\".\", \"_\") for c_name in df_spark.columns))\n",
    "\n",
    "#clean timeseries column. cast TimeSeries to new scheme where 1.dot in names are replaced with underscores and 2. Strange characters such as '@' or '#' are removed\n",
    "ts_schema = df_spark.select(\"TimeSeries\").dtypes[0][1]\n",
    "replacements = [('\\.', '_'), ('[@#]', '')]\n",
    "for old, new in replacements:\n",
    "    ts_schema = re.sub(old, new, ts_schema)\n",
    "df_spark = df_spark.withColumn(\"TimeSeries\", (F.col(\"TimeSeries\").cast(ts_schema)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten column\n",
    "\n",
    "# explode timeseries column into struct\n",
    "df_spark = df_spark.withColumn(\"TimeSeries\", F.explode(\"TimeSeries\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- @xmlns: string (nullable = true)\n",
      " |-- mRID: string (nullable = true)\n",
      " |-- revisionNumber: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- process_processType: string (nullable = true)\n",
      " |-- sender_MarketParticipant_mRID: struct (nullable = true)\n",
      " |    |-- #text: string (nullable = true)\n",
      " |    |-- @codingScheme: string (nullable = true)\n",
      " |-- sender_MarketParticipant_marketRole_type: string (nullable = true)\n",
      " |-- receiver_MarketParticipant_mRID: struct (nullable = true)\n",
      " |    |-- #text: string (nullable = true)\n",
      " |    |-- @codingScheme: string (nullable = true)\n",
      " |-- receiver_MarketParticipant_marketRole_type: string (nullable = true)\n",
      " |-- createdDateTime: string (nullable = true)\n",
      " |-- time_Period_timeInterval: struct (nullable = true)\n",
      " |    |-- end: string (nullable = true)\n",
      " |    |-- start: string (nullable = true)\n",
      " |-- TimeSeries: struct (nullable = true)\n",
      " |    |-- MktPSRType: struct (nullable = true)\n",
      " |    |    |-- psrType: string (nullable = true)\n",
      " |    |-- Period: struct (nullable = true)\n",
      " |    |    |-- Point: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- position: string (nullable = true)\n",
      " |    |    |    |    |-- quantity: string (nullable = true)\n",
      " |    |    |-- resolution: string (nullable = true)\n",
      " |    |    |-- timeInterval: struct (nullable = true)\n",
      " |    |    |    |-- end: string (nullable = true)\n",
      " |    |    |    |-- start: string (nullable = true)\n",
      " |    |-- businessType: string (nullable = true)\n",
      " |    |-- curveType: string (nullable = true)\n",
      " |    |-- inBiddingZone_Domain_mRID: struct (nullable = true)\n",
      " |    |    |-- text: string (nullable = true)\n",
      " |    |    |-- codingScheme: string (nullable = true)\n",
      " |    |-- mRID: string (nullable = true)\n",
      " |    |-- objectAggregation: string (nullable = true)\n",
      " |    |-- outBiddingZone_Domain_mRID: struct (nullable = true)\n",
      " |    |    |-- text: string (nullable = true)\n",
      " |    |    |-- codingScheme: string (nullable = true)\n",
      " |    |-- quantity_Measure_Unit_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- @xmlns: string (nullable = true)\n",
      " |-- mRID: string (nullable = true)\n",
      " |-- revisionNumber: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- process_processType: string (nullable = true)\n",
      " |-- sender_MarketParticipant_mRID: struct (nullable = true)\n",
      " |    |-- #text: string (nullable = true)\n",
      " |    |-- @codingScheme: string (nullable = true)\n",
      " |-- sender_MarketParticipant_marketRole_type: string (nullable = true)\n",
      " |-- receiver_MarketParticipant_mRID: struct (nullable = true)\n",
      " |    |-- #text: string (nullable = true)\n",
      " |    |-- @codingScheme: string (nullable = true)\n",
      " |-- receiver_MarketParticipant_marketRole_type: string (nullable = true)\n",
      " |-- createdDateTime: string (nullable = true)\n",
      " |-- time_Period_timeInterval: struct (nullable = true)\n",
      " |    |-- end: string (nullable = true)\n",
      " |    |-- start: string (nullable = true)\n",
      " |-- TimeSeries_MktPSRType: struct (nullable = true)\n",
      " |    |-- psrType: string (nullable = true)\n",
      " |-- TimeSeries_Period: struct (nullable = true)\n",
      " |    |-- Point: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- position: string (nullable = true)\n",
      " |    |    |    |-- quantity: string (nullable = true)\n",
      " |    |-- resolution: string (nullable = true)\n",
      " |    |-- timeInterval: struct (nullable = true)\n",
      " |    |    |-- end: string (nullable = true)\n",
      " |    |    |-- start: string (nullable = true)\n",
      " |-- TimeSeries_businessType: string (nullable = true)\n",
      " |-- TimeSeries_curveType: string (nullable = true)\n",
      " |-- TimeSeries_inBiddingZone_Domain_mRID: struct (nullable = true)\n",
      " |    |-- text: string (nullable = true)\n",
      " |    |-- codingScheme: string (nullable = true)\n",
      " |-- TimeSeries_mRID: string (nullable = true)\n",
      " |-- TimeSeries_objectAggregation: string (nullable = true)\n",
      " |-- TimeSeries_outBiddingZone_Domain_mRID: struct (nullable = true)\n",
      " |    |-- text: string (nullable = true)\n",
      " |    |-- codingScheme: string (nullable = true)\n",
      " |-- TimeSeries_quantity_Measure_Unit_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# flatten TimeSeries if there is TimeSeries, flatten AttributeInstanceComponent if there is AttributeInstanceComponent\n",
    "def unpack_df(nested_df):\n",
    "    component_columns = [\"TimeSeries\", \"AttributeInstanceComponent\"]\n",
    "    general_cols = [c for c in nested_df.columns if c not in component_columns]\n",
    "    if \"TimeSeries\" in nested_df.columns:\n",
    "        data_cols_name = \"TimeSeries\"\n",
    "        data_cols = [c for c in nested_df.select(\"TimeSeries.*\").columns]\n",
    "    else:\n",
    "        pass\n",
    "    unpacked_df = nested_df.select(general_cols \\\n",
    "                                   + [F.col(data_cols_name+\".\"+c).alias(data_cols_name+\"_\"+c)\\\n",
    "                                      for c in data_cols])\n",
    "    return unpacked_df\n",
    "    \n",
    "\n",
    "df_spark = unpack_df(df_spark)\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#tulis data ke bigquery via temporary gcs bucket\n",
    "df_spark.write \\\n",
    "  .format(\"bigquery\") \\\n",
    "  .option(\"project\",\"rafzul-analytics-1009\") \\\n",
    "  .option(\"temporaryGcsBucket\",\"entsoe_temp_1009\") \\\n",
    "  .mode(\"append\") \\\n",
    "  .save(\"rafzul-analytics-1009.entsoe_playground.fact_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if df_spark2.select(\"TimeSeries\"):\n",
    "#     # rename all column (specifically replace dot with underscore) on every struct nested in TimeSeries array column. use transform\n",
    "#     df_spark3 = df_spark3.withColumn(\"TimeSeries\", F.transform \\\n",
    "#     (\"TimeSeries\", lambda el,ind: \\\n",
    "#     F.struct \\\n",
    "#     (\"abc\" for c_name in el.columns)))\n",
    "    \n",
    "# # # #get list of names of all columns in every struct inside TimeSeries array column\n",
    "# # a = [x for x in df_spark2.select((\"TimeSeries\"))]\n",
    "# # print(a)\n",
    "\n",
    "# df_spark3.select(\"TimeSeries\").show(10, truncate=False)\n",
    "# a = F.struct(F.col(\"TimeSeries\").getItem(c_name).alias(c_name.replace(\".\", \"_\")) for i, c_name in enumerate(df_spark2.schema[\"TimeSeries\"].dataType.elementType.fieldNames()))\n",
    "# # print(a\n",
    "\n",
    "\n",
    "# F.struct(F.col(\"TimeSeries\") for c_name in df_spark2.schema[x].dataType.names\n",
    "\n",
    "\n",
    "# df_spark2.show()\n",
    "# df_spark2.printSchema()\n",
    "\n",
    "# # #get list of names of all columns in every struct inside TimeSeries array column\n",
    "# a = [(c_name,i) for (c_name,i) in enumerate(df_spark2.schema[\"TimeSeries\"].dataType.elementType.fieldNames())]\n",
    "# print(a)\n",
    "\n",
    "\n",
    "# df_spark2 = df_spark2.withColumn(\"TimeSeries\", F.transform(\"TimeSeries\", lambda x: F.struct(*[F.col(\"TimeSeries.\" + c_name.replace(\".\", \"_\")).alias(c_name) for c_name in x])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #for non timeseries column\n",
    "# df_spark2 = df_spark2.toDF(*(c_name.replace(\".\", \"_\") for c_name in df_spark2.columns))\n",
    "# b = df_spark2.select(F.col(\"TimeSeries\"))\n",
    "# #for timeseries one\n",
    "# a = F.struct(F.col(\"TimeSeries\").getItem(c_name).alias(c_name.replace(\".\", \"_\")) for i, c_name in enumerate(df_spark2.schema[\"TimeSeries\"].dataType.elementType.fieldNames()))\n",
    "# print(a)\n",
    "# if df_spark2.select(\"TimeSeries\"):\n",
    "#     # rename all column (specifically replace dot with underscore) on every struct nested in TimeSeries array column. use transform, use Timeseries schema and its datatype.name o\n",
    "#     df_spark2 = df_spark2.withColumn(\"TimeSeries\", F.transform(\"TimeSeries\", lambda x: (a)))\n",
    "    \n",
    "# # #get list of names of all columns in every struct inside TimeSeries array column\n",
    "# # a = [c_name for c_name in enumerate(df_spark2.schema[\"TimeSeries\"].dataType.elementType.fieldNames())]\n",
    "\n",
    "\n",
    "# df_spark3.show()\n",
    "# df_spark3.printSchema()\n",
    "\n",
    "\n",
    "\n",
    "# # df_spark2 = df_spark2.withColumn(\"TimeSeries\", F.transform(\"TimeSeries\", lambda x: F.struct(*[F.col(\"TimeSeries.\" + c_name.replace(\".\", \"_\")).alias(c_name) for c_name in x])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #cast schema with the dot replaced with underscore, programatically\n",
    "\n",
    "\n",
    "# ---- solution for  column names beside TimeSeries:\n",
    "# changed_column_general = [(column_name, column_name.replace(\".\", \"_\")) for column_name in df_schema.fieldNames() if \".\" in x]\n",
    "# for column_name, changed_column_name in changed_column_general:\n",
    "#     df_schema = df_spark2.select([F.col(c).alias(mappingcolumn_name, changed_column_name)\n",
    "# # UPDATE: there is even better solution\n",
    "\n",
    "# #changed non timeseries column\n",
    "# df_spark2 = df_spark2.toDF(*(c.replace(\".\", \"_\") for c in df_spark2.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "write() argument must be str, not RDD",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/rafzul/projects/entsoe-pipelines/prototype.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/rafzul/projects/entsoe-pipelines/prototype.ipynb#X34sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mexample_destination.json\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mw+\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m output_file:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/rafzul/projects/entsoe-pipelines/prototype.ipynb#X34sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     output_file\u001b[39m.\u001b[39;49mwrite(df_spark2\u001b[39m.\u001b[39;49mtoJSON())\n",
      "\u001b[0;31mTypeError\u001b[0m: write() argument must be str, not RDD"
     ]
    }
   ],
   "source": [
    "with open(\"example_destination.json\", \"w+\") as output_file:\n",
    "    output_file.write(df_spark2.toJSON())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #inferring schema and get the data type of each column and turn it into spark dataframe\n",
    "# datatype_infer = pd.DataFrame.from_dict(xml_file_dict[0], orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #flatten nested df at every layer\n",
    "# from pyspark.sql.types import *\n",
    "# from pyspark.sql.import functions as f\n",
    "\n",
    "# def flatten_structs(nested_df):\n",
    "#     stack = [(), nested_df]\n",
    "#     columns = []\n",
    "    \n",
    "#     while len(stack) > 0:\n",
    "#         parents\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_schema = df_spark.dtypes\n",
    "\n",
    "# firstrow_orig = df_spark.collect()[0]\n",
    "# new_header = [f\"{x} - {firstrow_orig.__getitem__(x)}\" for x in firstrow_orig.__fields__]\n",
    "\n",
    "# df_spark = df_spark.where(F.col(\"_c0\").isNotNull())\n",
    "# df_spark.show()\n",
    "\n",
    "# # #drop header and first column\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
