[2023-01-31T05:11:00.724+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-31T05:11:00.774+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-31T05:11:00.776+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T05:11:00.777+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T05:11:00.777+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T05:11:00.833+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 00:00:00+00:00
[2023-01-31T05:11:00.872+0000] {standard_task_runner.py:55} INFO - Started process 12270 to run task
[2023-01-31T05:11:00.915+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '883', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpx4e5hxhl']
[2023-01-31T05:11:00.944+0000] {standard_task_runner.py:83} INFO - Job 883: Subtask stage_total_generation
[2023-01-31T05:11:01.571+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 81d5fcd0285b
[2023-01-31T05:11:01.961+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-31T05:11:02.029+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T05:11:02.036+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-31T05:11:35.083+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:35 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T05:11:35.624+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T05:11:36.384+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:36 INFO ResourceUtils: ==============================================================
[2023-01-31T05:11:36.396+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:36 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T05:11:36.412+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:36 INFO ResourceUtils: ==============================================================
[2023-01-31T05:11:36.420+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:36 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T05:11:36.732+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:36 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T05:11:36.782+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:36 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T05:11:36.787+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:36 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T05:11:37.398+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:37 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T05:11:37.406+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:37 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T05:11:37.411+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:37 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T05:11:37.424+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:37 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T05:11:37.438+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:37 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T05:11:41.141+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:41 INFO Utils: Successfully started service 'sparkDriver' on port 34625.
[2023-01-31T05:11:41.384+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:41 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T05:11:41.789+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:41 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T05:11:42.199+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:42 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T05:11:42.207+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:42 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T05:11:42.286+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:42 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T05:11:42.715+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:42 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d1532aea-4161-4102-9f67-f9989e15408b
[2023-01-31T05:11:43.042+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:43 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T05:11:43.509+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:43 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T05:11:45.957+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:45 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T05:11:45.963+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:45 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2023-01-31T05:11:45.969+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:45 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[2023-01-31T05:11:46.008+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:46 INFO Utils: Successfully started service 'SparkUI' on port 4043.
[2023-01-31T05:11:46.209+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:46 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://81d5fcd0285b:34625/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675141895014
[2023-01-31T05:11:46.211+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:46 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://81d5fcd0285b:34625/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675141895014
[2023-01-31T05:11:46.699+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:46 INFO Executor: Starting executor ID driver on host 81d5fcd0285b
[2023-01-31T05:11:46.733+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:46 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T05:11:46.867+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:46 INFO Executor: Fetching spark://81d5fcd0285b:34625/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675141895014
[2023-01-31T05:11:47.271+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:47 INFO TransportClientFactory: Successfully created connection to 81d5fcd0285b/172.19.0.9:34625 after 310 ms (0 ms spent in bootstraps)
[2023-01-31T05:11:47.360+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:47 INFO Utils: Fetching spark://81d5fcd0285b:34625/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-3216caae-ae3d-4ef7-a3b4-aa880cc3cc36/userFiles-fa7f4a5d-83dd-45ec-b438-66ea66a3d037/fetchFileTemp14201276493440744716.tmp
[2023-01-31T05:11:50.252+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:50 INFO Executor: Adding file:/tmp/spark-3216caae-ae3d-4ef7-a3b4-aa880cc3cc36/userFiles-fa7f4a5d-83dd-45ec-b438-66ea66a3d037/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T05:11:50.255+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:50 INFO Executor: Fetching spark://81d5fcd0285b:34625/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675141895014
[2023-01-31T05:11:50.299+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:50 INFO Utils: Fetching spark://81d5fcd0285b:34625/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-3216caae-ae3d-4ef7-a3b4-aa880cc3cc36/userFiles-fa7f4a5d-83dd-45ec-b438-66ea66a3d037/fetchFileTemp12051005233678575271.tmp
[2023-01-31T05:11:52.652+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:52 INFO Executor: Adding file:/tmp/spark-3216caae-ae3d-4ef7-a3b4-aa880cc3cc36/userFiles-fa7f4a5d-83dd-45ec-b438-66ea66a3d037/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T05:11:52.730+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:52 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38993.
[2023-01-31T05:11:52.730+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:52 INFO NettyBlockTransferService: Server created on 81d5fcd0285b:38993
[2023-01-31T05:11:52.738+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:52 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T05:11:52.763+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:52 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 38993, None)
[2023-01-31T05:11:52.780+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:52 INFO BlockManagerMasterEndpoint: Registering block manager 81d5fcd0285b:38993 with 434.4 MiB RAM, BlockManagerId(driver, 81d5fcd0285b, 38993, None)
[2023-01-31T05:11:52.811+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:52 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 38993, None)
[2023-01-31T05:11:52.819+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:52 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 81d5fcd0285b, 38993, None)
[2023-01-31T05:11:59.275+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:59 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T05:11:59.320+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:59 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T05:12:20.632+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:20 INFO InMemoryFileIndex: It took 1002 ms to list leaf files for 1 paths.
[2023-01-31T05:12:22.376+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:22 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T05:12:23.302+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:23 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T05:12:23.313+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:23 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 81d5fcd0285b:38993 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T05:12:23.398+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:23 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T05:12:28.005+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:28 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T05:12:28.518+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:28 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T05:12:28.753+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:28 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T05:12:29.084+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:29 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T05:12:29.169+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:29 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T05:12:29.188+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:29 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:12:29.253+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:29 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:12:29.348+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:29 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T05:12:30.369+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:30 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T05:12:30.431+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:30 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T05:12:30.438+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:30 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 81d5fcd0285b:38993 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T05:12:30.442+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:30 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:12:30.633+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:12:30.650+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:30 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T05:12:32.231+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:32 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T05:12:32.829+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:32 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T05:12:35.024+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:35 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010000__202101010100.json:0+9369
[2023-01-31T05:12:37.568+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:37 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3437 bytes result sent to driver
[2023-01-31T05:12:37.696+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:37 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 5985 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:12:37.702+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:37 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T05:12:37.826+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:37 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 8.257 s
[2023-01-31T05:12:37.906+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:37 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:12:37.920+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T05:12:37.939+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:37 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 9.181840 s
[2023-01-31T05:12:42.270+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:42 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 81d5fcd0285b:38993 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T05:12:42.331+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:42 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 81d5fcd0285b:38993 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T05:13:09.094+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:09 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:09.108+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:09 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:13:09.129+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:09 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:13.521+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:13 INFO CodeGenerator: Code generated in 2330.60611 ms
[2023-01-31T05:13:13.689+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:13 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T05:13:13.893+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:13 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T05:13:13.902+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:13 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 81d5fcd0285b:38993 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T05:13:13.925+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:13 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T05:13:13.993+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:13 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:14.235+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T05:13:14.247+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T05:13:14.247+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T05:13:14.248+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:14.248+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:14.254+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T05:13:14.324+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T05:13:14.366+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T05:13:14.369+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 81d5fcd0285b:38993 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T05:13:14.375+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:14.396+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:14.407+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T05:13:14.428+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:14.437+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T05:13:15.115+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T05:13:16.373+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:16 INFO CodeGenerator: Code generated in 1073.662146 ms
[2023-01-31T05:13:17.494+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1628 bytes result sent to driver
[2023-01-31T05:13:17.513+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 3100 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:17.519+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 3.229 s
[2023-01-31T05:13:17.524+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T05:13:17.529+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:17.534+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T05:13:17.540+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 3.293245 s
[2023-01-31T05:13:18.393+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:18.398+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:13:18.403+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:19.331+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO CodeGenerator: Code generated in 651.982546 ms
[2023-01-31T05:13:19.399+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T05:13:19.457+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T05:13:19.463+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 81d5fcd0285b:38993 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:13:19.520+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T05:13:19.539+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:19.804+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T05:13:19.809+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T05:13:19.809+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T05:13:19.810+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:19.810+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:19.824+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T05:13:19.860+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T05:13:19.868+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T05:13:19.872+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 81d5fcd0285b:38993 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T05:13:19.874+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:19.887+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:19.896+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T05:13:19.906+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:19.908+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T05:13:20.023+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T05:13:20.342+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1585 bytes result sent to driver
[2023-01-31T05:13:20.365+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 460 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:20.372+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 0.528 s
[2023-01-31T05:13:20.373+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:20.374+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T05:13:20.375+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T05:13:20.384+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 0.572866 s
[2023-01-31T05:13:21.319+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:21.329+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T05:13:21.336+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:23.209+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO CodeGenerator: Code generated in 1350.58713 ms
[2023-01-31T05:13:23.233+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T05:13:23.377+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T05:13:23.382+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 81d5fcd0285b:38993 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:13:23.389+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T05:13:23.408+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:23.671+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T05:13:23.677+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T05:13:23.678+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T05:13:23.678+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:23.679+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:23.697+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T05:13:23.751+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.6 MiB)
[2023-01-31T05:13:23.763+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.6 MiB)
[2023-01-31T05:13:23.769+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 81d5fcd0285b:38993 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T05:13:23.773+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:23.785+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:23.786+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T05:13:23.880+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:23.881+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T05:13:23.926+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T05:13:24.185+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-01-31T05:13:24.240+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 338 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:24.243+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T05:13:24.243+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 0.520 s
[2023-01-31T05:13:24.244+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:24.244+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T05:13:24.244+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 0.568347 s
[2023-01-31T05:13:24.390+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:24.391+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:13:24.396+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:24.815+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO CodeGenerator: Code generated in 261.611175 ms
[2023-01-31T05:13:25.019+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.4 MiB)
[2023-01-31T05:13:25.340+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T05:13:25.342+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 81d5fcd0285b:38993 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:13:25.377+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T05:13:25.382+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:25.556+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T05:13:25.559+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T05:13:25.559+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T05:13:25.560+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:25.560+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:25.570+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T05:13:25.581+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-31T05:13:25.598+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-01-31T05:13:25.601+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 81d5fcd0285b:38993 (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T05:13:25.603+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:25.612+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:25.613+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T05:13:25.618+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:25.630+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T05:13:25.679+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T05:13:25.951+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1629 bytes result sent to driver
[2023-01-31T05:13:25.968+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 348 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:25.969+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T05:13:25.969+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 0.394 s
[2023-01-31T05:13:25.969+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:25.969+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T05:13:25.970+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 0.410593 s
[2023-01-31T05:13:29.131+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:29.148+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T05:13:29.158+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:29.927+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 81d5fcd0285b:38993 in memory (size: 12.7 KiB, free: 434.2 MiB)
[2023-01-31T05:13:30.026+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 81d5fcd0285b:38993 in memory (size: 7.6 KiB, free: 434.3 MiB)
[2023-01-31T05:13:30.139+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 81d5fcd0285b:38993 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T05:13:30.199+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 81d5fcd0285b:38993 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T05:13:30.251+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO CodeGenerator: Code generated in 618.15658 ms
[2023-01-31T05:13:30.351+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.3 MiB)
[2023-01-31T05:13:30.569+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T05:13:30.571+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 81d5fcd0285b:38993 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:13:30.573+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T05:13:30.604+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:31.263+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:31 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T05:13:31.267+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:31 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) with 1 output partitions
[2023-01-31T05:13:31.268+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:31 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201)
[2023-01-31T05:13:31.269+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:31 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:31.269+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:31 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:31.318+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:31 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201), which has no missing parents
[2023-01-31T05:13:31.426+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:31 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.2 MiB)
[2023-01-31T05:13:31.499+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:31 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.2 MiB)
[2023-01-31T05:13:31.505+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:31 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 81d5fcd0285b:38993 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T05:13:31.534+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:31 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:31.573+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:31.576+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:31 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T05:13:31.634+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:31 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:31.641+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:31 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T05:13:31.707+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:31 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T05:13:32.131+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:32 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2558 bytes result sent to driver
[2023-01-31T05:13:32.141+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:32 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 504 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:32.141+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:32 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) finished in 0.826 s
[2023-01-31T05:13:32.142+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:32 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:32.145+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:32 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T05:13:32.149+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T05:13:32.157+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:32 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201, took 0.889790 s
[2023-01-31T05:13:33.515+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:33.520+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T05:13:33.521+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:35.179+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:35 INFO CodeGenerator: Code generated in 1259.482233 ms
[2023-01-31T05:13:35.229+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:35 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 433.0 MiB)
[2023-01-31T05:13:35.373+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:35 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.0 MiB)
[2023-01-31T05:13:35.383+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:35 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 81d5fcd0285b:38993 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:13:35.393+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:35 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T05:13:35.414+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:35 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:35.890+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:35 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T05:13:35.891+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:35 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T05:13:35.892+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:35 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T05:13:35.892+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:35 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:35.893+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:35 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:35.901+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:35 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T05:13:35.934+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:35 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 432.9 MiB)
[2023-01-31T05:13:35.965+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:35 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.9 MiB)
[2023-01-31T05:13:35.969+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:35 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 81d5fcd0285b:38993 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T05:13:35.984+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:35 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:35.994+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:36.001+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T05:13:36.014+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:36.014+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T05:13:36.127+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T05:13:36.485+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-01-31T05:13:36.497+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 481 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:36.498+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T05:13:36.498+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 0.592 s
[2023-01-31T05:13:36.499+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:36.500+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T05:13:36.508+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 0.614019 s
[2023-01-31T05:13:36.964+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:36.965+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T05:13:36.965+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:37.936+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:37 INFO CodeGenerator: Code generated in 486.220321 ms
[2023-01-31T05:13:38.129+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.7 MiB)
[2023-01-31T05:13:38.577+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.7 MiB)
[2023-01-31T05:13:38.583+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 81d5fcd0285b:38993 (size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T05:13:38.591+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T05:13:38.606+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:38.690+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T05:13:38.694+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T05:13:38.695+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T05:13:38.695+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:38.695+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:38.714+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T05:13:38.735+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.7 MiB)
[2023-01-31T05:13:38.751+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.6 MiB)
[2023-01-31T05:13:38.754+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 81d5fcd0285b:38993 (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T05:13:38.757+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:38.760+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:38.761+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T05:13:38.769+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:38.769+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T05:13:38.800+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T05:13:39.027+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:39 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1729 bytes result sent to driver
[2023-01-31T05:13:39.033+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:39 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 267 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:39.037+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:39 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 0.316 s
[2023-01-31T05:13:39.038+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:39 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:39.038+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:39 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T05:13:39.038+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T05:13:39.043+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:39 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 0.348634 s
[2023-01-31T05:13:47.124+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:47 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 81d5fcd0285b:38993 in memory (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T05:13:47.208+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:47 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 81d5fcd0285b:38993 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:13:47.268+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:47 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 81d5fcd0285b:38993 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:13:47.331+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:47 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 81d5fcd0285b:38993 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:13:47.464+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:47 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 81d5fcd0285b:38993 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:13:47.545+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:47 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 81d5fcd0285b:38993 in memory (size: 13.4 KiB, free: 434.3 MiB)
[2023-01-31T05:13:47.603+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:47 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 81d5fcd0285b:38993 in memory (size: 12.6 KiB, free: 434.3 MiB)
[2023-01-31T05:13:47.668+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:47 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 81d5fcd0285b:38993 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:13:47.761+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:47 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 81d5fcd0285b:38993 in memory (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T05:14:05.002+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:14:05.076+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:14:05.077+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:14:05.086+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:14:05.089+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:14:05.089+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:14:05.104+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:14:05.870+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-31T05:14:05.872+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO DAGScheduler: Got job 8 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T05:14:05.872+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO DAGScheduler: Final stage: ResultStage 8 (save at BigQueryWriteHelper.java:105)
[2023-01-31T05:14:05.872+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:14:05.874+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:14:05.877+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO DAGScheduler: Submitting ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T05:14:06.044+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 209.9 KiB, free 434.0 MiB)
[2023-01-31T05:14:06.070+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 74.3 KiB, free 433.9 MiB)
[2023-01-31T05:14:06.082+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 81d5fcd0285b:38993 (size: 74.3 KiB, free: 434.3 MiB)
[2023-01-31T05:14:06.086+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:14:06.093+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:14:06.100+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2023-01-31T05:14:06.126+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7484 bytes) taskResourceAssignments Map()
[2023-01-31T05:14:06.131+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2023-01-31T05:14:06.660+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:14:06.660+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:14:06.671+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:14:06.672+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:14:06.672+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:14:06.682+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:14:06.712+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T05:14:06.731+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T05:14:07.132+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:07 INFO ParquetOutputFormat: Parquet block size to 134217728
[2023-01-31T05:14:07.133+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:07 INFO ParquetOutputFormat: Validation is off
[2023-01-31T05:14:07.148+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:07 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-01-31T05:14:07.149+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:07 INFO ParquetOutputFormat: Parquet properties are:
[2023-01-31T05:14:07.150+0000] {spark_submit.py:495} INFO - Parquet page size to 1048576
[2023-01-31T05:14:07.150+0000] {spark_submit.py:495} INFO - Parquet dictionary page size to 1048576
[2023-01-31T05:14:07.151+0000] {spark_submit.py:495} INFO - Dictionary is true
[2023-01-31T05:14:07.151+0000] {spark_submit.py:495} INFO - Writer version is: PARQUET_1_0
[2023-01-31T05:14:07.151+0000] {spark_submit.py:495} INFO - Page size checking is: estimated
[2023-01-31T05:14:07.152+0000] {spark_submit.py:495} INFO - Min row count for page size check is: 100
[2023-01-31T05:14:07.152+0000] {spark_submit.py:495} INFO - Max row count for page size check is: 10000
[2023-01-31T05:14:07.205+0000] {spark_submit.py:495} INFO - Truncate length for column indexes is: 64
[2023-01-31T05:14:07.216+0000] {spark_submit.py:495} INFO - Truncate length for statistics min/max  is: 2147483647
[2023-01-31T05:14:07.234+0000] {spark_submit.py:495} INFO - Bloom filter enabled: false
[2023-01-31T05:14:07.235+0000] {spark_submit.py:495} INFO - Max Bloom filter size for a column is 1048576
[2023-01-31T05:14:07.251+0000] {spark_submit.py:495} INFO - Bloom filter expected number of distinct values are: null
[2023-01-31T05:14:07.255+0000] {spark_submit.py:495} INFO - Page row count limit to 20000
[2023-01-31T05:14:07.256+0000] {spark_submit.py:495} INFO - Writing page checksums is: on
[2023-01-31T05:14:08.053+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:08 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-01-31T05:14:08.054+0000] {spark_submit.py:495} INFO - {
[2023-01-31T05:14:08.054+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-31T05:14:08.054+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-31T05:14:08.055+0000] {spark_submit.py:495} INFO - "name" : "measured_at",
[2023-01-31T05:14:08.055+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T05:14:08.056+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T05:14:08.056+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:08.056+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:08.064+0000] {spark_submit.py:495} INFO - "name" : "created_at",
[2023-01-31T05:14:08.065+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T05:14:08.067+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T05:14:08.068+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:08.069+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:08.070+0000] {spark_submit.py:495} INFO - "name" : "Biomass - Actual Aggregated",
[2023-01-31T05:14:08.071+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:08.071+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:08.072+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:08.072+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:08.073+0000] {spark_submit.py:495} INFO - "name" : "Fossil Brown coal/Lignite - Actual Aggregated",
[2023-01-31T05:14:08.073+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:08.074+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:08.074+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:08.075+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:08.075+0000] {spark_submit.py:495} INFO - "name" : "Fossil Gas - Actual Aggregated",
[2023-01-31T05:14:08.075+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:08.076+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:08.076+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:08.077+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:08.083+0000] {spark_submit.py:495} INFO - "name" : "Fossil Hard coal - Actual Aggregated",
[2023-01-31T05:14:08.084+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:08.084+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:08.085+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:08.085+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:08.086+0000] {spark_submit.py:495} INFO - "name" : "Fossil Oil - Actual Aggregated",
[2023-01-31T05:14:08.086+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:08.087+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:08.087+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:08.088+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:08.089+0000] {spark_submit.py:495} INFO - "name" : "Geothermal - Actual Aggregated",
[2023-01-31T05:14:08.089+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:08.090+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:08.091+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:08.092+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:08.092+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Aggregated",
[2023-01-31T05:14:08.093+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:08.093+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:08.094+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:08.094+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:08.095+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Consumption",
[2023-01-31T05:14:08.095+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:08.096+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:08.097+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:08.097+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:08.098+0000] {spark_submit.py:495} INFO - "name" : "Hydro Run-of-river and poundage - Actual Aggregated",
[2023-01-31T05:14:08.098+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:08.103+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:08.104+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:08.105+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:08.106+0000] {spark_submit.py:495} INFO - "name" : "Hydro Water Reservoir - Actual Aggregated",
[2023-01-31T05:14:08.106+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:08.107+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:08.107+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:08.109+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:08.110+0000] {spark_submit.py:495} INFO - "name" : "Nuclear - Actual Aggregated",
[2023-01-31T05:14:08.110+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:08.111+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:08.111+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:08.112+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:08.113+0000] {spark_submit.py:495} INFO - "name" : "Other - Actual Aggregated",
[2023-01-31T05:14:08.113+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:08.114+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:08.114+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:08.115+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:08.115+0000] {spark_submit.py:495} INFO - "name" : "Other renewable - Actual Aggregated",
[2023-01-31T05:14:08.116+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:08.116+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:08.117+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:08.117+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:08.118+0000] {spark_submit.py:495} INFO - "name" : "Solar - Actual Aggregated",
[2023-01-31T05:14:08.118+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:08.119+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:08.119+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:08.123+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:08.124+0000] {spark_submit.py:495} INFO - "name" : "Waste - Actual Aggregated",
[2023-01-31T05:14:08.125+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:08.125+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:08.126+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:08.126+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:08.127+0000] {spark_submit.py:495} INFO - "name" : "Wind Offshore - Actual Aggregated",
[2023-01-31T05:14:08.127+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:08.128+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:08.128+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:08.129+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:08.130+0000] {spark_submit.py:495} INFO - "name" : "Wind Onshore - Actual Aggregated",
[2023-01-31T05:14:08.130+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:08.140+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:08.141+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:08.142+0000] {spark_submit.py:495} INFO - } ]
[2023-01-31T05:14:08.142+0000] {spark_submit.py:495} INFO - }
[2023-01-31T05:14:08.143+0000] {spark_submit.py:495} INFO - and corresponding Parquet message type:
[2023-01-31T05:14:08.146+0000] {spark_submit.py:495} INFO - message spark_schema {
[2023-01-31T05:14:08.146+0000] {spark_submit.py:495} INFO - required int96 measured_at;
[2023-01-31T05:14:08.154+0000] {spark_submit.py:495} INFO - required int96 created_at;
[2023-01-31T05:14:08.158+0000] {spark_submit.py:495} INFO - optional double Biomass - Actual Aggregated;
[2023-01-31T05:14:08.158+0000] {spark_submit.py:495} INFO - optional double Fossil Brown coal/Lignite - Actual Aggregated;
[2023-01-31T05:14:08.164+0000] {spark_submit.py:495} INFO - optional double Fossil Gas - Actual Aggregated;
[2023-01-31T05:14:08.165+0000] {spark_submit.py:495} INFO - optional double Fossil Hard coal - Actual Aggregated;
[2023-01-31T05:14:08.168+0000] {spark_submit.py:495} INFO - optional double Fossil Oil - Actual Aggregated;
[2023-01-31T05:14:08.169+0000] {spark_submit.py:495} INFO - optional double Geothermal - Actual Aggregated;
[2023-01-31T05:14:08.172+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Aggregated;
[2023-01-31T05:14:08.175+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Consumption;
[2023-01-31T05:14:08.176+0000] {spark_submit.py:495} INFO - optional double Hydro Run-of-river and poundage - Actual Aggregated;
[2023-01-31T05:14:08.178+0000] {spark_submit.py:495} INFO - optional double Hydro Water Reservoir - Actual Aggregated;
[2023-01-31T05:14:08.179+0000] {spark_submit.py:495} INFO - optional double Nuclear - Actual Aggregated;
[2023-01-31T05:14:08.182+0000] {spark_submit.py:495} INFO - optional double Other - Actual Aggregated;
[2023-01-31T05:14:08.182+0000] {spark_submit.py:495} INFO - optional double Other renewable - Actual Aggregated;
[2023-01-31T05:14:08.186+0000] {spark_submit.py:495} INFO - optional double Solar - Actual Aggregated;
[2023-01-31T05:14:08.186+0000] {spark_submit.py:495} INFO - optional double Waste - Actual Aggregated;
[2023-01-31T05:14:08.190+0000] {spark_submit.py:495} INFO - optional double Wind Offshore - Actual Aggregated;
[2023-01-31T05:14:08.193+0000] {spark_submit.py:495} INFO - optional double Wind Onshore - Actual Aggregated;
[2023-01-31T05:14:08.197+0000] {spark_submit.py:495} INFO - }
[2023-01-31T05:14:08.197+0000] {spark_submit.py:495} INFO - 
[2023-01-31T05:14:08.200+0000] {spark_submit.py:495} INFO - 
[2023-01-31T05:14:09.156+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:09 INFO CodecPool: Got brand-new compressor [.snappy]
[2023-01-31T05:14:13.679+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:13 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675141906520-08f61b7b-7392-4221-a20c-b79ca2098b97/_temporary/0/_temporary/' directory.
[2023-01-31T05:14:13.680+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:13 INFO FileOutputCommitter: Saved output of task 'attempt_202301310514052941788269259312219_0008_m_000000_8' to gs://entsoe_temp_1009/.spark-bigquery-local-1675141906520-08f61b7b-7392-4221-a20c-b79ca2098b97/_temporary/0/task_202301310514052941788269259312219_0008_m_000000
[2023-01-31T05:14:13.684+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:13 INFO SparkHadoopMapRedUtil: attempt_202301310514052941788269259312219_0008_m_000000_8: Committed. Elapsed time: 1760 ms.
[2023-01-31T05:14:13.754+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:13 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2588 bytes result sent to driver
[2023-01-31T05:14:13.761+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:13 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 7654 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:14:13.769+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:13 INFO DAGScheduler: ResultStage 8 (save at BigQueryWriteHelper.java:105) finished in 7.877 s
[2023-01-31T05:14:13.769+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:13 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:14:13.772+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:13 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2023-01-31T05:14:13.775+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2023-01-31T05:14:13.787+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:13 INFO DAGScheduler: Job 8 finished: save at BigQueryWriteHelper.java:105, took 7.915045 s
[2023-01-31T05:14:13.795+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:13 INFO FileFormatWriter: Start to commit write Job 670c00bd-60cb-43f6-99be-913f1320f473.
[2023-01-31T05:14:15.309+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:15 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675141906520-08f61b7b-7392-4221-a20c-b79ca2098b97/_temporary/0/task_202301310514052941788269259312219_0008_m_000000/' directory.
[2023-01-31T05:14:15.753+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:15 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675141906520-08f61b7b-7392-4221-a20c-b79ca2098b97/' directory.
[2023-01-31T05:14:16.059+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:16 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 81d5fcd0285b:38993 in memory (size: 74.3 KiB, free: 434.4 MiB)
[2023-01-31T05:14:16.575+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:16 INFO FileFormatWriter: Write Job 670c00bd-60cb-43f6-99be-913f1320f473 committed. Elapsed time: 2772 ms.
[2023-01-31T05:14:16.584+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:16 INFO FileFormatWriter: Finished processing stats for write job 670c00bd-60cb-43f6-99be-913f1320f473.
[2023-01-31T05:14:17.917+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:17 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=entsoe_analytics_1009, projectId=rafzul-analytics-1009, tableId=TEST_total_generation_staging}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=null, ignoreUnknownValue=null, sourceUris=[gs://entsoe_temp_1009/.spark-bigquery-local-1675141906520-08f61b7b-7392-4221-a20c-b79ca2098b97/part-00000-94058c6b-8dd7-4989-b346-2dfb3db6722e-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=true, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=rafzul-analytics-1009, job=5df6b2e2-4cd1-4c92-833d-1c2db94219e1, location=US}
[2023-01-31T05:14:24.432+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:24 INFO BigQueryClient: Done loading to rafzul-analytics-1009.entsoe_analytics_1009.TEST_total_generation_staging. jobId: JobId{project=rafzul-analytics-1009, job=5df6b2e2-4cd1-4c92-833d-1c2db94219e1, location=US}
[2023-01-31T05:14:25.101+0000] {spark_submit.py:495} INFO - MANTEP!!!!
[2023-01-31T05:14:25.444+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:25 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-31T05:14:25.519+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:25 INFO SparkUI: Stopped Spark web UI at http://81d5fcd0285b:4043
[2023-01-31T05:14:25.571+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:25 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-31T05:14:25.617+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:25 INFO MemoryStore: MemoryStore cleared
[2023-01-31T05:14:25.623+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:25 INFO BlockManager: BlockManager stopped
[2023-01-31T05:14:25.634+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:25 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-31T05:14:25.645+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:25 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-31T05:14:25.663+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:25 INFO SparkContext: Successfully stopped SparkContext
[2023-01-31T05:14:25.664+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:25 INFO ShutdownHookManager: Shutdown hook called
[2023-01-31T05:14:25.665+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-5901fca3-9ec1-4978-a526-64678c25725a
[2023-01-31T05:14:25.678+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-3216caae-ae3d-4ef7-a3b4-aa880cc3cc36/pyspark-c71f21ce-82a4-4f95-ac45-2c1eff1650e6
[2023-01-31T05:14:25.689+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-3216caae-ae3d-4ef7-a3b4-aa880cc3cc36
[2023-01-31T05:14:25.946+0000] {taskinstance.py:1322} INFO - Marking task as SUCCESS. dag_id=entsoe-energydata-backfill, task_id=stage_total_generation, execution_date=20210101T000000, start_date=20230131T051100, end_date=20230131T051425
[2023-01-31T05:14:26.010+0000] {local_task_job.py:159} INFO - Task exited with return code 0
[2023-01-31T05:14:26.051+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T05:30:37.982+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-31T05:30:38.035+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-31T05:30:38.035+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T05:30:38.036+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T05:30:38.037+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T05:30:38.123+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 00:00:00+00:00
[2023-01-31T05:30:38.184+0000] {standard_task_runner.py:55} INFO - Started process 15715 to run task
[2023-01-31T05:30:38.236+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '893', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp_xhw1zii']
[2023-01-31T05:30:38.255+0000] {standard_task_runner.py:83} INFO - Job 893: Subtask stage_total_generation
[2023-01-31T05:30:38.756+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 81d5fcd0285b
[2023-01-31T05:30:39.223+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-31T05:30:39.290+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T05:30:39.297+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-31T05:31:52.529+0000] {spark_submit.py:495} INFO - 23/01/31 05:31:52 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T05:31:54.819+0000] {spark_submit.py:495} INFO - 23/01/31 05:31:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T05:31:56.208+0000] {spark_submit.py:495} INFO - 23/01/31 05:31:56 INFO ResourceUtils: ==============================================================
[2023-01-31T05:31:56.227+0000] {spark_submit.py:495} INFO - 23/01/31 05:31:56 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T05:31:56.261+0000] {spark_submit.py:495} INFO - 23/01/31 05:31:56 INFO ResourceUtils: ==============================================================
[2023-01-31T05:31:56.280+0000] {spark_submit.py:495} INFO - 23/01/31 05:31:56 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T05:31:57.472+0000] {spark_submit.py:495} INFO - 23/01/31 05:31:57 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T05:31:57.614+0000] {spark_submit.py:495} INFO - 23/01/31 05:31:57 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T05:31:57.654+0000] {spark_submit.py:495} INFO - 23/01/31 05:31:57 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T05:31:58.981+0000] {spark_submit.py:495} INFO - 23/01/31 05:31:58 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T05:31:58.999+0000] {spark_submit.py:495} INFO - 23/01/31 05:31:58 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T05:31:59.016+0000] {spark_submit.py:495} INFO - 23/01/31 05:31:59 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T05:31:59.038+0000] {spark_submit.py:495} INFO - 23/01/31 05:31:59 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T05:31:59.043+0000] {spark_submit.py:495} INFO - 23/01/31 05:31:59 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T05:32:07.307+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:07 INFO Utils: Successfully started service 'sparkDriver' on port 45423.
[2023-01-31T05:32:08.212+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:08 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T05:32:08.987+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:08 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T05:32:10.538+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:10 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T05:32:10.585+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:10 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T05:32:10.741+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:10 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T05:32:11.325+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:11 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ed2bf000-47e9-4288-9600-1e2d31645c21
[2023-01-31T05:32:11.702+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:11 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T05:32:12.139+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:12 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T05:32:17.837+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:17 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T05:32:17.843+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:17 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2023-01-31T05:32:17.926+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:17 INFO Utils: Successfully started service 'SparkUI' on port 4042.
[2023-01-31T05:32:18.562+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:18 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://81d5fcd0285b:45423/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675143112374
[2023-01-31T05:32:18.592+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:18 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://81d5fcd0285b:45423/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675143112374
[2023-01-31T05:32:20.779+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:20 INFO Executor: Starting executor ID driver on host 81d5fcd0285b
[2023-01-31T05:32:20.918+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:20 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T05:32:21.060+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:21 INFO Executor: Fetching spark://81d5fcd0285b:45423/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675143112374
[2023-01-31T05:32:23.026+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:23 INFO TransportClientFactory: Successfully created connection to 81d5fcd0285b/172.19.0.9:45423 after 1440 ms (0 ms spent in bootstraps)
[2023-01-31T05:32:23.331+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:23 INFO Utils: Fetching spark://81d5fcd0285b:45423/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-ff60e954-afef-4f2e-af78-833002cbf4b9/userFiles-a3d5a584-bad9-4d49-819d-a5183ad6364f/fetchFileTemp11849476045600837585.tmp
[2023-01-31T05:32:32.450+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:32 INFO Executor: Adding file:/tmp/spark-ff60e954-afef-4f2e-af78-833002cbf4b9/userFiles-a3d5a584-bad9-4d49-819d-a5183ad6364f/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T05:32:32.456+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:32 INFO Executor: Fetching spark://81d5fcd0285b:45423/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675143112374
[2023-01-31T05:32:32.492+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:32 INFO Utils: Fetching spark://81d5fcd0285b:45423/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-ff60e954-afef-4f2e-af78-833002cbf4b9/userFiles-a3d5a584-bad9-4d49-819d-a5183ad6364f/fetchFileTemp8336713122105592706.tmp
[2023-01-31T05:32:35.846+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:35 INFO Executor: Told to re-register on heartbeat
[2023-01-31T05:32:35.870+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:35 INFO BlockManager: BlockManager null re-registering with master
[2023-01-31T05:32:36.247+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:36 INFO BlockManagerMaster: Registering BlockManager null
[2023-01-31T05:32:36.645+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:36 ERROR Inbox: Ignoring error
[2023-01-31T05:32:36.646+0000] {spark_submit.py:495} INFO - java.lang.NullPointerException
[2023-01-31T05:32:36.647+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)
[2023-01-31T05:32:36.647+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)
[2023-01-31T05:32:36.648+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-01-31T05:32:36.664+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-01-31T05:32:36.665+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-01-31T05:32:36.665+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-01-31T05:32:36.665+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-01-31T05:32:36.666+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-01-31T05:32:36.666+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-01-31T05:32:36.667+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-31T05:32:36.667+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-31T05:32:36.667+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-31T05:32:36.742+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:36 WARN Executor: Issue communicating with driver in heartbeater
[2023-01-31T05:32:36.778+0000] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-01-31T05:32:36.817+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-01-31T05:32:36.817+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-01-31T05:32:36.818+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2023-01-31T05:32:36.819+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2023-01-31T05:32:36.819+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:79)
[2023-01-31T05:32:36.820+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:643)
[2023-01-31T05:32:36.837+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2023-01-31T05:32:36.838+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2023-01-31T05:32:36.839+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
[2023-01-31T05:32:36.839+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2023-01-31T05:32:36.872+0000] {spark_submit.py:495} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2023-01-31T05:32:36.889+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-01-31T05:32:36.889+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2023-01-31T05:32:36.890+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2023-01-31T05:32:36.891+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-31T05:32:36.891+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-31T05:32:36.892+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-31T05:32:36.892+0000] {spark_submit.py:495} INFO - Caused by: java.lang.NullPointerException
[2023-01-31T05:32:36.937+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)
[2023-01-31T05:32:36.938+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)
[2023-01-31T05:32:36.939+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-01-31T05:32:36.939+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-01-31T05:32:36.940+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-01-31T05:32:36.973+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-01-31T05:32:36.974+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-01-31T05:32:36.975+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-01-31T05:32:36.975+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-01-31T05:32:36.976+0000] {spark_submit.py:495} INFO - ... 3 more
[2023-01-31T05:32:37.655+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:37 INFO Executor: Adding file:/tmp/spark-ff60e954-afef-4f2e-af78-833002cbf4b9/userFiles-a3d5a584-bad9-4d49-819d-a5183ad6364f/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T05:32:37.930+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:37 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33637.
[2023-01-31T05:32:37.935+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:37 INFO NettyBlockTransferService: Server created on 81d5fcd0285b:33637
[2023-01-31T05:32:38.003+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:37 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T05:32:38.122+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:38 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 33637, None)
[2023-01-31T05:32:38.189+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:38 INFO BlockManagerMasterEndpoint: Registering block manager 81d5fcd0285b:33637 with 434.4 MiB RAM, BlockManagerId(driver, 81d5fcd0285b, 33637, None)
[2023-01-31T05:32:38.279+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:38 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 33637, None)
[2023-01-31T05:32:38.316+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:38 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 81d5fcd0285b, 33637, None)
[2023-01-31T05:33:00.428+0000] {spark_submit.py:495} INFO - 23/01/31 05:33:00 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T05:33:00.951+0000] {spark_submit.py:495} INFO - 23/01/31 05:33:00 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T05:33:58.356+0000] {spark_submit.py:495} INFO - 23/01/31 05:33:58 INFO InMemoryFileIndex: It took 2986 ms to list leaf files for 1 paths.
[2023-01-31T05:34:07.949+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:07 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T05:34:10.618+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:10 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T05:34:10.959+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:10 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 81d5fcd0285b:33637 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T05:34:11.101+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:11 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T05:34:26.722+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:26 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T05:34:27.017+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:27 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T05:34:27.478+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:27 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T05:34:27.960+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:27 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T05:34:27.970+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:27 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T05:34:27.984+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:27 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:34:28.014+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:28 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:34:28.121+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:28 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T05:34:30.868+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:30 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T05:34:30.971+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:30 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T05:34:30.976+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:30 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 81d5fcd0285b:33637 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T05:34:31.002+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:31 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:34:31.657+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:34:31.677+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:31 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T05:34:33.583+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:33 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T05:34:33.997+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:33 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T05:34:36.667+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:36 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010000__202101010100.json:0+9369
[2023-01-31T05:34:43.661+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:43 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3480 bytes result sent to driver
[2023-01-31T05:34:43.946+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:43 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 10706 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:34:44.034+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:44 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 15.035 s
[2023-01-31T05:34:44.061+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:44 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T05:34:44.066+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:44 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:34:44.144+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T05:34:44.337+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:44 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 16.837268 s
[2023-01-31T05:36:16.885+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:16 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 81d5fcd0285b:33637 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T05:36:17.105+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:17 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 81d5fcd0285b:33637 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T05:36:19.867+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:19 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:36:19.907+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:19 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:36:19.933+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:19 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:36:32.103+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:32 INFO CodeGenerator: Code generated in 5655.244457 ms
[2023-01-31T05:36:32.995+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:32 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T05:36:33.291+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:33 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T05:36:33.299+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:33 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 81d5fcd0285b:33637 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T05:36:33.312+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:33 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T05:36:33.917+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:36:36.625+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:36 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T05:36:36.632+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:36 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T05:36:36.633+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:36 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T05:36:36.633+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:36 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:36:36.656+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:36 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:36:36.679+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:36 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T05:36:36.883+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:36 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T05:36:37.065+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:37 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T05:36:37.069+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:37 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 81d5fcd0285b:33637 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T05:36:37.092+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:37 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:36:37.134+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:36:37.136+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:37 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T05:36:37.292+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:37 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:36:37.324+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:37 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T05:36:40.754+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:40 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T05:36:47.175+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:47 INFO CodeGenerator: Code generated in 4901.318411 ms
[2023-01-31T05:36:49.262+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:49 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1671 bytes result sent to driver
[2023-01-31T05:36:49.371+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:49 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 12166 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:36:49.374+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:49 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T05:36:49.383+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:49 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 12.623 s
[2023-01-31T05:36:49.407+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:49 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:36:49.432+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T05:36:49.437+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:49 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 12.806910 s
[2023-01-31T05:36:51.020+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:51 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:36:51.024+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:51 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:36:51.046+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:51 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:36:51.378+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:51 INFO CodeGenerator: Code generated in 85.97066 ms
[2023-01-31T05:36:51.400+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:51 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T05:36:51.567+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:51 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T05:36:51.577+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:51 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 81d5fcd0285b:33637 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:36:51.592+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:51 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T05:36:51.616+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:51 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:36:51.714+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:51 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T05:36:51.718+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:51 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T05:36:51.719+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:51 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T05:36:51.720+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:51 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:36:51.732+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:51 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:36:51.762+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:51 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T05:36:51.970+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:51 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T05:36:52.156+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:52 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T05:36:52.162+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:52 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 81d5fcd0285b:33637 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T05:36:52.179+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:52 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:36:52.206+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:36:52.233+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:52 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T05:36:52.249+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:52 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:36:52.269+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:52 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T05:36:52.314+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:52 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T05:36:53.496+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:53 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1585 bytes result sent to driver
[2023-01-31T05:36:53.675+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:53 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1430 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:36:53.676+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:53 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T05:36:53.683+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:53 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 1.815 s
[2023-01-31T05:36:53.691+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:53 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:36:53.696+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T05:36:53.716+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:53 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 1.998611 s
[2023-01-31T05:36:58.573+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:58 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:36:58.575+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:58 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T05:36:58.580+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:58 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:36:59.290+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:59 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 81d5fcd0285b:33637 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T05:36:59.412+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:59 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 81d5fcd0285b:33637 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T05:37:03.044+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:03 INFO CodeGenerator: Code generated in 2067.447602 ms
[2023-01-31T05:37:03.161+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:03 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T05:37:03.649+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:03 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T05:37:03.664+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:03 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 81d5fcd0285b:33637 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:37:03.679+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:03 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T05:37:03.698+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:03 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:37:05.051+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:05 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T05:37:05.056+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:05 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T05:37:05.125+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:05 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T05:37:05.150+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:05 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:37:05.151+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:05 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:37:05.223+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:05 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T05:37:05.224+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:05 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.7 MiB)
[2023-01-31T05:37:05.333+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:05 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.7 MiB)
[2023-01-31T05:37:05.380+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:05 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 81d5fcd0285b:33637 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T05:37:05.419+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:05 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:37:05.537+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:37:05.573+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:05 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T05:37:05.590+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:05 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:37:05.616+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:05 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T05:37:06.089+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:06 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T05:37:08.167+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:08 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-01-31T05:37:08.260+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:08 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 2667 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:37:08.282+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:08 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T05:37:08.288+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:08 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 3.173 s
[2023-01-31T05:37:08.301+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:08 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:37:08.304+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T05:37:08.346+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:08 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 3.281586 s
[2023-01-31T05:37:09.206+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:09 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:37:09.207+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:09 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:37:09.208+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:09 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:37:10.177+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:10 INFO CodeGenerator: Code generated in 716.578759 ms
[2023-01-31T05:37:10.307+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:10 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.5 MiB)
[2023-01-31T05:37:11.031+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T05:37:11.057+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 81d5fcd0285b:33637 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:37:11.133+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T05:37:11.269+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:37:11.809+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T05:37:11.817+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T05:37:11.818+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T05:37:11.820+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:37:11.825+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:37:11.841+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T05:37:11.874+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-31T05:37:11.917+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-01-31T05:37:11.926+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 81d5fcd0285b:33637 (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T05:37:11.935+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:37:11.945+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:37:11.949+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T05:37:11.965+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:37:11.976+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T05:37:12.163+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:12 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T05:37:12.994+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:12 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1586 bytes result sent to driver
[2023-01-31T05:37:13.295+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:13 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 1312 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:37:13.296+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:13 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T05:37:13.299+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:13 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 1.440 s
[2023-01-31T05:37:13.300+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:13 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:37:13.361+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T05:37:13.373+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:13 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 1.553339 s
[2023-01-31T05:37:13.553+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:13 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 81d5fcd0285b:33637 in memory (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T05:37:19.680+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:19 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:37:19.687+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:19 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T05:37:19.694+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:19 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:37:20.396+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:20 INFO CodeGenerator: Code generated in 456.64185 ms
[2023-01-31T05:37:20.601+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:20 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.3 MiB)
[2023-01-31T05:37:21.450+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:21 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T05:37:21.460+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:21 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 81d5fcd0285b:33637 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:37:21.492+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:21 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T05:37:21.633+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:21 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:37:22.195+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:22 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T05:37:22.219+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:22 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) with 1 output partitions
[2023-01-31T05:37:22.220+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:22 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201)
[2023-01-31T05:37:22.220+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:22 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:37:22.240+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:22 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:37:22.269+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:22 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201), which has no missing parents
[2023-01-31T05:37:22.293+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:22 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.2 MiB)
[2023-01-31T05:37:22.315+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:22 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.2 MiB)
[2023-01-31T05:37:22.328+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:22 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 81d5fcd0285b:33637 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T05:37:22.344+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:22 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:37:22.352+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:37:22.354+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:22 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T05:37:22.374+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:22 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:37:22.457+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:22 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T05:37:23.359+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:23 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T05:37:27.088+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:27 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2558 bytes result sent to driver
[2023-01-31T05:37:27.102+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:27 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 4726 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:37:27.112+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:27 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T05:37:27.117+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:27 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) finished in 4.848 s
[2023-01-31T05:37:27.127+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:27 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:37:27.128+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T05:37:27.134+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:27 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201, took 4.937908 s
[2023-01-31T05:37:27.996+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:27 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 81d5fcd0285b:33637 in memory (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T05:37:28.146+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:28 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 81d5fcd0285b:33637 in memory (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T05:37:29.527+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:37:29.584+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T05:37:29.586+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:37:30.562+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:30 INFO CodeGenerator: Code generated in 531.351255 ms
[2023-01-31T05:37:30.685+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:30 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 433.1 MiB)
[2023-01-31T05:37:30.974+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:30 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.0 MiB)
[2023-01-31T05:37:30.978+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:30 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 81d5fcd0285b:33637 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:37:30.982+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:30 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T05:37:30.996+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:37:31.221+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:31 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T05:37:31.224+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:31 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T05:37:31.225+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:31 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T05:37:31.225+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:31 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:37:31.226+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:31 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:37:31.232+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:31 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T05:37:31.268+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:31 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 433.0 MiB)
[2023-01-31T05:37:31.317+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:31 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 433.0 MiB)
[2023-01-31T05:37:31.334+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:31 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 81d5fcd0285b:33637 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T05:37:31.339+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:31 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:37:31.361+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:37:31.367+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:31 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T05:37:31.382+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:31 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:37:31.389+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:31 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T05:37:31.469+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:31 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T05:37:32.629+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:32 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-01-31T05:37:32.670+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:32 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 1289 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:37:32.672+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:32 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T05:37:32.675+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:32 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 1.433 s
[2023-01-31T05:37:32.676+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:32 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:37:32.676+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T05:37:32.677+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:32 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 1.452870 s
[2023-01-31T05:37:33.925+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:33 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:37:33.937+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:33 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T05:37:33.939+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:33 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:37:36.509+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:36 INFO CodeGenerator: Code generated in 2282.525998 ms
[2023-01-31T05:37:36.746+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:36 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.8 MiB)
[2023-01-31T05:37:37.967+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:37 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.7 MiB)
[2023-01-31T05:37:37.971+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:37 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 81d5fcd0285b:33637 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:37:37.984+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:37 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T05:37:38.101+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:37:38.385+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:38 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 81d5fcd0285b:33637 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T05:37:38.725+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:38 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T05:37:38.733+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:38 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T05:37:38.734+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:38 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T05:37:38.735+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:38 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:37:38.740+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:38 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:37:38.756+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:38 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T05:37:38.857+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:38 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.8 MiB)
[2023-01-31T05:37:38.891+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:38 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.7 MiB)
[2023-01-31T05:37:38.906+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:38 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 81d5fcd0285b:33637 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T05:37:38.930+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:38 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:37:38.946+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:37:38.947+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:38 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T05:37:38.997+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:38 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:37:39.014+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:39 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T05:37:39.224+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:39 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T05:37:40.232+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:40 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1772 bytes result sent to driver
[2023-01-31T05:37:40.283+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:40 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 1300 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:37:40.285+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:40 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T05:37:40.288+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:40 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 1.512 s
[2023-01-31T05:37:40.298+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:40 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:37:40.299+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T05:37:40.333+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:40 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 1.579839 s
[2023-01-31T05:38:08.948+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:08 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 81d5fcd0285b:33637 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:38:09.121+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:09 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 81d5fcd0285b:33637 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:38:09.396+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:09 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 81d5fcd0285b:33637 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:38:09.559+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:09 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 81d5fcd0285b:33637 in memory (size: 12.6 KiB, free: 434.3 MiB)
[2023-01-31T05:38:09.713+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:09 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 81d5fcd0285b:33637 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:38:40.829+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:40 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:38:41.069+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:38:41.073+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:38:41.129+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:41 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:38:41.169+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:38:41.170+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:38:41.210+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:38:43.142+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:43 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-31T05:38:43.148+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:43 INFO DAGScheduler: Got job 8 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T05:38:43.149+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:43 INFO DAGScheduler: Final stage: ResultStage 8 (save at BigQueryWriteHelper.java:105)
[2023-01-31T05:38:43.150+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:43 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:38:43.150+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:43 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:38:43.158+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:43 INFO DAGScheduler: Submitting ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T05:38:43.399+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:43 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 209.9 KiB, free 433.5 MiB)
[2023-01-31T05:38:43.413+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:43 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 74.3 KiB, free 433.4 MiB)
[2023-01-31T05:38:43.425+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:43 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 81d5fcd0285b:33637 (size: 74.3 KiB, free: 434.2 MiB)
[2023-01-31T05:38:43.431+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:43 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:38:43.435+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:38:43.439+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:43 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2023-01-31T05:38:43.474+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:43 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7484 bytes) taskResourceAssignments Map()
[2023-01-31T05:38:43.488+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:43 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2023-01-31T05:38:43.941+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:38:43.943+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:38:43.953+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:43 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:38:43.955+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:38:43.960+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:38:43.972+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:38:44.046+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:44 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T05:38:44.076+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:44 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T05:38:44.394+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:44 INFO ParquetOutputFormat: Parquet block size to 134217728
[2023-01-31T05:38:44.396+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:44 INFO ParquetOutputFormat: Validation is off
[2023-01-31T05:38:44.397+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:44 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-01-31T05:38:44.399+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:44 INFO ParquetOutputFormat: Parquet properties are:
[2023-01-31T05:38:44.400+0000] {spark_submit.py:495} INFO - Parquet page size to 1048576
[2023-01-31T05:38:44.400+0000] {spark_submit.py:495} INFO - Parquet dictionary page size to 1048576
[2023-01-31T05:38:44.401+0000] {spark_submit.py:495} INFO - Dictionary is true
[2023-01-31T05:38:44.401+0000] {spark_submit.py:495} INFO - Writer version is: PARQUET_1_0
[2023-01-31T05:38:44.404+0000] {spark_submit.py:495} INFO - Page size checking is: estimated
[2023-01-31T05:38:44.405+0000] {spark_submit.py:495} INFO - Min row count for page size check is: 100
[2023-01-31T05:38:44.406+0000] {spark_submit.py:495} INFO - Max row count for page size check is: 10000
[2023-01-31T05:38:44.407+0000] {spark_submit.py:495} INFO - Truncate length for column indexes is: 64
[2023-01-31T05:38:44.407+0000] {spark_submit.py:495} INFO - Truncate length for statistics min/max  is: 2147483647
[2023-01-31T05:38:44.413+0000] {spark_submit.py:495} INFO - Bloom filter enabled: false
[2023-01-31T05:38:44.414+0000] {spark_submit.py:495} INFO - Max Bloom filter size for a column is 1048576
[2023-01-31T05:38:44.414+0000] {spark_submit.py:495} INFO - Bloom filter expected number of distinct values are: null
[2023-01-31T05:38:44.415+0000] {spark_submit.py:495} INFO - Page row count limit to 20000
[2023-01-31T05:38:44.416+0000] {spark_submit.py:495} INFO - Writing page checksums is: on
[2023-01-31T05:38:44.613+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:44 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-01-31T05:38:44.613+0000] {spark_submit.py:495} INFO - {
[2023-01-31T05:38:44.614+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-31T05:38:44.614+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-31T05:38:44.615+0000] {spark_submit.py:495} INFO - "name" : "measured_at",
[2023-01-31T05:38:44.615+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T05:38:44.616+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T05:38:44.616+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:44.642+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:44.645+0000] {spark_submit.py:495} INFO - "name" : "created_at",
[2023-01-31T05:38:44.648+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T05:38:44.650+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T05:38:44.652+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:44.654+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:44.657+0000] {spark_submit.py:495} INFO - "name" : "Biomass - Actual Aggregated",
[2023-01-31T05:38:44.660+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:44.661+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:44.664+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:44.665+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:44.689+0000] {spark_submit.py:495} INFO - "name" : "Fossil Brown coal/Lignite - Actual Aggregated",
[2023-01-31T05:38:44.705+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:44.706+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:44.706+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:44.707+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:44.707+0000] {spark_submit.py:495} INFO - "name" : "Fossil Gas - Actual Aggregated",
[2023-01-31T05:38:44.708+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:44.708+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:44.720+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:44.720+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:44.721+0000] {spark_submit.py:495} INFO - "name" : "Fossil Hard coal - Actual Aggregated",
[2023-01-31T05:38:44.726+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:44.727+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:44.728+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:44.729+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:44.729+0000] {spark_submit.py:495} INFO - "name" : "Fossil Oil - Actual Aggregated",
[2023-01-31T05:38:44.730+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:44.737+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:44.738+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:44.738+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:44.739+0000] {spark_submit.py:495} INFO - "name" : "Geothermal - Actual Aggregated",
[2023-01-31T05:38:44.739+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:44.740+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:44.740+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:44.741+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:44.742+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Aggregated",
[2023-01-31T05:38:44.742+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:44.743+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:44.743+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:44.743+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:44.744+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Consumption",
[2023-01-31T05:38:44.744+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:44.806+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:44.807+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:44.810+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:44.813+0000] {spark_submit.py:495} INFO - "name" : "Hydro Run-of-river and poundage - Actual Aggregated",
[2023-01-31T05:38:44.814+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:44.817+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:44.820+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:44.823+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:44.823+0000] {spark_submit.py:495} INFO - "name" : "Hydro Water Reservoir - Actual Aggregated",
[2023-01-31T05:38:44.826+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:44.829+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:44.830+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:44.833+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:44.836+0000] {spark_submit.py:495} INFO - "name" : "Nuclear - Actual Aggregated",
[2023-01-31T05:38:44.836+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:44.844+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:44.893+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:44.903+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:44.907+0000] {spark_submit.py:495} INFO - "name" : "Other - Actual Aggregated",
[2023-01-31T05:38:44.911+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:44.911+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:44.912+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:44.913+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:44.913+0000] {spark_submit.py:495} INFO - "name" : "Other renewable - Actual Aggregated",
[2023-01-31T05:38:44.914+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:44.914+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:44.915+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:44.916+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:44.916+0000] {spark_submit.py:495} INFO - "name" : "Solar - Actual Aggregated",
[2023-01-31T05:38:44.917+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:44.917+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:44.918+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:44.918+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:44.919+0000] {spark_submit.py:495} INFO - "name" : "Waste - Actual Aggregated",
[2023-01-31T05:38:44.919+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:44.920+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:44.920+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:44.921+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:44.922+0000] {spark_submit.py:495} INFO - "name" : "Wind Offshore - Actual Aggregated",
[2023-01-31T05:38:44.922+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:44.923+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:44.923+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:44.924+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:44.924+0000] {spark_submit.py:495} INFO - "name" : "Wind Onshore - Actual Aggregated",
[2023-01-31T05:38:44.925+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:44.926+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:44.926+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:44.927+0000] {spark_submit.py:495} INFO - } ]
[2023-01-31T05:38:44.927+0000] {spark_submit.py:495} INFO - }
[2023-01-31T05:38:44.928+0000] {spark_submit.py:495} INFO - and corresponding Parquet message type:
[2023-01-31T05:38:44.928+0000] {spark_submit.py:495} INFO - message spark_schema {
[2023-01-31T05:38:44.929+0000] {spark_submit.py:495} INFO - required int96 measured_at;
[2023-01-31T05:38:44.929+0000] {spark_submit.py:495} INFO - required int96 created_at;
[2023-01-31T05:38:44.930+0000] {spark_submit.py:495} INFO - optional double Biomass - Actual Aggregated;
[2023-01-31T05:38:44.931+0000] {spark_submit.py:495} INFO - optional double Fossil Brown coal/Lignite - Actual Aggregated;
[2023-01-31T05:38:44.931+0000] {spark_submit.py:495} INFO - optional double Fossil Gas - Actual Aggregated;
[2023-01-31T05:38:44.932+0000] {spark_submit.py:495} INFO - optional double Fossil Hard coal - Actual Aggregated;
[2023-01-31T05:38:44.933+0000] {spark_submit.py:495} INFO - optional double Fossil Oil - Actual Aggregated;
[2023-01-31T05:38:44.933+0000] {spark_submit.py:495} INFO - optional double Geothermal - Actual Aggregated;
[2023-01-31T05:38:44.934+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Aggregated;
[2023-01-31T05:38:44.934+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Consumption;
[2023-01-31T05:38:44.935+0000] {spark_submit.py:495} INFO - optional double Hydro Run-of-river and poundage - Actual Aggregated;
[2023-01-31T05:38:44.935+0000] {spark_submit.py:495} INFO - optional double Hydro Water Reservoir - Actual Aggregated;
[2023-01-31T05:38:44.936+0000] {spark_submit.py:495} INFO - optional double Nuclear - Actual Aggregated;
[2023-01-31T05:38:44.937+0000] {spark_submit.py:495} INFO - optional double Other - Actual Aggregated;
[2023-01-31T05:38:44.937+0000] {spark_submit.py:495} INFO - optional double Other renewable - Actual Aggregated;
[2023-01-31T05:38:44.938+0000] {spark_submit.py:495} INFO - optional double Solar - Actual Aggregated;
[2023-01-31T05:38:44.939+0000] {spark_submit.py:495} INFO - optional double Waste - Actual Aggregated;
[2023-01-31T05:38:44.939+0000] {spark_submit.py:495} INFO - optional double Wind Offshore - Actual Aggregated;
[2023-01-31T05:38:44.940+0000] {spark_submit.py:495} INFO - optional double Wind Onshore - Actual Aggregated;
[2023-01-31T05:38:44.940+0000] {spark_submit.py:495} INFO - }
[2023-01-31T05:38:44.941+0000] {spark_submit.py:495} INFO - 
[2023-01-31T05:38:44.942+0000] {spark_submit.py:495} INFO - 
[2023-01-31T05:38:48.788+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:48 INFO CodecPool: Got brand-new compressor [.snappy]
[2023-01-31T05:38:49.527+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:49 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 81d5fcd0285b:33637 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:38:49.837+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:49 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 81d5fcd0285b:33637 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:39:02.817+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:02 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675143139697-f270d7d1-7928-41b4-9ff4-87f3d1e71beb/_temporary/0/_temporary/' directory.
[2023-01-31T05:39:02.819+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:02 INFO FileOutputCommitter: Saved output of task 'attempt_202301310538425724908640995875172_0008_m_000000_8' to gs://entsoe_temp_1009/.spark-bigquery-local-1675143139697-f270d7d1-7928-41b4-9ff4-87f3d1e71beb/_temporary/0/task_202301310538425724908640995875172_0008_m_000000
[2023-01-31T05:39:02.823+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:02 INFO SparkHadoopMapRedUtil: attempt_202301310538425724908640995875172_0008_m_000000_8: Committed. Elapsed time: 3562 ms.
[2023-01-31T05:39:03.000+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:02 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2588 bytes result sent to driver
[2023-01-31T05:39:03.009+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:03 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 19557 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:39:03.014+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:03 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2023-01-31T05:39:03.015+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:03 INFO DAGScheduler: ResultStage 8 (save at BigQueryWriteHelper.java:105) finished in 19.835 s
[2023-01-31T05:39:03.016+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:03 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:39:03.016+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2023-01-31T05:39:03.042+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:03 INFO DAGScheduler: Job 8 finished: save at BigQueryWriteHelper.java:105, took 19.868770 s
[2023-01-31T05:39:03.043+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:03 INFO FileFormatWriter: Start to commit write Job cccb6af8-5cae-45ff-9b27-a70c0baa5348.
[2023-01-31T05:39:04.339+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:04 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675143139697-f270d7d1-7928-41b4-9ff4-87f3d1e71beb/_temporary/0/task_202301310538425724908640995875172_0008_m_000000/' directory.
[2023-01-31T05:39:05.729+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:05 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675143139697-f270d7d1-7928-41b4-9ff4-87f3d1e71beb/' directory.
[2023-01-31T05:39:07.611+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:07 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 81d5fcd0285b:33637 in memory (size: 74.3 KiB, free: 434.4 MiB)
[2023-01-31T05:39:07.720+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:07 INFO FileFormatWriter: Write Job cccb6af8-5cae-45ff-9b27-a70c0baa5348 committed. Elapsed time: 4688 ms.
[2023-01-31T05:39:07.756+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:07 INFO FileFormatWriter: Finished processing stats for write job cccb6af8-5cae-45ff-9b27-a70c0baa5348.
[2023-01-31T05:39:13.716+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:13 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=entsoe_analytics_1009, projectId=rafzul-analytics-1009, tableId=TEST_total_generation_staging}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=null, ignoreUnknownValue=null, sourceUris=[gs://entsoe_temp_1009/.spark-bigquery-local-1675143139697-f270d7d1-7928-41b4-9ff4-87f3d1e71beb/part-00000-894c846c-e0a3-474c-b2fd-a11a8e227feb-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=true, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=rafzul-analytics-1009, job=a03b745e-41b0-4c9d-84bd-12fbcd4eabff, location=US}
[2023-01-31T05:39:18.085+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:18 INFO BigQueryClient: Done loading to rafzul-analytics-1009.entsoe_analytics_1009.TEST_total_generation_staging. jobId: JobId{project=rafzul-analytics-1009, job=a03b745e-41b0-4c9d-84bd-12fbcd4eabff, location=US}
[2023-01-31T05:39:19.413+0000] {spark_submit.py:495} INFO - MANTEP!!!!
[2023-01-31T05:39:21.400+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:21 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-31T05:39:21.633+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:21 INFO SparkUI: Stopped Spark web UI at http://81d5fcd0285b:4042
[2023-01-31T05:39:21.864+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:21 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-31T05:39:22.101+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:22 INFO MemoryStore: MemoryStore cleared
[2023-01-31T05:39:22.103+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:22 INFO BlockManager: BlockManager stopped
[2023-01-31T05:39:22.142+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:22 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-31T05:39:22.183+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:22 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-31T05:39:22.276+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:22 INFO SparkContext: Successfully stopped SparkContext
[2023-01-31T05:39:22.277+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:22 INFO ShutdownHookManager: Shutdown hook called
[2023-01-31T05:39:22.337+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-ff60e954-afef-4f2e-af78-833002cbf4b9
[2023-01-31T05:39:22.338+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-ff60e954-afef-4f2e-af78-833002cbf4b9/pyspark-155aaf1d-3ead-45d8-9d27-b7ece4ca07a8
[2023-01-31T05:39:22.362+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-73f7e64d-4683-4f27-8b71-f1335f39ed01
[2023-01-31T05:39:24.518+0000] {taskinstance.py:1322} INFO - Marking task as SUCCESS. dag_id=entsoe-energydata-backfill, task_id=stage_total_generation, execution_date=20210101T000000, start_date=20230131T053037, end_date=20230131T053924
[2023-01-31T05:39:24.866+0000] {local_task_job.py:159} INFO - Task exited with return code 0
[2023-01-31T05:39:25.102+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T07:39:04.035+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-31T07:39:04.206+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-31T07:39:04.207+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T07:39:04.207+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T07:39:04.208+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T07:39:04.383+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 00:00:00+00:00
[2023-01-31T07:39:04.471+0000] {standard_task_runner.py:55} INFO - Started process 25821 to run task
[2023-01-31T07:39:04.508+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '912', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp8cwas3vk']
[2023-01-31T07:39:04.550+0000] {standard_task_runner.py:83} INFO - Job 912: Subtask stage_total_generation
[2023-01-31T07:39:05.544+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 81d5fcd0285b
[2023-01-31T07:39:07.398+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-31T07:39:07.542+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T07:39:07.570+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-31T07:40:21.014+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:20 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T07:40:22.911+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T07:40:27.217+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:27 INFO ResourceUtils: ==============================================================
[2023-01-31T07:40:27.218+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:27 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T07:40:27.221+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:27 INFO ResourceUtils: ==============================================================
[2023-01-31T07:40:27.221+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:27 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T07:40:28.628+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:28 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T07:40:28.897+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:28 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T07:40:28.934+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:28 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T07:40:31.517+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:31 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T07:40:31.517+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:31 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T07:40:31.518+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:31 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T07:40:31.578+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:31 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T07:40:31.588+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:31 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T07:40:38.024+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:38 INFO Utils: Successfully started service 'sparkDriver' on port 39127.
[2023-01-31T07:40:39.931+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:39 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T07:40:41.313+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:41 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T07:40:43.333+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:43 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T07:40:43.385+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:43 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T07:40:43.781+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:43 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T07:40:45.504+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:45 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-36a2da1e-7b69-4d26-8f04-fc2b8ba83b74
[2023-01-31T07:40:46.145+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:46 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T07:40:46.931+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:46 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T07:40:51.516+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:51 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T07:40:51.519+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:51 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2023-01-31T07:40:51.532+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:51 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[2023-01-31T07:40:51.601+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:51 INFO Utils: Successfully started service 'SparkUI' on port 4043.
[2023-01-31T07:40:51.856+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:51 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://81d5fcd0285b:39127/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675150820556
[2023-01-31T07:40:51.858+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:51 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://81d5fcd0285b:39127/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675150820556
[2023-01-31T07:40:54.625+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:54 INFO Executor: Starting executor ID driver on host 81d5fcd0285b
[2023-01-31T07:40:55.389+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:55 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T07:40:56.653+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:56 INFO Executor: Fetching spark://81d5fcd0285b:39127/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675150820556
[2023-01-31T07:40:58.981+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:58 INFO TransportClientFactory: Successfully created connection to 81d5fcd0285b/172.19.0.9:39127 after 1269 ms (0 ms spent in bootstraps)
[2023-01-31T07:40:59.358+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:59 INFO Utils: Fetching spark://81d5fcd0285b:39127/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-cf519ba5-cefe-4069-9a87-7c60caed26fa/userFiles-bd46d63d-69e8-4cf9-8493-0e2c526825b6/fetchFileTemp15490262226496471649.tmp
[2023-01-31T07:41:09.755+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:09 INFO Executor: Adding file:/tmp/spark-cf519ba5-cefe-4069-9a87-7c60caed26fa/userFiles-bd46d63d-69e8-4cf9-8493-0e2c526825b6/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T07:41:09.763+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:09 INFO Executor: Fetching spark://81d5fcd0285b:39127/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675150820556
[2023-01-31T07:41:09.775+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:09 INFO Utils: Fetching spark://81d5fcd0285b:39127/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-cf519ba5-cefe-4069-9a87-7c60caed26fa/userFiles-bd46d63d-69e8-4cf9-8493-0e2c526825b6/fetchFileTemp4348750413952964773.tmp
[2023-01-31T07:41:13.601+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:13 INFO Executor: Told to re-register on heartbeat
[2023-01-31T07:41:13.604+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:13 INFO BlockManager: BlockManager null re-registering with master
[2023-01-31T07:41:13.614+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:13 INFO BlockManagerMaster: Registering BlockManager null
[2023-01-31T07:41:13.844+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:13 WARN Executor: Issue communicating with driver in heartbeater
[2023-01-31T07:41:13.846+0000] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-01-31T07:41:13.847+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-01-31T07:41:13.847+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-01-31T07:41:13.848+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2023-01-31T07:41:13.848+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2023-01-31T07:41:13.849+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:79)
[2023-01-31T07:41:13.850+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:643)
[2023-01-31T07:41:13.850+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2023-01-31T07:41:13.851+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2023-01-31T07:41:13.851+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
[2023-01-31T07:41:13.852+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2023-01-31T07:41:13.893+0000] {spark_submit.py:495} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2023-01-31T07:41:13.894+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-01-31T07:41:13.895+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2023-01-31T07:41:13.895+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2023-01-31T07:41:13.896+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-31T07:41:13.930+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-31T07:41:13.931+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-31T07:41:13.932+0000] {spark_submit.py:495} INFO - Caused by: java.lang.NullPointerException
[2023-01-31T07:41:13.941+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)
[2023-01-31T07:41:13.942+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)
[2023-01-31T07:41:13.942+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-01-31T07:41:13.943+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-01-31T07:41:13.943+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-01-31T07:41:13.944+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-01-31T07:41:14.011+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-01-31T07:41:14.021+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-01-31T07:41:14.022+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-01-31T07:41:14.032+0000] {spark_submit.py:495} INFO - ... 3 more
[2023-01-31T07:41:14.033+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:13 ERROR Inbox: Ignoring error
[2023-01-31T07:41:14.034+0000] {spark_submit.py:495} INFO - java.lang.NullPointerException
[2023-01-31T07:41:14.056+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)
[2023-01-31T07:41:14.057+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)
[2023-01-31T07:41:14.058+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-01-31T07:41:14.058+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-01-31T07:41:14.059+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-01-31T07:41:14.059+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-01-31T07:41:14.060+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-01-31T07:41:14.060+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-01-31T07:41:14.100+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-01-31T07:41:14.107+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-31T07:41:14.113+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-31T07:41:14.123+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-31T07:41:15.797+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:15 INFO Executor: Adding file:/tmp/spark-cf519ba5-cefe-4069-9a87-7c60caed26fa/userFiles-bd46d63d-69e8-4cf9-8493-0e2c526825b6/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T07:41:15.877+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:15 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35777.
[2023-01-31T07:41:15.877+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:15 INFO NettyBlockTransferService: Server created on 81d5fcd0285b:35777
[2023-01-31T07:41:15.890+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:15 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T07:41:16.120+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:16 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 35777, None)
[2023-01-31T07:41:16.237+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:16 INFO BlockManagerMasterEndpoint: Registering block manager 81d5fcd0285b:35777 with 434.4 MiB RAM, BlockManagerId(driver, 81d5fcd0285b, 35777, None)
[2023-01-31T07:41:16.348+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:16 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 35777, None)
[2023-01-31T07:41:16.457+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:16 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 81d5fcd0285b, 35777, None)
[2023-01-31T07:41:34.682+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:34 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T07:41:34.881+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:34 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T07:42:09.807+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:09 INFO InMemoryFileIndex: It took 2208 ms to list leaf files for 1 paths.
[2023-01-31T07:42:17.753+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:17 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T07:42:19.025+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:19 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T07:42:19.056+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:19 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 81d5fcd0285b:35777 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T07:42:19.092+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:19 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T07:42:28.762+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:28 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T07:42:29.080+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:29 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T07:42:29.427+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:29 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T07:42:30.259+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:30 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T07:42:30.266+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:30 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T07:42:30.275+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:30 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:42:30.406+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:30 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:42:30.564+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:30 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T07:42:32.022+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:32 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T07:42:32.101+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:32 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T07:42:32.113+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:32 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 81d5fcd0285b:35777 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T07:42:32.115+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:32 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:42:32.555+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:42:32.589+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:32 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T07:42:34.498+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:34 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T07:42:35.122+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:35 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T07:42:38.763+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:38 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010000__202101010100.json:0+9369
[2023-01-31T07:42:43.669+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:43 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3480 bytes result sent to driver
[2023-01-31T07:42:43.804+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:43 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 9926 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:42:43.823+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:43 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T07:42:43.862+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:43 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 12.684 s
[2023-01-31T07:42:44.070+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:44 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:42:44.118+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T07:42:44.187+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:44 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 14.770249 s
[2023-01-31T07:43:02.575+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:02 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 81d5fcd0285b:35777 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T07:43:02.882+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:02 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 81d5fcd0285b:35777 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T07:43:24.324+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:24 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:43:24.333+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:24 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T07:43:24.339+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:24 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:43:26.421+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:26 INFO CodeGenerator: Code generated in 1498.806235 ms
[2023-01-31T07:43:26.496+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:26 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T07:43:26.652+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:26 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T07:43:26.658+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:26 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 81d5fcd0285b:35777 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T07:43:26.674+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:26 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T07:43:26.805+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:26 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:43:27.657+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T07:43:27.691+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T07:43:27.692+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T07:43:27.692+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:43:27.692+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:43:27.733+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T07:43:27.872+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T07:43:27.896+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T07:43:27.902+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 81d5fcd0285b:35777 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T07:43:27.909+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:43:27.927+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:43:27.927+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T07:43:27.980+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:43:27.989+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T07:43:28.876+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:28 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T07:43:30.105+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO CodeGenerator: Code generated in 934.578903 ms
[2023-01-31T07:43:30.764+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1628 bytes result sent to driver
[2023-01-31T07:43:30.780+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2835 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:43:30.786+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T07:43:30.802+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 3.029 s
[2023-01-31T07:43:30.803+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:43:30.804+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T07:43:30.812+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 3.120768 s
[2023-01-31T07:43:31.606+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:31 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:43:31.606+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:31 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T07:43:31.608+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:31 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:43:31.911+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:31 INFO CodeGenerator: Code generated in 208.06036 ms
[2023-01-31T07:43:31.985+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:31 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T07:43:32.123+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T07:43:32.139+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 81d5fcd0285b:35777 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:43:32.145+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T07:43:32.160+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:43:32.360+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T07:43:32.362+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T07:43:32.363+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T07:43:32.363+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:43:32.363+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:43:32.373+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T07:43:32.441+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T07:43:32.497+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T07:43:32.507+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 81d5fcd0285b:35777 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T07:43:32.525+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:43:32.527+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:43:32.528+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T07:43:32.535+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:43:32.540+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T07:43:32.636+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T07:43:33.122+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1585 bytes result sent to driver
[2023-01-31T07:43:33.172+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 638 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:43:33.177+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 0.786 s
[2023-01-31T07:43:33.178+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:43:33.182+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T07:43:33.190+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T07:43:33.190+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 0.825432 s
[2023-01-31T07:43:34.546+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:43:34.563+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T07:43:34.572+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:43:36.105+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:36 INFO CodeGenerator: Code generated in 962.533115 ms
[2023-01-31T07:43:36.189+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:36 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T07:43:36.368+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:36 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T07:43:36.385+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:36 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 81d5fcd0285b:35777 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:43:36.404+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:36 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T07:43:36.435+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:36 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:43:36.709+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:36 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 81d5fcd0285b:35777 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T07:43:36.790+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:36 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 81d5fcd0285b:35777 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T07:43:37.109+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:37 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T07:43:37.113+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:37 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T07:43:37.116+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:37 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T07:43:37.116+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:37 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:43:37.117+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:37 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:43:37.124+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:37 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T07:43:37.250+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:37 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.7 MiB)
[2023-01-31T07:43:37.321+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:37 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.7 MiB)
[2023-01-31T07:43:37.329+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:37 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 81d5fcd0285b:35777 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T07:43:37.339+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:37 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:43:37.370+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:43:37.376+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:37 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T07:43:37.398+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:37 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:43:37.399+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:37 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T07:43:37.547+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:37 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T07:43:37.968+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:37 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-01-31T07:43:38.013+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:38 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 608 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:43:38.032+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:38 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 0.895 s
[2023-01-31T07:43:38.044+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:38 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:43:38.052+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:38 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T07:43:38.063+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T07:43:38.073+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:38 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 0.960232 s
[2023-01-31T07:43:38.631+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:38 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:43:38.633+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:38 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T07:43:38.636+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:38 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:43:39.196+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:39 INFO CodeGenerator: Code generated in 401.3837 ms
[2023-01-31T07:43:39.252+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:39 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.5 MiB)
[2023-01-31T07:43:39.379+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:39 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T07:43:39.401+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:39 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 81d5fcd0285b:35777 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:43:39.434+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:39 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T07:43:39.448+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:39 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:43:40.077+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T07:43:40.087+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T07:43:40.088+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T07:43:40.089+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:43:40.095+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:43:40.122+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T07:43:40.276+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-31T07:43:40.381+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-01-31T07:43:40.388+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 81d5fcd0285b:35777 (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T07:43:40.414+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:43:40.447+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:43:40.451+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T07:43:40.511+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:43:40.576+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T07:43:40.651+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T07:43:41.208+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:41 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1586 bytes result sent to driver
[2023-01-31T07:43:41.217+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:41 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 717 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:43:41.232+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:41 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 1.078 s
[2023-01-31T07:43:41.233+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:41 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:43:41.236+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:41 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T07:43:41.240+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T07:43:41.249+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:41 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 1.161283 s
[2023-01-31T07:43:45.337+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:43:45.344+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T07:43:45.352+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:43:48.039+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:48 INFO CodeGenerator: Code generated in 1625.741816 ms
[2023-01-31T07:43:48.201+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:48 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.2 MiB)
[2023-01-31T07:43:48.847+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:48 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T07:43:48.890+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:48 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 81d5fcd0285b:35777 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T07:43:48.944+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:48 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T07:43:48.974+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:43:49.979+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:49 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T07:43:49.989+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:49 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) with 1 output partitions
[2023-01-31T07:43:49.992+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:49 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201)
[2023-01-31T07:43:49.994+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:49 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:43:49.997+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:49 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:43:49.998+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:49 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201), which has no missing parents
[2023-01-31T07:43:50.133+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.1 MiB)
[2023-01-31T07:43:50.184+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.1 MiB)
[2023-01-31T07:43:50.185+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 81d5fcd0285b:35777 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T07:43:50.188+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:43:50.193+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:43:50.198+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T07:43:50.204+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:43:50.205+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T07:43:50.314+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T07:43:51.265+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:51 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2558 bytes result sent to driver
[2023-01-31T07:43:51.461+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:51 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 1201 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:43:51.462+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:51 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) finished in 1.402 s
[2023-01-31T07:43:51.462+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:51 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:43:51.466+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:51 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T07:43:51.466+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T07:43:51.494+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:51 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201, took 1.510747 s
[2023-01-31T07:43:52.905+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:52 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:43:52.961+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:52 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T07:43:52.962+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:52 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:43:53.869+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:53 INFO CodeGenerator: Code generated in 251.077073 ms
[2023-01-31T07:43:53.952+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:53 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 432.9 MiB)
[2023-01-31T07:43:54.103+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:54 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.9 MiB)
[2023-01-31T07:43:54.109+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:54 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 81d5fcd0285b:35777 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T07:43:54.115+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:54 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T07:43:54.121+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:54 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:43:54.258+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:54 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T07:43:54.311+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:54 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T07:43:54.336+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:54 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T07:43:54.378+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:54 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:43:54.379+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:54 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:43:54.500+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:54 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T07:43:54.816+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:54 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 432.9 MiB)
[2023-01-31T07:43:54.880+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:54 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.8 MiB)
[2023-01-31T07:43:54.899+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:54 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 81d5fcd0285b:35777 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T07:43:54.917+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:54 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:43:54.945+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:43:54.947+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:54 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T07:43:55.163+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:55 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:43:55.225+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:55 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T07:43:55.577+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:55 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T07:43:57.157+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:57 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-01-31T07:43:57.191+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:57 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 2055 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:43:57.192+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:57 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T07:43:57.206+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:57 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 2.614 s
[2023-01-31T07:43:57.207+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:57 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:43:57.207+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T07:43:57.208+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:57 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 2.948298 s
[2023-01-31T07:43:59.505+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:59 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:43:59.506+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:59 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T07:43:59.506+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:59 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:44:03.533+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:03 INFO CodeGenerator: Code generated in 3745.364408 ms
[2023-01-31T07:44:03.553+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:03 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.7 MiB)
[2023-01-31T07:44:03.840+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:03 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.6 MiB)
[2023-01-31T07:44:03.841+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:03 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 81d5fcd0285b:35777 (size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T07:44:03.894+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:03 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T07:44:03.910+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:03 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:44:04.114+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T07:44:04.119+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T07:44:04.120+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T07:44:04.121+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:44:04.142+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:44:04.148+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T07:44:04.160+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.6 MiB)
[2023-01-31T07:44:04.301+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.6 MiB)
[2023-01-31T07:44:04.304+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 81d5fcd0285b:35777 (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T07:44:04.317+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:44:04.332+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:44:04.336+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T07:44:04.354+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:44:04.378+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T07:44:04.645+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T07:44:05.659+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:05 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1729 bytes result sent to driver
[2023-01-31T07:44:05.723+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:05 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 1355 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:44:05.772+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:05 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T07:44:05.773+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:05 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 1.586 s
[2023-01-31T07:44:05.774+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:05 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:44:05.775+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T07:44:05.776+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:05 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 1.646934 s
[2023-01-31T07:44:17.951+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:17 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 81d5fcd0285b:35777 in memory (size: 7.6 KiB, free: 434.1 MiB)
[2023-01-31T07:44:18.284+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:18 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 81d5fcd0285b:35777 in memory (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T07:44:18.772+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:18 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 81d5fcd0285b:35777 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T07:44:19.099+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:19 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 81d5fcd0285b:35777 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T07:44:19.317+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:19 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 81d5fcd0285b:35777 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T07:44:19.481+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:19 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 81d5fcd0285b:35777 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T07:44:19.911+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:19 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 81d5fcd0285b:35777 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:44:20.170+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:20 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 81d5fcd0285b:35777 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:44:20.329+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:20 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 81d5fcd0285b:35777 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:44:20.524+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:20 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 81d5fcd0285b:35777 in memory (size: 13.4 KiB, free: 434.4 MiB)
[2023-01-31T07:44:20.744+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:20 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 81d5fcd0285b:35777 in memory (size: 12.7 KiB, free: 434.4 MiB)
[2023-01-31T07:45:08.851+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:08 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T07:45:08.979+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T07:45:08.982+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T07:45:08.996+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:08 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T07:45:08.999+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T07:45:09.004+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T07:45:09.046+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T07:45:10.513+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:10 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-31T07:45:10.513+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:10 INFO DAGScheduler: Got job 8 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T07:45:10.513+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:10 INFO DAGScheduler: Final stage: ResultStage 8 (save at BigQueryWriteHelper.java:105)
[2023-01-31T07:45:10.514+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:10 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:45:10.514+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:10 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:45:10.519+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:10 INFO DAGScheduler: Submitting ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T07:45:11.016+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 209.9 KiB, free 434.0 MiB)
[2023-01-31T07:45:11.086+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 74.3 KiB, free 433.9 MiB)
[2023-01-31T07:45:11.100+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 81d5fcd0285b:35777 (size: 74.3 KiB, free: 434.3 MiB)
[2023-01-31T07:45:11.101+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:45:11.106+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:45:11.106+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2023-01-31T07:45:11.122+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7484 bytes) taskResourceAssignments Map()
[2023-01-31T07:45:11.143+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2023-01-31T07:45:11.517+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T07:45:11.521+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T07:45:11.536+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T07:45:11.537+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T07:45:11.537+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T07:45:11.546+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T07:45:11.616+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T07:45:11.682+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T07:45:12.093+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:12 INFO ParquetOutputFormat: Parquet block size to 134217728
[2023-01-31T07:45:12.094+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:12 INFO ParquetOutputFormat: Validation is off
[2023-01-31T07:45:12.095+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:12 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-01-31T07:45:12.110+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:12 INFO ParquetOutputFormat: Parquet properties are:
[2023-01-31T07:45:12.111+0000] {spark_submit.py:495} INFO - Parquet page size to 1048576
[2023-01-31T07:45:12.111+0000] {spark_submit.py:495} INFO - Parquet dictionary page size to 1048576
[2023-01-31T07:45:12.111+0000] {spark_submit.py:495} INFO - Dictionary is true
[2023-01-31T07:45:12.111+0000] {spark_submit.py:495} INFO - Writer version is: PARQUET_1_0
[2023-01-31T07:45:12.112+0000] {spark_submit.py:495} INFO - Page size checking is: estimated
[2023-01-31T07:45:12.112+0000] {spark_submit.py:495} INFO - Min row count for page size check is: 100
[2023-01-31T07:45:12.113+0000] {spark_submit.py:495} INFO - Max row count for page size check is: 10000
[2023-01-31T07:45:12.113+0000] {spark_submit.py:495} INFO - Truncate length for column indexes is: 64
[2023-01-31T07:45:12.113+0000] {spark_submit.py:495} INFO - Truncate length for statistics min/max  is: 2147483647
[2023-01-31T07:45:12.113+0000] {spark_submit.py:495} INFO - Bloom filter enabled: false
[2023-01-31T07:45:12.113+0000] {spark_submit.py:495} INFO - Max Bloom filter size for a column is 1048576
[2023-01-31T07:45:12.114+0000] {spark_submit.py:495} INFO - Bloom filter expected number of distinct values are: null
[2023-01-31T07:45:12.114+0000] {spark_submit.py:495} INFO - Page row count limit to 20000
[2023-01-31T07:45:12.114+0000] {spark_submit.py:495} INFO - Writing page checksums is: on
[2023-01-31T07:45:12.696+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:12 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-01-31T07:45:12.697+0000] {spark_submit.py:495} INFO - {
[2023-01-31T07:45:12.697+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-31T07:45:12.698+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-31T07:45:12.711+0000] {spark_submit.py:495} INFO - "name" : "measured_at",
[2023-01-31T07:45:12.711+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T07:45:12.711+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T07:45:12.712+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.712+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.719+0000] {spark_submit.py:495} INFO - "name" : "created_at",
[2023-01-31T07:45:12.720+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T07:45:12.720+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T07:45:12.720+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.721+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.721+0000] {spark_submit.py:495} INFO - "name" : "Biomass - Actual Aggregated",
[2023-01-31T07:45:12.721+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.721+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.721+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.721+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.722+0000] {spark_submit.py:495} INFO - "name" : "Fossil Brown coal/Lignite - Actual Aggregated",
[2023-01-31T07:45:12.722+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.722+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.722+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.722+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.722+0000] {spark_submit.py:495} INFO - "name" : "Fossil Gas - Actual Aggregated",
[2023-01-31T07:45:12.723+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.723+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.723+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.723+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.723+0000] {spark_submit.py:495} INFO - "name" : "Fossil Hard coal - Actual Aggregated",
[2023-01-31T07:45:12.723+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.724+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.724+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.724+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.724+0000] {spark_submit.py:495} INFO - "name" : "Fossil Oil - Actual Aggregated",
[2023-01-31T07:45:12.724+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.779+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.783+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.784+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.802+0000] {spark_submit.py:495} INFO - "name" : "Geothermal - Actual Aggregated",
[2023-01-31T07:45:12.803+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.808+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.808+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.808+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.824+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Aggregated",
[2023-01-31T07:45:12.827+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.829+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.850+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.856+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.861+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Consumption",
[2023-01-31T07:45:12.863+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.866+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.866+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.867+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.867+0000] {spark_submit.py:495} INFO - "name" : "Hydro Run-of-river and poundage - Actual Aggregated",
[2023-01-31T07:45:12.867+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.869+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.869+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.869+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.869+0000] {spark_submit.py:495} INFO - "name" : "Hydro Water Reservoir - Actual Aggregated",
[2023-01-31T07:45:12.869+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.870+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.870+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.870+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.870+0000] {spark_submit.py:495} INFO - "name" : "Nuclear - Actual Aggregated",
[2023-01-31T07:45:12.870+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.870+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.871+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.871+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.871+0000] {spark_submit.py:495} INFO - "name" : "Other - Actual Aggregated",
[2023-01-31T07:45:12.871+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.871+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.871+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.872+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.872+0000] {spark_submit.py:495} INFO - "name" : "Other renewable - Actual Aggregated",
[2023-01-31T07:45:12.872+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.872+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.872+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.876+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.877+0000] {spark_submit.py:495} INFO - "name" : "Solar - Actual Aggregated",
[2023-01-31T07:45:12.877+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.877+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.878+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.878+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.878+0000] {spark_submit.py:495} INFO - "name" : "Waste - Actual Aggregated",
[2023-01-31T07:45:12.878+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.878+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.878+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.879+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.879+0000] {spark_submit.py:495} INFO - "name" : "Wind Offshore - Actual Aggregated",
[2023-01-31T07:45:12.879+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.879+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.879+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.879+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.880+0000] {spark_submit.py:495} INFO - "name" : "Wind Onshore - Actual Aggregated",
[2023-01-31T07:45:12.902+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.905+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.906+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.906+0000] {spark_submit.py:495} INFO - } ]
[2023-01-31T07:45:12.911+0000] {spark_submit.py:495} INFO - }
[2023-01-31T07:45:12.911+0000] {spark_submit.py:495} INFO - and corresponding Parquet message type:
[2023-01-31T07:45:12.911+0000] {spark_submit.py:495} INFO - message spark_schema {
[2023-01-31T07:45:12.911+0000] {spark_submit.py:495} INFO - required int96 measured_at;
[2023-01-31T07:45:12.912+0000] {spark_submit.py:495} INFO - required int96 created_at;
[2023-01-31T07:45:12.912+0000] {spark_submit.py:495} INFO - optional double Biomass - Actual Aggregated;
[2023-01-31T07:45:12.912+0000] {spark_submit.py:495} INFO - optional double Fossil Brown coal/Lignite - Actual Aggregated;
[2023-01-31T07:45:12.912+0000] {spark_submit.py:495} INFO - optional double Fossil Gas - Actual Aggregated;
[2023-01-31T07:45:12.912+0000] {spark_submit.py:495} INFO - optional double Fossil Hard coal - Actual Aggregated;
[2023-01-31T07:45:12.912+0000] {spark_submit.py:495} INFO - optional double Fossil Oil - Actual Aggregated;
[2023-01-31T07:45:12.913+0000] {spark_submit.py:495} INFO - optional double Geothermal - Actual Aggregated;
[2023-01-31T07:45:12.913+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Aggregated;
[2023-01-31T07:45:12.913+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Consumption;
[2023-01-31T07:45:12.913+0000] {spark_submit.py:495} INFO - optional double Hydro Run-of-river and poundage - Actual Aggregated;
[2023-01-31T07:45:12.913+0000] {spark_submit.py:495} INFO - optional double Hydro Water Reservoir - Actual Aggregated;
[2023-01-31T07:45:12.913+0000] {spark_submit.py:495} INFO - optional double Nuclear - Actual Aggregated;
[2023-01-31T07:45:12.913+0000] {spark_submit.py:495} INFO - optional double Other - Actual Aggregated;
[2023-01-31T07:45:12.914+0000] {spark_submit.py:495} INFO - optional double Other renewable - Actual Aggregated;
[2023-01-31T07:45:12.914+0000] {spark_submit.py:495} INFO - optional double Solar - Actual Aggregated;
[2023-01-31T07:45:12.914+0000] {spark_submit.py:495} INFO - optional double Waste - Actual Aggregated;
[2023-01-31T07:45:12.914+0000] {spark_submit.py:495} INFO - optional double Wind Offshore - Actual Aggregated;
[2023-01-31T07:45:12.914+0000] {spark_submit.py:495} INFO - optional double Wind Onshore - Actual Aggregated;
[2023-01-31T07:45:12.914+0000] {spark_submit.py:495} INFO - }
[2023-01-31T07:45:12.915+0000] {spark_submit.py:495} INFO - 
[2023-01-31T07:45:12.915+0000] {spark_submit.py:495} INFO - 
[2023-01-31T07:45:14.387+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:14 INFO CodecPool: Got brand-new compressor [.snappy]
[2023-01-31T07:45:20.839+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:20 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675150853071-f43d0aeb-350a-41d9-a4aa-dbbc47552624/_temporary/0/_temporary/' directory.
[2023-01-31T07:45:20.840+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:20 INFO FileOutputCommitter: Saved output of task 'attempt_202301310745101109696374716676219_0008_m_000000_8' to gs://entsoe_temp_1009/.spark-bigquery-local-1675150853071-f43d0aeb-350a-41d9-a4aa-dbbc47552624/_temporary/0/task_202301310745101109696374716676219_0008_m_000000
[2023-01-31T07:45:20.849+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:20 INFO SparkHadoopMapRedUtil: attempt_202301310745101109696374716676219_0008_m_000000_8: Committed. Elapsed time: 1729 ms.
[2023-01-31T07:45:20.916+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:20 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2588 bytes result sent to driver
[2023-01-31T07:45:20.928+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:20 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 9818 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:45:20.930+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:20 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2023-01-31T07:45:20.941+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:20 INFO DAGScheduler: ResultStage 8 (save at BigQueryWriteHelper.java:105) finished in 10.417 s
[2023-01-31T07:45:20.942+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:20 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:45:20.942+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2023-01-31T07:45:20.951+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:20 INFO DAGScheduler: Job 8 finished: save at BigQueryWriteHelper.java:105, took 10.438601 s
[2023-01-31T07:45:20.961+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:20 INFO FileFormatWriter: Start to commit write Job 16664d95-6675-42eb-9d7e-784b153be803.
[2023-01-31T07:45:22.712+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:22 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675150853071-f43d0aeb-350a-41d9-a4aa-dbbc47552624/_temporary/0/task_202301310745101109696374716676219_0008_m_000000/' directory.
[2023-01-31T07:45:24.272+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:24 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675150853071-f43d0aeb-350a-41d9-a4aa-dbbc47552624/' directory.
[2023-01-31T07:45:25.229+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:25 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 81d5fcd0285b:35777 in memory (size: 74.3 KiB, free: 434.4 MiB)
[2023-01-31T07:45:25.333+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:25 INFO FileFormatWriter: Write Job 16664d95-6675-42eb-9d7e-784b153be803 committed. Elapsed time: 4302 ms.
[2023-01-31T07:45:25.640+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:25 INFO FileFormatWriter: Finished processing stats for write job 16664d95-6675-42eb-9d7e-784b153be803.
[2023-01-31T07:45:27.985+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:27 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=entsoe_analytics_1009, projectId=rafzul-analytics-1009, tableId=TEST_total_generation_staging}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=null, ignoreUnknownValue=null, sourceUris=[gs://entsoe_temp_1009/.spark-bigquery-local-1675150853071-f43d0aeb-350a-41d9-a4aa-dbbc47552624/part-00000-7b05a57b-1285-4985-b54f-b516d313f0c5-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=true, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=rafzul-analytics-1009, job=53d27423-96b8-4ddf-bc32-0983386d3665, location=US}
[2023-01-31T07:45:33.703+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:33 INFO BigQueryClient: Done loading to rafzul-analytics-1009.entsoe_analytics_1009.TEST_total_generation_staging. jobId: JobId{project=rafzul-analytics-1009, job=53d27423-96b8-4ddf-bc32-0983386d3665, location=US}
[2023-01-31T07:45:35.195+0000] {spark_submit.py:495} INFO - MANTEP!!!!
[2023-01-31T07:45:35.689+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:35 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-31T07:45:35.781+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:35 INFO SparkUI: Stopped Spark web UI at http://81d5fcd0285b:4043
[2023-01-31T07:45:35.843+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:35 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-31T07:45:35.901+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:35 INFO MemoryStore: MemoryStore cleared
[2023-01-31T07:45:35.904+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:35 INFO BlockManager: BlockManager stopped
[2023-01-31T07:45:35.919+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:35 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-31T07:45:35.931+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:35 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-31T07:45:35.961+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:35 INFO SparkContext: Successfully stopped SparkContext
[2023-01-31T07:45:35.964+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:35 INFO ShutdownHookManager: Shutdown hook called
[2023-01-31T07:45:35.968+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:35 INFO ShutdownHookManager: Deleting directory /tmp/spark-cf519ba5-cefe-4069-9a87-7c60caed26fa/pyspark-d52cdb92-2ef7-4849-bd26-aa4c84c3f959
[2023-01-31T07:45:35.987+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:35 INFO ShutdownHookManager: Deleting directory /tmp/spark-cf519ba5-cefe-4069-9a87-7c60caed26fa
[2023-01-31T07:45:36.006+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:36 INFO ShutdownHookManager: Deleting directory /tmp/spark-7ec0f3b1-99e7-4cce-b955-ac079915a196
[2023-01-31T07:45:36.553+0000] {taskinstance.py:1322} INFO - Marking task as SUCCESS. dag_id=entsoe-energydata-backfill, task_id=stage_total_generation, execution_date=20210101T000000, start_date=20230131T073904, end_date=20230131T074536
[2023-01-31T07:45:36.675+0000] {local_task_job.py:159} INFO - Task exited with return code 0
[2023-01-31T07:45:36.762+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T08:21:35.471+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-31T08:21:35.571+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-31T08:21:35.572+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T08:21:35.572+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T08:21:35.572+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T08:21:35.682+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 00:00:00+00:00
[2023-01-31T08:21:35.717+0000] {standard_task_runner.py:55} INFO - Started process 2079 to run task
[2023-01-31T08:21:35.734+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '927', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpslrmjmn6']
[2023-01-31T08:21:35.745+0000] {standard_task_runner.py:83} INFO - Job 927: Subtask stage_total_generation
[2023-01-31T08:21:36.011+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 81d5fcd0285b
[2023-01-31T08:21:36.129+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-31T08:21:36.144+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T08:21:36.145+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-31T08:21:56.794+0000] {spark_submit.py:495} INFO - {{ start_date }}
[2023-01-31T08:21:56.795+0000] {spark_submit.py:495} INFO - mulai periode: {{ start_date }}
[2023-01-31T08:21:56.795+0000] {spark_submit.py:495} INFO - selesai periode: {{ end_date }}
[2023-01-31T08:21:57.086+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:57 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T08:21:57.478+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T08:21:58.190+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:58 INFO ResourceUtils: ==============================================================
[2023-01-31T08:21:58.195+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:58 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T08:21:58.203+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:58 INFO ResourceUtils: ==============================================================
[2023-01-31T08:21:58.209+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:58 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T08:21:58.367+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:58 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T08:21:58.407+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:58 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T08:21:58.418+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:58 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T08:21:59.129+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:59 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T08:21:59.134+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:59 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T08:21:59.185+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:59 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T08:21:59.185+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:59 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T08:21:59.210+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:59 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T08:22:00.975+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:00 INFO Utils: Successfully started service 'sparkDriver' on port 35131.
[2023-01-31T08:22:01.187+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:01 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T08:22:01.494+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:01 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T08:22:02.136+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:02 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T08:22:02.146+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:02 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T08:22:02.205+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:02 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T08:22:02.347+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:02 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-51a8b491-77a7-44aa-88bc-6ac1da371385
[2023-01-31T08:22:02.513+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:02 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T08:22:02.730+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:02 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T08:22:04.310+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:04 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T08:22:04.355+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:04 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2023-01-31T08:22:04.638+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:04 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://81d5fcd0285b:35131/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675153317061
[2023-01-31T08:22:04.639+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:04 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://81d5fcd0285b:35131/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675153317061
[2023-01-31T08:22:05.730+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:05 INFO Executor: Starting executor ID driver on host 81d5fcd0285b
[2023-01-31T08:22:05.813+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:05 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T08:22:06.165+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:06 INFO Executor: Fetching spark://81d5fcd0285b:35131/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675153317061
[2023-01-31T08:22:07.646+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:07 INFO TransportClientFactory: Successfully created connection to 81d5fcd0285b/172.19.0.5:35131 after 684 ms (0 ms spent in bootstraps)
[2023-01-31T08:22:07.757+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:07 INFO Utils: Fetching spark://81d5fcd0285b:35131/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-1030458b-30a0-4573-a8c8-7cd8ef979b67/userFiles-f332cb41-67ce-4d73-bdc4-a4cc85bf03d2/fetchFileTemp9576214548597729678.tmp
[2023-01-31T08:22:09.208+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:09 INFO Executor: Adding file:/tmp/spark-1030458b-30a0-4573-a8c8-7cd8ef979b67/userFiles-f332cb41-67ce-4d73-bdc4-a4cc85bf03d2/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T08:22:09.209+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:09 INFO Executor: Fetching spark://81d5fcd0285b:35131/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675153317061
[2023-01-31T08:22:09.209+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:09 INFO Utils: Fetching spark://81d5fcd0285b:35131/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-1030458b-30a0-4573-a8c8-7cd8ef979b67/userFiles-f332cb41-67ce-4d73-bdc4-a4cc85bf03d2/fetchFileTemp12552092378649089766.tmp
[2023-01-31T08:22:10.434+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:10 INFO Executor: Adding file:/tmp/spark-1030458b-30a0-4573-a8c8-7cd8ef979b67/userFiles-f332cb41-67ce-4d73-bdc4-a4cc85bf03d2/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T08:22:10.474+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:10 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40199.
[2023-01-31T08:22:10.475+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:10 INFO NettyBlockTransferService: Server created on 81d5fcd0285b:40199
[2023-01-31T08:22:10.479+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:10 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T08:22:10.517+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:10 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 40199, None)
[2023-01-31T08:22:10.539+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:10 INFO BlockManagerMasterEndpoint: Registering block manager 81d5fcd0285b:40199 with 434.4 MiB RAM, BlockManagerId(driver, 81d5fcd0285b, 40199, None)
[2023-01-31T08:22:10.545+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:10 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 40199, None)
[2023-01-31T08:22:10.550+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:10 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 81d5fcd0285b, 40199, None)
[2023-01-31T08:22:15.510+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:15 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T08:22:15.606+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:15 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T08:22:30.016+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:30 INFO InMemoryFileIndex: It took 632 ms to list leaf files for 1 paths.
[2023-01-31T08:22:30.946+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:30 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T08:22:31.297+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:31 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T08:22:31.306+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:31 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 81d5fcd0285b:40199 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T08:22:31.316+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:31 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T08:22:33.512+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:33 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T08:22:33.668+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:33 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T08:22:33.754+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:33 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T08:22:33.822+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:33 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T08:22:33.823+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:33 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T08:22:33.823+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:33 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:22:33.833+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:33 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:22:33.858+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:33 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T08:22:34.126+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:34 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T08:22:34.149+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:34 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T08:22:34.152+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:34 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 81d5fcd0285b:40199 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T08:22:34.158+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:34 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:22:34.254+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:22:34.264+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:34 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T08:22:34.816+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:34 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T08:22:35.009+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:35 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T08:22:36.416+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:36 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010000__202101010100.json:0+9369
[2023-01-31T08:22:38.920+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:38 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3480 bytes result sent to driver
[2023-01-31T08:22:39.163+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:39 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 4528 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:22:39.257+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:39 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T08:22:39.274+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:39 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 5.364 s
[2023-01-31T08:22:39.348+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:39 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:22:39.349+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T08:22:39.365+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:39 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 5.603154 s
[2023-01-31T08:23:03.589+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:03 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:03.626+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:03 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T08:23:03.644+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:03 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:08.206+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:08 INFO CodeGenerator: Code generated in 2467.579904 ms
[2023-01-31T08:23:08.222+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:08 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T08:23:08.269+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:08 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T08:23:08.273+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:08 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 81d5fcd0285b:40199 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T08:23:08.279+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:08 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T08:23:08.419+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:08 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:09.942+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:09 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T08:23:09.943+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:09 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T08:23:09.953+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:09 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T08:23:09.962+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:09 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:09.963+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:09 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:09.979+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:09 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T08:23:10.082+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:10 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T08:23:10.119+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:10 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T08:23:10.124+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:10 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 81d5fcd0285b:40199 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T08:23:10.183+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:10 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:10.187+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:10.187+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:10 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T08:23:10.219+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:10 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:10.224+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:10 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T08:23:11.700+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:11 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T08:23:13.220+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:13 INFO CodeGenerator: Code generated in 1185.854382 ms
[2023-01-31T08:23:14.513+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:14 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1671 bytes result sent to driver
[2023-01-31T08:23:14.582+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:14 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 4388 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:14.591+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:14 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 4.584 s
[2023-01-31T08:23:14.598+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:14 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:14.613+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:14 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T08:23:14.631+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T08:23:14.632+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:14 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 4.686319 s
[2023-01-31T08:23:15.701+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:15 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:15.876+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:15 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T08:23:15.879+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:15 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:16.876+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:16 INFO CodeGenerator: Code generated in 550.070433 ms
[2023-01-31T08:23:17.182+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:17 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T08:23:18.000+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:17 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T08:23:18.018+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:18 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 81d5fcd0285b:40199 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T08:23:18.019+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:18 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T08:23:18.019+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:18 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:18.291+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:18 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T08:23:18.365+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:18 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T08:23:18.365+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:18 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T08:23:18.365+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:18 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:18.366+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:18 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:18.373+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:18 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T08:23:18.394+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:18 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.7 MiB)
[2023-01-31T08:23:18.456+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:18 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.7 MiB)
[2023-01-31T08:23:18.462+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:18 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 81d5fcd0285b:40199 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T08:23:18.465+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:18 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:18.474+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:18.475+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:18 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T08:23:18.492+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:18 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:18.499+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:18 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T08:23:18.737+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:18 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T08:23:19.445+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:19 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1585 bytes result sent to driver
[2023-01-31T08:23:19.473+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:19 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 988 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:19.486+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:19 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 1.105 s
[2023-01-31T08:23:19.487+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:19 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:19.488+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:19 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T08:23:19.521+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T08:23:19.530+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:19 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 1.165270 s
[2023-01-31T08:23:21.117+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:21.130+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T08:23:21.131+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:24.927+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:24 INFO CodeGenerator: Code generated in 2451.764294 ms
[2023-01-31T08:23:25.006+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.5 MiB)
[2023-01-31T08:23:25.288+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T08:23:25.289+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 81d5fcd0285b:40199 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T08:23:25.335+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T08:23:25.370+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:25.916+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T08:23:25.921+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T08:23:25.921+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T08:23:25.922+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:25.922+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:25.923+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T08:23:25.932+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.4 MiB)
[2023-01-31T08:23:25.998+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.4 MiB)
[2023-01-31T08:23:26.002+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 81d5fcd0285b:40199 (size: 12.7 KiB, free: 434.2 MiB)
[2023-01-31T08:23:26.003+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:26.065+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 81d5fcd0285b:40199 in memory (size: 7.7 KiB, free: 434.2 MiB)
[2023-01-31T08:23:26.067+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:26.067+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T08:23:26.069+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:26.073+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T08:23:26.214+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T08:23:26.846+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-01-31T08:23:26.881+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 812 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:26.891+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T08:23:26.897+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 0.968 s
[2023-01-31T08:23:26.898+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:26.898+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T08:23:26.898+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 0.975813 s
[2023-01-31T08:23:27.386+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:27.387+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T08:23:27.387+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:28.123+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO CodeGenerator: Code generated in 456.923469 ms
[2023-01-31T08:23:28.285+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.2 MiB)
[2023-01-31T08:23:28.475+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T08:23:28.479+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 81d5fcd0285b:40199 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T08:23:28.488+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T08:23:28.500+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:28.804+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T08:23:28.806+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T08:23:28.807+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T08:23:28.809+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:28.810+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:28.818+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T08:23:28.837+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.1 MiB)
[2023-01-31T08:23:28.926+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.1 MiB)
[2023-01-31T08:23:28.939+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 81d5fcd0285b:40199 (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T08:23:28.959+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:28.961+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:28.973+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T08:23:28.998+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:29.006+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T08:23:29.131+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T08:23:29.497+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1629 bytes result sent to driver
[2023-01-31T08:23:29.501+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 503 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:29.502+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T08:23:29.510+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 0.677 s
[2023-01-31T08:23:29.511+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:29.512+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T08:23:29.512+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 0.707514 s
[2023-01-31T08:23:30.912+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:30 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 81d5fcd0285b:40199 in memory (size: 12.7 KiB, free: 434.2 MiB)
[2023-01-31T08:23:31.065+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 81d5fcd0285b:40199 in memory (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T08:23:31.935+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:31.936+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T08:23:31.942+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:33.084+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO CodeGenerator: Code generated in 580.002814 ms
[2023-01-31T08:23:33.121+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.0 MiB)
[2023-01-31T08:23:33.244+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.0 MiB)
[2023-01-31T08:23:33.256+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 81d5fcd0285b:40199 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T08:23:33.264+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T08:23:33.283+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:33.473+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T08:23:33.478+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) with 1 output partitions
[2023-01-31T08:23:33.479+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201)
[2023-01-31T08:23:33.479+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:33.479+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:33.492+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201), which has no missing parents
[2023-01-31T08:23:33.505+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 432.9 MiB)
[2023-01-31T08:23:33.513+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 432.9 MiB)
[2023-01-31T08:23:33.529+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 81d5fcd0285b:40199 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T08:23:33.538+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:33.543+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:33.558+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T08:23:33.562+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:33.562+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T08:23:33.667+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T08:23:34.147+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2558 bytes result sent to driver
[2023-01-31T08:23:34.150+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 589 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:34.150+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T08:23:34.154+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) finished in 0.658 s
[2023-01-31T08:23:34.155+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:34.155+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T08:23:34.155+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201, took 0.678799 s
[2023-01-31T08:23:34.719+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:34.720+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T08:23:34.720+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:35.489+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:35 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 81d5fcd0285b:40199 in memory (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T08:23:35.837+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:35 INFO CodeGenerator: Code generated in 935.106037 ms
[2023-01-31T08:23:35.893+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:35 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 432.8 MiB)
[2023-01-31T08:23:36.170+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.8 MiB)
[2023-01-31T08:23:36.171+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 81d5fcd0285b:40199 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T08:23:36.212+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T08:23:36.257+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:36.622+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T08:23:36.630+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T08:23:36.631+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T08:23:36.633+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:36.634+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:36.648+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T08:23:36.662+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 432.7 MiB)
[2023-01-31T08:23:36.684+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.7 MiB)
[2023-01-31T08:23:36.696+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 81d5fcd0285b:40199 (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T08:23:36.712+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:36.714+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:36.715+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T08:23:36.718+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:36.719+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T08:23:36.793+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T08:23:37.065+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:37 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-01-31T08:23:37.075+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:37 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 353 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:37.082+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:37 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T08:23:37.083+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:37 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 0.422 s
[2023-01-31T08:23:37.084+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:37 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:37.085+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T08:23:37.085+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:37 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 0.451762 s
[2023-01-31T08:23:37.532+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:37 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:37.567+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:37 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T08:23:37.592+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:37 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:39.170+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO CodeGenerator: Code generated in 1156.353508 ms
[2023-01-31T08:23:39.222+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.5 MiB)
[2023-01-31T08:23:39.254+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.5 MiB)
[2023-01-31T08:23:39.257+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 81d5fcd0285b:40199 (size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T08:23:39.266+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T08:23:39.267+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:39.340+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T08:23:39.347+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T08:23:39.347+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T08:23:39.347+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:39.348+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:39.354+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T08:23:39.377+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.4 MiB)
[2023-01-31T08:23:39.382+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.4 MiB)
[2023-01-31T08:23:39.442+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 81d5fcd0285b:40199 (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T08:23:39.446+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:39.455+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:39.462+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T08:23:39.464+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:39.464+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T08:23:39.648+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T08:23:40.055+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:40 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1729 bytes result sent to driver
[2023-01-31T08:23:40.064+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:40 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 600 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:40.064+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:40 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T08:23:40.064+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:40 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 0.707 s
[2023-01-31T08:23:40.064+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:40 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:40.064+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T08:23:40.064+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:40 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 0.720959 s
[2023-01-31T08:23:43.982+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:43 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 81d5fcd0285b:40199 in memory (size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T08:23:44.141+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:44 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 81d5fcd0285b:40199 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T08:23:44.376+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:44 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 81d5fcd0285b:40199 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T08:23:44.557+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:44 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 81d5fcd0285b:40199 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T08:23:44.769+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:44 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 81d5fcd0285b:40199 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T08:23:44.836+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:44 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 81d5fcd0285b:40199 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T08:23:47.932+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:47 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 81d5fcd0285b:40199 in memory (size: 34.5 KiB, free: 434.3 MiB)
