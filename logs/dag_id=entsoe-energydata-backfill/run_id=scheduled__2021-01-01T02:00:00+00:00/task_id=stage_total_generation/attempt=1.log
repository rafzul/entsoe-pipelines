[2023-01-31T05:10:59.754+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T02:00:00+00:00 [queued]>
[2023-01-31T05:10:59.786+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T02:00:00+00:00 [queued]>
[2023-01-31T05:10:59.787+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T05:10:59.788+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T05:10:59.788+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T05:10:59.834+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 02:00:00+00:00
[2023-01-31T05:10:59.863+0000] {standard_task_runner.py:55} INFO - Started process 12257 to run task
[2023-01-31T05:10:59.915+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T02:00:00+00:00', '--job-id', '880', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpntn1nq_l']
[2023-01-31T05:10:59.928+0000] {standard_task_runner.py:83} INFO - Job 880: Subtask stage_total_generation
[2023-01-31T05:11:00.375+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T02:00:00+00:00 [running]> on host 81d5fcd0285b
[2023-01-31T05:11:00.893+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T02:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T02:00:00+00:00
[2023-01-31T05:11:00.945+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T05:11:00.952+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010200 202101010300 DE_TENNET
[2023-01-31T05:11:36.203+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:36 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T05:11:36.760+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T05:11:38.739+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:38 INFO ResourceUtils: ==============================================================
[2023-01-31T05:11:38.744+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:38 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T05:11:38.757+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:38 INFO ResourceUtils: ==============================================================
[2023-01-31T05:11:38.760+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:38 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T05:11:38.934+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:38 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T05:11:38.960+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:38 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T05:11:38.964+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:38 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T05:11:39.484+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:39 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T05:11:39.490+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:39 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T05:11:39.493+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:39 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T05:11:39.495+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:39 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T05:11:39.502+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:39 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T05:11:42.480+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:42 INFO Utils: Successfully started service 'sparkDriver' on port 41667.
[2023-01-31T05:11:42.844+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:42 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T05:11:44.049+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:44 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T05:11:44.218+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:44 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T05:11:44.228+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:44 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T05:11:44.292+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:44 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T05:11:44.643+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:44 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-3009e718-309b-4b4e-9387-905f4292759f
[2023-01-31T05:11:44.760+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:44 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T05:11:45.159+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:45 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T05:11:46.748+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:46 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T05:11:46.754+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:46 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2023-01-31T05:11:46.777+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:46 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[2023-01-31T05:11:46.782+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:46 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[2023-01-31T05:11:46.961+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:46 INFO Utils: Successfully started service 'SparkUI' on port 4044.
[2023-01-31T05:11:47.432+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:47 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://81d5fcd0285b:41667/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675141896151
[2023-01-31T05:11:47.442+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:47 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://81d5fcd0285b:41667/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675141896151
[2023-01-31T05:11:48.358+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:48 INFO Executor: Starting executor ID driver on host 81d5fcd0285b
[2023-01-31T05:11:48.526+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:48 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T05:11:48.700+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:48 INFO Executor: Fetching spark://81d5fcd0285b:41667/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675141896151
[2023-01-31T05:11:49.415+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:49 INFO TransportClientFactory: Successfully created connection to 81d5fcd0285b/172.19.0.9:41667 after 522 ms (0 ms spent in bootstraps)
[2023-01-31T05:11:49.555+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:49 INFO Utils: Fetching spark://81d5fcd0285b:41667/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-fe60af0e-31ad-4df9-872e-21b4625d57ad/userFiles-99a915c3-08f7-439a-9542-796fb0a7771a/fetchFileTemp10000598706084160378.tmp
[2023-01-31T05:11:51.612+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:51 INFO Executor: Adding file:/tmp/spark-fe60af0e-31ad-4df9-872e-21b4625d57ad/userFiles-99a915c3-08f7-439a-9542-796fb0a7771a/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T05:11:51.614+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:51 INFO Executor: Fetching spark://81d5fcd0285b:41667/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675141896151
[2023-01-31T05:11:51.640+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:51 INFO Utils: Fetching spark://81d5fcd0285b:41667/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-fe60af0e-31ad-4df9-872e-21b4625d57ad/userFiles-99a915c3-08f7-439a-9542-796fb0a7771a/fetchFileTemp13606814349045300554.tmp
[2023-01-31T05:11:53.216+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:53 INFO Executor: Adding file:/tmp/spark-fe60af0e-31ad-4df9-872e-21b4625d57ad/userFiles-99a915c3-08f7-439a-9542-796fb0a7771a/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T05:11:53.289+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:53 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35833.
[2023-01-31T05:11:53.289+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:53 INFO NettyBlockTransferService: Server created on 81d5fcd0285b:35833
[2023-01-31T05:11:53.297+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:53 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T05:11:53.381+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:53 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 35833, None)
[2023-01-31T05:11:53.404+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:53 INFO BlockManagerMasterEndpoint: Registering block manager 81d5fcd0285b:35833 with 434.4 MiB RAM, BlockManagerId(driver, 81d5fcd0285b, 35833, None)
[2023-01-31T05:11:53.438+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:53 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 35833, None)
[2023-01-31T05:11:53.444+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:53 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 81d5fcd0285b, 35833, None)
[2023-01-31T05:12:01.224+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:01 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T05:12:01.322+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:01 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T05:12:23.782+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:23 INFO InMemoryFileIndex: It took 679 ms to list leaf files for 1 paths.
[2023-01-31T05:12:24.514+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:24 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T05:12:25.573+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:25 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T05:12:25.649+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:25 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 81d5fcd0285b:35833 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T05:12:25.693+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:25 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T05:12:32.885+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:32 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T05:12:33.497+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:33 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T05:12:33.832+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:33 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T05:12:33.891+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:33 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T05:12:33.904+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:33 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T05:12:33.907+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:33 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:12:33.923+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:33 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:12:33.973+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:33 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T05:12:34.748+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:34 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T05:12:34.782+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:34 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T05:12:34.787+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:34 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 81d5fcd0285b:35833 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T05:12:34.804+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:34 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:12:35.067+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:12:35.076+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:35 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T05:12:36.013+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:36 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T05:12:36.249+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:36 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T05:12:38.199+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:38 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010200__202101010300.json:0+9369
[2023-01-31T05:12:41.513+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:41 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3437 bytes result sent to driver
[2023-01-31T05:12:41.597+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:41 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 5927 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:12:41.623+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:41 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T05:12:41.775+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:41 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 7.500 s
[2023-01-31T05:12:41.815+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:41 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:12:41.818+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T05:12:41.836+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:41 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 8.002652 s
[2023-01-31T05:12:46.664+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:46 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 81d5fcd0285b:35833 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T05:12:46.770+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:46 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 81d5fcd0285b:35833 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T05:13:10.101+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:10 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:10.113+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:10 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:13:10.131+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:10 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:13.589+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:13 INFO CodeGenerator: Code generated in 1572.350777 ms
[2023-01-31T05:13:13.720+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:13 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T05:13:13.888+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:13 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T05:13:13.892+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:13 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 81d5fcd0285b:35833 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T05:13:13.906+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:13 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T05:13:14.124+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:14.941+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T05:13:14.944+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T05:13:14.945+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T05:13:14.945+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:14.945+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:14.948+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T05:13:15.032+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T05:13:15.051+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T05:13:15.053+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 81d5fcd0285b:35833 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T05:13:15.058+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:15.060+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:15.069+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T05:13:15.079+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:15.092+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T05:13:15.643+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010200__202101010300.json, range: 0-9369, partition values: [empty row]
[2023-01-31T05:13:16.822+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:16 INFO CodeGenerator: Code generated in 1008.681768 ms
[2023-01-31T05:13:17.724+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1628 bytes result sent to driver
[2023-01-31T05:13:17.769+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2691 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:17.770+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T05:13:17.780+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 2.794 s
[2023-01-31T05:13:17.782+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:17.782+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T05:13:17.791+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 2.849441 s
[2023-01-31T05:13:18.545+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:18.545+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:13:18.546+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:19.349+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO CodeGenerator: Code generated in 588.291722 ms
[2023-01-31T05:13:19.469+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T05:13:19.874+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T05:13:19.881+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 81d5fcd0285b:35833 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:13:19.885+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T05:13:19.914+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:20.138+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T05:13:20.142+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T05:13:20.142+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T05:13:20.142+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:20.143+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:20.152+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T05:13:20.183+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T05:13:20.204+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T05:13:20.209+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 81d5fcd0285b:35833 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T05:13:20.215+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:20.228+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:20.228+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T05:13:20.237+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:20.238+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T05:13:20.360+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010200__202101010300.json, range: 0-9369, partition values: [empty row]
[2023-01-31T05:13:20.752+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1585 bytes result sent to driver
[2023-01-31T05:13:20.772+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 538 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:20.784+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T05:13:20.788+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 0.637 s
[2023-01-31T05:13:20.789+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:20.789+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T05:13:20.793+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 0.651272 s
[2023-01-31T05:13:21.952+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:21.984+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T05:13:21.996+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:23.817+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO CodeGenerator: Code generated in 942.650675 ms
[2023-01-31T05:13:23.839+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T05:13:23.984+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T05:13:23.987+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 81d5fcd0285b:35833 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:13:24.007+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T05:13:24.035+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:24.318+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T05:13:24.322+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T05:13:24.323+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T05:13:24.323+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:24.324+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:24.334+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T05:13:24.357+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.6 MiB)
[2023-01-31T05:13:24.402+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.6 MiB)
[2023-01-31T05:13:24.422+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 81d5fcd0285b:35833 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T05:13:24.425+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:24.434+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:24.457+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T05:13:24.463+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:24.469+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T05:13:24.639+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010200__202101010300.json, range: 0-9369, partition values: [empty row]
[2023-01-31T05:13:25.390+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-01-31T05:13:25.398+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 936 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:25.406+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 1.066 s
[2023-01-31T05:13:25.407+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T05:13:25.407+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:25.413+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T05:13:25.413+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 1.092797 s
[2023-01-31T05:13:25.771+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:25.771+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:13:25.771+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:25.936+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO CodeGenerator: Code generated in 104.684548 ms
[2023-01-31T05:13:25.977+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.4 MiB)
[2023-01-31T05:13:26.180+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T05:13:26.189+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 81d5fcd0285b:35833 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:13:26.195+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T05:13:26.206+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:26.375+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T05:13:26.384+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T05:13:26.394+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T05:13:26.394+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:26.395+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:26.395+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T05:13:26.480+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-31T05:13:26.569+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-01-31T05:13:26.578+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 81d5fcd0285b:35833 (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T05:13:26.585+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:26.587+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:26.587+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T05:13:26.591+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:26.595+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T05:13:26.719+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010200__202101010300.json, range: 0-9369, partition values: [empty row]
[2023-01-31T05:13:27.304+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:27 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1586 bytes result sent to driver
[2023-01-31T05:13:27.310+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:27 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 719 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:27.322+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:27 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 0.889 s
[2023-01-31T05:13:27.323+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:27 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:27.325+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:27 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T05:13:27.331+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T05:13:27.356+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:27 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 0.955003 s
[2023-01-31T05:13:29.857+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:29.858+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T05:13:29.861+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:32.056+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:32 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 81d5fcd0285b:35833 in memory (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T05:13:32.365+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:32 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 81d5fcd0285b:35833 in memory (size: 7.7 KiB, free: 434.2 MiB)
[2023-01-31T05:13:32.390+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:32 INFO CodeGenerator: Code generated in 2167.495295 ms
[2023-01-31T05:13:32.493+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:32 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 81d5fcd0285b:35833 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T05:13:32.499+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:32 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.2 MiB)
[2023-01-31T05:13:32.669+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:32 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 81d5fcd0285b:35833 in memory (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T05:13:32.809+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:32 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T05:13:32.901+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:32 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 81d5fcd0285b:35833 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:13:32.906+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:32 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T05:13:33.096+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:33.275+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T05:13:33.278+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) with 1 output partitions
[2023-01-31T05:13:33.279+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201)
[2023-01-31T05:13:33.279+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:33.279+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:33.292+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201), which has no missing parents
[2023-01-31T05:13:33.329+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.2 MiB)
[2023-01-31T05:13:33.351+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.2 MiB)
[2023-01-31T05:13:33.358+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 81d5fcd0285b:35833 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T05:13:33.364+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:33.371+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:33.374+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T05:13:33.386+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:33.387+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T05:13:33.464+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010200__202101010300.json, range: 0-9369, partition values: [empty row]
[2023-01-31T05:13:33.935+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2524 bytes result sent to driver
[2023-01-31T05:13:33.946+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 562 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:33.947+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T05:13:33.955+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) finished in 0.657 s
[2023-01-31T05:13:33.956+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:33.956+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T05:13:33.956+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201, took 0.675249 s
[2023-01-31T05:13:34.759+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:34 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:34.769+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:34 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T05:13:34.772+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:34 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:35.473+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:35 INFO CodeGenerator: Code generated in 611.688648 ms
[2023-01-31T05:13:35.653+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:35 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 433.0 MiB)
[2023-01-31T05:13:35.688+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:35 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.0 MiB)
[2023-01-31T05:13:35.690+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:35 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 81d5fcd0285b:35833 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:13:35.691+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:35 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T05:13:35.698+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:35 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:35.732+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:35 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T05:13:35.733+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:35 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T05:13:35.734+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:35 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T05:13:35.734+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:35 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:35.734+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:35 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:35.736+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:35 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T05:13:35.744+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:35 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 432.9 MiB)
[2023-01-31T05:13:35.746+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:35 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.9 MiB)
[2023-01-31T05:13:35.748+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:35 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 81d5fcd0285b:35833 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T05:13:35.750+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:35 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:35.753+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:35.754+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:35 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T05:13:35.758+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:35 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:35.759+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:35 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T05:13:35.777+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:35 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010200__202101010300.json, range: 0-9369, partition values: [empty row]
[2023-01-31T05:13:36.072+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-01-31T05:13:36.075+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 318 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:36.075+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T05:13:36.081+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 0.340 s
[2023-01-31T05:13:36.081+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:36.083+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T05:13:36.084+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 0.346559 s
[2023-01-31T05:13:36.600+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:36.611+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T05:13:36.620+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:37.412+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:37 INFO CodeGenerator: Code generated in 610.324585 ms
[2023-01-31T05:13:37.463+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:37 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.7 MiB)
[2023-01-31T05:13:37.682+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:37 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.7 MiB)
[2023-01-31T05:13:37.684+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:37 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 81d5fcd0285b:35833 (size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T05:13:37.698+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:37 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T05:13:37.720+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:37 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:38.090+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T05:13:38.097+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T05:13:38.097+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T05:13:38.098+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:38.124+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:38.133+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T05:13:38.202+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.7 MiB)
[2023-01-31T05:13:38.232+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.6 MiB)
[2023-01-31T05:13:38.236+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 81d5fcd0285b:35833 (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T05:13:38.248+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:38.260+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:38.265+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T05:13:38.275+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:38.286+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T05:13:38.369+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010200__202101010300.json, range: 0-9369, partition values: [empty row]
[2023-01-31T05:13:38.789+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1729 bytes result sent to driver
[2023-01-31T05:13:38.797+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 520 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:38.798+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 0.631 s
[2023-01-31T05:13:38.799+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:38.801+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T05:13:38.804+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T05:13:38.816+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 0.715629 s
[2023-01-31T05:13:46.091+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:46 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 81d5fcd0285b:35833 in memory (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T05:13:46.151+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:46 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 81d5fcd0285b:35833 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:13:46.193+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:46 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 81d5fcd0285b:35833 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:13:46.235+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:46 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 81d5fcd0285b:35833 in memory (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T05:13:46.306+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:46 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 81d5fcd0285b:35833 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:13:46.395+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:46 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 81d5fcd0285b:35833 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:13:46.464+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:46 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 81d5fcd0285b:35833 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:13:46.531+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:46 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 81d5fcd0285b:35833 in memory (size: 12.6 KiB, free: 434.3 MiB)
[2023-01-31T05:13:46.575+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:46 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 81d5fcd0285b:35833 in memory (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T05:14:04.372+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:04 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:14:04.461+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:14:04.464+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:14:04.465+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:04 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:14:04.465+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:14:04.466+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:14:04.478+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:14:05.794+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-31T05:14:05.797+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO DAGScheduler: Got job 8 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T05:14:05.797+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO DAGScheduler: Final stage: ResultStage 8 (save at BigQueryWriteHelper.java:105)
[2023-01-31T05:14:05.798+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:14:05.798+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:14:05.803+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO DAGScheduler: Submitting ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T05:14:06.084+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 209.9 KiB, free 434.0 MiB)
[2023-01-31T05:14:06.119+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 74.3 KiB, free 433.9 MiB)
[2023-01-31T05:14:06.125+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 81d5fcd0285b:35833 (size: 74.3 KiB, free: 434.3 MiB)
[2023-01-31T05:14:06.132+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:14:06.140+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:14:06.143+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2023-01-31T05:14:06.167+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7484 bytes) taskResourceAssignments Map()
[2023-01-31T05:14:06.170+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2023-01-31T05:14:06.550+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:14:06.561+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:14:06.563+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:14:06.565+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:14:06.569+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:14:06.571+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:14:06.599+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T05:14:06.621+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T05:14:06.770+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO ParquetOutputFormat: Parquet block size to 134217728
[2023-01-31T05:14:06.770+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO ParquetOutputFormat: Validation is off
[2023-01-31T05:14:06.773+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-01-31T05:14:06.773+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO ParquetOutputFormat: Parquet properties are:
[2023-01-31T05:14:06.774+0000] {spark_submit.py:495} INFO - Parquet page size to 1048576
[2023-01-31T05:14:06.774+0000] {spark_submit.py:495} INFO - Parquet dictionary page size to 1048576
[2023-01-31T05:14:06.775+0000] {spark_submit.py:495} INFO - Dictionary is true
[2023-01-31T05:14:06.777+0000] {spark_submit.py:495} INFO - Writer version is: PARQUET_1_0
[2023-01-31T05:14:06.778+0000] {spark_submit.py:495} INFO - Page size checking is: estimated
[2023-01-31T05:14:06.778+0000] {spark_submit.py:495} INFO - Min row count for page size check is: 100
[2023-01-31T05:14:06.780+0000] {spark_submit.py:495} INFO - Max row count for page size check is: 10000
[2023-01-31T05:14:06.781+0000] {spark_submit.py:495} INFO - Truncate length for column indexes is: 64
[2023-01-31T05:14:06.782+0000] {spark_submit.py:495} INFO - Truncate length for statistics min/max  is: 2147483647
[2023-01-31T05:14:06.783+0000] {spark_submit.py:495} INFO - Bloom filter enabled: false
[2023-01-31T05:14:06.783+0000] {spark_submit.py:495} INFO - Max Bloom filter size for a column is 1048576
[2023-01-31T05:14:06.785+0000] {spark_submit.py:495} INFO - Bloom filter expected number of distinct values are: null
[2023-01-31T05:14:06.786+0000] {spark_submit.py:495} INFO - Page row count limit to 20000
[2023-01-31T05:14:06.787+0000] {spark_submit.py:495} INFO - Writing page checksums is: on
[2023-01-31T05:14:07.257+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:07 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-01-31T05:14:07.259+0000] {spark_submit.py:495} INFO - {
[2023-01-31T05:14:07.260+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-31T05:14:07.261+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-31T05:14:07.261+0000] {spark_submit.py:495} INFO - "name" : "measured_at",
[2023-01-31T05:14:07.262+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T05:14:07.265+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T05:14:07.268+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:07.276+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:07.282+0000] {spark_submit.py:495} INFO - "name" : "created_at",
[2023-01-31T05:14:07.289+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T05:14:07.292+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T05:14:07.296+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:07.348+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:07.349+0000] {spark_submit.py:495} INFO - "name" : "Biomass - Actual Aggregated",
[2023-01-31T05:14:07.388+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:07.409+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:07.410+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:07.410+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:07.411+0000] {spark_submit.py:495} INFO - "name" : "Fossil Brown coal/Lignite - Actual Aggregated",
[2023-01-31T05:14:07.411+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:07.419+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:07.469+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:07.470+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:07.470+0000] {spark_submit.py:495} INFO - "name" : "Fossil Gas - Actual Aggregated",
[2023-01-31T05:14:07.471+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:07.471+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:07.490+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:07.514+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:07.515+0000] {spark_submit.py:495} INFO - "name" : "Fossil Hard coal - Actual Aggregated",
[2023-01-31T05:14:07.516+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:07.532+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:07.539+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:07.556+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:07.556+0000] {spark_submit.py:495} INFO - "name" : "Fossil Oil - Actual Aggregated",
[2023-01-31T05:14:07.578+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:07.595+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:07.632+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:07.636+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:07.662+0000] {spark_submit.py:495} INFO - "name" : "Geothermal - Actual Aggregated",
[2023-01-31T05:14:07.702+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:07.703+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:07.716+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:07.717+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:07.717+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Aggregated",
[2023-01-31T05:14:07.717+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:07.718+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:07.719+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:07.719+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:07.720+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Consumption",
[2023-01-31T05:14:07.720+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:07.720+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:07.721+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:07.721+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:07.722+0000] {spark_submit.py:495} INFO - "name" : "Hydro Run-of-river and poundage - Actual Aggregated",
[2023-01-31T05:14:07.722+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:07.723+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:07.723+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:07.724+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:07.725+0000] {spark_submit.py:495} INFO - "name" : "Hydro Water Reservoir - Actual Aggregated",
[2023-01-31T05:14:07.725+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:07.726+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:07.726+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:07.727+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:07.728+0000] {spark_submit.py:495} INFO - "name" : "Nuclear - Actual Aggregated",
[2023-01-31T05:14:07.728+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:07.729+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:07.729+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:07.730+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:07.730+0000] {spark_submit.py:495} INFO - "name" : "Other - Actual Aggregated",
[2023-01-31T05:14:07.731+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:07.732+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:07.732+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:07.733+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:07.733+0000] {spark_submit.py:495} INFO - "name" : "Other renewable - Actual Aggregated",
[2023-01-31T05:14:07.734+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:07.734+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:07.735+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:07.736+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:07.736+0000] {spark_submit.py:495} INFO - "name" : "Solar - Actual Aggregated",
[2023-01-31T05:14:07.737+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:07.738+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:07.738+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:07.739+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:07.739+0000] {spark_submit.py:495} INFO - "name" : "Waste - Actual Aggregated",
[2023-01-31T05:14:07.739+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:07.740+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:07.740+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:07.741+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:07.741+0000] {spark_submit.py:495} INFO - "name" : "Wind Offshore - Actual Aggregated",
[2023-01-31T05:14:07.742+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:07.742+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:07.743+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:07.743+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:07.744+0000] {spark_submit.py:495} INFO - "name" : "Wind Onshore - Actual Aggregated",
[2023-01-31T05:14:07.744+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:07.745+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:07.747+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:07.747+0000] {spark_submit.py:495} INFO - } ]
[2023-01-31T05:14:07.749+0000] {spark_submit.py:495} INFO - }
[2023-01-31T05:14:07.750+0000] {spark_submit.py:495} INFO - and corresponding Parquet message type:
[2023-01-31T05:14:07.751+0000] {spark_submit.py:495} INFO - message spark_schema {
[2023-01-31T05:14:07.751+0000] {spark_submit.py:495} INFO - required int96 measured_at;
[2023-01-31T05:14:07.752+0000] {spark_submit.py:495} INFO - required int96 created_at;
[2023-01-31T05:14:07.754+0000] {spark_submit.py:495} INFO - optional double Biomass - Actual Aggregated;
[2023-01-31T05:14:07.755+0000] {spark_submit.py:495} INFO - optional double Fossil Brown coal/Lignite - Actual Aggregated;
[2023-01-31T05:14:07.756+0000] {spark_submit.py:495} INFO - optional double Fossil Gas - Actual Aggregated;
[2023-01-31T05:14:07.756+0000] {spark_submit.py:495} INFO - optional double Fossil Hard coal - Actual Aggregated;
[2023-01-31T05:14:07.757+0000] {spark_submit.py:495} INFO - optional double Fossil Oil - Actual Aggregated;
[2023-01-31T05:14:07.758+0000] {spark_submit.py:495} INFO - optional double Geothermal - Actual Aggregated;
[2023-01-31T05:14:07.758+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Aggregated;
[2023-01-31T05:14:07.759+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Consumption;
[2023-01-31T05:14:07.762+0000] {spark_submit.py:495} INFO - optional double Hydro Run-of-river and poundage - Actual Aggregated;
[2023-01-31T05:14:07.763+0000] {spark_submit.py:495} INFO - optional double Hydro Water Reservoir - Actual Aggregated;
[2023-01-31T05:14:07.763+0000] {spark_submit.py:495} INFO - optional double Nuclear - Actual Aggregated;
[2023-01-31T05:14:07.763+0000] {spark_submit.py:495} INFO - optional double Other - Actual Aggregated;
[2023-01-31T05:14:07.764+0000] {spark_submit.py:495} INFO - optional double Other renewable - Actual Aggregated;
[2023-01-31T05:14:07.764+0000] {spark_submit.py:495} INFO - optional double Solar - Actual Aggregated;
[2023-01-31T05:14:07.765+0000] {spark_submit.py:495} INFO - optional double Waste - Actual Aggregated;
[2023-01-31T05:14:07.765+0000] {spark_submit.py:495} INFO - optional double Wind Offshore - Actual Aggregated;
[2023-01-31T05:14:07.766+0000] {spark_submit.py:495} INFO - optional double Wind Onshore - Actual Aggregated;
[2023-01-31T05:14:07.766+0000] {spark_submit.py:495} INFO - }
[2023-01-31T05:14:07.767+0000] {spark_submit.py:495} INFO - 
[2023-01-31T05:14:07.767+0000] {spark_submit.py:495} INFO - 
[2023-01-31T05:14:08.941+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:08 INFO CodecPool: Got brand-new compressor [.snappy]
[2023-01-31T05:14:14.433+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:14 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675141907808-67b274e6-a510-4cfc-ad8b-89cd4ba519de/_temporary/0/_temporary/' directory.
[2023-01-31T05:14:14.434+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:14 INFO FileOutputCommitter: Saved output of task 'attempt_202301310514055491672308642963228_0008_m_000000_8' to gs://entsoe_temp_1009/.spark-bigquery-local-1675141907808-67b274e6-a510-4cfc-ad8b-89cd4ba519de/_temporary/0/task_202301310514055491672308642963228_0008_m_000000
[2023-01-31T05:14:14.439+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:14 INFO SparkHadoopMapRedUtil: attempt_202301310514055491672308642963228_0008_m_000000_8: Committed. Elapsed time: 2023 ms.
[2023-01-31T05:14:14.654+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:14 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2588 bytes result sent to driver
[2023-01-31T05:14:14.666+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:14 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 8516 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:14:14.670+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:14 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2023-01-31T05:14:14.711+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:14 INFO DAGScheduler: ResultStage 8 (save at BigQueryWriteHelper.java:105) finished in 8.865 s
[2023-01-31T05:14:14.737+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:14 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:14:14.737+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2023-01-31T05:14:14.738+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:14 INFO DAGScheduler: Job 8 finished: save at BigQueryWriteHelper.java:105, took 8.936858 s
[2023-01-31T05:14:14.763+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:14 INFO FileFormatWriter: Start to commit write Job 10659f3e-f7be-44cc-b5b1-b046b1dbe3ff.
[2023-01-31T05:14:16.083+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:16 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675141907808-67b274e6-a510-4cfc-ad8b-89cd4ba519de/_temporary/0/task_202301310514055491672308642963228_0008_m_000000/' directory.
[2023-01-31T05:14:16.480+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:16 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675141907808-67b274e6-a510-4cfc-ad8b-89cd4ba519de/' directory.
[2023-01-31T05:14:16.966+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:16 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 81d5fcd0285b:35833 in memory (size: 74.3 KiB, free: 434.4 MiB)
[2023-01-31T05:14:17.378+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:17 INFO FileFormatWriter: Write Job 10659f3e-f7be-44cc-b5b1-b046b1dbe3ff committed. Elapsed time: 2599 ms.
[2023-01-31T05:14:17.391+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:17 INFO FileFormatWriter: Finished processing stats for write job 10659f3e-f7be-44cc-b5b1-b046b1dbe3ff.
[2023-01-31T05:14:18.726+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:18 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=entsoe_analytics_1009, projectId=rafzul-analytics-1009, tableId=TEST_total_generation_staging}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=null, ignoreUnknownValue=null, sourceUris=[gs://entsoe_temp_1009/.spark-bigquery-local-1675141907808-67b274e6-a510-4cfc-ad8b-89cd4ba519de/part-00000-648e981e-c705-4a3c-a645-21c7279e2c37-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=true, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=rafzul-analytics-1009, job=bdb33b61-3347-4646-bdc6-20a7008dae60, location=US}
[2023-01-31T05:14:24.070+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:24 INFO BigQueryClient: Done loading to rafzul-analytics-1009.entsoe_analytics_1009.TEST_total_generation_staging. jobId: JobId{project=rafzul-analytics-1009, job=bdb33b61-3347-4646-bdc6-20a7008dae60, location=US}
[2023-01-31T05:14:24.724+0000] {spark_submit.py:495} INFO - MANTEP!!!!
[2023-01-31T05:14:25.054+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:25 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-31T05:14:25.083+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:25 INFO SparkUI: Stopped Spark web UI at http://81d5fcd0285b:4044
[2023-01-31T05:14:25.119+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:25 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-31T05:14:25.159+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:25 INFO MemoryStore: MemoryStore cleared
[2023-01-31T05:14:25.162+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:25 INFO BlockManager: BlockManager stopped
[2023-01-31T05:14:25.178+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:25 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-31T05:14:25.188+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:25 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-31T05:14:25.243+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:25 INFO SparkContext: Successfully stopped SparkContext
[2023-01-31T05:14:25.243+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:25 INFO ShutdownHookManager: Shutdown hook called
[2023-01-31T05:14:25.244+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-fe60af0e-31ad-4df9-872e-21b4625d57ad/pyspark-e35d5e5a-d640-46e2-9b72-188a950f785b
[2023-01-31T05:14:25.263+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-fe60af0e-31ad-4df9-872e-21b4625d57ad
[2023-01-31T05:14:25.278+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-378420e1-c7aa-4b3b-9fad-0a1b1deffd69
[2023-01-31T05:14:25.662+0000] {taskinstance.py:1322} INFO - Marking task as SUCCESS. dag_id=entsoe-energydata-backfill, task_id=stage_total_generation, execution_date=20210101T020000, start_date=20230131T051059, end_date=20230131T051425
[2023-01-31T05:14:25.734+0000] {local_task_job.py:159} INFO - Task exited with return code 0
[2023-01-31T05:14:25.786+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T05:30:46.003+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T02:00:00+00:00 [queued]>
[2023-01-31T05:30:46.112+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T02:00:00+00:00 [queued]>
[2023-01-31T05:30:46.113+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T05:30:46.114+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T05:30:46.116+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T05:30:46.336+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 02:00:00+00:00
[2023-01-31T05:30:46.456+0000] {standard_task_runner.py:55} INFO - Started process 15812 to run task
[2023-01-31T05:30:46.514+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T02:00:00+00:00', '--job-id', '897', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpm_9aqrrf']
[2023-01-31T05:30:46.554+0000] {standard_task_runner.py:83} INFO - Job 897: Subtask stage_total_generation
[2023-01-31T05:30:48.286+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T02:00:00+00:00 [running]> on host 81d5fcd0285b
[2023-01-31T05:30:49.348+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T02:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T02:00:00+00:00
[2023-01-31T05:30:49.548+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T05:30:49.579+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010200 202101010300 DE_TENNET
[2023-01-31T05:31:58.121+0000] {spark_submit.py:495} INFO - 23/01/31 05:31:58 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T05:31:59.649+0000] {spark_submit.py:495} INFO - 23/01/31 05:31:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T05:32:02.940+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:02 INFO ResourceUtils: ==============================================================
[2023-01-31T05:32:02.944+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:02 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T05:32:02.967+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:02 INFO ResourceUtils: ==============================================================
[2023-01-31T05:32:03.081+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:03 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T05:32:03.540+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:03 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T05:32:03.681+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:03 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T05:32:03.710+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:03 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T05:32:05.521+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:05 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T05:32:05.531+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:05 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T05:32:05.545+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:05 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T05:32:05.546+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:05 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T05:32:05.547+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:05 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T05:32:11.447+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:11 INFO Utils: Successfully started service 'sparkDriver' on port 34829.
[2023-01-31T05:32:12.522+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:12 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T05:32:12.996+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:12 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T05:32:15.048+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:15 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T05:32:15.056+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:15 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T05:32:15.206+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:15 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T05:32:15.418+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:15 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b370c82f-97e8-4015-9f76-522ef8bf4599
[2023-01-31T05:32:15.529+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:15 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T05:32:15.738+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:15 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T05:32:21.804+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:21 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T05:32:21.824+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:21 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2023-01-31T05:32:21.841+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:21 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[2023-01-31T05:32:22.058+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:22 INFO Utils: Successfully started service 'SparkUI' on port 4043.
[2023-01-31T05:32:23.277+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:23 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://81d5fcd0285b:34829/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675143117923
[2023-01-31T05:32:23.280+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:23 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://81d5fcd0285b:34829/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675143117923
[2023-01-31T05:32:25.224+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:25 INFO Executor: Starting executor ID driver on host 81d5fcd0285b
[2023-01-31T05:32:25.531+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:25 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T05:32:25.737+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:25 INFO Executor: Fetching spark://81d5fcd0285b:34829/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675143117923
[2023-01-31T05:32:26.990+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:26 INFO TransportClientFactory: Successfully created connection to 81d5fcd0285b/172.19.0.9:34829 after 788 ms (0 ms spent in bootstraps)
[2023-01-31T05:32:27.130+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:27 INFO Utils: Fetching spark://81d5fcd0285b:34829/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-b63de28c-789a-4d7b-b0df-8d0eb0c9a92e/userFiles-c9b14e3e-a4e8-4a70-b06e-53f84af6af10/fetchFileTemp1057106037270930466.tmp
[2023-01-31T05:32:34.007+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:34 INFO Executor: Adding file:/tmp/spark-b63de28c-789a-4d7b-b0df-8d0eb0c9a92e/userFiles-c9b14e3e-a4e8-4a70-b06e-53f84af6af10/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T05:32:34.049+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:34 INFO Executor: Fetching spark://81d5fcd0285b:34829/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675143117923
[2023-01-31T05:32:34.132+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:34 INFO Utils: Fetching spark://81d5fcd0285b:34829/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-b63de28c-789a-4d7b-b0df-8d0eb0c9a92e/userFiles-c9b14e3e-a4e8-4a70-b06e-53f84af6af10/fetchFileTemp5609057485490961800.tmp
[2023-01-31T05:32:38.405+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:38 INFO Executor: Adding file:/tmp/spark-b63de28c-789a-4d7b-b0df-8d0eb0c9a92e/userFiles-c9b14e3e-a4e8-4a70-b06e-53f84af6af10/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T05:32:38.569+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:38 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33303.
[2023-01-31T05:32:38.569+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:38 INFO NettyBlockTransferService: Server created on 81d5fcd0285b:33303
[2023-01-31T05:32:38.583+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:38 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T05:32:38.653+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:38 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 33303, None)
[2023-01-31T05:32:38.686+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:38 INFO BlockManagerMasterEndpoint: Registering block manager 81d5fcd0285b:33303 with 434.4 MiB RAM, BlockManagerId(driver, 81d5fcd0285b, 33303, None)
[2023-01-31T05:32:38.716+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:38 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 33303, None)
[2023-01-31T05:32:38.724+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:38 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 81d5fcd0285b, 33303, None)
[2023-01-31T05:32:42.681+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:42 INFO Executor: Told to re-register on heartbeat
[2023-01-31T05:32:42.687+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:42 INFO BlockManager: BlockManager BlockManagerId(driver, 81d5fcd0285b, 33303, None) re-registering with master
[2023-01-31T05:32:42.698+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:42 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 33303, None)
[2023-01-31T05:32:42.917+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:42 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 33303, None)
[2023-01-31T05:32:42.917+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:42 INFO BlockManager: Reporting 0 blocks to the master.
[2023-01-31T05:33:04.439+0000] {spark_submit.py:495} INFO - 23/01/31 05:33:04 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T05:33:05.004+0000] {spark_submit.py:495} INFO - 23/01/31 05:33:05 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T05:33:58.905+0000] {spark_submit.py:495} INFO - 23/01/31 05:33:58 INFO InMemoryFileIndex: It took 2836 ms to list leaf files for 1 paths.
[2023-01-31T05:34:06.067+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:06 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T05:34:07.982+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:07 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T05:34:08.166+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:08 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 81d5fcd0285b:33303 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T05:34:08.419+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:08 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T05:34:25.793+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:25 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T05:34:25.930+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:25 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T05:34:26.761+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:26 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T05:34:27.459+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:27 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T05:34:27.473+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:27 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T05:34:27.485+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:27 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:34:27.572+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:27 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:34:27.759+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:27 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T05:34:29.137+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:29 INFO AsyncEventQueue: Process of event SparkListenerJobStart(0,1675143267575,ArraySeq(org.apache.spark.scheduler.StageInfo@14ca6c8d),{spark.master=local, spark.driver.port=34829, spark.submit.pyFiles=, spark.app.startTime=1675143117923, spark.rdd.compress=True, spark.executor.id=driver, spark.app.name=gcp_playground, spark.submit.deployMode=client, spark.hadoop.google.cloud.auth.service.account.json.keyfile=/.google/credentials/google_credentials.json, spark.driver.host=81d5fcd0285b, spark.app.id=local-1675143143852, spark.app.submitTime=1675143098205, spark.hadoop.fs.AbstractFileSystem.gs.impl=google.cloud.hadoop.fs.gcs.GoogleHadoopFS, spark.sql.session.timeZone=UTC, spark.hadoop.google.cloud.auth.service.account.enable=true, spark.app.initial.jar.urls=spark://81d5fcd0285b:34829/jars/gcs-connector-hadoop3-latest.jar,spark://81d5fcd0285b:34829/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar, spark.executor.extraJavaOptions=-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED, spark.jars=/opt/spark/jars/gcs-connector-hadoop3-latest.jar, /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar, spark.sql.warehouse.dir=file:/opt/***/spark-warehouse, spark.serializer.objectStreamReset=100, spark.driver.extraJavaOptions=-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED}) by listener AppStatusListener took 1.412041323s.
[2023-01-31T05:34:30.838+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:30 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T05:34:30.966+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:30 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T05:34:30.977+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:30 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 81d5fcd0285b:33303 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T05:34:31.024+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:31 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:34:31.265+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:34:31.269+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:31 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T05:34:32.296+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:32 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T05:34:32.863+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:32 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T05:34:37.974+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:37 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010200__202101010300.json:0+9369
[2023-01-31T05:34:46.484+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:46 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3437 bytes result sent to driver
[2023-01-31T05:34:47.526+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:47 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 15622 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:34:47.538+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:47 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T05:34:48.077+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:48 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 19.499 s
[2023-01-31T05:34:48.622+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:48 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:34:48.657+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T05:34:49.111+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:49 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 22.348984 s
[2023-01-31T05:35:09.095+0000] {spark_submit.py:495} INFO - 23/01/31 05:35:09 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 81d5fcd0285b:33303 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T05:35:09.228+0000] {spark_submit.py:495} INFO - 23/01/31 05:35:09 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 81d5fcd0285b:33303 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T05:36:25.539+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:25 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:36:25.556+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:25 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:36:25.569+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:25 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:36:35.313+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:35 INFO CodeGenerator: Code generated in 7236.926722 ms
[2023-01-31T05:36:35.649+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:35 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T05:36:35.946+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:35 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T05:36:35.965+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:35 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 81d5fcd0285b:33303 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T05:36:36.056+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:35 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T05:36:36.952+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:36 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:36:38.537+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:38 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T05:36:38.541+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:38 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T05:36:38.542+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:38 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T05:36:38.542+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:38 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:36:38.561+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:38 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:36:38.579+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:38 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T05:36:38.759+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:38 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T05:36:38.779+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:38 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T05:36:38.782+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:38 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 81d5fcd0285b:33303 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T05:36:38.788+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:38 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:36:38.794+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:36:38.795+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:38 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T05:36:38.930+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:38 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:36:38.970+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:38 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T05:36:42.148+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:42 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010200__202101010300.json, range: 0-9369, partition values: [empty row]
[2023-01-31T05:36:47.009+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:47 INFO CodeGenerator: Code generated in 4135.29925 ms
[2023-01-31T05:36:49.476+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:49 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1671 bytes result sent to driver
[2023-01-31T05:36:49.511+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:49 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 10694 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:36:49.523+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:49 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T05:36:49.529+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:49 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 10.800 s
[2023-01-31T05:36:49.534+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:49 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:36:49.538+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T05:36:49.549+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:49 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 11.014553 s
[2023-01-31T05:36:50.142+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:50 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:36:50.144+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:50 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:36:50.173+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:50 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:36:51.180+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:51 INFO CodeGenerator: Code generated in 515.085179 ms
[2023-01-31T05:36:51.420+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:51 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T05:36:51.975+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:51 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T05:36:51.979+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:51 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 81d5fcd0285b:33303 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:36:51.985+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:51 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T05:36:51.998+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:51 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:36:52.135+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:52 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T05:36:52.145+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:52 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T05:36:52.152+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:52 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T05:36:52.165+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:52 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:36:52.192+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:52 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:36:52.225+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:52 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T05:36:52.301+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:52 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T05:36:52.323+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:52 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T05:36:52.329+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:52 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 81d5fcd0285b:33303 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T05:36:52.337+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:52 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:36:52.343+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:36:52.345+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:52 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T05:36:52.362+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:52 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:36:52.380+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:52 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T05:36:52.899+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:52 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010200__202101010300.json, range: 0-9369, partition values: [empty row]
[2023-01-31T05:36:54.408+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:54 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1585 bytes result sent to driver
[2023-01-31T05:36:54.453+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:54 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 2094 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:36:54.465+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:54 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T05:36:54.484+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:54 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 2.231 s
[2023-01-31T05:36:54.496+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:54 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:36:54.499+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T05:36:54.505+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:54 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 2.361846 s
[2023-01-31T05:36:56.300+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:56 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:36:56.307+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:56 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T05:36:56.314+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:56 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:36:57.962+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:57 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 81d5fcd0285b:33303 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T05:36:58.051+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:58 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 81d5fcd0285b:33303 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T05:37:00.397+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:00 INFO CodeGenerator: Code generated in 2924.689159 ms
[2023-01-31T05:37:00.417+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:00 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T05:37:01.276+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:01 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T05:37:01.306+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:01 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 81d5fcd0285b:33303 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:37:01.324+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:01 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T05:37:01.428+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:01 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:37:02.418+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:02 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T05:37:02.432+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:02 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T05:37:02.439+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:02 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T05:37:02.440+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:02 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:37:02.452+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:02 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:37:02.467+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:02 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T05:37:02.611+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:02 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.7 MiB)
[2023-01-31T05:37:02.694+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:02 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.7 MiB)
[2023-01-31T05:37:02.724+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:02 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 81d5fcd0285b:33303 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T05:37:02.754+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:02 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:37:02.774+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:37:02.788+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:02 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T05:37:02.865+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:02 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:37:02.897+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:02 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T05:37:03.293+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:03 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010200__202101010300.json, range: 0-9369, partition values: [empty row]
[2023-01-31T05:37:05.432+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:05 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-01-31T05:37:05.562+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:05 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 2677 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:37:05.585+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:05 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T05:37:05.592+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:05 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 3.034 s
[2023-01-31T05:37:05.599+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:05 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:37:05.604+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T05:37:05.613+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:05 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 3.181329 s
[2023-01-31T05:37:06.957+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:06 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:37:06.961+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:06 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:37:06.964+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:06 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:37:07.325+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:07 INFO CodeGenerator: Code generated in 310.484845 ms
[2023-01-31T05:37:07.561+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:07 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.5 MiB)
[2023-01-31T05:37:08.317+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:08 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T05:37:08.334+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:08 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 81d5fcd0285b:33303 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:37:08.335+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:08 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T05:37:08.375+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:08 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:37:09.490+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:09 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T05:37:09.500+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:09 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T05:37:09.517+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:09 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T05:37:09.521+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:09 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:37:09.526+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:09 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:37:09.557+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:09 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T05:37:09.666+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:09 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-31T05:37:09.927+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:09 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-01-31T05:37:09.937+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:09 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 81d5fcd0285b:33303 (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T05:37:09.978+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:09 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:37:10.024+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:37:10.044+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:10 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T05:37:10.089+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:10 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:37:10.101+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:10 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T05:37:10.756+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:10 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010200__202101010300.json, range: 0-9369, partition values: [empty row]
[2023-01-31T05:37:12.698+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:12 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1586 bytes result sent to driver
[2023-01-31T05:37:12.781+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:12 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 2692 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:37:12.782+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:12 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T05:37:12.791+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:12 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 3.213 s
[2023-01-31T05:37:12.801+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:12 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:37:12.867+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T05:37:12.923+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:12 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 3.299786 s
[2023-01-31T05:37:20.201+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:20 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 81d5fcd0285b:33303 in memory (size: 7.6 KiB, free: 434.3 MiB)
[2023-01-31T05:37:20.448+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:20 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 81d5fcd0285b:33303 in memory (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T05:37:24.213+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:24 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:37:24.238+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:24 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T05:37:24.239+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:24 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:37:27.670+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:27 INFO CodeGenerator: Code generated in 1440.552557 ms
[2023-01-31T05:37:27.691+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:27 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.3 MiB)
[2023-01-31T05:37:27.759+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:27 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T05:37:27.770+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:27 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 81d5fcd0285b:33303 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:37:27.781+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:27 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T05:37:27.806+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:27 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:37:29.128+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T05:37:29.131+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) with 1 output partitions
[2023-01-31T05:37:29.132+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201)
[2023-01-31T05:37:29.132+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:37:29.132+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:37:29.146+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201), which has no missing parents
[2023-01-31T05:37:29.198+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.2 MiB)
[2023-01-31T05:37:29.224+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.2 MiB)
[2023-01-31T05:37:29.230+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 81d5fcd0285b:33303 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T05:37:29.239+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:37:29.246+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:37:29.248+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T05:37:29.261+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:37:29.263+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T05:37:29.374+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010200__202101010300.json, range: 0-9369, partition values: [empty row]
[2023-01-31T05:37:29.810+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2524 bytes result sent to driver
[2023-01-31T05:37:29.822+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 556 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:37:29.823+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) finished in 0.662 s
[2023-01-31T05:37:29.823+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T05:37:29.828+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:37:29.841+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T05:37:29.849+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201, took 0.713382 s
[2023-01-31T05:37:30.801+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:30 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:37:30.801+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:30 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T05:37:30.808+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:30 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:37:31.274+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:31 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 81d5fcd0285b:33303 in memory (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T05:37:32.691+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:32 INFO CodeGenerator: Code generated in 1707.204641 ms
[2023-01-31T05:37:32.885+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:32 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 433.1 MiB)
[2023-01-31T05:37:33.543+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:33 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.0 MiB)
[2023-01-31T05:37:33.562+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:33 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 81d5fcd0285b:33303 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:37:33.589+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:33 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T05:37:33.669+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:37:35.143+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:35 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T05:37:35.162+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:35 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T05:37:35.163+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:35 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T05:37:35.163+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:35 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:37:35.164+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:35 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:37:35.206+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:35 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T05:37:35.380+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:35 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 433.0 MiB)
[2023-01-31T05:37:35.499+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:35 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 433.0 MiB)
[2023-01-31T05:37:35.566+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:35 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 81d5fcd0285b:33303 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T05:37:35.570+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:35 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:37:35.582+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:37:35.593+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:35 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T05:37:35.617+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:35 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:37:35.622+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:35 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T05:37:36.313+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:36 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010200__202101010300.json, range: 0-9369, partition values: [empty row]
[2023-01-31T05:37:37.655+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:37 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1827 bytes result sent to driver
[2023-01-31T05:37:37.660+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:37 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 2045 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:37:37.662+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:37 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T05:37:37.739+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:37 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 2.409 s
[2023-01-31T05:37:37.741+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:37 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:37:37.743+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T05:37:37.749+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:37 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 2.592535 s
[2023-01-31T05:37:38.632+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:38 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:37:38.640+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:38 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T05:37:38.662+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:38 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:37:40.116+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:40 INFO CodeGenerator: Code generated in 784.089555 ms
[2023-01-31T05:37:40.196+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:40 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.8 MiB)
[2023-01-31T05:37:40.256+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:40 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.7 MiB)
[2023-01-31T05:37:40.275+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:40 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 81d5fcd0285b:33303 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:37:40.279+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:40 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T05:37:40.289+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:40 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:37:40.544+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:40 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T05:37:40.581+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:40 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T05:37:40.582+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:40 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T05:37:40.583+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:40 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:37:40.583+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:40 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:37:40.624+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:40 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T05:37:40.891+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:40 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.7 MiB)
[2023-01-31T05:37:40.967+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:40 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.7 MiB)
[2023-01-31T05:37:40.980+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:40 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 81d5fcd0285b:33303 (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T05:37:40.998+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:40 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:37:41.056+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:37:41.060+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:41 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T05:37:41.128+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:41 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:37:41.138+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:41 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T05:37:41.329+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:41 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010200__202101010300.json, range: 0-9369, partition values: [empty row]
[2023-01-31T05:37:42.401+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:42 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 81d5fcd0285b:33303 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T05:37:42.430+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:42 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1772 bytes result sent to driver
[2023-01-31T05:37:42.444+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:42 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 1315 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:37:42.447+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:42 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T05:37:42.458+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:42 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 1.771 s
[2023-01-31T05:37:42.462+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:42 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:37:42.464+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T05:37:42.497+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:42 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 1.946906 s
[2023-01-31T05:37:56.903+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:56 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 81d5fcd0285b:33303 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:37:57.069+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:57 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 81d5fcd0285b:33303 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:37:57.155+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:57 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 81d5fcd0285b:33303 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:37:57.357+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:57 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 81d5fcd0285b:33303 in memory (size: 12.6 KiB, free: 434.3 MiB)
[2023-01-31T05:37:57.457+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:57 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 81d5fcd0285b:33303 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:38:47.301+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:47 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:38:48.244+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:38:48.288+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:38:48.397+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:48 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:38:48.413+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:38:48.414+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:38:48.491+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:38:51.337+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:51 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-31T05:38:51.344+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:51 INFO DAGScheduler: Got job 8 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T05:38:51.346+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:51 INFO DAGScheduler: Final stage: ResultStage 8 (save at BigQueryWriteHelper.java:105)
[2023-01-31T05:38:51.347+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:51 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:38:51.352+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:51 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:38:51.355+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:51 INFO DAGScheduler: Submitting ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T05:38:52.579+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:52 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 209.9 KiB, free 433.5 MiB)
[2023-01-31T05:38:52.778+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:52 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 74.2 KiB, free 433.4 MiB)
[2023-01-31T05:38:52.797+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:52 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 81d5fcd0285b:33303 (size: 74.2 KiB, free: 434.2 MiB)
[2023-01-31T05:38:52.800+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:52 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:38:52.820+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:38:52.856+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:52 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2023-01-31T05:38:52.968+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:52 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7484 bytes) taskResourceAssignments Map()
[2023-01-31T05:38:53.008+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:53 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2023-01-31T05:38:54.076+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:38:54.078+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:38:54.083+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:54 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:38:54.085+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:38:54.087+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:38:54.092+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:38:54.214+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:54 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T05:38:54.240+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:54 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T05:38:54.721+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:54 INFO ParquetOutputFormat: Parquet block size to 134217728
[2023-01-31T05:38:54.722+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:54 INFO ParquetOutputFormat: Validation is off
[2023-01-31T05:38:54.726+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:54 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-01-31T05:38:54.727+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:54 INFO ParquetOutputFormat: Parquet properties are:
[2023-01-31T05:38:54.730+0000] {spark_submit.py:495} INFO - Parquet page size to 1048576
[2023-01-31T05:38:54.731+0000] {spark_submit.py:495} INFO - Parquet dictionary page size to 1048576
[2023-01-31T05:38:54.731+0000] {spark_submit.py:495} INFO - Dictionary is true
[2023-01-31T05:38:54.732+0000] {spark_submit.py:495} INFO - Writer version is: PARQUET_1_0
[2023-01-31T05:38:54.733+0000] {spark_submit.py:495} INFO - Page size checking is: estimated
[2023-01-31T05:38:54.733+0000] {spark_submit.py:495} INFO - Min row count for page size check is: 100
[2023-01-31T05:38:54.734+0000] {spark_submit.py:495} INFO - Max row count for page size check is: 10000
[2023-01-31T05:38:54.734+0000] {spark_submit.py:495} INFO - Truncate length for column indexes is: 64
[2023-01-31T05:38:54.735+0000] {spark_submit.py:495} INFO - Truncate length for statistics min/max  is: 2147483647
[2023-01-31T05:38:54.750+0000] {spark_submit.py:495} INFO - Bloom filter enabled: false
[2023-01-31T05:38:54.751+0000] {spark_submit.py:495} INFO - Max Bloom filter size for a column is 1048576
[2023-01-31T05:38:54.752+0000] {spark_submit.py:495} INFO - Bloom filter expected number of distinct values are: null
[2023-01-31T05:38:54.755+0000] {spark_submit.py:495} INFO - Page row count limit to 20000
[2023-01-31T05:38:54.758+0000] {spark_submit.py:495} INFO - Writing page checksums is: on
[2023-01-31T05:38:55.505+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:55 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-01-31T05:38:55.506+0000] {spark_submit.py:495} INFO - {
[2023-01-31T05:38:55.506+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-31T05:38:55.507+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-31T05:38:55.508+0000] {spark_submit.py:495} INFO - "name" : "measured_at",
[2023-01-31T05:38:55.512+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T05:38:55.516+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T05:38:55.523+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:55.526+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:55.530+0000] {spark_submit.py:495} INFO - "name" : "created_at",
[2023-01-31T05:38:55.536+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T05:38:55.541+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T05:38:55.545+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:55.550+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:55.550+0000] {spark_submit.py:495} INFO - "name" : "Biomass - Actual Aggregated",
[2023-01-31T05:38:55.557+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:55.558+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:55.559+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:55.559+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:55.560+0000] {spark_submit.py:495} INFO - "name" : "Fossil Brown coal/Lignite - Actual Aggregated",
[2023-01-31T05:38:55.560+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:55.569+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:55.570+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:55.571+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:55.571+0000] {spark_submit.py:495} INFO - "name" : "Fossil Gas - Actual Aggregated",
[2023-01-31T05:38:55.572+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:55.572+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:55.573+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:55.574+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:55.574+0000] {spark_submit.py:495} INFO - "name" : "Fossil Hard coal - Actual Aggregated",
[2023-01-31T05:38:55.575+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:55.576+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:55.576+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:55.577+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:55.578+0000] {spark_submit.py:495} INFO - "name" : "Fossil Oil - Actual Aggregated",
[2023-01-31T05:38:55.579+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:55.579+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:55.580+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:55.581+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:55.581+0000] {spark_submit.py:495} INFO - "name" : "Geothermal - Actual Aggregated",
[2023-01-31T05:38:55.582+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:55.583+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:55.583+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:55.584+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:55.589+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Aggregated",
[2023-01-31T05:38:55.589+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:55.590+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:55.591+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:55.591+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:55.592+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Consumption",
[2023-01-31T05:38:55.597+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:55.598+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:55.599+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:55.600+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:55.605+0000] {spark_submit.py:495} INFO - "name" : "Hydro Run-of-river and poundage - Actual Aggregated",
[2023-01-31T05:38:55.606+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:55.606+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:55.607+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:55.608+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:55.608+0000] {spark_submit.py:495} INFO - "name" : "Hydro Water Reservoir - Actual Aggregated",
[2023-01-31T05:38:55.613+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:55.619+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:55.625+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:55.625+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:55.633+0000] {spark_submit.py:495} INFO - "name" : "Nuclear - Actual Aggregated",
[2023-01-31T05:38:55.633+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:55.634+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:55.658+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:55.659+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:55.660+0000] {spark_submit.py:495} INFO - "name" : "Other - Actual Aggregated",
[2023-01-31T05:38:55.661+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:55.661+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:55.662+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:55.667+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:55.668+0000] {spark_submit.py:495} INFO - "name" : "Other renewable - Actual Aggregated",
[2023-01-31T05:38:55.669+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:55.670+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:55.670+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:55.732+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:55.738+0000] {spark_submit.py:495} INFO - "name" : "Solar - Actual Aggregated",
[2023-01-31T05:38:55.739+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:55.742+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:55.745+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:55.764+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:55.765+0000] {spark_submit.py:495} INFO - "name" : "Waste - Actual Aggregated",
[2023-01-31T05:38:55.767+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:55.768+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:55.772+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:55.774+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:55.775+0000] {spark_submit.py:495} INFO - "name" : "Wind Offshore - Actual Aggregated",
[2023-01-31T05:38:55.777+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:55.778+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:55.779+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:55.779+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:55.780+0000] {spark_submit.py:495} INFO - "name" : "Wind Onshore - Actual Aggregated",
[2023-01-31T05:38:55.781+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:55.782+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:55.783+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:55.784+0000] {spark_submit.py:495} INFO - } ]
[2023-01-31T05:38:55.784+0000] {spark_submit.py:495} INFO - }
[2023-01-31T05:38:55.785+0000] {spark_submit.py:495} INFO - and corresponding Parquet message type:
[2023-01-31T05:38:55.785+0000] {spark_submit.py:495} INFO - message spark_schema {
[2023-01-31T05:38:55.786+0000] {spark_submit.py:495} INFO - required int96 measured_at;
[2023-01-31T05:38:55.786+0000] {spark_submit.py:495} INFO - required int96 created_at;
[2023-01-31T05:38:55.787+0000] {spark_submit.py:495} INFO - optional double Biomass - Actual Aggregated;
[2023-01-31T05:38:55.788+0000] {spark_submit.py:495} INFO - optional double Fossil Brown coal/Lignite - Actual Aggregated;
[2023-01-31T05:38:55.788+0000] {spark_submit.py:495} INFO - optional double Fossil Gas - Actual Aggregated;
[2023-01-31T05:38:55.789+0000] {spark_submit.py:495} INFO - optional double Fossil Hard coal - Actual Aggregated;
[2023-01-31T05:38:55.789+0000] {spark_submit.py:495} INFO - optional double Fossil Oil - Actual Aggregated;
[2023-01-31T05:38:55.790+0000] {spark_submit.py:495} INFO - optional double Geothermal - Actual Aggregated;
[2023-01-31T05:38:55.790+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Aggregated;
[2023-01-31T05:38:55.791+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Consumption;
[2023-01-31T05:38:55.791+0000] {spark_submit.py:495} INFO - optional double Hydro Run-of-river and poundage - Actual Aggregated;
[2023-01-31T05:38:55.792+0000] {spark_submit.py:495} INFO - optional double Hydro Water Reservoir - Actual Aggregated;
[2023-01-31T05:38:55.792+0000] {spark_submit.py:495} INFO - optional double Nuclear - Actual Aggregated;
[2023-01-31T05:38:55.794+0000] {spark_submit.py:495} INFO - optional double Other - Actual Aggregated;
[2023-01-31T05:38:55.797+0000] {spark_submit.py:495} INFO - optional double Other renewable - Actual Aggregated;
[2023-01-31T05:38:55.797+0000] {spark_submit.py:495} INFO - optional double Solar - Actual Aggregated;
[2023-01-31T05:38:55.798+0000] {spark_submit.py:495} INFO - optional double Waste - Actual Aggregated;
[2023-01-31T05:38:55.798+0000] {spark_submit.py:495} INFO - optional double Wind Offshore - Actual Aggregated;
[2023-01-31T05:38:55.799+0000] {spark_submit.py:495} INFO - optional double Wind Onshore - Actual Aggregated;
[2023-01-31T05:38:55.799+0000] {spark_submit.py:495} INFO - }
[2023-01-31T05:38:55.800+0000] {spark_submit.py:495} INFO - 
[2023-01-31T05:38:55.800+0000] {spark_submit.py:495} INFO - 
[2023-01-31T05:38:59.369+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:59 INFO CodecPool: Got brand-new compressor [.snappy]
[2023-01-31T05:39:00.118+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:00 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 81d5fcd0285b:33303 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:39:00.670+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:00 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 81d5fcd0285b:33303 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:39:12.769+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:12 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675143143852-fabc6c00-1b53-43c5-8d4b-2cbdadef879a/_temporary/0/_temporary/' directory.
[2023-01-31T05:39:12.774+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:12 INFO FileOutputCommitter: Saved output of task 'attempt_20230131053850303150723559144472_0008_m_000000_8' to gs://entsoe_temp_1009/.spark-bigquery-local-1675143143852-fabc6c00-1b53-43c5-8d4b-2cbdadef879a/_temporary/0/task_20230131053850303150723559144472_0008_m_000000
[2023-01-31T05:39:12.790+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:12 INFO SparkHadoopMapRedUtil: attempt_20230131053850303150723559144472_0008_m_000000_8: Committed. Elapsed time: 3588 ms.
[2023-01-31T05:39:13.315+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:13 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2588 bytes result sent to driver
[2023-01-31T05:39:13.337+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:13 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 20476 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:39:13.341+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:13 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2023-01-31T05:39:13.349+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:13 INFO DAGScheduler: ResultStage 8 (save at BigQueryWriteHelper.java:105) finished in 21.943 s
[2023-01-31T05:39:13.369+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:13 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:39:13.369+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2023-01-31T05:39:13.373+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:13 INFO DAGScheduler: Job 8 finished: save at BigQueryWriteHelper.java:105, took 22.030049 s
[2023-01-31T05:39:13.406+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:13 INFO FileFormatWriter: Start to commit write Job fe0ed13d-c377-41b9-bec7-a71d68732960.
[2023-01-31T05:39:14.606+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:14 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675143143852-fabc6c00-1b53-43c5-8d4b-2cbdadef879a/_temporary/0/task_20230131053850303150723559144472_0008_m_000000/' directory.
[2023-01-31T05:39:15.398+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:15 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675143143852-fabc6c00-1b53-43c5-8d4b-2cbdadef879a/' directory.
[2023-01-31T05:39:17.327+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:17 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 81d5fcd0285b:33303 in memory (size: 74.2 KiB, free: 434.4 MiB)
[2023-01-31T05:39:17.853+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:17 INFO FileFormatWriter: Write Job fe0ed13d-c377-41b9-bec7-a71d68732960 committed. Elapsed time: 4360 ms.
[2023-01-31T05:39:17.947+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:17 INFO FileFormatWriter: Finished processing stats for write job fe0ed13d-c377-41b9-bec7-a71d68732960.
[2023-01-31T05:39:23.210+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:23 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=entsoe_analytics_1009, projectId=rafzul-analytics-1009, tableId=TEST_total_generation_staging}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=null, ignoreUnknownValue=null, sourceUris=[gs://entsoe_temp_1009/.spark-bigquery-local-1675143143852-fabc6c00-1b53-43c5-8d4b-2cbdadef879a/part-00000-4a7191f4-4ad7-492f-9495-5300949456bc-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=true, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=rafzul-analytics-1009, job=b47bf810-e221-41b9-942d-1d4dd31724fa, location=US}
[2023-01-31T05:39:28.095+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:28 INFO BigQueryClient: Done loading to rafzul-analytics-1009.entsoe_analytics_1009.TEST_total_generation_staging. jobId: JobId{project=rafzul-analytics-1009, job=b47bf810-e221-41b9-942d-1d4dd31724fa, location=US}
[2023-01-31T05:39:29.245+0000] {spark_submit.py:495} INFO - MANTEP!!!!
[2023-01-31T05:39:30.559+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:30 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-31T05:39:30.730+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:30 INFO SparkUI: Stopped Spark web UI at http://81d5fcd0285b:4043
[2023-01-31T05:39:30.929+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:30 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-31T05:39:31.180+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:31 INFO MemoryStore: MemoryStore cleared
[2023-01-31T05:39:31.183+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:31 INFO BlockManager: BlockManager stopped
[2023-01-31T05:39:31.224+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:31 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-31T05:39:31.241+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:31 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-31T05:39:31.310+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:31 INFO SparkContext: Successfully stopped SparkContext
[2023-01-31T05:39:31.312+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:31 INFO ShutdownHookManager: Shutdown hook called
[2023-01-31T05:39:31.314+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-1f5c2559-212e-4c28-bd7f-2c4d145868f8
[2023-01-31T05:39:31.370+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-b63de28c-789a-4d7b-b0df-8d0eb0c9a92e/pyspark-e9b7302b-c720-43ba-a04f-f568327250b8
[2023-01-31T05:39:31.409+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-b63de28c-789a-4d7b-b0df-8d0eb0c9a92e
[2023-01-31T05:39:32.123+0000] {taskinstance.py:1322} INFO - Marking task as SUCCESS. dag_id=entsoe-energydata-backfill, task_id=stage_total_generation, execution_date=20210101T020000, start_date=20230131T053046, end_date=20230131T053932
[2023-01-31T05:39:32.246+0000] {local_task_job.py:159} INFO - Task exited with return code 0
[2023-01-31T05:39:32.335+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T07:39:01.406+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T02:00:00+00:00 [queued]>
[2023-01-31T07:39:01.520+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T02:00:00+00:00 [queued]>
[2023-01-31T07:39:01.521+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T07:39:01.522+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T07:39:01.523+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T07:39:01.682+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 02:00:00+00:00
[2023-01-31T07:39:01.762+0000] {standard_task_runner.py:55} INFO - Started process 25804 to run task
[2023-01-31T07:39:01.801+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T02:00:00+00:00', '--job-id', '909', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp87diegzx']
[2023-01-31T07:39:01.815+0000] {standard_task_runner.py:83} INFO - Job 909: Subtask stage_total_generation
[2023-01-31T07:39:03.132+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T02:00:00+00:00 [running]> on host 81d5fcd0285b
[2023-01-31T07:39:04.047+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T02:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T02:00:00+00:00
[2023-01-31T07:39:04.139+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T07:39:04.165+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010200 202101010300 DE_TENNET
[2023-01-31T07:40:24.805+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:24 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T07:40:28.103+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T07:40:32.652+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:32 INFO ResourceUtils: ==============================================================
[2023-01-31T07:40:32.669+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:32 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T07:40:32.675+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:32 INFO ResourceUtils: ==============================================================
[2023-01-31T07:40:32.691+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:32 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T07:40:33.027+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:33 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T07:40:33.188+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:33 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T07:40:33.245+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:33 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T07:40:34.393+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:34 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T07:40:34.403+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:34 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T07:40:34.412+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:34 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T07:40:34.449+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:34 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T07:40:34.457+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:34 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T07:40:47.575+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:47 INFO Utils: Successfully started service 'sparkDriver' on port 43395.
[2023-01-31T07:40:49.348+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:49 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T07:40:51.188+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:51 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T07:40:52.003+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:52 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T07:40:52.020+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:52 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T07:40:52.117+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:52 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T07:40:52.517+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:52 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-4511c983-962d-4987-8c04-2f33336f5177
[2023-01-31T07:40:52.868+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:52 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T07:40:53.524+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:53 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T07:41:03.177+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:03 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T07:41:03.185+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:03 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2023-01-31T07:41:03.197+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:03 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[2023-01-31T07:41:03.211+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:03 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[2023-01-31T07:41:03.224+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:03 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[2023-01-31T07:41:03.664+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:03 INFO Utils: Successfully started service 'SparkUI' on port 4045.
[2023-01-31T07:41:04.105+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:04 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://81d5fcd0285b:43395/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675150824670
[2023-01-31T07:41:04.106+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:04 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://81d5fcd0285b:43395/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675150824670
[2023-01-31T07:41:05.257+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:05 INFO Executor: Starting executor ID driver on host 81d5fcd0285b
[2023-01-31T07:41:05.342+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:05 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T07:41:05.706+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:05 INFO Executor: Fetching spark://81d5fcd0285b:43395/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675150824670
[2023-01-31T07:41:07.098+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:07 INFO TransportClientFactory: Successfully created connection to 81d5fcd0285b/172.19.0.9:43395 after 661 ms (0 ms spent in bootstraps)
[2023-01-31T07:41:07.654+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:07 INFO Utils: Fetching spark://81d5fcd0285b:43395/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-51c8cce9-1109-42d9-bdb0-15631ad29eeb/userFiles-ab044af5-15e3-424a-9087-c075c3fb52e4/fetchFileTemp8883527556972349929.tmp
[2023-01-31T07:41:12.241+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:12 INFO Executor: Adding file:/tmp/spark-51c8cce9-1109-42d9-bdb0-15631ad29eeb/userFiles-ab044af5-15e3-424a-9087-c075c3fb52e4/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T07:41:12.250+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:12 INFO Executor: Fetching spark://81d5fcd0285b:43395/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675150824670
[2023-01-31T07:41:12.312+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:12 INFO Utils: Fetching spark://81d5fcd0285b:43395/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-51c8cce9-1109-42d9-bdb0-15631ad29eeb/userFiles-ab044af5-15e3-424a-9087-c075c3fb52e4/fetchFileTemp3861487287918465222.tmp
[2023-01-31T07:41:19.573+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:19 INFO Executor: Adding file:/tmp/spark-51c8cce9-1109-42d9-bdb0-15631ad29eeb/userFiles-ab044af5-15e3-424a-9087-c075c3fb52e4/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T07:41:19.863+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:19 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40621.
[2023-01-31T07:41:19.872+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:19 INFO NettyBlockTransferService: Server created on 81d5fcd0285b:40621
[2023-01-31T07:41:19.946+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:19 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T07:41:20.356+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:20 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 40621, None)
[2023-01-31T07:41:20.409+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:20 INFO BlockManagerMasterEndpoint: Registering block manager 81d5fcd0285b:40621 with 434.4 MiB RAM, BlockManagerId(driver, 81d5fcd0285b, 40621, None)
[2023-01-31T07:41:20.537+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:20 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 40621, None)
[2023-01-31T07:41:20.545+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:20 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 81d5fcd0285b, 40621, None)
[2023-01-31T07:41:21.603+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:21 INFO Executor: Told to re-register on heartbeat
[2023-01-31T07:41:21.611+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:21 INFO BlockManager: BlockManager BlockManagerId(driver, 81d5fcd0285b, 40621, None) re-registering with master
[2023-01-31T07:41:21.617+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:21 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 40621, None)
[2023-01-31T07:41:21.779+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:21 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 40621, None)
[2023-01-31T07:41:21.792+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:21 INFO BlockManager: Reporting 0 blocks to the master.
[2023-01-31T07:41:42.007+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:42 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T07:41:42.245+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:42 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T07:42:24.737+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:24 INFO InMemoryFileIndex: It took 602 ms to list leaf files for 1 paths.
[2023-01-31T07:42:28.489+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:28 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T07:42:29.433+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:29 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T07:42:29.559+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:29 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 81d5fcd0285b:40621 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T07:42:29.681+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:29 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T07:42:36.303+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:36 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T07:42:37.302+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:37 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T07:42:37.662+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:37 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T07:42:37.787+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:37 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T07:42:37.789+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:37 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T07:42:37.791+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:37 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:42:37.803+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:37 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:42:37.839+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:37 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T07:42:39.222+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:39 INFO AsyncEventQueue: Process of event SparkListenerJobStart(0,1675150957804,ArraySeq(org.apache.spark.scheduler.StageInfo@2dbc9218),{spark.master=local, spark.driver.port=43395, spark.submit.pyFiles=, spark.app.startTime=1675150824670, spark.rdd.compress=True, spark.executor.id=driver, spark.app.name=gcp_playground, spark.submit.deployMode=client, spark.hadoop.google.cloud.auth.service.account.json.keyfile=/.google/credentials/google_credentials.json, spark.driver.host=81d5fcd0285b, spark.app.id=local-1675150864789, spark.app.submitTime=1675150805515, spark.hadoop.fs.AbstractFileSystem.gs.impl=google.cloud.hadoop.fs.gcs.GoogleHadoopFS, spark.sql.session.timeZone=UTC, spark.hadoop.google.cloud.auth.service.account.enable=true, spark.app.initial.jar.urls=spark://81d5fcd0285b:43395/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar,spark://81d5fcd0285b:43395/jars/gcs-connector-hadoop3-latest.jar, spark.executor.extraJavaOptions=-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED, spark.jars=/opt/spark/jars/gcs-connector-hadoop3-latest.jar, /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar, spark.sql.warehouse.dir=file:/opt/***/spark-warehouse, spark.serializer.objectStreamReset=100, spark.driver.extraJavaOptions=-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED}) by listener AppStatusListener took 1.368552366s.
[2023-01-31T07:42:40.124+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:40 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T07:42:40.356+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:40 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T07:42:40.393+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:40 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 81d5fcd0285b:40621 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T07:42:40.472+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:40 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:42:42.161+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:42:42.205+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:42 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T07:42:45.189+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:45 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T07:42:45.515+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:45 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T07:42:50.437+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:50 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010200__202101010300.json:0+9369
[2023-01-31T07:42:57.909+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:57 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3480 bytes result sent to driver
[2023-01-31T07:42:58.137+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:58 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 13943 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:42:58.183+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:58 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T07:42:58.233+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:58 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 19.541 s
[2023-01-31T07:42:58.283+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:58 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:42:58.284+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T07:42:58.306+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:58 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 20.633967 s
[2023-01-31T07:43:03.309+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:03 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 81d5fcd0285b:40621 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T07:43:03.418+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:03 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 81d5fcd0285b:40621 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T07:43:25.395+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:25 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:43:25.446+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:25 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T07:43:25.503+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:25 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:43:27.216+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO CodeGenerator: Code generated in 948.551621 ms
[2023-01-31T07:43:27.254+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T07:43:27.383+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T07:43:27.399+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 81d5fcd0285b:40621 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T07:43:27.403+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T07:43:27.493+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:43:27.987+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T07:43:27.993+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T07:43:27.993+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T07:43:27.994+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:43:27.994+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:43:28.027+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:28 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T07:43:28.226+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:28 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T07:43:28.339+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:28 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T07:43:28.393+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:28 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 81d5fcd0285b:40621 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T07:43:28.399+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:28 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:43:28.434+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:43:28.437+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:28 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T07:43:28.561+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:28 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:43:28.577+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:28 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T07:43:30.117+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010200__202101010300.json, range: 0-9369, partition values: [empty row]
[2023-01-31T07:43:30.982+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO CodeGenerator: Code generated in 485.155177 ms
[2023-01-31T07:43:32.665+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1671 bytes result sent to driver
[2023-01-31T07:43:32.695+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 4191 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:43:32.703+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T07:43:32.773+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 4.582 s
[2023-01-31T07:43:32.800+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:43:32.800+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T07:43:32.805+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 4.812345 s
[2023-01-31T07:43:33.533+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:43:33.534+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T07:43:33.536+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:43:34.162+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO CodeGenerator: Code generated in 513.811892 ms
[2023-01-31T07:43:34.237+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T07:43:34.382+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T07:43:34.384+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 81d5fcd0285b:40621 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:43:34.412+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T07:43:34.433+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:43:34.791+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T07:43:34.800+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T07:43:34.803+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T07:43:34.804+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:43:34.805+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:43:34.810+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T07:43:34.840+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T07:43:34.855+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T07:43:34.873+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 81d5fcd0285b:40621 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T07:43:34.885+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:43:34.896+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:43:34.898+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T07:43:34.913+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:43:34.915+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T07:43:35.047+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:35 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010200__202101010300.json, range: 0-9369, partition values: [empty row]
[2023-01-31T07:43:35.589+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:35 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1585 bytes result sent to driver
[2023-01-31T07:43:35.624+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:35 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 712 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:43:35.628+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:35 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T07:43:35.635+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:35 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 0.813 s
[2023-01-31T07:43:35.635+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:35 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:43:35.636+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T07:43:35.641+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:35 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 0.847519 s
[2023-01-31T07:43:38.487+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:38 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:43:38.492+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:38 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T07:43:38.493+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:38 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:43:42.622+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:42 INFO CodeGenerator: Code generated in 2456.345014 ms
[2023-01-31T07:43:43.234+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:43 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T07:43:43.776+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:43 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T07:43:43.777+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:43 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 81d5fcd0285b:40621 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:43:43.777+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:43 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T07:43:43.782+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:43 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:43:43.974+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:43 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T07:43:43.977+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:43 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T07:43:43.978+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:43 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T07:43:43.978+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:43 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:43:43.986+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:43 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:43:44.036+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:44 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T07:43:44.410+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:44 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.6 MiB)
[2023-01-31T07:43:44.683+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:44 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.6 MiB)
[2023-01-31T07:43:44.710+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:44 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 81d5fcd0285b:40621 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T07:43:44.728+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:44 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:43:44.729+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:43:44.730+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:44 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T07:43:44.730+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:44 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:43:44.732+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:44 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T07:43:45.157+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010200__202101010300.json, range: 0-9369, partition values: [empty row]
[2023-01-31T07:43:46.703+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:46 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-01-31T07:43:46.784+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:46 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 2053 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:43:46.794+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:46 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 2.702 s
[2023-01-31T07:43:46.802+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:46 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:43:46.805+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:46 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T07:43:46.809+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T07:43:46.825+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:46 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 2.844883 s
[2023-01-31T07:43:47.181+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:47 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:43:47.182+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:47 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T07:43:47.182+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:47 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:43:48.126+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:48 INFO CodeGenerator: Code generated in 675.945394 ms
[2023-01-31T07:43:48.149+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:48 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 81d5fcd0285b:40621 in memory (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T07:43:48.227+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:48 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.5 MiB)
[2023-01-31T07:43:48.344+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:48 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 81d5fcd0285b:40621 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T07:43:48.502+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:48 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 81d5fcd0285b:40621 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T07:43:49.111+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:49 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.5 MiB)
[2023-01-31T07:43:49.121+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:49 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 81d5fcd0285b:40621 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:43:49.141+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:49 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T07:43:49.208+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:49 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:43:50.280+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T07:43:50.359+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T07:43:50.363+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T07:43:50.364+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:43:50.366+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:43:50.445+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T07:43:50.534+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.5 MiB)
[2023-01-31T07:43:50.555+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.5 MiB)
[2023-01-31T07:43:50.568+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 81d5fcd0285b:40621 (size: 7.6 KiB, free: 434.3 MiB)
[2023-01-31T07:43:50.577+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:43:50.606+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:43:50.613+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T07:43:50.710+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:43:50.735+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T07:43:51.050+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:51 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010200__202101010300.json, range: 0-9369, partition values: [empty row]
[2023-01-31T07:43:51.803+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:51 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1585 bytes result sent to driver
[2023-01-31T07:43:51.825+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:51 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 1192 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:43:51.829+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:51 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 1.363 s
[2023-01-31T07:43:51.832+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:51 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:43:51.851+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:51 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T07:43:51.852+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T07:43:51.924+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:51 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 1.568525 s
[2023-01-31T07:44:01.511+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:01 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:44:01.520+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:01 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T07:44:01.524+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:01 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:44:04.872+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO CodeGenerator: Code generated in 2361.409616 ms
[2023-01-31T07:44:05.124+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:05 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.3 MiB)
[2023-01-31T07:44:06.643+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:06 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T07:44:06.710+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:06 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 81d5fcd0285b:40621 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T07:44:06.808+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:06 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T07:44:06.909+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:06 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:44:09.254+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:09 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T07:44:09.261+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:09 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) with 1 output partitions
[2023-01-31T07:44:09.262+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:09 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201)
[2023-01-31T07:44:09.263+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:09 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:44:09.285+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:09 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:44:09.391+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:09 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201), which has no missing parents
[2023-01-31T07:44:09.903+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:09 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.2 MiB)
[2023-01-31T07:44:10.013+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:10 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.2 MiB)
[2023-01-31T07:44:10.051+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:10 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 81d5fcd0285b:40621 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T07:44:10.058+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:10 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:44:10.082+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:44:10.095+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:10 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T07:44:10.106+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:10 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:44:10.109+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:10 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T07:44:10.343+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:10 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010200__202101010300.json, range: 0-9369, partition values: [empty row]
[2023-01-31T07:44:12.111+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:12 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2567 bytes result sent to driver
[2023-01-31T07:44:12.147+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:12 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 2024 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:44:12.148+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:12 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T07:44:12.149+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:12 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) finished in 2.675 s
[2023-01-31T07:44:12.149+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:12 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:44:12.150+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T07:44:12.150+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:12 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201, took 2.891246 s
[2023-01-31T07:44:14.089+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:14 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:44:14.093+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:14 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T07:44:14.094+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:14 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:44:15.101+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:15 INFO CodeGenerator: Code generated in 883.850989 ms
[2023-01-31T07:44:15.322+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:15 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 433.0 MiB)
[2023-01-31T07:44:16.796+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:16 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.9 MiB)
[2023-01-31T07:44:16.830+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:16 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 81d5fcd0285b:40621 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T07:44:16.937+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:16 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T07:44:16.977+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:16 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:44:17.747+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:17 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 81d5fcd0285b:40621 in memory (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T07:44:18.106+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:18 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 81d5fcd0285b:40621 in memory (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T07:44:18.609+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:18 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T07:44:18.717+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:18 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T07:44:18.718+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:18 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T07:44:18.718+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:18 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:44:18.719+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:18 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:44:18.985+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:18 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T07:44:19.209+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:19 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 433.0 MiB)
[2023-01-31T07:44:19.555+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:19 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 433.0 MiB)
[2023-01-31T07:44:19.581+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:19 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 81d5fcd0285b:40621 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T07:44:19.681+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:19 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:44:19.764+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:44:19.789+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:19 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T07:44:19.793+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:19 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:44:19.793+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:19 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T07:44:20.046+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:20 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010200__202101010300.json, range: 0-9369, partition values: [empty row]
[2023-01-31T07:44:21.477+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:21 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-01-31T07:44:21.484+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:21 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 1702 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:44:21.485+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:21 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T07:44:21.493+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:21 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 2.481 s
[2023-01-31T07:44:21.504+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:21 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:44:21.505+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T07:44:21.506+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:21 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 2.885833 s
[2023-01-31T07:44:22.891+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:22 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:44:22.939+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:22 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T07:44:22.946+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:22 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:44:26.133+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:26 INFO CodeGenerator: Code generated in 1949.214254 ms
[2023-01-31T07:44:26.589+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:26 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.8 MiB)
[2023-01-31T07:44:27.694+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:27 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.7 MiB)
[2023-01-31T07:44:27.695+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:27 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 81d5fcd0285b:40621 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T07:44:27.704+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:27 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T07:44:27.812+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:27 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:44:28.357+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:28 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T07:44:28.357+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:28 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T07:44:28.358+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:28 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T07:44:28.358+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:28 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:44:28.359+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:28 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:44:28.359+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:28 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T07:44:28.456+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:28 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.7 MiB)
[2023-01-31T07:44:28.613+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:28 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.7 MiB)
[2023-01-31T07:44:28.618+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:28 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 81d5fcd0285b:40621 (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T07:44:28.641+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:28 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:44:28.710+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:44:28.713+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:28 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T07:44:28.734+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:28 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:44:28.737+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:28 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T07:44:28.864+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:28 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010200__202101010300.json, range: 0-9369, partition values: [empty row]
[2023-01-31T07:44:29.927+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:29 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1729 bytes result sent to driver
[2023-01-31T07:44:29.961+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:29 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 1210 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:44:29.962+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:29 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T07:44:29.963+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:29 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 1.587 s
[2023-01-31T07:44:29.968+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:29 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:44:29.969+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T07:44:30.031+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:29 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 1.638874 s
[2023-01-31T07:44:40.775+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:40 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 81d5fcd0285b:40621 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T07:44:41.050+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:41 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 81d5fcd0285b:40621 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T07:44:41.244+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:41 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 81d5fcd0285b:40621 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T07:44:41.425+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:41 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 81d5fcd0285b:40621 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:44:41.592+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:41 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 81d5fcd0285b:40621 in memory (size: 12.6 KiB, free: 434.3 MiB)
[2023-01-31T07:44:41.760+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:41 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 81d5fcd0285b:40621 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:44:42.034+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:42 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 81d5fcd0285b:40621 in memory (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T07:44:42.331+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:42 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 81d5fcd0285b:40621 in memory (size: 12.6 KiB, free: 434.4 MiB)
[2023-01-31T07:45:15.954+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:15 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T07:45:16.163+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T07:45:16.163+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T07:45:16.185+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:16 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T07:45:16.185+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T07:45:16.186+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T07:45:16.246+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T07:45:17.565+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:17 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-31T07:45:17.566+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:17 INFO DAGScheduler: Got job 8 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T07:45:17.566+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:17 INFO DAGScheduler: Final stage: ResultStage 8 (save at BigQueryWriteHelper.java:105)
[2023-01-31T07:45:17.567+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:17 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:45:17.567+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:17 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:45:17.573+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:17 INFO DAGScheduler: Submitting ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T07:45:17.692+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:17 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 209.9 KiB, free 434.0 MiB)
[2023-01-31T07:45:17.716+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:17 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 74.3 KiB, free 433.9 MiB)
[2023-01-31T07:45:17.719+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:17 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 81d5fcd0285b:40621 (size: 74.3 KiB, free: 434.3 MiB)
[2023-01-31T07:45:17.721+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:17 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:45:17.724+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:45:17.726+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:17 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2023-01-31T07:45:17.740+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:17 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7484 bytes) taskResourceAssignments Map()
[2023-01-31T07:45:17.744+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:17 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2023-01-31T07:45:18.051+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T07:45:18.051+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T07:45:18.059+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T07:45:18.060+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T07:45:18.063+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T07:45:18.066+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T07:45:18.086+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T07:45:18.095+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T07:45:18.191+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO ParquetOutputFormat: Parquet block size to 134217728
[2023-01-31T07:45:18.198+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO ParquetOutputFormat: Validation is off
[2023-01-31T07:45:18.199+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-01-31T07:45:18.200+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO ParquetOutputFormat: Parquet properties are:
[2023-01-31T07:45:18.200+0000] {spark_submit.py:495} INFO - Parquet page size to 1048576
[2023-01-31T07:45:18.200+0000] {spark_submit.py:495} INFO - Parquet dictionary page size to 1048576
[2023-01-31T07:45:18.201+0000] {spark_submit.py:495} INFO - Dictionary is true
[2023-01-31T07:45:18.201+0000] {spark_submit.py:495} INFO - Writer version is: PARQUET_1_0
[2023-01-31T07:45:18.201+0000] {spark_submit.py:495} INFO - Page size checking is: estimated
[2023-01-31T07:45:18.201+0000] {spark_submit.py:495} INFO - Min row count for page size check is: 100
[2023-01-31T07:45:18.202+0000] {spark_submit.py:495} INFO - Max row count for page size check is: 10000
[2023-01-31T07:45:18.202+0000] {spark_submit.py:495} INFO - Truncate length for column indexes is: 64
[2023-01-31T07:45:18.202+0000] {spark_submit.py:495} INFO - Truncate length for statistics min/max  is: 2147483647
[2023-01-31T07:45:18.202+0000] {spark_submit.py:495} INFO - Bloom filter enabled: false
[2023-01-31T07:45:18.202+0000] {spark_submit.py:495} INFO - Max Bloom filter size for a column is 1048576
[2023-01-31T07:45:18.203+0000] {spark_submit.py:495} INFO - Bloom filter expected number of distinct values are: null
[2023-01-31T07:45:18.203+0000] {spark_submit.py:495} INFO - Page row count limit to 20000
[2023-01-31T07:45:18.203+0000] {spark_submit.py:495} INFO - Writing page checksums is: on
[2023-01-31T07:45:18.428+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-01-31T07:45:18.429+0000] {spark_submit.py:495} INFO - {
[2023-01-31T07:45:18.429+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-31T07:45:18.429+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-31T07:45:18.430+0000] {spark_submit.py:495} INFO - "name" : "measured_at",
[2023-01-31T07:45:18.430+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T07:45:18.430+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T07:45:18.430+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.430+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.431+0000] {spark_submit.py:495} INFO - "name" : "created_at",
[2023-01-31T07:45:18.431+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T07:45:18.431+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T07:45:18.431+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.431+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.431+0000] {spark_submit.py:495} INFO - "name" : "Biomass - Actual Aggregated",
[2023-01-31T07:45:18.432+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.432+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.432+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.432+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.432+0000] {spark_submit.py:495} INFO - "name" : "Fossil Brown coal/Lignite - Actual Aggregated",
[2023-01-31T07:45:18.432+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.433+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.433+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.433+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.433+0000] {spark_submit.py:495} INFO - "name" : "Fossil Gas - Actual Aggregated",
[2023-01-31T07:45:18.433+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.433+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.433+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.434+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.434+0000] {spark_submit.py:495} INFO - "name" : "Fossil Hard coal - Actual Aggregated",
[2023-01-31T07:45:18.434+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.434+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.434+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.435+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.435+0000] {spark_submit.py:495} INFO - "name" : "Fossil Oil - Actual Aggregated",
[2023-01-31T07:45:18.442+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.443+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.443+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.443+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.443+0000] {spark_submit.py:495} INFO - "name" : "Geothermal - Actual Aggregated",
[2023-01-31T07:45:18.449+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.449+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.449+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.450+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.450+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Aggregated",
[2023-01-31T07:45:18.450+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.450+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.450+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.450+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.450+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Consumption",
[2023-01-31T07:45:18.451+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.451+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.451+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.451+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.451+0000] {spark_submit.py:495} INFO - "name" : "Hydro Run-of-river and poundage - Actual Aggregated",
[2023-01-31T07:45:18.451+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.452+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.452+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.452+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.452+0000] {spark_submit.py:495} INFO - "name" : "Hydro Water Reservoir - Actual Aggregated",
[2023-01-31T07:45:18.452+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.452+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.452+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.453+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.453+0000] {spark_submit.py:495} INFO - "name" : "Nuclear - Actual Aggregated",
[2023-01-31T07:45:18.453+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.453+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.453+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.453+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.453+0000] {spark_submit.py:495} INFO - "name" : "Other - Actual Aggregated",
[2023-01-31T07:45:18.454+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.454+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.454+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.454+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.454+0000] {spark_submit.py:495} INFO - "name" : "Other renewable - Actual Aggregated",
[2023-01-31T07:45:18.454+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.454+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.455+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.455+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.455+0000] {spark_submit.py:495} INFO - "name" : "Solar - Actual Aggregated",
[2023-01-31T07:45:18.455+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.455+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.455+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.455+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.473+0000] {spark_submit.py:495} INFO - "name" : "Waste - Actual Aggregated",
[2023-01-31T07:45:18.473+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.473+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.473+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.474+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.474+0000] {spark_submit.py:495} INFO - "name" : "Wind Offshore - Actual Aggregated",
[2023-01-31T07:45:18.474+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.474+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.474+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.474+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.474+0000] {spark_submit.py:495} INFO - "name" : "Wind Onshore - Actual Aggregated",
[2023-01-31T07:45:18.474+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.475+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.475+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.475+0000] {spark_submit.py:495} INFO - } ]
[2023-01-31T07:45:18.475+0000] {spark_submit.py:495} INFO - }
[2023-01-31T07:45:18.475+0000] {spark_submit.py:495} INFO - and corresponding Parquet message type:
[2023-01-31T07:45:18.475+0000] {spark_submit.py:495} INFO - message spark_schema {
[2023-01-31T07:45:18.475+0000] {spark_submit.py:495} INFO - required int96 measured_at;
[2023-01-31T07:45:18.475+0000] {spark_submit.py:495} INFO - required int96 created_at;
[2023-01-31T07:45:18.475+0000] {spark_submit.py:495} INFO - optional double Biomass - Actual Aggregated;
[2023-01-31T07:45:18.476+0000] {spark_submit.py:495} INFO - optional double Fossil Brown coal/Lignite - Actual Aggregated;
[2023-01-31T07:45:18.476+0000] {spark_submit.py:495} INFO - optional double Fossil Gas - Actual Aggregated;
[2023-01-31T07:45:18.476+0000] {spark_submit.py:495} INFO - optional double Fossil Hard coal - Actual Aggregated;
[2023-01-31T07:45:18.476+0000] {spark_submit.py:495} INFO - optional double Fossil Oil - Actual Aggregated;
[2023-01-31T07:45:18.476+0000] {spark_submit.py:495} INFO - optional double Geothermal - Actual Aggregated;
[2023-01-31T07:45:18.476+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Aggregated;
[2023-01-31T07:45:18.476+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Consumption;
[2023-01-31T07:45:18.476+0000] {spark_submit.py:495} INFO - optional double Hydro Run-of-river and poundage - Actual Aggregated;
[2023-01-31T07:45:18.476+0000] {spark_submit.py:495} INFO - optional double Hydro Water Reservoir - Actual Aggregated;
[2023-01-31T07:45:18.481+0000] {spark_submit.py:495} INFO - optional double Nuclear - Actual Aggregated;
[2023-01-31T07:45:18.482+0000] {spark_submit.py:495} INFO - optional double Other - Actual Aggregated;
[2023-01-31T07:45:18.482+0000] {spark_submit.py:495} INFO - optional double Other renewable - Actual Aggregated;
[2023-01-31T07:45:18.482+0000] {spark_submit.py:495} INFO - optional double Solar - Actual Aggregated;
[2023-01-31T07:45:18.482+0000] {spark_submit.py:495} INFO - optional double Waste - Actual Aggregated;
[2023-01-31T07:45:18.482+0000] {spark_submit.py:495} INFO - optional double Wind Offshore - Actual Aggregated;
[2023-01-31T07:45:18.483+0000] {spark_submit.py:495} INFO - optional double Wind Onshore - Actual Aggregated;
[2023-01-31T07:45:18.483+0000] {spark_submit.py:495} INFO - }
[2023-01-31T07:45:18.483+0000] {spark_submit.py:495} INFO - 
[2023-01-31T07:45:18.483+0000] {spark_submit.py:495} INFO - 
[2023-01-31T07:45:19.632+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:19 INFO CodecPool: Got brand-new compressor [.snappy]
[2023-01-31T07:45:28.905+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:28 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675150864789-6fd9dd0e-babd-472b-abda-fdd73ca8d726/_temporary/0/_temporary/' directory.
[2023-01-31T07:45:28.906+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:28 INFO FileOutputCommitter: Saved output of task 'attempt_202301310745178007248164904776713_0008_m_000000_8' to gs://entsoe_temp_1009/.spark-bigquery-local-1675150864789-6fd9dd0e-babd-472b-abda-fdd73ca8d726/_temporary/0/task_202301310745178007248164904776713_0008_m_000000
[2023-01-31T07:45:28.912+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:28 INFO SparkHadoopMapRedUtil: attempt_202301310745178007248164904776713_0008_m_000000_8: Committed. Elapsed time: 2800 ms.
[2023-01-31T07:45:29.002+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:29 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2588 bytes result sent to driver
[2023-01-31T07:45:29.041+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:29 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 11297 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:45:29.042+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:29 INFO DAGScheduler: ResultStage 8 (save at BigQueryWriteHelper.java:105) finished in 11.464 s
[2023-01-31T07:45:29.042+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:29 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:45:29.058+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:29 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2023-01-31T07:45:29.062+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2023-01-31T07:45:29.069+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:29 INFO DAGScheduler: Job 8 finished: save at BigQueryWriteHelper.java:105, took 11.502740 s
[2023-01-31T07:45:29.090+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:29 INFO FileFormatWriter: Start to commit write Job 5a70a4d7-9401-4583-8b5e-eee8106de026.
[2023-01-31T07:45:30.506+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:30 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675150864789-6fd9dd0e-babd-472b-abda-fdd73ca8d726/_temporary/0/task_202301310745178007248164904776713_0008_m_000000/' directory.
[2023-01-31T07:45:30.940+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:30 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675150864789-6fd9dd0e-babd-472b-abda-fdd73ca8d726/' directory.
[2023-01-31T07:45:31.284+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:31 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 81d5fcd0285b:40621 in memory (size: 74.3 KiB, free: 434.4 MiB)
[2023-01-31T07:45:32.065+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:32 INFO FileFormatWriter: Write Job 5a70a4d7-9401-4583-8b5e-eee8106de026 committed. Elapsed time: 2969 ms.
[2023-01-31T07:45:32.108+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:32 INFO FileFormatWriter: Finished processing stats for write job 5a70a4d7-9401-4583-8b5e-eee8106de026.
[2023-01-31T07:45:34.309+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:34 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=entsoe_analytics_1009, projectId=rafzul-analytics-1009, tableId=TEST_total_generation_staging}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=null, ignoreUnknownValue=null, sourceUris=[gs://entsoe_temp_1009/.spark-bigquery-local-1675150864789-6fd9dd0e-babd-472b-abda-fdd73ca8d726/part-00000-0796d8c8-7663-4889-be4e-9349002f17f6-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=true, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=rafzul-analytics-1009, job=a8873bea-20be-4204-bab6-8cdb3f3155ca, location=US}
[2023-01-31T07:45:38.025+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:38 INFO BigQueryClient: Done loading to rafzul-analytics-1009.entsoe_analytics_1009.TEST_total_generation_staging. jobId: JobId{project=rafzul-analytics-1009, job=a8873bea-20be-4204-bab6-8cdb3f3155ca, location=US}
[2023-01-31T07:45:40.655+0000] {spark_submit.py:495} INFO - MANTEP!!!!
[2023-01-31T07:45:41.044+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:41 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-31T07:45:41.085+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:41 INFO SparkUI: Stopped Spark web UI at http://81d5fcd0285b:4045
[2023-01-31T07:45:41.135+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:41 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-31T07:45:41.181+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:41 INFO MemoryStore: MemoryStore cleared
[2023-01-31T07:45:41.184+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:41 INFO BlockManager: BlockManager stopped
[2023-01-31T07:45:41.198+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:41 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-31T07:45:41.208+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:41 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-31T07:45:41.231+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:41 INFO SparkContext: Successfully stopped SparkContext
[2023-01-31T07:45:41.232+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:41 INFO ShutdownHookManager: Shutdown hook called
[2023-01-31T07:45:41.235+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-5cf6fffa-7fd1-4d04-9646-348a17d9d65c
[2023-01-31T07:45:41.255+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-51c8cce9-1109-42d9-bdb0-15631ad29eeb/pyspark-139f85fa-cb62-4bc4-87c8-55854d8f9fa3
[2023-01-31T07:45:41.270+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-51c8cce9-1109-42d9-bdb0-15631ad29eeb
[2023-01-31T07:45:41.621+0000] {taskinstance.py:1322} INFO - Marking task as SUCCESS. dag_id=entsoe-energydata-backfill, task_id=stage_total_generation, execution_date=20210101T020000, start_date=20230131T073901, end_date=20230131T074541
[2023-01-31T07:45:41.711+0000] {local_task_job.py:159} INFO - Task exited with return code 0
[2023-01-31T07:45:41.776+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T08:21:35.281+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T02:00:00+00:00 [queued]>
[2023-01-31T08:21:35.310+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T02:00:00+00:00 [queued]>
[2023-01-31T08:21:35.311+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T08:21:35.311+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T08:21:35.311+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T08:21:35.345+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 02:00:00+00:00
[2023-01-31T08:21:35.376+0000] {standard_task_runner.py:55} INFO - Started process 2076 to run task
[2023-01-31T08:21:35.395+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T02:00:00+00:00', '--job-id', '923', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp1zv6mcfg']
[2023-01-31T08:21:35.401+0000] {standard_task_runner.py:83} INFO - Job 923: Subtask stage_total_generation
[2023-01-31T08:21:35.764+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T02:00:00+00:00 [running]> on host 81d5fcd0285b
[2023-01-31T08:21:36.039+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T02:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T02:00:00+00:00
[2023-01-31T08:21:36.054+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T08:21:36.057+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010200 202101010300 DE_TENNET
[2023-01-31T08:21:56.250+0000] {spark_submit.py:495} INFO - {{ start_date }}
[2023-01-31T08:21:56.251+0000] {spark_submit.py:495} INFO - mulai periode: {{ start_date }}
[2023-01-31T08:21:56.251+0000] {spark_submit.py:495} INFO - selesai periode: {{ end_date }}
[2023-01-31T08:21:56.534+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:56 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T08:21:56.987+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T08:21:57.607+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:57 INFO ResourceUtils: ==============================================================
[2023-01-31T08:21:57.611+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:57 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T08:21:57.611+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:57 INFO ResourceUtils: ==============================================================
[2023-01-31T08:21:57.612+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:57 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T08:21:57.938+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:57 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T08:21:57.984+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:57 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T08:21:57.993+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:57 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T08:21:58.280+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:58 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T08:21:58.294+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:58 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T08:21:58.296+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:58 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T08:21:58.297+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:58 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T08:21:58.307+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T08:22:01.099+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:01 INFO Utils: Successfully started service 'sparkDriver' on port 44677.
[2023-01-31T08:22:01.603+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:01 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T08:22:01.922+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:01 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T08:22:02.266+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:02 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T08:22:02.271+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:02 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T08:22:02.322+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:02 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T08:22:02.577+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:02 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-269116a0-88d2-4f1e-80ca-369a4fb51d98
[2023-01-31T08:22:02.761+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:02 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T08:22:02.942+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:02 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T08:22:07.110+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:07 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T08:22:07.111+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:07 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2023-01-31T08:22:07.111+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:07 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[2023-01-31T08:22:07.262+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:07 INFO Utils: Successfully started service 'SparkUI' on port 4043.
[2023-01-31T08:22:07.436+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:07 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://81d5fcd0285b:44677/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675153316495
[2023-01-31T08:22:07.436+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:07 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://81d5fcd0285b:44677/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675153316495
[2023-01-31T08:22:07.906+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:07 INFO Executor: Starting executor ID driver on host 81d5fcd0285b
[2023-01-31T08:22:07.942+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:07 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T08:22:08.070+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:08 INFO Executor: Fetching spark://81d5fcd0285b:44677/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675153316495
[2023-01-31T08:22:08.427+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:08 INFO TransportClientFactory: Successfully created connection to 81d5fcd0285b/172.19.0.5:44677 after 163 ms (0 ms spent in bootstraps)
[2023-01-31T08:22:08.470+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:08 INFO Utils: Fetching spark://81d5fcd0285b:44677/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-26a495d4-9a4b-433c-b97d-255055012676/userFiles-51883fd1-ab6a-4cf9-b33c-4e54c10c7dbb/fetchFileTemp7704102012157126690.tmp
[2023-01-31T08:22:09.721+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:09 INFO Executor: Adding file:/tmp/spark-26a495d4-9a4b-433c-b97d-255055012676/userFiles-51883fd1-ab6a-4cf9-b33c-4e54c10c7dbb/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T08:22:09.725+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:09 INFO Executor: Fetching spark://81d5fcd0285b:44677/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675153316495
[2023-01-31T08:22:09.743+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:09 INFO Utils: Fetching spark://81d5fcd0285b:44677/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-26a495d4-9a4b-433c-b97d-255055012676/userFiles-51883fd1-ab6a-4cf9-b33c-4e54c10c7dbb/fetchFileTemp4129735993672715032.tmp
[2023-01-31T08:22:10.562+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:10 INFO Executor: Adding file:/tmp/spark-26a495d4-9a4b-433c-b97d-255055012676/userFiles-51883fd1-ab6a-4cf9-b33c-4e54c10c7dbb/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T08:22:10.610+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:10 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43269.
[2023-01-31T08:22:10.611+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:10 INFO NettyBlockTransferService: Server created on 81d5fcd0285b:43269
[2023-01-31T08:22:10.623+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:10 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T08:22:10.654+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:10 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 43269, None)
[2023-01-31T08:22:10.674+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:10 INFO BlockManagerMasterEndpoint: Registering block manager 81d5fcd0285b:43269 with 434.4 MiB RAM, BlockManagerId(driver, 81d5fcd0285b, 43269, None)
[2023-01-31T08:22:10.690+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:10 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 43269, None)
[2023-01-31T08:22:10.694+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:10 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 81d5fcd0285b, 43269, None)
[2023-01-31T08:22:16.282+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:16 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T08:22:16.317+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:16 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T08:22:31.870+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:31 INFO InMemoryFileIndex: It took 757 ms to list leaf files for 1 paths.
[2023-01-31T08:22:32.470+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:32 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T08:22:32.716+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:32 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T08:22:32.726+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:32 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 81d5fcd0285b:43269 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T08:22:32.734+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:32 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T08:22:35.412+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:35 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T08:22:35.521+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:35 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T08:22:35.749+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:35 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T08:22:35.945+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:35 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T08:22:35.984+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:35 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T08:22:35.986+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:35 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:22:36.019+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:36 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:22:36.042+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:36 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T08:22:36.310+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:36 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T08:22:36.323+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:36 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T08:22:36.327+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:36 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 81d5fcd0285b:43269 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T08:22:36.333+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:36 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:22:36.424+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:22:36.429+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:36 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T08:22:36.668+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:36 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T08:22:36.716+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:36 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T08:22:37.441+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:37 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010200__202101010300.json:0+9369
[2023-01-31T08:22:38.712+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:38 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3437 bytes result sent to driver
[2023-01-31T08:22:38.806+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:38 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2212 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:22:38.812+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:38 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T08:22:38.829+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:38 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 2.710 s
[2023-01-31T08:22:38.838+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:38 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:22:38.839+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T08:22:38.846+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:38 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 3.092850 s
[2023-01-31T08:22:40.805+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:40 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 81d5fcd0285b:43269 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T08:22:40.904+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:40 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 81d5fcd0285b:43269 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T08:23:02.918+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:02 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:02.923+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:02 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T08:23:02.934+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:02 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:05.097+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:05 INFO CodeGenerator: Code generated in 986.433228 ms
[2023-01-31T08:23:05.136+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:05 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T08:23:05.180+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:05 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T08:23:05.188+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:05 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 81d5fcd0285b:43269 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T08:23:05.192+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:05 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T08:23:05.245+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:05 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:06.479+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:06 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T08:23:06.480+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:06 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T08:23:06.480+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:06 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T08:23:06.480+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:06 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:06.480+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:06 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:06.498+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:06 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T08:23:06.600+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:06 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T08:23:06.734+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:06 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T08:23:06.760+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:06 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 81d5fcd0285b:43269 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T08:23:06.782+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:06 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:06.847+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:06.848+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:06 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T08:23:06.929+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:06 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:06.959+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:06 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T08:23:07.543+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:07 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010200__202101010300.json, range: 0-9369, partition values: [empty row]
[2023-01-31T08:23:09.232+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:09 INFO CodeGenerator: Code generated in 1335.589802 ms
[2023-01-31T08:23:11.108+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:11 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1671 bytes result sent to driver
[2023-01-31T08:23:11.124+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:11 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 4255 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:11.131+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:11 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 4.608 s
[2023-01-31T08:23:11.134+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:11 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:11.153+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:11 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T08:23:11.154+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T08:23:11.163+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:11 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 4.686363 s
[2023-01-31T08:23:11.770+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:11 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:11.771+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:11 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T08:23:11.786+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:11 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:12.277+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:12 INFO CodeGenerator: Code generated in 343.546594 ms
[2023-01-31T08:23:12.393+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:12 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T08:23:12.726+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:12 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T08:23:12.740+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:12 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 81d5fcd0285b:43269 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T08:23:12.742+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:12 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T08:23:12.754+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:12.946+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:12 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T08:23:12.948+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:12 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T08:23:12.948+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:12 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T08:23:12.949+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:12 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:12.949+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:12 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:12.970+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:12 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T08:23:12.994+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:12 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T08:23:13.008+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:13 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T08:23:13.012+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:13 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 81d5fcd0285b:43269 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T08:23:13.020+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:13 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:13.022+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:13.025+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:13 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T08:23:13.054+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:13 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:13.060+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:13 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T08:23:13.150+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:13 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010200__202101010300.json, range: 0-9369, partition values: [empty row]
[2023-01-31T08:23:13.894+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:13 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1585 bytes result sent to driver
[2023-01-31T08:23:14.101+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:14 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1058 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:14.106+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:14 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 1.151 s
[2023-01-31T08:23:14.113+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:14 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:14.203+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:14 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T08:23:14.210+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T08:23:14.211+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:14 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 1.259888 s
[2023-01-31T08:23:16.454+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:16 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:16.455+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:16 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T08:23:16.455+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:16 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:19.702+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:19 INFO CodeGenerator: Code generated in 1566.705809 ms
[2023-01-31T08:23:19.718+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:19 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T08:23:19.796+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:19 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T08:23:19.806+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:19 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 81d5fcd0285b:43269 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T08:23:19.816+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:19 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T08:23:19.984+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:19 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:20.636+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T08:23:20.698+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T08:23:20.700+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T08:23:20.702+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:20.703+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:20.703+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T08:23:20.776+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.6 MiB)
[2023-01-31T08:23:20.833+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.6 MiB)
[2023-01-31T08:23:20.838+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 81d5fcd0285b:43269 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T08:23:20.846+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:20.847+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:20.847+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T08:23:20.851+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:20.852+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T08:23:21.044+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010200__202101010300.json, range: 0-9369, partition values: [empty row]
[2023-01-31T08:23:22.147+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:22 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-01-31T08:23:22.147+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:22 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 1287 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:22.148+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:22 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 1.400 s
[2023-01-31T08:23:22.148+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:22 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:22.148+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:22 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T08:23:22.148+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T08:23:22.149+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:22 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 1.509433 s
[2023-01-31T08:23:22.259+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:22 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:22.264+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:22 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T08:23:22.270+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:22 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:22.524+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:22 INFO CodeGenerator: Code generated in 170.48268 ms
[2023-01-31T08:23:22.626+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:22 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.4 MiB)
[2023-01-31T08:23:22.800+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:22 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T08:23:22.824+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:22 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 81d5fcd0285b:43269 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T08:23:22.890+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:22 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T08:23:22.891+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:22 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:22.916+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:22 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T08:23:22.917+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:22 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T08:23:22.917+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:22 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T08:23:22.921+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:22 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:22.922+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:22 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:22.922+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:22 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T08:23:22.922+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:22 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-31T08:23:22.924+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:22 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-01-31T08:23:22.930+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:22 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 81d5fcd0285b:43269 (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T08:23:22.934+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:22 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:22.938+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:22.942+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:22 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T08:23:22.945+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:22 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:22.947+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:22 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T08:23:23.004+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:23 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010200__202101010300.json, range: 0-9369, partition values: [empty row]
[2023-01-31T08:23:23.358+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:23 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1586 bytes result sent to driver
[2023-01-31T08:23:23.392+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:23 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 446 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:23.392+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:23 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T08:23:23.399+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:23 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 0.489 s
[2023-01-31T08:23:23.403+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:23 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:23.404+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T08:23:23.410+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:23 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 0.513213 s
[2023-01-31T08:23:26.476+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:26.509+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T08:23:26.519+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:26.784+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 81d5fcd0285b:43269 in memory (size: 7.7 KiB, free: 434.2 MiB)
[2023-01-31T08:23:26.927+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 81d5fcd0285b:43269 in memory (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T08:23:27.018+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 81d5fcd0285b:43269 in memory (size: 7.6 KiB, free: 434.3 MiB)
[2023-01-31T08:23:27.245+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO CodeGenerator: Code generated in 482.728243 ms
[2023-01-31T08:23:27.279+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.3 MiB)
[2023-01-31T08:23:27.386+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T08:23:27.390+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 81d5fcd0285b:43269 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T08:23:27.396+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T08:23:27.410+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:27.540+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T08:23:27.542+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) with 1 output partitions
[2023-01-31T08:23:27.543+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201)
[2023-01-31T08:23:27.543+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:27.544+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:27.553+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201), which has no missing parents
[2023-01-31T08:23:27.587+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.2 MiB)
[2023-01-31T08:23:27.604+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.2 MiB)
[2023-01-31T08:23:27.609+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 81d5fcd0285b:43269 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T08:23:27.619+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:27.621+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:27.623+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T08:23:27.640+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:27.645+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T08:23:27.925+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010200__202101010300.json, range: 0-9369, partition values: [empty row]
[2023-01-31T08:23:28.392+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2567 bytes result sent to driver
[2023-01-31T08:23:28.424+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 785 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:28.425+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T08:23:28.428+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) finished in 0.878 s
[2023-01-31T08:23:28.429+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:28.429+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T08:23:28.456+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201, took 0.917680 s
[2023-01-31T08:23:28.745+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:28.753+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T08:23:28.757+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:29.364+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO CodeGenerator: Code generated in 386.277979 ms
[2023-01-31T08:23:29.468+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 433.0 MiB)
[2023-01-31T08:23:29.746+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.9 MiB)
[2023-01-31T08:23:29.764+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 81d5fcd0285b:43269 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T08:23:29.769+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T08:23:29.794+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:29.911+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T08:23:29.914+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T08:23:29.916+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T08:23:29.917+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:29.917+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:29.923+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T08:23:29.940+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 432.9 MiB)
[2023-01-31T08:23:29.976+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.9 MiB)
[2023-01-31T08:23:29.981+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 81d5fcd0285b:43269 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T08:23:29.992+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:29.994+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:29.996+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T08:23:29.997+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:29.997+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T08:23:30.097+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:30 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010200__202101010300.json, range: 0-9369, partition values: [empty row]
[2023-01-31T08:23:30.470+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:30 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-01-31T08:23:30.481+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:30 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 483 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:30.482+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:30 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T08:23:30.483+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:30 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 0.555 s
[2023-01-31T08:23:30.484+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:30 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:30.485+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T08:23:30.486+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:30 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 0.567401 s
[2023-01-31T08:23:31.049+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:31.051+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T08:23:31.058+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:32.226+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO CodeGenerator: Code generated in 721.058518 ms
[2023-01-31T08:23:32.285+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.7 MiB)
[2023-01-31T08:23:32.667+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.7 MiB)
[2023-01-31T08:23:32.680+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 81d5fcd0285b:43269 (size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T08:23:32.688+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T08:23:32.741+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:32.890+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T08:23:32.891+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T08:23:32.891+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T08:23:32.892+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:32.893+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:32.900+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T08:23:32.904+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.6 MiB)
[2023-01-31T08:23:32.919+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.6 MiB)
[2023-01-31T08:23:32.923+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 81d5fcd0285b:43269 (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T08:23:32.931+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:32.934+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:32.937+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T08:23:32.944+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:32.945+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T08:23:33.058+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010200__202101010300.json, range: 0-9369, partition values: [empty row]
[2023-01-31T08:23:33.308+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1729 bytes result sent to driver
[2023-01-31T08:23:33.317+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 373 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:33.317+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T08:23:33.319+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 0.418 s
[2023-01-31T08:23:33.319+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:33.320+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T08:23:33.320+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 0.430425 s
[2023-01-31T08:23:34.306+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 81d5fcd0285b:43269 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T08:23:34.480+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 81d5fcd0285b:43269 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T08:23:34.772+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 81d5fcd0285b:43269 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T08:23:34.984+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 81d5fcd0285b:43269 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T08:23:35.075+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:35 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 81d5fcd0285b:43269 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T08:23:35.192+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:35 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 81d5fcd0285b:43269 in memory (size: 13.4 KiB, free: 434.3 MiB)
[2023-01-31T08:23:35.246+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:35 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 81d5fcd0285b:43269 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T08:23:35.390+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:35 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 81d5fcd0285b:43269 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T08:23:35.462+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:35 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 81d5fcd0285b:43269 in memory (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T12:01:42.018+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T02:00:00+00:00 [queued]>
[2023-01-31T12:01:42.048+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T02:00:00+00:00 [queued]>
[2023-01-31T12:01:42.049+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T12:01:42.049+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T12:01:42.050+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T12:01:42.119+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 02:00:00+00:00
[2023-01-31T12:01:42.151+0000] {standard_task_runner.py:55} INFO - Started process 156 to run task
[2023-01-31T12:01:42.225+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T02:00:00+00:00', '--job-id', '979', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmprlmukthh']
[2023-01-31T12:01:42.253+0000] {standard_task_runner.py:83} INFO - Job 979: Subtask stage_total_generation
[2023-01-31T12:01:42.617+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T02:00:00+00:00 [running]> on host 8124f810dec3
[2023-01-31T12:01:43.241+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T02:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T02:00:00+00:00
[2023-01-31T12:01:43.327+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T12:01:43.329+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 398, in submit
    spark_submit_cmd = self._build_spark_submit_command(application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 344, in _build_spark_submit_command
    self.log.info("Spark-Submit cmd: %s", self._mask_cmd(connection_cmd))
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 256, in _mask_cmd
    " ".join(connection_cmd),
TypeError: sequence item 10: expected str instance, DateTime found
[2023-01-31T12:01:43.373+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=entsoe-energydata-backfill, task_id=stage_total_generation, execution_date=20210101T020000, start_date=20230131T120142, end_date=20230131T120143
[2023-01-31T12:01:43.415+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 979 for task stage_total_generation (sequence item 10: expected str instance, DateTime found; 156)
[2023-01-31T12:01:43.491+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-31T12:01:43.614+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T12:08:50.426+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T02:00:00+00:00 [queued]>
[2023-01-31T12:08:50.483+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T02:00:00+00:00 [queued]>
[2023-01-31T12:08:50.484+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T12:08:50.485+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T12:08:50.485+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T12:08:50.524+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 02:00:00+00:00
[2023-01-31T12:08:50.559+0000] {standard_task_runner.py:55} INFO - Started process 600 to run task
[2023-01-31T12:08:50.623+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T02:00:00+00:00', '--job-id', '998', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmphh2083d3']
[2023-01-31T12:08:50.656+0000] {standard_task_runner.py:83} INFO - Job 998: Subtask stage_total_generation
[2023-01-31T12:08:51.019+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T02:00:00+00:00 [running]> on host 8124f810dec3
[2023-01-31T12:08:51.781+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T02:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T02:00:00+00:00
[2023-01-31T12:08:51.808+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T12:08:51.810+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010200 202101010300 DE_TENNET 202101010000 202101010600
[2023-01-31T12:09:24.214+0000] {spark_submit.py:495} INFO - mulai periode: 202101010000
[2023-01-31T12:09:24.218+0000] {spark_submit.py:495} INFO - selesai periode: 202101010600
[2023-01-31T12:09:24.641+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:24 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T12:09:25.296+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T12:09:25.820+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:25 INFO ResourceUtils: ==============================================================
[2023-01-31T12:09:25.834+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:25 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T12:09:25.851+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:25 INFO ResourceUtils: ==============================================================
[2023-01-31T12:09:25.851+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:25 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T12:09:26.050+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:26 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T12:09:26.138+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:26 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T12:09:26.139+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:26 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T12:09:26.662+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:26 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T12:09:26.667+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:26 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T12:09:26.672+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:26 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T12:09:26.676+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:26 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T12:09:26.681+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:26 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T12:09:29.134+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:29 INFO Utils: Successfully started service 'sparkDriver' on port 35233.
[2023-01-31T12:09:30.032+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:30 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T12:09:30.781+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:30 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T12:09:30.925+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:30 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T12:09:30.933+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:30 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T12:09:31.038+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:31 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T12:09:31.435+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:31 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-2870a1d8-a8ce-4952-8a6f-6b86d1729647
[2023-01-31T12:09:31.678+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:31 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T12:09:31.961+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:31 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T12:09:35.072+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:35 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T12:09:35.122+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:35 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2023-01-31T12:09:35.157+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:35 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[2023-01-31T12:09:35.230+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:35 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[2023-01-31T12:09:35.248+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:35 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[2023-01-31T12:09:35.295+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:35 INFO Utils: Successfully started service 'SparkUI' on port 4045.
[2023-01-31T12:09:35.678+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:35 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://8124f810dec3:35233/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675166964522
[2023-01-31T12:09:35.682+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:35 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://8124f810dec3:35233/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675166964522
[2023-01-31T12:09:36.126+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:36 INFO Executor: Starting executor ID driver on host 8124f810dec3
[2023-01-31T12:09:36.154+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:36 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T12:09:36.267+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:36 INFO Executor: Fetching spark://8124f810dec3:35233/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675166964522
[2023-01-31T12:09:37.122+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:37 INFO TransportClientFactory: Successfully created connection to 8124f810dec3/172.20.0.9:35233 after 403 ms (0 ms spent in bootstraps)
[2023-01-31T12:09:37.262+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:37 INFO Utils: Fetching spark://8124f810dec3:35233/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-0831f003-3491-46f6-b19d-a41317cea1ee/userFiles-bd2c42eb-5d92-4bd6-99d9-14df7c87d13a/fetchFileTemp4391695277213579767.tmp
[2023-01-31T12:09:41.246+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:41 INFO Executor: Adding file:/tmp/spark-0831f003-3491-46f6-b19d-a41317cea1ee/userFiles-bd2c42eb-5d92-4bd6-99d9-14df7c87d13a/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T12:09:41.266+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:41 INFO Executor: Fetching spark://8124f810dec3:35233/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675166964522
[2023-01-31T12:09:41.277+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:41 INFO Utils: Fetching spark://8124f810dec3:35233/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-0831f003-3491-46f6-b19d-a41317cea1ee/userFiles-bd2c42eb-5d92-4bd6-99d9-14df7c87d13a/fetchFileTemp4475807444887265275.tmp
[2023-01-31T12:09:42.474+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:42 INFO Executor: Adding file:/tmp/spark-0831f003-3491-46f6-b19d-a41317cea1ee/userFiles-bd2c42eb-5d92-4bd6-99d9-14df7c87d13a/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T12:09:42.613+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:42 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42079.
[2023-01-31T12:09:42.613+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:42 INFO NettyBlockTransferService: Server created on 8124f810dec3:42079
[2023-01-31T12:09:42.618+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:42 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T12:09:42.679+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:42 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 42079, None)
[2023-01-31T12:09:42.734+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:42 INFO BlockManagerMasterEndpoint: Registering block manager 8124f810dec3:42079 with 434.4 MiB RAM, BlockManagerId(driver, 8124f810dec3, 42079, None)
[2023-01-31T12:09:42.746+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:42 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 42079, None)
[2023-01-31T12:09:42.747+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:42 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 8124f810dec3, 42079, None)
[2023-01-31T12:09:51.498+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:51 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T12:09:51.639+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:51 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T12:10:09.378+0000] {spark_submit.py:495} INFO - 23/01/31 12:10:09 INFO InMemoryFileIndex: It took 1035 ms to list leaf files for 1 paths.
[2023-01-31T12:10:15.069+0000] {spark_submit.py:495} INFO - 23/01/31 12:10:15 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T12:10:24.443+0000] {spark_submit.py:495} INFO - 23/01/31 12:10:24 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T12:10:25.637+0000] {spark_submit.py:495} INFO - 23/01/31 12:10:25 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 8124f810dec3:42079 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T12:10:26.031+0000] {spark_submit.py:495} INFO - 23/01/31 12:10:26 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T12:13:26.230+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:26 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T12:13:26.458+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:26 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T12:13:26.856+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:26 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T12:13:26.988+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:26 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T12:13:26.990+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:26 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T12:13:26.995+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:26 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T12:13:27.008+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:27 INFO DAGScheduler: Missing parents: List()
[2023-01-31T12:13:27.074+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:27 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T12:13:29.072+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:29 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T12:13:29.570+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:29 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T12:13:29.585+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:29 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 8124f810dec3:42079 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T12:13:29.643+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:29 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T12:13:30.067+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T12:13:30.073+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:30 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T12:13:30.894+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:30 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (8124f810dec3, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T12:13:31.515+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:31 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T12:13:34.873+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:34 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010200__202101010300.json:0+9369
[2023-01-31T12:14:13.327+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:13 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3437 bytes result sent to driver
[2023-01-31T12:14:14.446+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:14 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 43723 ms on 8124f810dec3 (executor driver) (1/1)
[2023-01-31T12:14:14.487+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:14 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T12:14:14.884+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:14 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 47.630 s
[2023-01-31T12:14:14.904+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:14 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T12:14:14.906+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T12:14:14.914+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:14 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 48.054400 s
[2023-01-31T12:14:17.545+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:17 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 8124f810dec3:42079 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T12:14:17.605+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:17 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 8124f810dec3:42079 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T12:15:10.453+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:10 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T12:15:10.461+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:10 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T12:15:10.478+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:10 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T12:15:11.875+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:11 INFO CodeGenerator: Code generated in 612.522947 ms
[2023-01-31T12:15:11.927+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:11 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T12:15:11.989+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:11 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T12:15:11.992+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:11 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 8124f810dec3:42079 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T12:15:11.999+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:11 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T12:15:12.121+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T12:15:12.581+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:12 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T12:15:12.585+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:12 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T12:15:12.586+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:12 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T12:15:12.586+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:12 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T12:15:12.586+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:12 INFO DAGScheduler: Missing parents: List()
[2023-01-31T12:15:12.620+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:12 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T12:15:12.632+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:12 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T12:15:12.646+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:12 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T12:15:12.649+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:12 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 8124f810dec3:42079 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T12:15:12.655+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:12 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T12:15:12.657+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T12:15:12.658+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:12 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T12:15:12.688+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:12 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (8124f810dec3, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T12:15:12.699+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:12 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T12:15:13.176+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:13 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010200__202101010300.json, range: 0-9369, partition values: [empty row]
[2023-01-31T12:15:14.307+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:14 INFO CodeGenerator: Code generated in 901.514898 ms
[2023-01-31T12:15:14.744+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:14 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1628 bytes result sent to driver
[2023-01-31T12:15:14.758+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:14 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2090 ms on 8124f810dec3 (executor driver) (1/1)
[2023-01-31T12:15:14.765+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:14 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 2.134 s
[2023-01-31T12:15:14.766+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:14 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T12:15:14.771+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:14 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T12:15:14.772+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T12:15:14.777+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:14 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 2.194964 s
[2023-01-31T12:15:15.091+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T12:15:15.091+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T12:15:15.093+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T12:15:15.245+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO CodeGenerator: Code generated in 73.928713 ms
[2023-01-31T12:15:15.275+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T12:15:15.483+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T12:15:15.488+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 8124f810dec3:42079 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:15:15.495+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T12:15:15.496+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T12:15:15.558+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T12:15:15.563+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T12:15:15.563+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T12:15:15.564+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T12:15:15.564+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO DAGScheduler: Missing parents: List()
[2023-01-31T12:15:15.590+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T12:15:15.752+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T12:15:15.835+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T12:15:15.837+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 8124f810dec3:42079 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:15:15.840+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T12:15:15.842+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T12:15:15.843+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T12:15:15.857+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (8124f810dec3, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T12:15:15.873+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T12:15:16.011+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:16 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010200__202101010300.json, range: 0-9369, partition values: [empty row]
[2023-01-31T12:15:16.598+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:16 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1585 bytes result sent to driver
[2023-01-31T12:15:16.666+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:16 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 800 ms on 8124f810dec3 (executor driver) (1/1)
[2023-01-31T12:15:16.691+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:16 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 1.056 s
[2023-01-31T12:15:16.711+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:16 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T12:15:16.747+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:16 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T12:15:16.750+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T12:15:16.767+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:16 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 1.190748 s
[2023-01-31T12:15:19.723+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:19 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T12:15:19.756+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:19 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T12:15:19.767+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:19 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T12:15:20.936+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:20 INFO CodeGenerator: Code generated in 281.66859 ms
[2023-01-31T12:15:20.954+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:20 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T12:15:21.017+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T12:15:21.019+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 8124f810dec3:42079 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:15:21.032+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T12:15:21.047+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T12:15:21.225+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T12:15:21.228+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T12:15:21.231+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T12:15:21.231+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T12:15:21.232+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO DAGScheduler: Missing parents: List()
[2023-01-31T12:15:21.236+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T12:15:21.247+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 8124f810dec3:42079 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:15:21.248+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.6 MiB)
[2023-01-31T12:15:21.255+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.6 MiB)
[2023-01-31T12:15:21.255+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 8124f810dec3:42079 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T12:15:21.258+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T12:15:21.265+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T12:15:21.268+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T12:15:21.273+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (8124f810dec3, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T12:15:21.275+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T12:15:21.318+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 8124f810dec3:42079 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:15:21.319+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010200__202101010300.json, range: 0-9369, partition values: [empty row]
[2023-01-31T12:15:21.499+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-01-31T12:15:21.508+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 236 ms on 8124f810dec3 (executor driver) (1/1)
[2023-01-31T12:15:21.515+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 0.271 s
[2023-01-31T12:15:21.516+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T12:15:21.522+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T12:15:21.526+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T12:15:21.526+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 0.298536 s
[2023-01-31T12:15:21.705+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T12:15:21.705+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T12:15:21.707+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T12:15:21.738+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO CodeGenerator: Code generated in 20.192107 ms
[2023-01-31T12:15:21.798+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.5 MiB)
[2023-01-31T12:15:21.933+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T12:15:21.943+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 8124f810dec3:42079 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:15:21.946+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T12:15:21.957+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T12:15:22.026+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:22 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T12:15:22.029+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:22 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T12:15:22.029+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:22 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T12:15:22.031+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:22 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T12:15:22.031+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:22 INFO DAGScheduler: Missing parents: List()
[2023-01-31T12:15:22.041+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:22 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T12:15:22.118+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:22 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-31T12:15:22.152+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:22 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-01-31T12:15:22.202+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:22 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 8124f810dec3:42079 (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T12:15:22.219+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:22 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T12:15:22.222+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T12:15:22.241+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:22 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T12:15:22.246+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:22 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (8124f810dec3, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T12:15:22.247+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:22 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T12:15:22.334+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:22 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010200__202101010300.json, range: 0-9369, partition values: [empty row]
[2023-01-31T12:15:22.649+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:22 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1586 bytes result sent to driver
[2023-01-31T12:15:22.657+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:22 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 412 ms on 8124f810dec3 (executor driver) (1/1)
[2023-01-31T12:15:22.664+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:22 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 0.608 s
[2023-01-31T12:15:22.665+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:22 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T12:15:22.666+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:22 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T12:15:22.670+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T12:15:22.671+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:22 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 0.644505 s
[2023-01-31T12:15:23.910+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:23 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T12:15:23.916+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:23 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T12:15:23.921+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:23 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T12:15:24.233+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:24 INFO CodeGenerator: Code generated in 195.637798 ms
[2023-01-31T12:15:24.268+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:24 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.2 MiB)
[2023-01-31T12:15:24.339+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:24 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T12:15:24.342+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:24 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 8124f810dec3:42079 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:15:24.348+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:24 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T12:15:24.353+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:24 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T12:15:24.495+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:24 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T12:15:24.498+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:24 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T12:15:24.499+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:24 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T12:15:24.499+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:24 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T12:15:24.501+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:24 INFO DAGScheduler: Missing parents: List()
[2023-01-31T12:15:24.507+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:24 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T12:15:24.558+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:24 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.1 MiB)
[2023-01-31T12:15:24.570+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:24 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.1 MiB)
[2023-01-31T12:15:24.579+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:24 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 8124f810dec3:42079 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T12:15:24.585+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:24 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T12:15:24.588+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T12:15:24.591+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:24 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T12:15:24.602+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:24 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (8124f810dec3, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T12:15:24.605+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:24 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T12:15:24.805+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:24 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010200__202101010300.json, range: 0-9369, partition values: [empty row]
[2023-01-31T12:15:25.140+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:25 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2524 bytes result sent to driver
[2023-01-31T12:15:25.149+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:25 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 550 ms on 8124f810dec3 (executor driver) (1/1)
[2023-01-31T12:15:25.150+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:25 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T12:15:25.171+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:25 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 0.637 s
[2023-01-31T12:15:25.171+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:25 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T12:15:25.172+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T12:15:25.172+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:25 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 0.673831 s
[2023-01-31T12:15:25.606+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:25 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T12:15:25.609+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:25 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T12:15:25.611+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:25 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T12:15:26.466+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:26 INFO CodeGenerator: Code generated in 722.265262 ms
[2023-01-31T12:15:26.518+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:26 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 432.9 MiB)
[2023-01-31T12:15:26.842+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:26 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.9 MiB)
[2023-01-31T12:15:26.852+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:26 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 8124f810dec3:42079 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:15:26.853+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:26 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T12:15:26.872+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:26 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T12:15:27.322+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:27 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T12:15:27.333+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:27 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T12:15:27.334+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:27 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T12:15:27.334+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:27 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T12:15:27.335+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:27 INFO DAGScheduler: Missing parents: List()
[2023-01-31T12:15:27.336+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:27 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T12:15:27.370+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:27 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 432.9 MiB)
[2023-01-31T12:15:27.436+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:27 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.8 MiB)
[2023-01-31T12:15:27.442+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:27 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 8124f810dec3:42079 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T12:15:27.455+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:27 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T12:15:27.469+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T12:15:27.476+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:27 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T12:15:27.490+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:27 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (8124f810dec3, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T12:15:27.502+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:27 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T12:15:27.632+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:27 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010200__202101010300.json, range: 0-9369, partition values: [empty row]
[2023-01-31T12:15:27.929+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:27 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-01-31T12:15:27.938+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:27 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 448 ms on 8124f810dec3 (executor driver) (1/1)
[2023-01-31T12:15:27.939+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:27 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 0.595 s
[2023-01-31T12:15:27.940+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:27 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T12:15:27.940+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:27 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T12:15:27.940+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T12:15:27.944+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:27 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 0.621292 s
[2023-01-31T12:15:28.348+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T12:15:28.368+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T12:15:28.376+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T12:15:29.502+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:29 INFO CodeGenerator: Code generated in 843.355248 ms
[2023-01-31T12:15:29.572+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:29 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.7 MiB)
[2023-01-31T12:15:29.726+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:29 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.6 MiB)
[2023-01-31T12:15:29.732+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:29 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 8124f810dec3:42079 (size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T12:15:29.733+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:29 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:204
[2023-01-31T12:15:29.745+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:29 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T12:15:29.847+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:29 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:204
[2023-01-31T12:15:29.890+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:29 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:204) with 1 output partitions
[2023-01-31T12:15:29.892+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:29 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:204)
[2023-01-31T12:15:29.894+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:29 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T12:15:29.895+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:29 INFO DAGScheduler: Missing parents: List()
[2023-01-31T12:15:29.898+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:29 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:204), which has no missing parents
[2023-01-31T12:15:29.920+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:29 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.6 MiB)
[2023-01-31T12:15:30.113+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:30 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.6 MiB)
[2023-01-31T12:15:30.124+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:30 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 8124f810dec3:42079 (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T12:15:30.131+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:30 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T12:15:30.137+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:204) (first 15 tasks are for partitions Vector(0))
[2023-01-31T12:15:30.143+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:30 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T12:15:30.185+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:30 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (8124f810dec3, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T12:15:30.192+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:30 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T12:15:30.237+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:30 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010200__202101010300.json, range: 0-9369, partition values: [empty row]
[2023-01-31T12:15:31.393+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:31 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1729 bytes result sent to driver
[2023-01-31T12:15:31.742+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:31 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 1451 ms on 8124f810dec3 (executor driver) (1/1)
[2023-01-31T12:15:31.812+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:31 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T12:15:31.813+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:31 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:204) finished in 1.851 s
[2023-01-31T12:15:31.813+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:31 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T12:15:31.814+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T12:15:31.873+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:31 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:204, took 1.987594 s
[2023-01-31T12:18:51.386+0000] {base_job.py:232} ERROR - LocalTaskJob heartbeat got an exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3361, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 325, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 888, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 491, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 271, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 386, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 684, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 210, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 680, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/jobs/base_job.py", line 204, in heartbeat
    session.merge(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in merge
    return self._merge(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3131, in _merge
    merged = self.get(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2848, in get
    return self._get_impl(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2970, in _get_impl
    return db_load_fn(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/loading.py", line 530, in load_on_pk_identity
    session.execute(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1552, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3315, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3394, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3364, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2198, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 210, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3361, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 325, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 888, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 491, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 271, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 386, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 684, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 210, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 680, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-01-31T12:20:31.202+0000] {spark_submit.py:495} INFO - 23/01/31 12:20:31 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,ArraySeq(),HashMap((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@6c9a9cfd)) by listener AppStatusListener took 1.051258335s.
[2023-01-31T12:20:31.287+0000] {spark_submit.py:495} INFO - 23/01/31 12:20:31 WARN Executor: Issue communicating with driver in heartbeater
[2023-01-31T12:20:31.287+0000] {spark_submit.py:495} INFO - org.apache.spark.rpc.RpcTimeoutException: Future timed out after [10000 milliseconds]. This timeout is controlled by spark.executor.heartbeatInterval
[2023-01-31T12:20:31.288+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
[2023-01-31T12:20:31.288+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
[2023-01-31T12:20:31.288+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
[2023-01-31T12:20:31.288+0000] {spark_submit.py:495} INFO - at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)
[2023-01-31T12:20:31.288+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)
[2023-01-31T12:20:31.289+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2023-01-31T12:20:31.289+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1053)
[2023-01-31T12:20:31.289+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2023-01-31T12:20:31.289+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
[2023-01-31T12:20:31.289+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2023-01-31T12:20:31.289+0000] {spark_submit.py:495} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2023-01-31T12:20:31.290+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-01-31T12:20:31.290+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2023-01-31T12:20:31.290+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2023-01-31T12:20:31.367+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-31T12:20:31.367+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-31T12:20:31.368+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-31T12:20:31.368+0000] {spark_submit.py:495} INFO - Caused by: java.util.concurrent.TimeoutException: Future timed out after [10000 milliseconds]
[2023-01-31T12:20:31.368+0000] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryAwait0(Promise.scala:248)
[2023-01-31T12:20:31.368+0000] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:261)
[2023-01-31T12:20:31.368+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:293)
[2023-01-31T12:20:31.369+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-01-31T12:20:31.369+0000] {spark_submit.py:495} INFO - ... 12 more
[2023-01-31T12:20:32.499+0000] {spark_submit.py:495} INFO - 23/01/31 12:20:32 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 261656 ms exceeds timeout 120000 ms
[2023-01-31T12:20:54.254+0000] {base_job.py:232} ERROR - LocalTaskJob heartbeat got an exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3361, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 325, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 888, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 491, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 271, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 386, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 684, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 210, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 680, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/jobs/base_job.py", line 222, in heartbeat
    session.merge(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in merge
    return self._merge(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3131, in _merge
    merged = self.get(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2848, in get
    return self._get_impl(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2970, in _get_impl
    return db_load_fn(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/loading.py", line 530, in load_on_pk_identity
    session.execute(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1552, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3315, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3394, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3364, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2198, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 210, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3361, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 325, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 888, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 491, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 271, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 386, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 684, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 210, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 680, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-01-31T12:21:02.768+0000] {local_task_job.py:143} ERROR - Heartbeat time limit exceeded!
[2023-01-31T12:21:48.599+0000] {process_utils.py:129} INFO - Sending Signals.SIGTERM to group 600. PIDs of all processes in the group: [603, 941, 600]
[2023-01-31T12:21:58.171+0000] {process_utils.py:84} INFO - Sending the signal Signals.SIGTERM to group 600
[2023-01-31T12:21:59.726+0000] {process_utils.py:79} INFO - Process psutil.Process(pid=941, status='terminated', started='12:09:16') (941) terminated with exit code None
[2023-01-31T12:21:59.096+0000] {taskinstance.py:1483} ERROR - Received SIGTERM. Terminating subprocesses.
[2023-01-31T12:21:59.985+0000] {spark_submit.py:620} INFO - Sending kill signal to spark-submit
[2023-01-31T12:22:00.409+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 414, in submit
    self._process_spark_submit_log(iter(self._submit_sp.stdout))  # type: ignore
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 463, in _process_spark_submit_log
    for line in itr:
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 1485, in signal_handler
    raise AirflowException("Task received SIGTERM signal")
airflow.exceptions.AirflowException: Task received SIGTERM signal
[2023-01-31T12:22:00.732+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=entsoe-energydata-backfill, task_id=stage_total_generation, execution_date=20210101T020000, start_date=20230131T120850, end_date=20230131T122200
[2023-01-31T12:22:01.110+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 998 for task stage_total_generation (Task received SIGTERM signal; 600)
[2023-01-31T12:22:01.419+0000] {process_utils.py:79} INFO - Process psutil.Process(pid=600, status='terminated', exitcode=1, started='12:08:49') (600) terminated with exit code 1
[2023-01-31T12:22:01.786+0000] {process_utils.py:79} INFO - Process psutil.Process(pid=603, status='terminated', started='12:08:51') (603) terminated with exit code None
[2023-01-31T13:03:42.752+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T02:00:00+00:00 [queued]>
[2023-01-31T13:03:42.778+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T02:00:00+00:00 [queued]>
[2023-01-31T13:03:42.779+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T13:03:42.779+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T13:03:42.779+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T13:03:42.815+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 02:00:00+00:00
[2023-01-31T13:03:42.828+0000] {standard_task_runner.py:55} INFO - Started process 226 to run task
[2023-01-31T13:03:42.849+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T02:00:00+00:00', '--job-id', '1028', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp0_k28s15']
[2023-01-31T13:03:42.868+0000] {standard_task_runner.py:83} INFO - Job 1028: Subtask stage_total_generation
[2023-01-31T13:03:43.205+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T02:00:00+00:00 [running]> on host b37fe3cbf330
[2023-01-31T13:03:43.460+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T02:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T02:00:00+00:00
[2023-01-31T13:03:43.477+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T13:03:43.479+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010200 202101010300 DE_TENNET 202101010000 202101010600
[2023-01-31T13:04:16.262+0000] {spark_submit.py:495} INFO - mulai periode: 202101010000
[2023-01-31T13:04:16.266+0000] {spark_submit.py:495} INFO - selesai periode: 202101010600
[2023-01-31T13:04:17.154+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:17 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T13:04:19.170+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T13:04:22.132+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:22 INFO ResourceUtils: ==============================================================
[2023-01-31T13:04:22.134+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:22 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T13:04:22.166+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:22 INFO ResourceUtils: ==============================================================
[2023-01-31T13:04:22.172+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:22 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T13:04:22.617+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:22 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T13:04:22.669+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:22 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T13:04:22.678+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:22 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T13:04:23.224+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:23 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T13:04:23.226+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:23 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T13:04:23.228+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:23 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T13:04:23.231+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:23 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T13:04:23.234+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T13:04:29.294+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:29 INFO Utils: Successfully started service 'sparkDriver' on port 43007.
[2023-01-31T13:04:30.497+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:30 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T13:04:31.452+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:31 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T13:04:32.023+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:32 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T13:04:32.030+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:32 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T13:04:32.108+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:32 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T13:04:32.291+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:32 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-531ba5eb-9668-44cf-93e8-256ce452bd20
[2023-01-31T13:04:32.496+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:32 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T13:04:32.637+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:32 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T13:04:35.627+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:35 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T13:04:35.685+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:35 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2023-01-31T13:04:35.876+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:35 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://b37fe3cbf330:43007/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675170257009
[2023-01-31T13:04:35.878+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:35 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://b37fe3cbf330:43007/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675170257009
[2023-01-31T13:04:38.122+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:38 INFO Executor: Starting executor ID driver on host b37fe3cbf330
[2023-01-31T13:04:38.312+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:38 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T13:04:38.701+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:38 INFO Executor: Fetching spark://b37fe3cbf330:43007/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675170257009
[2023-01-31T13:04:39.411+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:39 INFO TransportClientFactory: Successfully created connection to b37fe3cbf330/172.19.0.8:43007 after 507 ms (0 ms spent in bootstraps)
[2023-01-31T13:04:39.561+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:39 INFO Utils: Fetching spark://b37fe3cbf330:43007/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-a5d03d40-aae1-4692-beda-028404b7228c/userFiles-cbab8c0b-b97f-4f4a-b361-015617b15c42/fetchFileTemp5943994747498378861.tmp
[2023-01-31T13:04:45.846+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:45 INFO Executor: Adding file:/tmp/spark-a5d03d40-aae1-4692-beda-028404b7228c/userFiles-cbab8c0b-b97f-4f4a-b361-015617b15c42/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T13:04:45.850+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:45 INFO Executor: Fetching spark://b37fe3cbf330:43007/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675170257009
[2023-01-31T13:04:45.869+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:45 INFO Utils: Fetching spark://b37fe3cbf330:43007/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-a5d03d40-aae1-4692-beda-028404b7228c/userFiles-cbab8c0b-b97f-4f4a-b361-015617b15c42/fetchFileTemp5895638698928212315.tmp
[2023-01-31T13:04:49.370+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:49 INFO Executor: Adding file:/tmp/spark-a5d03d40-aae1-4692-beda-028404b7228c/userFiles-cbab8c0b-b97f-4f4a-b361-015617b15c42/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T13:04:49.522+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:49 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37277.
[2023-01-31T13:04:49.523+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:49 INFO NettyBlockTransferService: Server created on b37fe3cbf330:37277
[2023-01-31T13:04:49.572+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:49 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T13:04:49.688+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:49 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, b37fe3cbf330, 37277, None)
[2023-01-31T13:04:49.714+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:49 INFO BlockManagerMasterEndpoint: Registering block manager b37fe3cbf330:37277 with 434.4 MiB RAM, BlockManagerId(driver, b37fe3cbf330, 37277, None)
[2023-01-31T13:04:49.760+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:49 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, b37fe3cbf330, 37277, None)
[2023-01-31T13:04:49.766+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:49 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, b37fe3cbf330, 37277, None)
[2023-01-31T13:05:00.218+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:00 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T13:05:00.276+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:00 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T13:05:26.162+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:26 INFO InMemoryFileIndex: It took 601 ms to list leaf files for 1 paths.
[2023-01-31T13:05:26.897+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:26 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T13:05:27.182+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:27 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T13:05:27.195+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:27 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on b37fe3cbf330:37277 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T13:05:27.227+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:27 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T13:05:29.932+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:29 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T13:05:30.030+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:30 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T13:05:30.147+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:30 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T13:05:30.257+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:30 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T13:05:30.259+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:30 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T13:05:30.268+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:30 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T13:05:30.293+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:30 INFO DAGScheduler: Missing parents: List()
[2023-01-31T13:05:30.356+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:30 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T13:05:30.792+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:30 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T13:05:30.807+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:30 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T13:05:30.809+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:30 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on b37fe3cbf330:37277 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T13:05:30.810+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:30 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T13:05:30.878+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T13:05:30.885+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:30 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T13:05:31.066+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:31 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T13:05:31.231+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:31 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T13:05:32.130+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:32 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010200__202101010300.json:0+9369
[2023-01-31T13:05:33.486+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:33 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3437 bytes result sent to driver
[2023-01-31T13:05:33.577+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:33 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2562 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T13:05:33.602+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:33 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T13:05:33.654+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:33 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 3.118 s
[2023-01-31T13:05:33.683+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:33 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T13:05:33.685+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T13:05:33.707+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:33 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 3.558870 s
[2023-01-31T13:05:34.025+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:34 INFO BlockManagerInfo: Removed broadcast_1_piece0 on b37fe3cbf330:37277 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T13:05:51.626+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:51 INFO BlockManagerInfo: Removed broadcast_0_piece0 on b37fe3cbf330:37277 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T13:06:15.978+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:15 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T13:06:16.000+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:15 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T13:06:16.049+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:16 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T13:06:18.187+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:18 INFO CodeGenerator: Code generated in 1104.500718 ms
[2023-01-31T13:06:18.210+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:18 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T13:06:18.271+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:18 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T13:06:18.273+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:18 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on b37fe3cbf330:37277 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T13:06:18.275+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:18 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T13:06:18.305+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:18 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T13:06:18.764+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:18 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T13:06:18.768+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:18 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T13:06:18.768+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:18 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T13:06:18.768+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:18 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T13:06:18.768+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:18 INFO DAGScheduler: Missing parents: List()
[2023-01-31T13:06:18.775+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:18 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T13:06:18.809+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:18 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T13:06:18.824+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:18 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T13:06:18.826+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:18 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on b37fe3cbf330:37277 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T13:06:18.830+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:18 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T13:06:18.833+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T13:06:18.835+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:18 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T13:06:18.855+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:18 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T13:06:18.860+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:18 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T13:06:19.223+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:19 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010200__202101010300.json, range: 0-9369, partition values: [empty row]
[2023-01-31T13:06:19.753+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:19 INFO CodeGenerator: Code generated in 414.128291 ms
[2023-01-31T13:06:20.317+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:20 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1628 bytes result sent to driver
[2023-01-31T13:06:20.333+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:20 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1488 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T13:06:20.339+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:20 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 1.545 s
[2023-01-31T13:06:20.339+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:20 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T13:06:20.343+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:20 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T13:06:20.344+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T13:06:20.348+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:20 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 1.580738 s
[2023-01-31T13:06:21.021+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:21 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T13:06:21.022+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:21 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T13:06:21.066+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:21 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T13:06:21.174+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:21 INFO CodeGenerator: Code generated in 58.253605 ms
[2023-01-31T13:06:21.195+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:21 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T13:06:21.236+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:21 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T13:06:21.254+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:21 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on b37fe3cbf330:37277 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T13:06:21.258+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:21 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T13:06:21.261+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:21 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T13:06:21.425+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:21 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T13:06:21.427+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:21 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T13:06:21.428+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:21 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T13:06:21.429+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:21 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T13:06:21.430+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:21 INFO DAGScheduler: Missing parents: List()
[2023-01-31T13:06:21.441+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:21 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T13:06:21.502+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:21 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T13:06:21.525+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:21 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T13:06:21.531+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:21 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on b37fe3cbf330:37277 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T13:06:21.541+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:21 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T13:06:21.546+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T13:06:21.547+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:21 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T13:06:21.569+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:21 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T13:06:21.573+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:21 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T13:06:21.660+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:21 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010200__202101010300.json, range: 0-9369, partition values: [empty row]
[2023-01-31T13:06:21.992+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:21 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1585 bytes result sent to driver
[2023-01-31T13:06:22.006+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:22 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 443 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T13:06:22.013+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:22 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 0.552 s
[2023-01-31T13:06:22.014+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:22 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T13:06:22.015+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:22 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T13:06:22.015+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T13:06:22.016+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:22 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 0.586196 s
[2023-01-31T13:06:23.157+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:23 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T13:06:23.162+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:23 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T13:06:23.169+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:23 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T13:06:24.087+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO CodeGenerator: Code generated in 651.352005 ms
[2023-01-31T13:06:24.110+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T13:06:24.200+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T13:06:24.203+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on b37fe3cbf330:37277 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T13:06:24.206+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T13:06:24.214+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T13:06:24.440+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO BlockManagerInfo: Removed broadcast_3_piece0 on b37fe3cbf330:37277 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T13:06:24.493+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO BlockManagerInfo: Removed broadcast_5_piece0 on b37fe3cbf330:37277 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T13:06:24.516+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T13:06:24.519+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T13:06:24.520+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T13:06:24.521+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T13:06:24.522+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO DAGScheduler: Missing parents: List()
[2023-01-31T13:06:24.526+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T13:06:24.572+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.7 MiB)
[2023-01-31T13:06:24.588+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.7 MiB)
[2023-01-31T13:06:24.593+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on b37fe3cbf330:37277 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T13:06:24.599+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T13:06:24.608+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T13:06:24.611+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T13:06:24.617+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T13:06:24.618+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T13:06:24.759+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010200__202101010300.json, range: 0-9369, partition values: [empty row]
[2023-01-31T13:06:25.037+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:25 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-01-31T13:06:25.103+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:25 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 488 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T13:06:25.110+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:25 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 0.572 s
[2023-01-31T13:06:25.114+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:25 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T13:06:25.115+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:25 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T13:06:25.116+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T13:06:25.119+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:25 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 0.602016 s
[2023-01-31T13:06:25.444+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:25 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T13:06:25.448+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:25 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T13:06:25.452+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:25 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T13:06:25.553+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:25 INFO CodeGenerator: Code generated in 42.327333 ms
[2023-01-31T13:06:25.587+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:25 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.5 MiB)
[2023-01-31T13:06:25.662+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:25 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T13:06:25.670+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:25 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on b37fe3cbf330:37277 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T13:06:25.672+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:25 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T13:06:25.680+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:25 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T13:06:25.806+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:25 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T13:06:25.808+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:25 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T13:06:25.809+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:25 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T13:06:25.809+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:25 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T13:06:25.809+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:25 INFO DAGScheduler: Missing parents: List()
[2023-01-31T13:06:25.814+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:25 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T13:06:25.847+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:25 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-31T13:06:25.861+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:25 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-01-31T13:06:25.866+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:25 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on b37fe3cbf330:37277 (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T13:06:25.869+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:25 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T13:06:25.871+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T13:06:25.875+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:25 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T13:06:25.883+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:25 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T13:06:25.883+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:25 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T13:06:25.928+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:25 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010200__202101010300.json, range: 0-9369, partition values: [empty row]
[2023-01-31T13:06:26.117+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1585 bytes result sent to driver
[2023-01-31T13:06:26.132+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 237 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T13:06:26.132+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T13:06:26.132+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 0.296 s
[2023-01-31T13:06:26.132+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T13:06:26.132+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T13:06:26.133+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 0.316757 s
[2023-01-31T13:06:27.516+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T13:06:27.517+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T13:06:27.530+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T13:06:27.994+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO CodeGenerator: Code generated in 232.031802 ms
[2023-01-31T13:06:28.058+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.2 MiB)
[2023-01-31T13:06:28.167+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T13:06:28.175+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on b37fe3cbf330:37277 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T13:06:28.178+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T13:06:28.187+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T13:06:28.284+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T13:06:28.296+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T13:06:28.297+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T13:06:28.297+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T13:06:28.298+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO DAGScheduler: Missing parents: List()
[2023-01-31T13:06:28.307+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T13:06:28.328+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.1 MiB)
[2023-01-31T13:06:28.337+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.1 MiB)
[2023-01-31T13:06:28.341+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on b37fe3cbf330:37277 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T13:06:28.345+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T13:06:28.349+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T13:06:28.351+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T13:06:28.360+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T13:06:28.360+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T13:06:28.420+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010200__202101010300.json, range: 0-9369, partition values: [empty row]
[2023-01-31T13:06:28.641+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2524 bytes result sent to driver
[2023-01-31T13:06:28.649+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 287 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T13:06:28.650+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 0.335 s
[2023-01-31T13:06:28.650+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T13:06:28.651+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T13:06:28.651+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T13:06:28.654+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 0.368493 s
[2023-01-31T13:06:28.901+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T13:06:28.903+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T13:06:28.910+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T13:06:29.374+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO CodeGenerator: Code generated in 247.674954 ms
[2023-01-31T13:06:29.403+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 432.9 MiB)
[2023-01-31T13:06:29.479+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.9 MiB)
[2023-01-31T13:06:29.503+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on b37fe3cbf330:37277 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T13:06:29.506+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T13:06:29.508+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T13:06:29.537+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T13:06:29.540+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T13:06:29.541+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T13:06:29.541+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T13:06:29.541+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO DAGScheduler: Missing parents: List()
[2023-01-31T13:06:29.543+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T13:06:29.558+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 432.9 MiB)
[2023-01-31T13:06:29.588+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.8 MiB)
[2023-01-31T13:06:29.594+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on b37fe3cbf330:37277 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T13:06:29.597+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T13:06:29.602+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T13:06:29.616+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T13:06:29.623+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T13:06:29.626+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T13:06:29.669+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010200__202101010300.json, range: 0-9369, partition values: [empty row]
[2023-01-31T13:06:29.861+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-01-31T13:06:29.867+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 241 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T13:06:29.868+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T13:06:29.868+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 0.319 s
[2023-01-31T13:06:29.868+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T13:06:29.868+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T13:06:29.868+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 0.327242 s
[2023-01-31T13:06:29.942+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T13:06:29.942+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T13:06:29.943+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T13:06:30.170+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO CodeGenerator: Code generated in 152.040245 ms
[2023-01-31T13:06:30.200+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.7 MiB)
[2023-01-31T13:06:30.343+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.6 MiB)
[2023-01-31T13:06:30.345+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on b37fe3cbf330:37277 (size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T13:06:30.349+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:204
[2023-01-31T13:06:30.350+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T13:06:30.380+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:204
[2023-01-31T13:06:30.382+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:204) with 1 output partitions
[2023-01-31T13:06:30.383+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:204)
[2023-01-31T13:06:30.383+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T13:06:30.383+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO DAGScheduler: Missing parents: List()
[2023-01-31T13:06:30.393+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:204), which has no missing parents
[2023-01-31T13:06:30.402+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.6 MiB)
[2023-01-31T13:06:30.409+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.6 MiB)
[2023-01-31T13:06:30.412+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on b37fe3cbf330:37277 (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T13:06:30.413+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T13:06:30.415+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:204) (first 15 tasks are for partitions Vector(0))
[2023-01-31T13:06:30.416+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T13:06:30.421+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T13:06:30.423+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T13:06:30.441+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010200__202101010300.json, range: 0-9369, partition values: [empty row]
[2023-01-31T13:06:30.672+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1729 bytes result sent to driver
[2023-01-31T13:06:30.688+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 270 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T13:06:30.689+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T13:06:30.692+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:204) finished in 0.301 s
[2023-01-31T13:06:30.693+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T13:06:30.693+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T13:06:30.693+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:204, took 0.312843 s
[2023-01-31T13:06:31.947+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:31 INFO BlockManagerInfo: Removed broadcast_2_piece0 on b37fe3cbf330:37277 in memory (size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T13:06:32.040+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:32 INFO BlockManagerInfo: Removed broadcast_6_piece0 on b37fe3cbf330:37277 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T13:06:32.122+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:32 INFO BlockManagerInfo: Removed broadcast_9_piece0 on b37fe3cbf330:37277 in memory (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T13:06:32.236+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:32 INFO BlockManagerInfo: Removed broadcast_7_piece0 on b37fe3cbf330:37277 in memory (size: 12.7 KiB, free: 434.2 MiB)
[2023-01-31T13:06:32.297+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:32 INFO BlockManagerInfo: Removed broadcast_8_piece0 on b37fe3cbf330:37277 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T13:06:32.352+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:32 INFO BlockManagerInfo: Removed broadcast_13_piece0 on b37fe3cbf330:37277 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T13:06:32.400+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:32 INFO BlockManagerInfo: Removed broadcast_15_piece0 on b37fe3cbf330:37277 in memory (size: 12.6 KiB, free: 434.3 MiB)
[2023-01-31T13:06:32.448+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:32 INFO BlockManagerInfo: Removed broadcast_4_piece0 on b37fe3cbf330:37277 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T13:06:32.489+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:32 INFO BlockManagerInfo: Removed broadcast_11_piece0 on b37fe3cbf330:37277 in memory (size: 13.4 KiB, free: 434.3 MiB)
[2023-01-31T13:06:37.839+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO BlockManagerInfo: Removed broadcast_12_piece0 on b37fe3cbf330:37277 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T13:06:37.871+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO BlockManagerInfo: Removed broadcast_14_piece0 on b37fe3cbf330:37277 in memory (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T13:06:49.434+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:49 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T13:06:49.502+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T13:06:49.504+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T13:06:49.509+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:49 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T13:06:49.511+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T13:06:49.512+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T13:06:49.520+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T13:06:50.368+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-31T13:06:50.375+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO DAGScheduler: Got job 8 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T13:06:50.375+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO DAGScheduler: Final stage: ResultStage 8 (save at BigQueryWriteHelper.java:105)
[2023-01-31T13:06:50.375+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T13:06:50.376+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO DAGScheduler: Missing parents: List()
[2023-01-31T13:06:50.381+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO DAGScheduler: Submitting ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T13:06:50.766+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 209.9 KiB, free 434.0 MiB)
[2023-01-31T13:06:50.818+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 74.3 KiB, free 433.9 MiB)
[2023-01-31T13:06:50.827+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on b37fe3cbf330:37277 (size: 74.3 KiB, free: 434.3 MiB)
[2023-01-31T13:06:50.844+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513
[2023-01-31T13:06:50.848+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T13:06:50.848+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2023-01-31T13:06:50.881+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7484 bytes) taskResourceAssignments Map()
[2023-01-31T13:06:50.897+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2023-01-31T13:06:51.259+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T13:06:51.260+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T13:06:51.270+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T13:06:51.274+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T13:06:51.276+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T13:06:51.283+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T13:06:51.331+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T13:06:51.365+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T13:06:51.493+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO ParquetOutputFormat: Parquet block size to 134217728
[2023-01-31T13:06:51.498+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO ParquetOutputFormat: Validation is off
[2023-01-31T13:06:51.499+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-01-31T13:06:51.503+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO ParquetOutputFormat: Parquet properties are:
[2023-01-31T13:06:51.503+0000] {spark_submit.py:495} INFO - Parquet page size to 1048576
[2023-01-31T13:06:51.504+0000] {spark_submit.py:495} INFO - Parquet dictionary page size to 1048576
[2023-01-31T13:06:51.504+0000] {spark_submit.py:495} INFO - Dictionary is true
[2023-01-31T13:06:51.504+0000] {spark_submit.py:495} INFO - Writer version is: PARQUET_1_0
[2023-01-31T13:06:51.504+0000] {spark_submit.py:495} INFO - Page size checking is: estimated
[2023-01-31T13:06:51.504+0000] {spark_submit.py:495} INFO - Min row count for page size check is: 100
[2023-01-31T13:06:51.505+0000] {spark_submit.py:495} INFO - Max row count for page size check is: 10000
[2023-01-31T13:06:51.505+0000] {spark_submit.py:495} INFO - Truncate length for column indexes is: 64
[2023-01-31T13:06:51.505+0000] {spark_submit.py:495} INFO - Truncate length for statistics min/max  is: 2147483647
[2023-01-31T13:06:51.505+0000] {spark_submit.py:495} INFO - Bloom filter enabled: false
[2023-01-31T13:06:51.505+0000] {spark_submit.py:495} INFO - Max Bloom filter size for a column is 1048576
[2023-01-31T13:06:51.505+0000] {spark_submit.py:495} INFO - Bloom filter expected number of distinct values are: null
[2023-01-31T13:06:51.506+0000] {spark_submit.py:495} INFO - Page row count limit to 20000
[2023-01-31T13:06:51.506+0000] {spark_submit.py:495} INFO - Writing page checksums is: on
[2023-01-31T13:06:51.828+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-01-31T13:06:51.829+0000] {spark_submit.py:495} INFO - {
[2023-01-31T13:06:51.829+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-31T13:06:51.829+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-31T13:06:51.829+0000] {spark_submit.py:495} INFO - "name" : "measured_at",
[2023-01-31T13:06:51.829+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T13:06:51.829+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T13:06:51.830+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.830+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.830+0000] {spark_submit.py:495} INFO - "name" : "created_at",
[2023-01-31T13:06:51.830+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T13:06:51.830+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T13:06:51.830+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.831+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.831+0000] {spark_submit.py:495} INFO - "name" : "Biomass - Actual Aggregated",
[2023-01-31T13:06:51.831+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.832+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.833+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.837+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.838+0000] {spark_submit.py:495} INFO - "name" : "Fossil Brown coal/Lignite - Actual Aggregated",
[2023-01-31T13:06:51.839+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.842+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.844+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.846+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.849+0000] {spark_submit.py:495} INFO - "name" : "Fossil Gas - Actual Aggregated",
[2023-01-31T13:06:51.850+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.851+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.853+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.854+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.863+0000] {spark_submit.py:495} INFO - "name" : "Fossil Hard coal - Actual Aggregated",
[2023-01-31T13:06:51.863+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.864+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.864+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.864+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.864+0000] {spark_submit.py:495} INFO - "name" : "Fossil Oil - Actual Aggregated",
[2023-01-31T13:06:51.864+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.865+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.865+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.866+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.866+0000] {spark_submit.py:495} INFO - "name" : "Geothermal - Actual Aggregated",
[2023-01-31T13:06:51.867+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.867+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.871+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.873+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.874+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Aggregated",
[2023-01-31T13:06:51.877+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.878+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.880+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.881+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.882+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Consumption",
[2023-01-31T13:06:51.885+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.890+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.890+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.891+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.891+0000] {spark_submit.py:495} INFO - "name" : "Hydro Run-of-river and poundage - Actual Aggregated",
[2023-01-31T13:06:51.892+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.892+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.893+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.893+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.894+0000] {spark_submit.py:495} INFO - "name" : "Hydro Water Reservoir - Actual Aggregated",
[2023-01-31T13:06:51.896+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.898+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.898+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.899+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.900+0000] {spark_submit.py:495} INFO - "name" : "Nuclear - Actual Aggregated",
[2023-01-31T13:06:51.901+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.906+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.908+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.910+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.911+0000] {spark_submit.py:495} INFO - "name" : "Other - Actual Aggregated",
[2023-01-31T13:06:51.912+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.912+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.914+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.914+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.914+0000] {spark_submit.py:495} INFO - "name" : "Other renewable - Actual Aggregated",
[2023-01-31T13:06:51.915+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.916+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.917+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.917+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.918+0000] {spark_submit.py:495} INFO - "name" : "Solar - Actual Aggregated",
[2023-01-31T13:06:51.919+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.919+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.919+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.920+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.921+0000] {spark_submit.py:495} INFO - "name" : "Waste - Actual Aggregated",
[2023-01-31T13:06:51.921+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.922+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.922+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.923+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.925+0000] {spark_submit.py:495} INFO - "name" : "Wind Offshore - Actual Aggregated",
[2023-01-31T13:06:51.926+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.927+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.929+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.929+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.929+0000] {spark_submit.py:495} INFO - "name" : "Wind Onshore - Actual Aggregated",
[2023-01-31T13:06:51.930+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.932+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.934+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.935+0000] {spark_submit.py:495} INFO - } ]
[2023-01-31T13:06:51.935+0000] {spark_submit.py:495} INFO - }
[2023-01-31T13:06:51.936+0000] {spark_submit.py:495} INFO - and corresponding Parquet message type:
[2023-01-31T13:06:51.937+0000] {spark_submit.py:495} INFO - message spark_schema {
[2023-01-31T13:06:51.938+0000] {spark_submit.py:495} INFO - required int96 measured_at;
[2023-01-31T13:06:51.939+0000] {spark_submit.py:495} INFO - required int96 created_at;
[2023-01-31T13:06:51.940+0000] {spark_submit.py:495} INFO - optional double Biomass - Actual Aggregated;
[2023-01-31T13:06:51.940+0000] {spark_submit.py:495} INFO - optional double Fossil Brown coal/Lignite - Actual Aggregated;
[2023-01-31T13:06:51.943+0000] {spark_submit.py:495} INFO - optional double Fossil Gas - Actual Aggregated;
[2023-01-31T13:06:51.943+0000] {spark_submit.py:495} INFO - optional double Fossil Hard coal - Actual Aggregated;
[2023-01-31T13:06:51.944+0000] {spark_submit.py:495} INFO - optional double Fossil Oil - Actual Aggregated;
[2023-01-31T13:06:51.944+0000] {spark_submit.py:495} INFO - optional double Geothermal - Actual Aggregated;
[2023-01-31T13:06:51.947+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Aggregated;
[2023-01-31T13:06:51.947+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Consumption;
[2023-01-31T13:06:51.948+0000] {spark_submit.py:495} INFO - optional double Hydro Run-of-river and poundage - Actual Aggregated;
[2023-01-31T13:06:51.948+0000] {spark_submit.py:495} INFO - optional double Hydro Water Reservoir - Actual Aggregated;
[2023-01-31T13:06:51.954+0000] {spark_submit.py:495} INFO - optional double Nuclear - Actual Aggregated;
[2023-01-31T13:06:51.955+0000] {spark_submit.py:495} INFO - optional double Other - Actual Aggregated;
[2023-01-31T13:06:51.955+0000] {spark_submit.py:495} INFO - optional double Other renewable - Actual Aggregated;
[2023-01-31T13:06:51.955+0000] {spark_submit.py:495} INFO - optional double Solar - Actual Aggregated;
[2023-01-31T13:06:51.957+0000] {spark_submit.py:495} INFO - optional double Waste - Actual Aggregated;
[2023-01-31T13:06:51.958+0000] {spark_submit.py:495} INFO - optional double Wind Offshore - Actual Aggregated;
[2023-01-31T13:06:51.959+0000] {spark_submit.py:495} INFO - optional double Wind Onshore - Actual Aggregated;
[2023-01-31T13:06:51.962+0000] {spark_submit.py:495} INFO - }
[2023-01-31T13:06:51.962+0000] {spark_submit.py:495} INFO - 
[2023-01-31T13:06:51.965+0000] {spark_submit.py:495} INFO - 
[2023-01-31T13:06:52.901+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:52 INFO CodecPool: Got brand-new compressor [.snappy]
[2023-01-31T13:06:56.451+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:56 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675170276946-e54c9234-c71f-462b-a253-9f065c7c54f2/_temporary/0/_temporary/' directory.
[2023-01-31T13:06:56.452+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:56 INFO FileOutputCommitter: Saved output of task 'attempt_202301311306504095714834419839296_0008_m_000000_8' to gs://entsoe_temp_1009/.spark-bigquery-local-1675170276946-e54c9234-c71f-462b-a253-9f065c7c54f2/_temporary/0/task_202301311306504095714834419839296_0008_m_000000
[2023-01-31T13:06:56.452+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:56 INFO SparkHadoopMapRedUtil: attempt_202301311306504095714834419839296_0008_m_000000_8: Committed. Elapsed time: 1454 ms.
[2023-01-31T13:06:56.461+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:56 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2588 bytes result sent to driver
[2023-01-31T13:06:56.463+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:56 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 5591 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T13:06:56.463+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:56 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2023-01-31T13:06:56.464+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:56 INFO DAGScheduler: ResultStage 8 (save at BigQueryWriteHelper.java:105) finished in 6.084 s
[2023-01-31T13:06:56.464+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:56 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T13:06:56.464+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2023-01-31T13:06:56.465+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:56 INFO DAGScheduler: Job 8 finished: save at BigQueryWriteHelper.java:105, took 6.094560 s
[2023-01-31T13:06:56.467+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:56 INFO FileFormatWriter: Start to commit write Job b25be1ea-a736-459a-a23b-5a92453a3b11.
[2023-01-31T13:06:57.165+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:57 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675170276946-e54c9234-c71f-462b-a253-9f065c7c54f2/_temporary/0/task_202301311306504095714834419839296_0008_m_000000/' directory.
[2023-01-31T13:06:58.248+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:58 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675170276946-e54c9234-c71f-462b-a253-9f065c7c54f2/' directory.
[2023-01-31T13:06:58.388+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:58 INFO BlockManagerInfo: Removed broadcast_16_piece0 on b37fe3cbf330:37277 in memory (size: 74.3 KiB, free: 434.4 MiB)
[2023-01-31T13:06:58.846+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:58 INFO FileFormatWriter: Write Job b25be1ea-a736-459a-a23b-5a92453a3b11 committed. Elapsed time: 2377 ms.
[2023-01-31T13:06:58.849+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:58 INFO FileFormatWriter: Finished processing stats for write job b25be1ea-a736-459a-a23b-5a92453a3b11.
[2023-01-31T13:07:00.745+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:00 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=entsoe_analytics_1009, projectId=rafzul-analytics-1009, tableId=TEST_total_generation_staging}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=null, ignoreUnknownValue=null, sourceUris=[gs://entsoe_temp_1009/.spark-bigquery-local-1675170276946-e54c9234-c71f-462b-a253-9f065c7c54f2/part-00000-ea9b92c4-f587-45c1-9899-5aad1156760f-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=true, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=rafzul-analytics-1009, job=bd4a18a2-3e02-4878-91fc-be72cad85146, location=US}
[2023-01-31T13:07:06.533+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:06 INFO BigQueryClient: Done loading to rafzul-analytics-1009.entsoe_analytics_1009.TEST_total_generation_staging. jobId: JobId{project=rafzul-analytics-1009, job=bd4a18a2-3e02-4878-91fc-be72cad85146, location=US}
[2023-01-31T13:07:08.827+0000] {spark_submit.py:495} INFO - MANTEP!!!!
[2023-01-31T13:07:09.171+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:09 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-31T13:07:09.210+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:09 INFO SparkUI: Stopped Spark web UI at http://b37fe3cbf330:4041
[2023-01-31T13:07:09.290+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:09 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-31T13:07:09.377+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:09 INFO MemoryStore: MemoryStore cleared
[2023-01-31T13:07:09.381+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:09 INFO BlockManager: BlockManager stopped
[2023-01-31T13:07:09.413+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:09 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-31T13:07:09.430+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:09 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-31T13:07:09.463+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:09 INFO SparkContext: Successfully stopped SparkContext
[2023-01-31T13:07:09.465+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:09 INFO ShutdownHookManager: Shutdown hook called
[2023-01-31T13:07:09.469+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-a5d03d40-aae1-4692-beda-028404b7228c/pyspark-993b4f46-3e63-457a-83d7-48ad6b217437
[2023-01-31T13:07:09.506+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-56d64702-3aaf-4e49-9503-c3eab01cf297
[2023-01-31T13:07:09.532+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-a5d03d40-aae1-4692-beda-028404b7228c
[2023-01-31T13:07:09.867+0000] {taskinstance.py:1322} INFO - Marking task as SUCCESS. dag_id=entsoe-energydata-backfill, task_id=stage_total_generation, execution_date=20210101T020000, start_date=20230131T130342, end_date=20230131T130709
[2023-01-31T13:07:09.938+0000] {local_task_job.py:159} INFO - Task exited with return code 0
[2023-01-31T13:07:09.984+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T14:09:45.346+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T02:00:00+00:00 [queued]>
[2023-01-31T14:09:45.380+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T02:00:00+00:00 [queued]>
[2023-01-31T14:09:45.381+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T14:09:45.381+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T14:09:45.381+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T14:09:45.405+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 02:00:00+00:00
[2023-01-31T14:09:45.419+0000] {standard_task_runner.py:55} INFO - Started process 6115 to run task
[2023-01-31T14:09:45.428+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T02:00:00+00:00', '--job-id', '1044', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmproxqgk76']
[2023-01-31T14:09:45.435+0000] {standard_task_runner.py:83} INFO - Job 1044: Subtask stage_total_generation
[2023-01-31T14:09:45.629+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T02:00:00+00:00 [running]> on host b37fe3cbf330
[2023-01-31T14:09:45.872+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T02:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T02:00:00+00:00
[2023-01-31T14:09:45.903+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T14:09:45.910+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 398, in submit
    spark_submit_cmd = self._build_spark_submit_command(application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 344, in _build_spark_submit_command
    self.log.info("Spark-Submit cmd: %s", self._mask_cmd(connection_cmd))
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 256, in _mask_cmd
    " ".join(connection_cmd),
TypeError: sequence item 10: expected str instance, DateTime found
[2023-01-31T14:09:45.949+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=entsoe-energydata-backfill, task_id=stage_total_generation, execution_date=20210101T020000, start_date=20230131T140945, end_date=20230131T140945
[2023-01-31T14:09:45.990+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 1044 for task stage_total_generation (sequence item 10: expected str instance, DateTime found; 6115)
[2023-01-31T14:09:46.091+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-31T14:09:46.163+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T14:10:50.452+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T02:00:00+00:00 [queued]>
[2023-01-31T14:10:50.578+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T02:00:00+00:00 [queued]>
[2023-01-31T14:10:50.578+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T14:10:50.579+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T14:10:50.579+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T14:10:50.627+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 02:00:00+00:00
[2023-01-31T14:10:50.667+0000] {standard_task_runner.py:55} INFO - Started process 6201 to run task
[2023-01-31T14:10:50.689+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T02:00:00+00:00', '--job-id', '1053', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp5ymc6z91']
[2023-01-31T14:10:50.694+0000] {standard_task_runner.py:83} INFO - Job 1053: Subtask stage_total_generation
[2023-01-31T14:10:50.883+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T02:00:00+00:00 [running]> on host b37fe3cbf330
[2023-01-31T14:10:51.125+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T02:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T02:00:00+00:00
[2023-01-31T14:10:51.172+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T14:10:51.176+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 398, in submit
    spark_submit_cmd = self._build_spark_submit_command(application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 344, in _build_spark_submit_command
    self.log.info("Spark-Submit cmd: %s", self._mask_cmd(connection_cmd))
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 256, in _mask_cmd
    " ".join(connection_cmd),
TypeError: sequence item 10: expected str instance, DateTime found
[2023-01-31T14:10:51.208+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=entsoe-energydata-backfill, task_id=stage_total_generation, execution_date=20210101T020000, start_date=20230131T141050, end_date=20230131T141051
[2023-01-31T14:10:51.255+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 1053 for task stage_total_generation (sequence item 10: expected str instance, DateTime found; 6201)
[2023-01-31T14:10:51.362+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-31T14:10:51.500+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
