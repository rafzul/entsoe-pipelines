[2023-01-31T05:11:01.603+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-31T05:11:01.653+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-31T05:11:01.654+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T05:11:01.654+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T05:11:01.655+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T05:11:01.720+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 01:00:00+00:00
[2023-01-31T05:11:01.750+0000] {standard_task_runner.py:55} INFO - Started process 12302 to run task
[2023-01-31T05:11:01.787+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '884', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpasl9ym7u']
[2023-01-31T05:11:01.805+0000] {standard_task_runner.py:83} INFO - Job 884: Subtask stage_total_generation
[2023-01-31T05:11:02.170+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 81d5fcd0285b
[2023-01-31T05:11:02.539+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-31T05:11:02.601+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T05:11:02.617+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-31T05:11:34.116+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:34 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T05:11:34.898+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T05:11:36.244+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:36 INFO ResourceUtils: ==============================================================
[2023-01-31T05:11:36.263+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:36 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T05:11:36.273+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:36 INFO ResourceUtils: ==============================================================
[2023-01-31T05:11:36.278+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:36 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T05:11:36.638+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:36 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T05:11:36.661+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:36 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T05:11:36.679+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:36 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T05:11:37.289+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:37 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T05:11:37.292+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:37 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T05:11:37.294+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:37 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T05:11:37.297+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:37 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T05:11:37.303+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:37 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T05:11:40.902+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:40 INFO Utils: Successfully started service 'sparkDriver' on port 38427.
[2023-01-31T05:11:41.239+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:41 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T05:11:41.717+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:41 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T05:11:42.088+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:42 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T05:11:42.110+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:42 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T05:11:42.215+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:42 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T05:11:42.409+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:42 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d7fcad89-fd04-48ec-8974-6905489c76fa
[2023-01-31T05:11:42.542+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:42 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T05:11:42.736+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:42 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T05:11:45.683+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:45 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T05:11:45.694+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:45 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2023-01-31T05:11:45.776+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:45 INFO Utils: Successfully started service 'SparkUI' on port 4042.
[2023-01-31T05:11:46.328+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:46 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://81d5fcd0285b:38427/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675141894041
[2023-01-31T05:11:46.345+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:46 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://81d5fcd0285b:38427/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675141894041
[2023-01-31T05:11:47.234+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:47 INFO Executor: Starting executor ID driver on host 81d5fcd0285b
[2023-01-31T05:11:47.283+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:47 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T05:11:47.494+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:47 INFO Executor: Fetching spark://81d5fcd0285b:38427/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675141894041
[2023-01-31T05:11:48.014+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:48 INFO TransportClientFactory: Successfully created connection to 81d5fcd0285b/172.19.0.9:38427 after 343 ms (0 ms spent in bootstraps)
[2023-01-31T05:11:48.108+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:48 INFO Utils: Fetching spark://81d5fcd0285b:38427/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-a9cec05e-0ca5-44a0-b848-00ad65ff68da/userFiles-a819ed0f-1466-45e5-a3a5-5cf99fe39274/fetchFileTemp4607501923503944968.tmp
[2023-01-31T05:11:50.197+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:50 INFO Executor: Adding file:/tmp/spark-a9cec05e-0ca5-44a0-b848-00ad65ff68da/userFiles-a819ed0f-1466-45e5-a3a5-5cf99fe39274/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T05:11:50.204+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:50 INFO Executor: Fetching spark://81d5fcd0285b:38427/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675141894041
[2023-01-31T05:11:50.227+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:50 INFO Utils: Fetching spark://81d5fcd0285b:38427/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-a9cec05e-0ca5-44a0-b848-00ad65ff68da/userFiles-a819ed0f-1466-45e5-a3a5-5cf99fe39274/fetchFileTemp3453613327379139609.tmp
[2023-01-31T05:11:52.012+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:52 INFO Executor: Adding file:/tmp/spark-a9cec05e-0ca5-44a0-b848-00ad65ff68da/userFiles-a819ed0f-1466-45e5-a3a5-5cf99fe39274/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T05:11:52.096+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:52 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37421.
[2023-01-31T05:11:52.097+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:52 INFO NettyBlockTransferService: Server created on 81d5fcd0285b:37421
[2023-01-31T05:11:52.119+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:52 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T05:11:52.232+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:52 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 37421, None)
[2023-01-31T05:11:52.289+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:52 INFO BlockManagerMasterEndpoint: Registering block manager 81d5fcd0285b:37421 with 434.4 MiB RAM, BlockManagerId(driver, 81d5fcd0285b, 37421, None)
[2023-01-31T05:11:52.320+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:52 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 37421, None)
[2023-01-31T05:11:52.326+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:52 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 81d5fcd0285b, 37421, None)
[2023-01-31T05:11:59.038+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:59 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T05:11:59.100+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:59 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T05:12:21.645+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:21 INFO InMemoryFileIndex: It took 660 ms to list leaf files for 1 paths.
[2023-01-31T05:12:23.418+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:23 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T05:12:24.275+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:24 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T05:12:24.306+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:24 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 81d5fcd0285b:37421 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T05:12:24.343+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:24 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T05:12:28.897+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:28 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T05:12:28.960+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:28 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T05:12:29.193+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:29 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T05:12:29.531+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:29 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T05:12:29.559+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:29 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T05:12:29.560+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:29 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:12:29.637+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:29 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:12:29.867+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:29 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T05:12:30.560+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:30 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T05:12:30.593+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:30 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T05:12:30.599+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:30 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 81d5fcd0285b:37421 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T05:12:30.605+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:30 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:12:30.732+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:12:30.739+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:30 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T05:12:31.941+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:31 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T05:12:32.076+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:32 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T05:12:33.510+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:33 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010100__202101010200.json:0+9369
[2023-01-31T05:12:35.252+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:35 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3437 bytes result sent to driver
[2023-01-31T05:12:35.359+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:35 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3780 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:12:35.371+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:35 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T05:12:35.462+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:35 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 5.362 s
[2023-01-31T05:12:35.555+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:35 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:12:35.556+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T05:12:35.581+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:35 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 6.385868 s
[2023-01-31T05:12:37.725+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:37 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 81d5fcd0285b:37421 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T05:12:37.804+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:37 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 81d5fcd0285b:37421 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T05:13:09.769+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:09 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:09.775+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:09 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:13:09.785+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:09 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:11.690+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:11 INFO CodeGenerator: Code generated in 989.825465 ms
[2023-01-31T05:13:11.745+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:11 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T05:13:11.823+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:11 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T05:13:11.826+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:11 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 81d5fcd0285b:37421 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T05:13:11.834+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:11 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T05:13:11.877+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:11 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:12.198+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:12 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T05:13:12.202+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:12 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T05:13:12.202+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:12 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T05:13:12.202+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:12 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:12.202+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:12 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:12.205+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:12 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T05:13:12.240+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:12 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T05:13:12.245+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:12 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T05:13:12.247+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:12 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 81d5fcd0285b:37421 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T05:13:12.250+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:12 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:12.255+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:12.263+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:12 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T05:13:12.275+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:12 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:12.280+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:12 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T05:13:12.871+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:12 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T05:13:13.734+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:13 INFO CodeGenerator: Code generated in 611.448414 ms
[2023-01-31T05:13:14.699+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1628 bytes result sent to driver
[2023-01-31T05:13:14.710+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2443 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:14.711+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T05:13:14.717+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 2.489 s
[2023-01-31T05:13:14.717+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:14.718+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T05:13:14.729+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 2.524018 s
[2023-01-31T05:13:15.152+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:15.158+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:13:15.175+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:15.387+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO CodeGenerator: Code generated in 106.065205 ms
[2023-01-31T05:13:15.439+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T05:13:15.636+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T05:13:15.638+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 81d5fcd0285b:37421 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:13:15.639+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T05:13:15.651+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:15.824+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T05:13:15.826+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T05:13:15.826+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T05:13:15.827+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:15.827+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:15.830+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T05:13:15.860+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T05:13:15.912+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T05:13:15.921+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 81d5fcd0285b:37421 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T05:13:15.925+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:15.935+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:15.939+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T05:13:15.953+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:15.955+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T05:13:16.057+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:16 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T05:13:16.493+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:16 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1585 bytes result sent to driver
[2023-01-31T05:13:16.521+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:16 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 569 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:16.522+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:16 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T05:13:16.529+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:16 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 0.686 s
[2023-01-31T05:13:16.532+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:16 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:16.532+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T05:13:16.536+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:16 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 0.711563 s
[2023-01-31T05:13:17.206+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:17.218+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T05:13:17.220+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:18.184+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO CodeGenerator: Code generated in 823.230402 ms
[2023-01-31T05:13:18.222+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T05:13:18.385+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T05:13:18.385+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 81d5fcd0285b:37421 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:13:18.398+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T05:13:18.407+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:18.507+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T05:13:18.513+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T05:13:18.516+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T05:13:18.517+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:18.519+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:18.532+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T05:13:18.577+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.6 MiB)
[2023-01-31T05:13:18.660+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.6 MiB)
[2023-01-31T05:13:18.671+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 81d5fcd0285b:37421 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T05:13:18.685+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:18.700+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:18.709+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T05:13:18.722+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:18.723+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T05:13:19.137+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T05:13:19.630+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1755 bytes result sent to driver
[2023-01-31T05:13:19.677+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 958 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:19.689+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 1.139 s
[2023-01-31T05:13:19.689+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:19.692+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T05:13:19.696+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T05:13:19.740+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 1.231705 s
[2023-01-31T05:13:20.124+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:20.126+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:13:20.132+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:20.353+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO CodeGenerator: Code generated in 130.020752 ms
[2023-01-31T05:13:20.407+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.4 MiB)
[2023-01-31T05:13:20.559+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T05:13:20.561+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 81d5fcd0285b:37421 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:13:20.566+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T05:13:20.598+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:20.894+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T05:13:20.898+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T05:13:20.898+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T05:13:20.898+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:20.899+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:20.906+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T05:13:20.947+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-31T05:13:21.009+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-01-31T05:13:21.016+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 81d5fcd0285b:37421 (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T05:13:21.017+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:21.028+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:21.030+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T05:13:21.036+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:21.037+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T05:13:21.106+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T05:13:21.376+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1586 bytes result sent to driver
[2023-01-31T05:13:21.383+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 347 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:21.384+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T05:13:21.386+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 0.475 s
[2023-01-31T05:13:21.387+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:21.387+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T05:13:21.387+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 0.490803 s
[2023-01-31T05:13:23.278+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:23.280+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T05:13:23.310+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:24.750+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 81d5fcd0285b:37421 in memory (size: 7.7 KiB, free: 434.2 MiB)
[2023-01-31T05:13:24.973+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 81d5fcd0285b:37421 in memory (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T05:13:25.128+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 81d5fcd0285b:37421 in memory (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T05:13:25.205+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 81d5fcd0285b:37421 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T05:13:25.310+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO CodeGenerator: Code generated in 1607.792941 ms
[2023-01-31T05:13:25.433+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.3 MiB)
[2023-01-31T05:13:25.583+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T05:13:25.587+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 81d5fcd0285b:37421 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:13:25.593+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T05:13:25.609+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:25.863+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T05:13:25.870+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) with 1 output partitions
[2023-01-31T05:13:25.871+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201)
[2023-01-31T05:13:25.872+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:25.872+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:25.873+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201), which has no missing parents
[2023-01-31T05:13:25.918+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.2 MiB)
[2023-01-31T05:13:25.936+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.2 MiB)
[2023-01-31T05:13:25.942+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 81d5fcd0285b:37421 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T05:13:25.956+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:25.965+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:25.966+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T05:13:25.981+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:25.982+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T05:13:26.077+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T05:13:26.311+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2602 bytes result sent to driver
[2023-01-31T05:13:26.315+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 335 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:26.316+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T05:13:26.323+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) finished in 0.442 s
[2023-01-31T05:13:26.324+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:26.324+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T05:13:26.325+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201, took 0.455162 s
[2023-01-31T05:13:26.526+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:26.528+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T05:13:26.534+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:27.214+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:27 INFO CodeGenerator: Code generated in 364.276636 ms
[2023-01-31T05:13:27.413+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:27 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 433.0 MiB)
[2023-01-31T05:13:27.706+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:27 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.0 MiB)
[2023-01-31T05:13:27.715+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:27 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 81d5fcd0285b:37421 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:13:27.720+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:27 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T05:13:27.742+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:27 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:28.080+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T05:13:28.081+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T05:13:28.082+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T05:13:28.082+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:28.085+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:28.095+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T05:13:28.148+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 432.9 MiB)
[2023-01-31T05:13:28.173+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.9 MiB)
[2023-01-31T05:13:28.179+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 81d5fcd0285b:37421 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T05:13:28.190+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:28.197+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:28.201+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T05:13:28.215+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:28.216+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T05:13:28.287+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T05:13:28.615+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1827 bytes result sent to driver
[2023-01-31T05:13:28.638+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 417 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:28.642+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T05:13:28.645+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 0.530 s
[2023-01-31T05:13:28.648+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:28.652+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T05:13:28.684+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 0.563693 s
[2023-01-31T05:13:29.037+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:29.049+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T05:13:29.052+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:29.962+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO CodeGenerator: Code generated in 547.891432 ms
[2023-01-31T05:13:30.014+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.7 MiB)
[2023-01-31T05:13:30.126+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.7 MiB)
[2023-01-31T05:13:30.130+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 81d5fcd0285b:37421 (size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T05:13:30.135+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T05:13:30.141+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:30.315+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T05:13:30.320+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T05:13:30.320+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T05:13:30.321+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:30.321+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:30.326+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T05:13:30.345+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.7 MiB)
[2023-01-31T05:13:30.364+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.6 MiB)
[2023-01-31T05:13:30.369+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 81d5fcd0285b:37421 (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T05:13:30.374+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:30.378+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:30.382+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T05:13:30.388+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:30.396+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T05:13:30.458+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T05:13:30.723+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1729 bytes result sent to driver
[2023-01-31T05:13:30.727+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 340 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:30.728+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T05:13:30.730+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 0.404 s
[2023-01-31T05:13:30.730+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:30.732+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T05:13:30.732+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 0.416450 s
[2023-01-31T05:13:41.080+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:41 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 81d5fcd0285b:37421 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:13:41.156+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:41 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 81d5fcd0285b:37421 in memory (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T05:13:41.230+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:41 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 81d5fcd0285b:37421 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T05:13:41.261+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:41 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 81d5fcd0285b:37421 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:13:41.298+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:41 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 81d5fcd0285b:37421 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:13:41.427+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:41 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 81d5fcd0285b:37421 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:13:41.502+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:41 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 81d5fcd0285b:37421 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:13:41.582+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:41 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 81d5fcd0285b:37421 in memory (size: 12.6 KiB, free: 434.3 MiB)
[2023-01-31T05:13:41.679+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:41 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 81d5fcd0285b:37421 in memory (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T05:13:59.695+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:59 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:13:59.778+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:13:59.779+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:13:59.788+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:59 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:13:59.790+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:13:59.792+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:13:59.806+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:14:00.751+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:00 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-31T05:14:00.754+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:00 INFO DAGScheduler: Got job 8 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T05:14:00.754+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:00 INFO DAGScheduler: Final stage: ResultStage 8 (save at BigQueryWriteHelper.java:105)
[2023-01-31T05:14:00.755+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:00 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:14:00.755+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:00 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:14:00.760+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:00 INFO DAGScheduler: Submitting ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T05:14:00.918+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:00 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 209.9 KiB, free 434.0 MiB)
[2023-01-31T05:14:01.028+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:00 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 74.3 KiB, free 433.9 MiB)
[2023-01-31T05:14:01.033+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 81d5fcd0285b:37421 (size: 74.3 KiB, free: 434.3 MiB)
[2023-01-31T05:14:01.033+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:14:01.034+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:14:01.034+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2023-01-31T05:14:01.035+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7484 bytes) taskResourceAssignments Map()
[2023-01-31T05:14:01.048+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2023-01-31T05:14:01.233+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:14:01.236+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:14:01.256+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:14:01.260+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:14:01.261+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:14:01.271+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:14:01.311+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T05:14:01.328+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T05:14:01.576+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO ParquetOutputFormat: Parquet block size to 134217728
[2023-01-31T05:14:01.581+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO ParquetOutputFormat: Validation is off
[2023-01-31T05:14:01.583+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-01-31T05:14:01.584+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO ParquetOutputFormat: Parquet properties are:
[2023-01-31T05:14:01.585+0000] {spark_submit.py:495} INFO - Parquet page size to 1048576
[2023-01-31T05:14:01.585+0000] {spark_submit.py:495} INFO - Parquet dictionary page size to 1048576
[2023-01-31T05:14:01.586+0000] {spark_submit.py:495} INFO - Dictionary is true
[2023-01-31T05:14:01.586+0000] {spark_submit.py:495} INFO - Writer version is: PARQUET_1_0
[2023-01-31T05:14:01.587+0000] {spark_submit.py:495} INFO - Page size checking is: estimated
[2023-01-31T05:14:01.587+0000] {spark_submit.py:495} INFO - Min row count for page size check is: 100
[2023-01-31T05:14:01.588+0000] {spark_submit.py:495} INFO - Max row count for page size check is: 10000
[2023-01-31T05:14:01.588+0000] {spark_submit.py:495} INFO - Truncate length for column indexes is: 64
[2023-01-31T05:14:01.597+0000] {spark_submit.py:495} INFO - Truncate length for statistics min/max  is: 2147483647
[2023-01-31T05:14:01.603+0000] {spark_submit.py:495} INFO - Bloom filter enabled: false
[2023-01-31T05:14:01.606+0000] {spark_submit.py:495} INFO - Max Bloom filter size for a column is 1048576
[2023-01-31T05:14:01.612+0000] {spark_submit.py:495} INFO - Bloom filter expected number of distinct values are: null
[2023-01-31T05:14:01.616+0000] {spark_submit.py:495} INFO - Page row count limit to 20000
[2023-01-31T05:14:01.617+0000] {spark_submit.py:495} INFO - Writing page checksums is: on
[2023-01-31T05:14:02.252+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:02 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-01-31T05:14:02.258+0000] {spark_submit.py:495} INFO - {
[2023-01-31T05:14:02.259+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-31T05:14:02.261+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-31T05:14:02.264+0000] {spark_submit.py:495} INFO - "name" : "measured_at",
[2023-01-31T05:14:02.267+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T05:14:02.284+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T05:14:02.285+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:02.298+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:02.301+0000] {spark_submit.py:495} INFO - "name" : "created_at",
[2023-01-31T05:14:02.319+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T05:14:02.333+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T05:14:02.342+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:02.343+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:02.360+0000] {spark_submit.py:495} INFO - "name" : "Biomass - Actual Aggregated",
[2023-01-31T05:14:02.366+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:02.370+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:02.373+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:02.374+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:02.379+0000] {spark_submit.py:495} INFO - "name" : "Fossil Brown coal/Lignite - Actual Aggregated",
[2023-01-31T05:14:02.381+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:02.384+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:02.386+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:02.387+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:02.387+0000] {spark_submit.py:495} INFO - "name" : "Fossil Gas - Actual Aggregated",
[2023-01-31T05:14:02.388+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:02.388+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:02.389+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:02.390+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:02.390+0000] {spark_submit.py:495} INFO - "name" : "Fossil Hard coal - Actual Aggregated",
[2023-01-31T05:14:02.391+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:02.391+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:02.391+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:02.392+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:02.392+0000] {spark_submit.py:495} INFO - "name" : "Fossil Oil - Actual Aggregated",
[2023-01-31T05:14:02.393+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:02.393+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:02.394+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:02.394+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:02.395+0000] {spark_submit.py:495} INFO - "name" : "Geothermal - Actual Aggregated",
[2023-01-31T05:14:02.395+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:02.409+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:02.410+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:02.410+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:02.415+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Aggregated",
[2023-01-31T05:14:02.416+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:02.425+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:02.425+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:02.426+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:02.427+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Consumption",
[2023-01-31T05:14:02.427+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:02.428+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:02.428+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:02.429+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:02.429+0000] {spark_submit.py:495} INFO - "name" : "Hydro Run-of-river and poundage - Actual Aggregated",
[2023-01-31T05:14:02.429+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:02.430+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:02.431+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:02.431+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:02.432+0000] {spark_submit.py:495} INFO - "name" : "Hydro Water Reservoir - Actual Aggregated",
[2023-01-31T05:14:02.432+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:02.433+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:02.433+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:02.433+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:02.434+0000] {spark_submit.py:495} INFO - "name" : "Nuclear - Actual Aggregated",
[2023-01-31T05:14:02.434+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:02.435+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:02.435+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:02.435+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:02.436+0000] {spark_submit.py:495} INFO - "name" : "Other - Actual Aggregated",
[2023-01-31T05:14:02.436+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:02.437+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:02.442+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:02.446+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:02.447+0000] {spark_submit.py:495} INFO - "name" : "Other renewable - Actual Aggregated",
[2023-01-31T05:14:02.448+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:02.449+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:02.449+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:02.450+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:02.450+0000] {spark_submit.py:495} INFO - "name" : "Solar - Actual Aggregated",
[2023-01-31T05:14:02.451+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:02.451+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:02.452+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:02.453+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:02.453+0000] {spark_submit.py:495} INFO - "name" : "Waste - Actual Aggregated",
[2023-01-31T05:14:02.454+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:02.454+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:02.455+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:02.455+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:02.456+0000] {spark_submit.py:495} INFO - "name" : "Wind Offshore - Actual Aggregated",
[2023-01-31T05:14:02.456+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:02.457+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:02.457+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:02.458+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:02.462+0000] {spark_submit.py:495} INFO - "name" : "Wind Onshore - Actual Aggregated",
[2023-01-31T05:14:02.467+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:02.467+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:02.469+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:02.469+0000] {spark_submit.py:495} INFO - } ]
[2023-01-31T05:14:02.470+0000] {spark_submit.py:495} INFO - }
[2023-01-31T05:14:02.471+0000] {spark_submit.py:495} INFO - and corresponding Parquet message type:
[2023-01-31T05:14:02.471+0000] {spark_submit.py:495} INFO - message spark_schema {
[2023-01-31T05:14:02.472+0000] {spark_submit.py:495} INFO - required int96 measured_at;
[2023-01-31T05:14:02.472+0000] {spark_submit.py:495} INFO - required int96 created_at;
[2023-01-31T05:14:02.473+0000] {spark_submit.py:495} INFO - optional double Biomass - Actual Aggregated;
[2023-01-31T05:14:02.474+0000] {spark_submit.py:495} INFO - optional double Fossil Brown coal/Lignite - Actual Aggregated;
[2023-01-31T05:14:02.477+0000] {spark_submit.py:495} INFO - optional double Fossil Gas - Actual Aggregated;
[2023-01-31T05:14:02.477+0000] {spark_submit.py:495} INFO - optional double Fossil Hard coal - Actual Aggregated;
[2023-01-31T05:14:02.479+0000] {spark_submit.py:495} INFO - optional double Fossil Oil - Actual Aggregated;
[2023-01-31T05:14:02.479+0000] {spark_submit.py:495} INFO - optional double Geothermal - Actual Aggregated;
[2023-01-31T05:14:02.481+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Aggregated;
[2023-01-31T05:14:02.486+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Consumption;
[2023-01-31T05:14:02.486+0000] {spark_submit.py:495} INFO - optional double Hydro Run-of-river and poundage - Actual Aggregated;
[2023-01-31T05:14:02.488+0000] {spark_submit.py:495} INFO - optional double Hydro Water Reservoir - Actual Aggregated;
[2023-01-31T05:14:02.489+0000] {spark_submit.py:495} INFO - optional double Nuclear - Actual Aggregated;
[2023-01-31T05:14:02.491+0000] {spark_submit.py:495} INFO - optional double Other - Actual Aggregated;
[2023-01-31T05:14:02.492+0000] {spark_submit.py:495} INFO - optional double Other renewable - Actual Aggregated;
[2023-01-31T05:14:02.492+0000] {spark_submit.py:495} INFO - optional double Solar - Actual Aggregated;
[2023-01-31T05:14:02.493+0000] {spark_submit.py:495} INFO - optional double Waste - Actual Aggregated;
[2023-01-31T05:14:02.497+0000] {spark_submit.py:495} INFO - optional double Wind Offshore - Actual Aggregated;
[2023-01-31T05:14:02.497+0000] {spark_submit.py:495} INFO - optional double Wind Onshore - Actual Aggregated;
[2023-01-31T05:14:02.498+0000] {spark_submit.py:495} INFO - }
[2023-01-31T05:14:02.498+0000] {spark_submit.py:495} INFO - 
[2023-01-31T05:14:02.499+0000] {spark_submit.py:495} INFO - 
[2023-01-31T05:14:03.464+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:03 INFO CodecPool: Got brand-new compressor [.snappy]
[2023-01-31T05:14:10.054+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:10 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675141906725-6a38f81b-3c04-4138-aebb-a4ce57b04d53/_temporary/0/_temporary/' directory.
[2023-01-31T05:14:10.060+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:10 INFO FileOutputCommitter: Saved output of task 'attempt_20230131051400728185770031411967_0008_m_000000_8' to gs://entsoe_temp_1009/.spark-bigquery-local-1675141906725-6a38f81b-3c04-4138-aebb-a4ce57b04d53/_temporary/0/task_20230131051400728185770031411967_0008_m_000000
[2023-01-31T05:14:10.064+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:10 INFO SparkHadoopMapRedUtil: attempt_20230131051400728185770031411967_0008_m_000000_8: Committed. Elapsed time: 2444 ms.
[2023-01-31T05:14:10.217+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:10 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2588 bytes result sent to driver
[2023-01-31T05:14:10.224+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:10 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 9196 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:14:10.225+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:10 INFO DAGScheduler: ResultStage 8 (save at BigQueryWriteHelper.java:105) finished in 9.458 s
[2023-01-31T05:14:10.226+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:10 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:14:10.226+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:10 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2023-01-31T05:14:10.232+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2023-01-31T05:14:10.235+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:10 INFO DAGScheduler: Job 8 finished: save at BigQueryWriteHelper.java:105, took 9.476811 s
[2023-01-31T05:14:10.239+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:10 INFO FileFormatWriter: Start to commit write Job d1518355-9fed-4afe-9192-149892bba72f.
[2023-01-31T05:14:11.173+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:11 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675141906725-6a38f81b-3c04-4138-aebb-a4ce57b04d53/_temporary/0/task_20230131051400728185770031411967_0008_m_000000/' directory.
[2023-01-31T05:14:11.874+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:11 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675141906725-6a38f81b-3c04-4138-aebb-a4ce57b04d53/' directory.
[2023-01-31T05:14:12.294+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:12 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 81d5fcd0285b:37421 in memory (size: 74.3 KiB, free: 434.4 MiB)
[2023-01-31T05:14:13.027+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:13 INFO FileFormatWriter: Write Job d1518355-9fed-4afe-9192-149892bba72f committed. Elapsed time: 2777 ms.
[2023-01-31T05:14:13.097+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:13 INFO FileFormatWriter: Finished processing stats for write job d1518355-9fed-4afe-9192-149892bba72f.
[2023-01-31T05:14:15.591+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:15 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=entsoe_analytics_1009, projectId=rafzul-analytics-1009, tableId=TEST_total_generation_staging}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=null, ignoreUnknownValue=null, sourceUris=[gs://entsoe_temp_1009/.spark-bigquery-local-1675141906725-6a38f81b-3c04-4138-aebb-a4ce57b04d53/part-00000-2b231d04-3e0e-4030-a9aa-6708d1cd0940-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=true, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=rafzul-analytics-1009, job=d4fc32f3-e935-4736-84b4-336f360371c9, location=US}
[2023-01-31T05:14:20.764+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:20 INFO BigQueryClient: Done loading to rafzul-analytics-1009.entsoe_analytics_1009.TEST_total_generation_staging. jobId: JobId{project=rafzul-analytics-1009, job=d4fc32f3-e935-4736-84b4-336f360371c9, location=US}
[2023-01-31T05:14:21.194+0000] {spark_submit.py:495} INFO - MANTEP!!!!
[2023-01-31T05:14:21.342+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:21 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-31T05:14:21.357+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:21 INFO SparkUI: Stopped Spark web UI at http://81d5fcd0285b:4042
[2023-01-31T05:14:21.376+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:21 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-31T05:14:21.398+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:21 INFO MemoryStore: MemoryStore cleared
[2023-01-31T05:14:21.399+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:21 INFO BlockManager: BlockManager stopped
[2023-01-31T05:14:21.405+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:21 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-31T05:14:21.410+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:21 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-31T05:14:21.420+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:21 INFO SparkContext: Successfully stopped SparkContext
[2023-01-31T05:14:21.421+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:21 INFO ShutdownHookManager: Shutdown hook called
[2023-01-31T05:14:21.422+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-e8d8e790-70a3-41c3-b4be-e9c2605edcc2
[2023-01-31T05:14:21.430+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-a9cec05e-0ca5-44a0-b848-00ad65ff68da
[2023-01-31T05:14:21.437+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-a9cec05e-0ca5-44a0-b848-00ad65ff68da/pyspark-8ff7af60-bc17-4d66-9768-1a67ace79b1b
[2023-01-31T05:14:21.606+0000] {taskinstance.py:1322} INFO - Marking task as SUCCESS. dag_id=entsoe-energydata-backfill, task_id=stage_total_generation, execution_date=20210101T010000, start_date=20230131T051101, end_date=20230131T051421
[2023-01-31T05:14:21.667+0000] {local_task_job.py:159} INFO - Task exited with return code 0
[2023-01-31T05:14:21.688+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T05:30:45.864+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-31T05:30:45.952+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-31T05:30:45.957+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T05:30:45.961+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T05:30:45.968+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T05:30:46.380+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 01:00:00+00:00
[2023-01-31T05:30:46.486+0000] {standard_task_runner.py:55} INFO - Started process 15814 to run task
[2023-01-31T05:30:46.509+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '896', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmphe0p_osj']
[2023-01-31T05:30:46.521+0000] {standard_task_runner.py:83} INFO - Job 896: Subtask stage_total_generation
[2023-01-31T05:30:47.911+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 81d5fcd0285b
[2023-01-31T05:30:48.682+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-31T05:30:48.890+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T05:30:48.920+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-31T05:32:05.062+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:04 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T05:32:07.110+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T05:32:10.982+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:10 INFO ResourceUtils: ==============================================================
[2023-01-31T05:32:10.987+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:10 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T05:32:10.996+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:10 INFO ResourceUtils: ==============================================================
[2023-01-31T05:32:11.000+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:10 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T05:32:11.275+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:11 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T05:32:11.316+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:11 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T05:32:11.334+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:11 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T05:32:11.979+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:11 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T05:32:11.984+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:11 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T05:32:11.989+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:11 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T05:32:11.993+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:11 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T05:32:11.999+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:11 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T05:32:20.201+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:20 INFO Utils: Successfully started service 'sparkDriver' on port 36923.
[2023-01-31T05:32:21.318+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:21 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T05:32:21.684+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:21 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T05:32:22.859+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:22 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T05:32:22.903+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:22 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T05:32:23.165+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:23 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T05:32:23.671+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:23 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f2685689-99e4-4b87-8e5c-25d7fbdae8b0
[2023-01-31T05:32:23.998+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:23 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T05:32:25.183+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:25 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T05:32:32.023+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:32 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T05:32:32.028+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:32 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2023-01-31T05:32:32.032+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:32 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[2023-01-31T05:32:32.041+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:32 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[2023-01-31T05:32:32.122+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:32 INFO Utils: Successfully started service 'SparkUI' on port 4044.
[2023-01-31T05:32:32.837+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:32 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://81d5fcd0285b:36923/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675143124413
[2023-01-31T05:32:32.840+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:32 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://81d5fcd0285b:36923/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675143124413
[2023-01-31T05:32:34.964+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:34 INFO Executor: Starting executor ID driver on host 81d5fcd0285b
[2023-01-31T05:32:35.353+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:35 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T05:32:35.590+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:35 INFO Executor: Fetching spark://81d5fcd0285b:36923/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675143124413
[2023-01-31T05:32:36.937+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:36 INFO TransportClientFactory: Successfully created connection to 81d5fcd0285b/172.19.0.9:36923 after 894 ms (0 ms spent in bootstraps)
[2023-01-31T05:32:37.105+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:37 INFO Utils: Fetching spark://81d5fcd0285b:36923/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-8d32b5c8-46f2-48c3-88a0-537d51ae3452/userFiles-f5b1038c-7a86-46a2-a5f6-e6b26d37ec93/fetchFileTemp4997795712983897854.tmp
[2023-01-31T05:32:43.755+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:43 INFO Executor: Adding file:/tmp/spark-8d32b5c8-46f2-48c3-88a0-537d51ae3452/userFiles-f5b1038c-7a86-46a2-a5f6-e6b26d37ec93/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T05:32:43.790+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:43 INFO Executor: Fetching spark://81d5fcd0285b:36923/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675143124413
[2023-01-31T05:32:43.830+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:43 INFO Utils: Fetching spark://81d5fcd0285b:36923/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-8d32b5c8-46f2-48c3-88a0-537d51ae3452/userFiles-f5b1038c-7a86-46a2-a5f6-e6b26d37ec93/fetchFileTemp15450079157130938286.tmp
[2023-01-31T05:32:50.705+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:50 INFO Executor: Adding file:/tmp/spark-8d32b5c8-46f2-48c3-88a0-537d51ae3452/userFiles-f5b1038c-7a86-46a2-a5f6-e6b26d37ec93/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T05:32:50.849+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:50 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44295.
[2023-01-31T05:32:50.850+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:50 INFO NettyBlockTransferService: Server created on 81d5fcd0285b:44295
[2023-01-31T05:32:50.925+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:50 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T05:32:51.033+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:51 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 44295, None)
[2023-01-31T05:32:51.150+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:51 INFO BlockManagerMasterEndpoint: Registering block manager 81d5fcd0285b:44295 with 434.4 MiB RAM, BlockManagerId(driver, 81d5fcd0285b, 44295, None)
[2023-01-31T05:32:51.209+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:51 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 44295, None)
[2023-01-31T05:32:51.256+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:51 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 81d5fcd0285b, 44295, None)
[2023-01-31T05:32:52.088+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:52 INFO Executor: Told to re-register on heartbeat
[2023-01-31T05:32:52.122+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:52 INFO BlockManager: BlockManager BlockManagerId(driver, 81d5fcd0285b, 44295, None) re-registering with master
[2023-01-31T05:32:52.126+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:52 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 44295, None)
[2023-01-31T05:32:52.198+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:52 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 44295, None)
[2023-01-31T05:32:52.200+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:52 INFO BlockManager: Reporting 0 blocks to the master.
[2023-01-31T05:33:08.785+0000] {spark_submit.py:495} INFO - 23/01/31 05:33:08 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T05:33:08.904+0000] {spark_submit.py:495} INFO - 23/01/31 05:33:08 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T05:34:10.341+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:10 INFO InMemoryFileIndex: It took 1860 ms to list leaf files for 1 paths.
[2023-01-31T05:34:13.604+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:13 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T05:34:15.315+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:15 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T05:34:15.381+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:15 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 81d5fcd0285b:44295 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T05:34:15.483+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:15 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T05:34:28.531+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:28 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T05:34:28.941+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:28 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T05:34:29.763+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:29 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T05:34:30.748+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:30 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T05:34:30.773+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:30 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T05:34:30.803+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:30 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:34:30.859+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:30 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:34:31.084+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:31 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T05:34:32.344+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:32 INFO AsyncEventQueue: Process of event SparkListenerJobStart(0,1675143270860,ArraySeq(org.apache.spark.scheduler.StageInfo@40892729),{spark.master=local, spark.driver.port=36923, spark.submit.pyFiles=, spark.app.startTime=1675143124413, spark.rdd.compress=True, spark.executor.id=driver, spark.app.name=gcp_playground, spark.submit.deployMode=client, spark.hadoop.google.cloud.auth.service.account.json.keyfile=/.google/credentials/google_credentials.json, spark.driver.host=81d5fcd0285b, spark.app.id=local-1675143153611, spark.app.submitTime=1675143103441, spark.hadoop.fs.AbstractFileSystem.gs.impl=google.cloud.hadoop.fs.gcs.GoogleHadoopFS, spark.sql.session.timeZone=UTC, spark.hadoop.google.cloud.auth.service.account.enable=true, spark.app.initial.jar.urls=spark://81d5fcd0285b:36923/jars/gcs-connector-hadoop3-latest.jar,spark://81d5fcd0285b:36923/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar, spark.executor.extraJavaOptions=-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED, spark.jars=/opt/spark/jars/gcs-connector-hadoop3-latest.jar, /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar, spark.sql.warehouse.dir=file:/opt/***/spark-warehouse, spark.serializer.objectStreamReset=100, spark.driver.extraJavaOptions=-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED}) by listener AppStatusListener took 1.295189893s.
[2023-01-31T05:34:34.978+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:34 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T05:34:35.233+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:35 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T05:34:35.282+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:35 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 81d5fcd0285b:44295 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T05:34:35.372+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:35 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:34:37.047+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:34:37.111+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:37 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T05:34:39.402+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:39 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T05:34:39.750+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:39 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T05:34:45.140+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:45 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010100__202101010200.json:0+9369
[2023-01-31T05:34:51.249+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:51 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3480 bytes result sent to driver
[2023-01-31T05:34:51.765+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:51 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 12926 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:34:51.794+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:51 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T05:34:51.875+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:51 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 19.841 s
[2023-01-31T05:34:51.899+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:51 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:34:51.903+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T05:34:52.023+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:52 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 22.179234 s
[2023-01-31T05:35:17.431+0000] {spark_submit.py:495} INFO - 23/01/31 05:35:17 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 81d5fcd0285b:44295 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T05:35:17.518+0000] {spark_submit.py:495} INFO - 23/01/31 05:35:17 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 81d5fcd0285b:44295 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T05:36:19.017+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:19 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:36:19.027+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:19 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:36:19.041+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:19 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:36:29.245+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:29 INFO CodeGenerator: Code generated in 6081.401696 ms
[2023-01-31T05:36:29.541+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:29 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T05:36:30.031+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:30 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T05:36:30.048+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:30 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 81d5fcd0285b:44295 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T05:36:30.084+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:30 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T05:36:30.699+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:36:31.697+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:31 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T05:36:31.711+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:31 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T05:36:31.713+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:31 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T05:36:31.715+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:31 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:36:31.740+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:31 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:36:31.762+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:31 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T05:36:32.123+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:32 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T05:36:32.598+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:32 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T05:36:32.614+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:32 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 81d5fcd0285b:44295 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T05:36:32.631+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:32 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:36:32.652+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:36:32.655+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:32 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T05:36:32.770+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:32 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:36:32.799+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:32 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T05:36:34.485+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:34 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T05:36:39.496+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:39 INFO CodeGenerator: Code generated in 3647.8571 ms
[2023-01-31T05:36:42.851+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:42 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1671 bytes result sent to driver
[2023-01-31T05:36:42.894+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:42 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 10215 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:36:42.905+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:42 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T05:36:42.926+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:42 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 10.980 s
[2023-01-31T05:36:42.929+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:42 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:36:42.935+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T05:36:42.945+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:42 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 11.243056 s
[2023-01-31T05:36:45.257+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:45 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:36:45.265+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:45 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:36:45.265+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:45 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:36:46.553+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:46 INFO CodeGenerator: Code generated in 703.115493 ms
[2023-01-31T05:36:46.709+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:46 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T05:36:46.969+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:46 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T05:36:46.993+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:46 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 81d5fcd0285b:44295 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:36:47.019+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:47 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T05:36:47.126+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:47 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:36:47.966+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:47 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T05:36:47.991+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:47 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T05:36:48.020+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:48 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T05:36:48.025+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:48 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:36:48.053+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:48 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:36:48.129+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:48 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T05:36:48.495+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:48 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T05:36:48.783+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:48 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T05:36:48.811+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:48 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 81d5fcd0285b:44295 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T05:36:48.833+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:48 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:36:48.859+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:36:48.885+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:48 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T05:36:48.894+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:48 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:36:48.986+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:48 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T05:36:49.559+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:49 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T05:36:51.376+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:51 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1585 bytes result sent to driver
[2023-01-31T05:36:51.485+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:51 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 2541 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:36:51.497+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:51 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 3.356 s
[2023-01-31T05:36:51.500+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:51 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:36:51.532+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:51 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T05:36:51.535+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T05:36:51.549+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:51 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 3.570556 s
[2023-01-31T05:36:57.782+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:57 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:36:57.868+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:57 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T05:36:57.885+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:57 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:37:03.574+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:03 INFO CodeGenerator: Code generated in 3106.970361 ms
[2023-01-31T05:37:03.667+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:03 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T05:37:03.770+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:03 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T05:37:03.771+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:03 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 81d5fcd0285b:44295 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:37:03.801+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:03 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T05:37:03.802+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:03 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:37:04.483+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:04 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 81d5fcd0285b:44295 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T05:37:04.839+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:04 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 81d5fcd0285b:44295 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T05:37:05.036+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:05 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T05:37:05.073+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:05 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T05:37:05.082+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:05 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T05:37:05.090+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:05 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:37:05.143+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:05 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:37:05.184+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:05 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T05:37:05.345+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:05 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.7 MiB)
[2023-01-31T05:37:05.379+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:05 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.7 MiB)
[2023-01-31T05:37:05.394+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:05 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 81d5fcd0285b:44295 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T05:37:05.417+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:05 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:37:05.439+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:37:05.441+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:05 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T05:37:05.459+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:05 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:37:05.505+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:05 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T05:37:05.898+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:05 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T05:37:07.505+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:07 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-01-31T05:37:07.585+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:07 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 2120 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:37:07.586+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:07 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 2.323 s
[2023-01-31T05:37:07.586+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:07 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:37:07.637+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:07 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T05:37:07.658+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T05:37:07.659+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:07 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 2.571065 s
[2023-01-31T05:37:09.432+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:09 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:37:09.433+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:09 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:37:09.433+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:09 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:37:10.352+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:10 INFO CodeGenerator: Code generated in 439.952172 ms
[2023-01-31T05:37:10.733+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:10 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.5 MiB)
[2023-01-31T05:37:11.537+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T05:37:11.604+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 81d5fcd0285b:44295 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:37:11.620+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T05:37:11.698+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:37:13.207+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:13 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T05:37:13.237+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:13 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T05:37:13.257+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:13 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T05:37:13.277+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:13 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:37:13.303+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:13 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:37:13.381+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:13 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T05:37:13.648+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:13 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-31T05:37:13.780+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:13 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-01-31T05:37:13.788+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:13 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 81d5fcd0285b:44295 (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T05:37:13.846+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:13 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:37:13.947+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:37:13.950+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:13 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T05:37:14.061+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:14 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:37:14.067+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:14 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T05:37:14.262+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:14 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T05:37:17.275+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:17 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1585 bytes result sent to driver
[2023-01-31T05:37:17.413+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:17 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 3356 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:37:17.423+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:17 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 3.976 s
[2023-01-31T05:37:17.464+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:17 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T05:37:17.467+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:17 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:37:17.484+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T05:37:17.493+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:17 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 4.270765 s
[2023-01-31T05:37:22.858+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:22 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:37:22.862+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:22 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T05:37:22.865+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:22 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:37:24.919+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:24 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 81d5fcd0285b:44295 in memory (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T05:37:25.302+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:25 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 81d5fcd0285b:44295 in memory (size: 7.6 KiB, free: 434.3 MiB)
[2023-01-31T05:37:27.591+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:27 INFO CodeGenerator: Code generated in 2548.343717 ms
[2023-01-31T05:37:27.611+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:27 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.3 MiB)
[2023-01-31T05:37:27.993+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:27 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T05:37:28.019+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:28 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 81d5fcd0285b:44295 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:37:28.034+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:28 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T05:37:28.052+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:28 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:37:28.318+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:28 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T05:37:28.426+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:28 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) with 1 output partitions
[2023-01-31T05:37:28.430+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:28 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201)
[2023-01-31T05:37:28.431+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:28 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:37:28.495+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:28 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:37:28.510+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:28 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201), which has no missing parents
[2023-01-31T05:37:28.809+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:28 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.2 MiB)
[2023-01-31T05:37:28.882+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:28 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.2 MiB)
[2023-01-31T05:37:28.885+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:28 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 81d5fcd0285b:44295 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T05:37:28.888+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:28 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:37:28.897+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:37:28.907+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:28 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T05:37:28.924+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:28 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:37:28.926+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:28 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T05:37:28.976+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:28 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T05:37:29.410+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2559 bytes result sent to driver
[2023-01-31T05:37:29.416+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 494 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:37:29.416+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T05:37:29.419+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) finished in 0.831 s
[2023-01-31T05:37:29.420+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:37:29.421+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T05:37:29.421+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201, took 1.100595 s
[2023-01-31T05:37:30.132+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:30 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:37:30.135+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:30 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T05:37:30.139+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:30 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:37:31.953+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:31 INFO CodeGenerator: Code generated in 1317.025548 ms
[2023-01-31T05:37:32.078+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:32 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 433.0 MiB)
[2023-01-31T05:37:32.270+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:32 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 81d5fcd0285b:44295 in memory (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T05:37:32.365+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:32 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.0 MiB)
[2023-01-31T05:37:32.370+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:32 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 81d5fcd0285b:44295 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:37:32.403+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:32 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T05:37:32.464+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:32 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:37:32.916+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:32 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T05:37:32.921+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:32 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T05:37:32.924+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:32 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T05:37:32.925+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:32 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:37:32.931+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:32 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:37:32.959+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:32 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T05:37:33.106+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:33 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 433.0 MiB)
[2023-01-31T05:37:33.173+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:33 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 433.0 MiB)
[2023-01-31T05:37:33.197+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:33 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 81d5fcd0285b:44295 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T05:37:33.227+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:33 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:37:33.269+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:37:33.272+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:33 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T05:37:33.311+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:33 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:37:33.317+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:33 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T05:37:33.714+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:33 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T05:37:34.686+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:34 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-01-31T05:37:34.693+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:34 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 1383 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:37:34.695+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:34 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T05:37:34.706+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:34 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 1.714 s
[2023-01-31T05:37:34.713+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:34 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:37:34.716+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T05:37:34.737+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:34 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 1.814459 s
[2023-01-31T05:37:35.351+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:35 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:37:35.362+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:35 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T05:37:35.370+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:35 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:37:36.573+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:36 INFO CodeGenerator: Code generated in 929.771343 ms
[2023-01-31T05:37:36.597+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:36 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.8 MiB)
[2023-01-31T05:37:36.669+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:36 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.7 MiB)
[2023-01-31T05:37:36.675+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:36 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 81d5fcd0285b:44295 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:37:36.680+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:36 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T05:37:36.689+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:36 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:37:37.204+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:37 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T05:37:37.221+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:37 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T05:37:37.222+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:37 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T05:37:37.223+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:37 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:37:37.223+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:37 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:37:37.243+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:37 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T05:37:37.293+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:37 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.7 MiB)
[2023-01-31T05:37:37.463+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:37 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.7 MiB)
[2023-01-31T05:37:37.487+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:37 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 81d5fcd0285b:44295 (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T05:37:37.492+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:37 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:37:37.505+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:37:37.509+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:37 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T05:37:37.535+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:37 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:37:37.543+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:37 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T05:37:37.661+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:37 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T05:37:38.137+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:38 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1772 bytes result sent to driver
[2023-01-31T05:37:38.145+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:38 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 607 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:37:38.147+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:38 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 0.884 s
[2023-01-31T05:37:38.166+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:38 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T05:37:38.175+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:38 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:37:38.178+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T05:37:38.197+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:38 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 0.940383 s
[2023-01-31T05:37:41.532+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:41 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 81d5fcd0285b:44295 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:37:41.669+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:41 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 81d5fcd0285b:44295 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:37:41.883+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:41 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 81d5fcd0285b:44295 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T05:37:42.098+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:42 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 81d5fcd0285b:44295 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T05:37:42.242+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:42 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 81d5fcd0285b:44295 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:37:42.369+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:42 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 81d5fcd0285b:44295 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:37:42.460+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:42 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 81d5fcd0285b:44295 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:37:42.562+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:42 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 81d5fcd0285b:44295 in memory (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T05:38:45.089+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:45 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:38:45.289+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:38:45.290+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:38:45.303+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:45 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:38:45.308+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:38:45.309+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:38:45.328+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:38:49.069+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:49 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-31T05:38:49.073+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:49 INFO DAGScheduler: Got job 8 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T05:38:49.086+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:49 INFO DAGScheduler: Final stage: ResultStage 8 (save at BigQueryWriteHelper.java:105)
[2023-01-31T05:38:49.088+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:49 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:38:49.108+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:49 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:38:49.170+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:49 INFO DAGScheduler: Submitting ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T05:38:50.405+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:50 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 209.9 KiB, free 434.0 MiB)
[2023-01-31T05:38:50.411+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:50 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 74.3 KiB, free 433.9 MiB)
[2023-01-31T05:38:50.496+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:50 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 81d5fcd0285b:44295 (size: 74.3 KiB, free: 434.3 MiB)
[2023-01-31T05:38:50.497+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:50 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:38:50.498+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:38:50.498+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:50 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2023-01-31T05:38:50.563+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:50 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7484 bytes) taskResourceAssignments Map()
[2023-01-31T05:38:50.574+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:50 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2023-01-31T05:38:51.059+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:38:51.060+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:38:51.067+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:38:51.069+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:38:51.073+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:38:51.086+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:38:51.142+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:51 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T05:38:51.181+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:51 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T05:38:51.828+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:51 INFO ParquetOutputFormat: Parquet block size to 134217728
[2023-01-31T05:38:51.828+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:51 INFO ParquetOutputFormat: Validation is off
[2023-01-31T05:38:51.831+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:51 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-01-31T05:38:51.832+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:51 INFO ParquetOutputFormat: Parquet properties are:
[2023-01-31T05:38:51.832+0000] {spark_submit.py:495} INFO - Parquet page size to 1048576
[2023-01-31T05:38:51.839+0000] {spark_submit.py:495} INFO - Parquet dictionary page size to 1048576
[2023-01-31T05:38:51.839+0000] {spark_submit.py:495} INFO - Dictionary is true
[2023-01-31T05:38:51.840+0000] {spark_submit.py:495} INFO - Writer version is: PARQUET_1_0
[2023-01-31T05:38:51.840+0000] {spark_submit.py:495} INFO - Page size checking is: estimated
[2023-01-31T05:38:51.862+0000] {spark_submit.py:495} INFO - Min row count for page size check is: 100
[2023-01-31T05:38:51.865+0000] {spark_submit.py:495} INFO - Max row count for page size check is: 10000
[2023-01-31T05:38:51.866+0000] {spark_submit.py:495} INFO - Truncate length for column indexes is: 64
[2023-01-31T05:38:51.882+0000] {spark_submit.py:495} INFO - Truncate length for statistics min/max  is: 2147483647
[2023-01-31T05:38:51.890+0000] {spark_submit.py:495} INFO - Bloom filter enabled: false
[2023-01-31T05:38:51.916+0000] {spark_submit.py:495} INFO - Max Bloom filter size for a column is 1048576
[2023-01-31T05:38:51.918+0000] {spark_submit.py:495} INFO - Bloom filter expected number of distinct values are: null
[2023-01-31T05:38:51.922+0000] {spark_submit.py:495} INFO - Page row count limit to 20000
[2023-01-31T05:38:51.924+0000] {spark_submit.py:495} INFO - Writing page checksums is: on
[2023-01-31T05:38:52.783+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:52 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-01-31T05:38:52.784+0000] {spark_submit.py:495} INFO - {
[2023-01-31T05:38:52.785+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-31T05:38:52.786+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-31T05:38:52.788+0000] {spark_submit.py:495} INFO - "name" : "measured_at",
[2023-01-31T05:38:52.789+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T05:38:52.804+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T05:38:52.813+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:52.818+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:52.820+0000] {spark_submit.py:495} INFO - "name" : "created_at",
[2023-01-31T05:38:52.827+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T05:38:52.832+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T05:38:52.839+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:52.845+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:52.858+0000] {spark_submit.py:495} INFO - "name" : "Biomass - Actual Aggregated",
[2023-01-31T05:38:52.874+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:52.879+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:52.888+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:52.897+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:52.919+0000] {spark_submit.py:495} INFO - "name" : "Fossil Brown coal/Lignite - Actual Aggregated",
[2023-01-31T05:38:52.938+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:52.943+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:52.947+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:52.953+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:52.957+0000] {spark_submit.py:495} INFO - "name" : "Fossil Gas - Actual Aggregated",
[2023-01-31T05:38:52.966+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:52.975+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:52.980+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:52.982+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:52.983+0000] {spark_submit.py:495} INFO - "name" : "Fossil Hard coal - Actual Aggregated",
[2023-01-31T05:38:52.985+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:52.986+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:52.986+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:52.987+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:52.987+0000] {spark_submit.py:495} INFO - "name" : "Fossil Oil - Actual Aggregated",
[2023-01-31T05:38:52.988+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:52.988+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:52.989+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:52.989+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:52.990+0000] {spark_submit.py:495} INFO - "name" : "Geothermal - Actual Aggregated",
[2023-01-31T05:38:52.991+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:52.991+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:52.992+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:52.992+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:52.993+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Aggregated",
[2023-01-31T05:38:52.993+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:52.994+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:52.995+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:52.995+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:52.996+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Consumption",
[2023-01-31T05:38:52.996+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:52.997+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:52.997+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:52.998+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:52.998+0000] {spark_submit.py:495} INFO - "name" : "Hydro Run-of-river and poundage - Actual Aggregated",
[2023-01-31T05:38:52.999+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:52.999+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:53.000+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:53.000+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:53.001+0000] {spark_submit.py:495} INFO - "name" : "Hydro Water Reservoir - Actual Aggregated",
[2023-01-31T05:38:53.001+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:53.002+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:53.002+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:53.003+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:53.003+0000] {spark_submit.py:495} INFO - "name" : "Nuclear - Actual Aggregated",
[2023-01-31T05:38:53.004+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:53.005+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:53.006+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:53.006+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:53.007+0000] {spark_submit.py:495} INFO - "name" : "Other - Actual Aggregated",
[2023-01-31T05:38:53.007+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:53.008+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:53.008+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:53.009+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:53.009+0000] {spark_submit.py:495} INFO - "name" : "Other renewable - Actual Aggregated",
[2023-01-31T05:38:53.010+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:53.010+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:53.011+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:53.011+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:53.012+0000] {spark_submit.py:495} INFO - "name" : "Solar - Actual Aggregated",
[2023-01-31T05:38:53.012+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:53.013+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:53.013+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:53.014+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:53.015+0000] {spark_submit.py:495} INFO - "name" : "Waste - Actual Aggregated",
[2023-01-31T05:38:53.015+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:53.016+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:53.016+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:53.016+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:53.017+0000] {spark_submit.py:495} INFO - "name" : "Wind Offshore - Actual Aggregated",
[2023-01-31T05:38:53.018+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:53.018+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:53.019+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:53.019+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:53.020+0000] {spark_submit.py:495} INFO - "name" : "Wind Onshore - Actual Aggregated",
[2023-01-31T05:38:53.020+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:53.021+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:53.021+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:53.022+0000] {spark_submit.py:495} INFO - } ]
[2023-01-31T05:38:53.023+0000] {spark_submit.py:495} INFO - }
[2023-01-31T05:38:53.023+0000] {spark_submit.py:495} INFO - and corresponding Parquet message type:
[2023-01-31T05:38:53.024+0000] {spark_submit.py:495} INFO - message spark_schema {
[2023-01-31T05:38:53.024+0000] {spark_submit.py:495} INFO - required int96 measured_at;
[2023-01-31T05:38:53.025+0000] {spark_submit.py:495} INFO - required int96 created_at;
[2023-01-31T05:38:53.025+0000] {spark_submit.py:495} INFO - optional double Biomass - Actual Aggregated;
[2023-01-31T05:38:53.026+0000] {spark_submit.py:495} INFO - optional double Fossil Brown coal/Lignite - Actual Aggregated;
[2023-01-31T05:38:53.026+0000] {spark_submit.py:495} INFO - optional double Fossil Gas - Actual Aggregated;
[2023-01-31T05:38:53.027+0000] {spark_submit.py:495} INFO - optional double Fossil Hard coal - Actual Aggregated;
[2023-01-31T05:38:53.027+0000] {spark_submit.py:495} INFO - optional double Fossil Oil - Actual Aggregated;
[2023-01-31T05:38:53.028+0000] {spark_submit.py:495} INFO - optional double Geothermal - Actual Aggregated;
[2023-01-31T05:38:53.028+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Aggregated;
[2023-01-31T05:38:53.029+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Consumption;
[2023-01-31T05:38:53.029+0000] {spark_submit.py:495} INFO - optional double Hydro Run-of-river and poundage - Actual Aggregated;
[2023-01-31T05:38:53.030+0000] {spark_submit.py:495} INFO - optional double Hydro Water Reservoir - Actual Aggregated;
[2023-01-31T05:38:53.030+0000] {spark_submit.py:495} INFO - optional double Nuclear - Actual Aggregated;
[2023-01-31T05:38:53.031+0000] {spark_submit.py:495} INFO - optional double Other - Actual Aggregated;
[2023-01-31T05:38:53.032+0000] {spark_submit.py:495} INFO - optional double Other renewable - Actual Aggregated;
[2023-01-31T05:38:53.032+0000] {spark_submit.py:495} INFO - optional double Solar - Actual Aggregated;
[2023-01-31T05:38:53.033+0000] {spark_submit.py:495} INFO - optional double Waste - Actual Aggregated;
[2023-01-31T05:38:53.033+0000] {spark_submit.py:495} INFO - optional double Wind Offshore - Actual Aggregated;
[2023-01-31T05:38:53.034+0000] {spark_submit.py:495} INFO - optional double Wind Onshore - Actual Aggregated;
[2023-01-31T05:38:53.034+0000] {spark_submit.py:495} INFO - }
[2023-01-31T05:38:53.035+0000] {spark_submit.py:495} INFO - 
[2023-01-31T05:38:53.036+0000] {spark_submit.py:495} INFO - 
[2023-01-31T05:38:56.270+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:56 INFO CodecPool: Got brand-new compressor [.snappy]
[2023-01-31T05:39:14.754+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:14 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675143153611-bc4991d9-9568-4e1a-90c9-74a335f770db/_temporary/0/_temporary/' directory.
[2023-01-31T05:39:14.759+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:14 INFO FileOutputCommitter: Saved output of task 'attempt_20230131053847974473178344799884_0008_m_000000_8' to gs://entsoe_temp_1009/.spark-bigquery-local-1675143153611-bc4991d9-9568-4e1a-90c9-74a335f770db/_temporary/0/task_20230131053847974473178344799884_0008_m_000000
[2023-01-31T05:39:14.768+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:14 INFO SparkHadoopMapRedUtil: attempt_20230131053847974473178344799884_0008_m_000000_8: Committed. Elapsed time: 3049 ms.
[2023-01-31T05:39:14.891+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:14 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2588 bytes result sent to driver
[2023-01-31T05:39:14.900+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:14 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 24351 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:39:14.909+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:14 INFO DAGScheduler: ResultStage 8 (save at BigQueryWriteHelper.java:105) finished in 25.615 s
[2023-01-31T05:39:14.911+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:14 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2023-01-31T05:39:14.916+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:14 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:39:14.922+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2023-01-31T05:39:14.933+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:14 INFO DAGScheduler: Job 8 finished: save at BigQueryWriteHelper.java:105, took 25.861172 s
[2023-01-31T05:39:14.942+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:14 INFO FileFormatWriter: Start to commit write Job 932f3d8f-adb6-4b95-8074-55dae15fb2eb.
[2023-01-31T05:39:17.377+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:17 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675143153611-bc4991d9-9568-4e1a-90c9-74a335f770db/_temporary/0/task_20230131053847974473178344799884_0008_m_000000/' directory.
[2023-01-31T05:39:18.245+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:18 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675143153611-bc4991d9-9568-4e1a-90c9-74a335f770db/' directory.
[2023-01-31T05:39:19.919+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:19 INFO FileFormatWriter: Write Job 932f3d8f-adb6-4b95-8074-55dae15fb2eb committed. Elapsed time: 4949 ms.
[2023-01-31T05:39:20.103+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:20 INFO FileFormatWriter: Finished processing stats for write job 932f3d8f-adb6-4b95-8074-55dae15fb2eb.
[2023-01-31T05:39:20.457+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:20 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 81d5fcd0285b:44295 in memory (size: 74.3 KiB, free: 434.4 MiB)
[2023-01-31T05:39:24.183+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:24 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=entsoe_analytics_1009, projectId=rafzul-analytics-1009, tableId=TEST_total_generation_staging}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=null, ignoreUnknownValue=null, sourceUris=[gs://entsoe_temp_1009/.spark-bigquery-local-1675143153611-bc4991d9-9568-4e1a-90c9-74a335f770db/part-00000-158bd315-41ca-49cb-ba8f-69be59385f66-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=true, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=rafzul-analytics-1009, job=5a7b4c3b-3381-47c9-9540-31105825613d, location=US}
[2023-01-31T05:39:27.417+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:27 INFO BigQueryClient: Done loading to rafzul-analytics-1009.entsoe_analytics_1009.TEST_total_generation_staging. jobId: JobId{project=rafzul-analytics-1009, job=5a7b4c3b-3381-47c9-9540-31105825613d, location=US}
[2023-01-31T05:39:28.285+0000] {spark_submit.py:495} INFO - MANTEP!!!!
[2023-01-31T05:39:29.747+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:29 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-31T05:39:29.986+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:29 INFO SparkUI: Stopped Spark web UI at http://81d5fcd0285b:4044
[2023-01-31T05:39:30.180+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:30 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-31T05:39:30.333+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:30 INFO MemoryStore: MemoryStore cleared
[2023-01-31T05:39:30.336+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:30 INFO BlockManager: BlockManager stopped
[2023-01-31T05:39:30.357+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:30 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-31T05:39:30.392+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:30 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-31T05:39:30.479+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:30 INFO SparkContext: Successfully stopped SparkContext
[2023-01-31T05:39:30.488+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:30 INFO ShutdownHookManager: Shutdown hook called
[2023-01-31T05:39:30.499+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:30 INFO ShutdownHookManager: Deleting directory /tmp/spark-8d32b5c8-46f2-48c3-88a0-537d51ae3452
[2023-01-31T05:39:30.543+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:30 INFO ShutdownHookManager: Deleting directory /tmp/spark-8d32b5c8-46f2-48c3-88a0-537d51ae3452/pyspark-6509b6b9-30c1-4ea3-9f3e-8ef5d294c630
[2023-01-31T05:39:30.617+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:30 INFO ShutdownHookManager: Deleting directory /tmp/spark-85fa5459-4fa0-46d1-9eaf-c65bc3c7cb0b
[2023-01-31T05:39:31.334+0000] {taskinstance.py:1322} INFO - Marking task as SUCCESS. dag_id=entsoe-energydata-backfill, task_id=stage_total_generation, execution_date=20210101T010000, start_date=20230131T053045, end_date=20230131T053931
[2023-01-31T05:39:31.468+0000] {local_task_job.py:159} INFO - Task exited with return code 0
[2023-01-31T05:39:31.589+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T07:38:54.156+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-31T07:38:54.438+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-31T07:38:54.449+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T07:38:54.459+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T07:38:54.467+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T07:38:55.018+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 01:00:00+00:00
[2023-01-31T07:38:55.155+0000] {standard_task_runner.py:55} INFO - Started process 25781 to run task
[2023-01-31T07:38:55.232+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '908', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpn0hf1icj']
[2023-01-31T07:38:55.277+0000] {standard_task_runner.py:83} INFO - Job 908: Subtask stage_total_generation
[2023-01-31T07:38:55.980+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 81d5fcd0285b
[2023-01-31T07:38:57.472+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-31T07:38:57.647+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T07:38:57.659+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-31T07:40:14.910+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:14 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T07:40:16.233+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T07:40:19.162+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:19 INFO ResourceUtils: ==============================================================
[2023-01-31T07:40:19.166+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:19 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T07:40:19.178+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:19 INFO ResourceUtils: ==============================================================
[2023-01-31T07:40:19.181+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:19 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T07:40:19.534+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:19 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T07:40:19.645+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:19 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T07:40:19.660+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:19 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T07:40:20.762+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:20 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T07:40:20.777+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:20 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T07:40:20.778+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:20 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T07:40:20.785+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:20 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T07:40:20.785+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:20 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T07:40:26.948+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:26 INFO Utils: Successfully started service 'sparkDriver' on port 45255.
[2023-01-31T07:40:28.056+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:28 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T07:40:30.228+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:30 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T07:40:30.439+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:30 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T07:40:30.514+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:30 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T07:40:30.846+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:30 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T07:40:31.803+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:31 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-2bd6f9df-ea29-43d5-a0fb-583d4aec5362
[2023-01-31T07:40:32.967+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:32 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T07:40:33.714+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:33 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T07:40:42.410+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:42 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-01-31T07:40:43.968+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:43 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://81d5fcd0285b:45255/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675150814747
[2023-01-31T07:40:43.969+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:43 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://81d5fcd0285b:45255/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675150814747
[2023-01-31T07:40:49.184+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:49 INFO Executor: Starting executor ID driver on host 81d5fcd0285b
[2023-01-31T07:40:49.450+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:49 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T07:40:49.890+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:49 INFO Executor: Fetching spark://81d5fcd0285b:45255/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675150814747
[2023-01-31T07:40:51.169+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:51 INFO TransportClientFactory: Successfully created connection to 81d5fcd0285b/172.19.0.9:45255 after 700 ms (0 ms spent in bootstraps)
[2023-01-31T07:40:51.577+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:51 INFO Utils: Fetching spark://81d5fcd0285b:45255/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-7ef585df-f3b3-4837-b64e-f28d62bfc0e4/userFiles-a808d029-06d6-472a-8131-c0ccb37c1c4b/fetchFileTemp10662363882990368823.tmp
[2023-01-31T07:41:01.365+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:01 INFO Executor: Adding file:/tmp/spark-7ef585df-f3b3-4837-b64e-f28d62bfc0e4/userFiles-a808d029-06d6-472a-8131-c0ccb37c1c4b/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T07:41:01.374+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:01 INFO Executor: Fetching spark://81d5fcd0285b:45255/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675150814747
[2023-01-31T07:41:01.383+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:01 INFO Utils: Fetching spark://81d5fcd0285b:45255/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-7ef585df-f3b3-4837-b64e-f28d62bfc0e4/userFiles-a808d029-06d6-472a-8131-c0ccb37c1c4b/fetchFileTemp3400020687664991591.tmp
[2023-01-31T07:41:07.645+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:07 INFO Executor: Told to re-register on heartbeat
[2023-01-31T07:41:07.649+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:07 INFO BlockManager: BlockManager null re-registering with master
[2023-01-31T07:41:07.679+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:07 INFO BlockManagerMaster: Registering BlockManager null
[2023-01-31T07:41:07.950+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:07 ERROR Inbox: Ignoring error
[2023-01-31T07:41:07.951+0000] {spark_submit.py:495} INFO - java.lang.NullPointerException
[2023-01-31T07:41:07.952+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)
[2023-01-31T07:41:07.955+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)
[2023-01-31T07:41:07.956+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-01-31T07:41:07.965+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-01-31T07:41:07.966+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-01-31T07:41:07.967+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-01-31T07:41:07.968+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-01-31T07:41:07.969+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-01-31T07:41:07.985+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-01-31T07:41:07.986+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-31T07:41:07.987+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-31T07:41:07.994+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-31T07:41:07.995+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:07 WARN Executor: Issue communicating with driver in heartbeater
[2023-01-31T07:41:07.996+0000] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-01-31T07:41:07.996+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-01-31T07:41:08.008+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-01-31T07:41:08.009+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2023-01-31T07:41:08.011+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2023-01-31T07:41:08.011+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:79)
[2023-01-31T07:41:08.012+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:643)
[2023-01-31T07:41:08.013+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2023-01-31T07:41:08.013+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2023-01-31T07:41:08.014+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
[2023-01-31T07:41:08.015+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2023-01-31T07:41:08.015+0000] {spark_submit.py:495} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2023-01-31T07:41:08.016+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-01-31T07:41:08.017+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2023-01-31T07:41:08.018+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2023-01-31T07:41:08.019+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-31T07:41:08.019+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-31T07:41:08.020+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-31T07:41:08.029+0000] {spark_submit.py:495} INFO - Caused by: java.lang.NullPointerException
[2023-01-31T07:41:08.029+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)
[2023-01-31T07:41:08.030+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)
[2023-01-31T07:41:08.030+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-01-31T07:41:08.031+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-01-31T07:41:08.042+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-01-31T07:41:08.043+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-01-31T07:41:08.043+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-01-31T07:41:08.044+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-01-31T07:41:08.058+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-01-31T07:41:08.058+0000] {spark_submit.py:495} INFO - ... 3 more
[2023-01-31T07:41:08.064+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:07 INFO Executor: Adding file:/tmp/spark-7ef585df-f3b3-4837-b64e-f28d62bfc0e4/userFiles-a808d029-06d6-472a-8131-c0ccb37c1c4b/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T07:41:08.121+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:08 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39809.
[2023-01-31T07:41:08.122+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:08 INFO NettyBlockTransferService: Server created on 81d5fcd0285b:39809
[2023-01-31T07:41:08.148+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:08 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T07:41:08.232+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:08 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 39809, None)
[2023-01-31T07:41:08.253+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:08 INFO BlockManagerMasterEndpoint: Registering block manager 81d5fcd0285b:39809 with 434.4 MiB RAM, BlockManagerId(driver, 81d5fcd0285b, 39809, None)
[2023-01-31T07:41:08.293+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:08 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 39809, None)
[2023-01-31T07:41:08.309+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:08 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 81d5fcd0285b, 39809, None)
[2023-01-31T07:41:32.333+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:32 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T07:41:32.698+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:32 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T07:42:11.651+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:11 INFO InMemoryFileIndex: It took 1993 ms to list leaf files for 1 paths.
[2023-01-31T07:42:18.061+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:18 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T07:42:19.177+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:19 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T07:42:19.206+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:19 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 81d5fcd0285b:39809 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T07:42:19.237+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:19 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T07:42:27.404+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:27 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T07:42:27.693+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:27 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T07:42:28.717+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:28 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T07:42:29.309+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:29 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T07:42:29.311+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:29 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T07:42:29.314+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:29 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:42:29.428+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:29 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:42:29.797+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:29 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T07:42:31.227+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:31 INFO AsyncEventQueue: Process of event SparkListenerJobStart(0,1675150949423,ArraySeq(org.apache.spark.scheduler.StageInfo@2299e343),{spark.master=local, spark.driver.port=45255, spark.submit.pyFiles=, spark.app.startTime=1675150814747, spark.rdd.compress=True, spark.executor.id=driver, spark.app.name=gcp_playground, spark.submit.deployMode=client, spark.hadoop.google.cloud.auth.service.account.json.keyfile=/.google/credentials/google_credentials.json, spark.driver.host=81d5fcd0285b, spark.app.id=local-1675150846325, spark.app.submitTime=1675150801439, spark.hadoop.fs.AbstractFileSystem.gs.impl=google.cloud.hadoop.fs.gcs.GoogleHadoopFS, spark.sql.session.timeZone=UTC, spark.hadoop.google.cloud.auth.service.account.enable=true, spark.app.initial.jar.urls=spark://81d5fcd0285b:45255/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar,spark://81d5fcd0285b:45255/jars/gcs-connector-hadoop3-latest.jar, spark.executor.extraJavaOptions=-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED, spark.jars=/opt/spark/jars/gcs-connector-hadoop3-latest.jar, /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar, spark.sql.warehouse.dir=file:/opt/***/spark-warehouse, spark.serializer.objectStreamReset=100, spark.driver.extraJavaOptions=-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED}) by listener AppStatusListener took 1.643165761s.
[2023-01-31T07:42:31.615+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:31 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T07:42:31.764+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:31 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T07:42:31.774+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:31 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 81d5fcd0285b:39809 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T07:42:31.794+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:31 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:42:32.460+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:42:32.488+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:32 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T07:42:33.604+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:33 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T07:42:33.791+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:33 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T07:42:37.638+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:37 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010100__202101010200.json:0+9369
[2023-01-31T07:42:45.324+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:45 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3480 bytes result sent to driver
[2023-01-31T07:42:45.643+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:45 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 12237 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:42:45.750+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:45 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T07:42:45.761+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:45 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 15.403 s
[2023-01-31T07:42:46.133+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:46 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:42:46.156+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T07:42:46.229+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:46 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 17.504316 s
[2023-01-31T07:43:09.048+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:09 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 81d5fcd0285b:39809 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T07:43:09.100+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:09 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 81d5fcd0285b:39809 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T07:43:22.835+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:22 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:43:22.844+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:22 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T07:43:22.858+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:22 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:43:25.818+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:25 INFO CodeGenerator: Code generated in 1735.150762 ms
[2023-01-31T07:43:25.857+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:25 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T07:43:25.898+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:25 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T07:43:25.901+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:25 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 81d5fcd0285b:39809 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T07:43:25.942+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:25 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T07:43:26.057+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:26 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:43:26.421+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:26 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T07:43:26.436+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:26 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T07:43:26.437+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:26 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T07:43:26.438+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:26 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:43:26.438+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:26 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:43:26.449+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:26 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T07:43:26.484+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:26 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T07:43:26.508+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:26 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T07:43:26.513+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:26 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 81d5fcd0285b:39809 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T07:43:26.533+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:26 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:43:26.535+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:43:26.536+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:26 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T07:43:26.575+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:26 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:43:26.583+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:26 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T07:43:27.102+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T07:43:29.599+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO CodeGenerator: Code generated in 2190.022421 ms
[2023-01-31T07:43:30.569+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1628 bytes result sent to driver
[2023-01-31T07:43:30.628+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 4074 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:43:30.629+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T07:43:30.639+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 4.171 s
[2023-01-31T07:43:30.649+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:43:30.649+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T07:43:30.656+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 4.224868 s
[2023-01-31T07:43:31.462+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:31 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:43:31.463+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:31 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T07:43:31.466+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:31 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:43:32.235+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO CodeGenerator: Code generated in 298.155124 ms
[2023-01-31T07:43:32.324+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T07:43:32.572+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T07:43:32.576+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 81d5fcd0285b:39809 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:43:32.588+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T07:43:32.611+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:43:32.798+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T07:43:32.799+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T07:43:32.799+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T07:43:32.800+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:43:32.800+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:43:32.809+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T07:43:32.929+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T07:43:32.988+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T07:43:33.003+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 81d5fcd0285b:39809 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T07:43:33.010+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:43:33.020+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:43:33.024+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T07:43:33.031+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:43:33.033+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T07:43:33.208+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T07:43:33.456+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 81d5fcd0285b:39809 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T07:43:34.027+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1628 bytes result sent to driver
[2023-01-31T07:43:34.077+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1029 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:43:34.093+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 1.214 s
[2023-01-31T07:43:34.093+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:43:34.094+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T07:43:34.094+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T07:43:34.109+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 1.298404 s
[2023-01-31T07:43:35.765+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:35 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:43:35.836+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:35 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T07:43:35.856+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:35 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:43:40.065+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO CodeGenerator: Code generated in 2360.430934 ms
[2023-01-31T07:43:40.202+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T07:43:40.599+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T07:43:40.618+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 81d5fcd0285b:39809 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:43:40.662+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T07:43:40.769+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:43:41.446+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:41 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T07:43:41.453+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:41 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T07:43:41.453+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:41 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T07:43:41.466+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:41 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:43:41.468+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:41 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:43:41.501+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:41 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T07:43:41.598+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:41 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.7 MiB)
[2023-01-31T07:43:41.657+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:41 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.6 MiB)
[2023-01-31T07:43:41.679+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:41 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 81d5fcd0285b:39809 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T07:43:41.697+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:41 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:43:41.719+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:43:41.749+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:41 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T07:43:41.766+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:41 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:43:41.777+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:41 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T07:43:42.204+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:42 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T07:43:43.447+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:43 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-01-31T07:43:43.557+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:43 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 1792 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:43:43.578+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:43 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T07:43:43.596+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:43 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 2.065 s
[2023-01-31T07:43:43.598+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:43 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:43:43.599+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T07:43:43.599+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:43 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 2.147979 s
[2023-01-31T07:43:44.565+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:44 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:43:44.583+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:44 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T07:43:44.592+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:44 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:43:44.936+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:44 INFO CodeGenerator: Code generated in 103.41744 ms
[2023-01-31T07:43:45.007+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:44 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.4 MiB)
[2023-01-31T07:43:45.193+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T07:43:45.209+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 81d5fcd0285b:39809 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T07:43:45.229+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T07:43:45.247+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:43:45.713+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T07:43:45.714+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T07:43:45.746+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T07:43:45.772+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:43:45.773+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:43:45.846+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T07:43:46.164+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:46 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-31T07:43:46.225+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:46 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-01-31T07:43:46.231+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:46 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 81d5fcd0285b:39809 (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T07:43:46.246+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:46 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:43:46.252+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:43:46.257+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:46 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T07:43:46.276+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:46 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:43:46.283+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:46 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T07:43:46.614+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:46 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T07:43:47.874+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:47 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1585 bytes result sent to driver
[2023-01-31T07:43:47.911+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:47 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 1639 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:43:47.929+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:47 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 1.987 s
[2023-01-31T07:43:47.929+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:47 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:43:47.942+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:47 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T07:43:47.945+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T07:43:47.955+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:47 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 2.248024 s
[2023-01-31T07:43:53.948+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:53 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:43:53.977+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:53 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T07:43:53.979+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:53 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:43:55.688+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:55 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 81d5fcd0285b:39809 in memory (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T07:43:55.943+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:55 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 81d5fcd0285b:39809 in memory (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T07:43:56.081+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:56 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 81d5fcd0285b:39809 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T07:43:57.014+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:57 INFO CodeGenerator: Code generated in 2207.669118 ms
[2023-01-31T07:43:57.351+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:57 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.3 MiB)
[2023-01-31T07:43:58.258+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:58 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T07:43:58.273+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:58 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 81d5fcd0285b:39809 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T07:43:58.285+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:58 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T07:43:58.331+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:58 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:43:59.336+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:59 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T07:43:59.351+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:59 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) with 1 output partitions
[2023-01-31T07:43:59.352+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:59 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201)
[2023-01-31T07:43:59.355+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:59 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:43:59.355+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:59 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:43:59.367+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:59 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201), which has no missing parents
[2023-01-31T07:43:59.530+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:59 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.2 MiB)
[2023-01-31T07:43:59.764+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:59 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.2 MiB)
[2023-01-31T07:43:59.808+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:59 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 81d5fcd0285b:39809 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T07:43:59.837+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:59 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:43:59.882+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:43:59.885+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:59 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T07:43:59.906+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:59 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:43:59.912+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:59 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T07:44:00.176+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:00 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T07:44:01.801+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:01 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2559 bytes result sent to driver
[2023-01-31T07:44:01.821+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:01 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 1915 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:44:01.822+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:01 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T07:44:01.830+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:01 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) finished in 2.452 s
[2023-01-31T07:44:01.833+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:01 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:44:01.834+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T07:44:01.834+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:01 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201, took 2.490502 s
[2023-01-31T07:44:02.519+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:02 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:44:02.534+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:02 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T07:44:02.547+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:02 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:44:03.789+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:03 INFO CodeGenerator: Code generated in 896.03878 ms
[2023-01-31T07:44:03.870+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:03 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 433.0 MiB)
[2023-01-31T07:44:03.991+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:03 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.0 MiB)
[2023-01-31T07:44:03.994+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:03 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 81d5fcd0285b:39809 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T07:44:03.995+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:03 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T07:44:04.007+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:44:04.220+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T07:44:04.247+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T07:44:04.249+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T07:44:04.250+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:44:04.250+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:44:04.410+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T07:44:04.453+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 432.9 MiB)
[2023-01-31T07:44:04.467+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.9 MiB)
[2023-01-31T07:44:04.475+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 81d5fcd0285b:39809 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T07:44:04.483+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:44:04.519+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:44:04.534+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T07:44:04.558+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:44:04.561+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T07:44:04.877+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T07:44:05.577+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:05 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-01-31T07:44:05.578+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:05 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 1007 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:44:05.579+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:05 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T07:44:05.580+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:05 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 1.137 s
[2023-01-31T07:44:05.580+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:05 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:44:05.586+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T07:44:05.587+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:05 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 1.371427 s
[2023-01-31T07:44:08.579+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:08 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:44:08.669+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:08 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T07:44:08.695+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:08 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:44:11.361+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:11 INFO CodeGenerator: Code generated in 1670.063674 ms
[2023-01-31T07:44:11.781+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:11 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.7 MiB)
[2023-01-31T07:44:12.434+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:12 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.7 MiB)
[2023-01-31T07:44:12.446+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:12 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 81d5fcd0285b:39809 (size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T07:44:12.458+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:12 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T07:44:12.462+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:44:13.729+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:13 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T07:44:13.729+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:13 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T07:44:13.730+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:13 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T07:44:13.730+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:13 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:44:13.731+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:13 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:44:13.731+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:13 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T07:44:13.732+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:13 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 81d5fcd0285b:39809 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T07:44:13.732+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:13 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.7 MiB)
[2023-01-31T07:44:13.814+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:13 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.7 MiB)
[2023-01-31T07:44:13.830+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:13 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 81d5fcd0285b:39809 (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T07:44:13.836+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:13 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:44:13.856+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:44:13.871+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:13 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T07:44:13.878+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:13 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 81d5fcd0285b:39809 in memory (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T07:44:13.897+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:13 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:44:13.905+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:13 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T07:44:14.188+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:14 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T07:44:15.689+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:15 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1772 bytes result sent to driver
[2023-01-31T07:44:15.690+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:15 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 1778 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:44:15.690+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:15 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T07:44:15.691+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:15 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 1.957 s
[2023-01-31T07:44:15.691+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:15 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:44:15.691+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T07:44:15.704+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:15 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 2.041025 s
[2023-01-31T07:44:39.754+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:39 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 81d5fcd0285b:39809 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T07:44:39.986+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:39 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 81d5fcd0285b:39809 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T07:44:40.386+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:40 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 81d5fcd0285b:39809 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:44:40.534+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:40 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 81d5fcd0285b:39809 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:44:40.697+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:40 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 81d5fcd0285b:39809 in memory (size: 12.6 KiB, free: 434.3 MiB)
[2023-01-31T07:44:41.061+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:41 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 81d5fcd0285b:39809 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:45:10.541+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:10 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T07:45:10.593+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T07:45:10.595+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T07:45:10.606+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:10 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T07:45:10.608+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T07:45:10.608+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T07:45:10.623+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T07:45:11.648+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-31T07:45:11.660+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO DAGScheduler: Got job 8 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T07:45:11.665+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO DAGScheduler: Final stage: ResultStage 8 (save at BigQueryWriteHelper.java:105)
[2023-01-31T07:45:11.665+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:45:11.666+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:45:11.677+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO DAGScheduler: Submitting ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T07:45:12.004+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 209.9 KiB, free 433.7 MiB)
[2023-01-31T07:45:12.038+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:12 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 74.2 KiB, free 433.7 MiB)
[2023-01-31T07:45:12.043+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:12 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 81d5fcd0285b:39809 (size: 74.2 KiB, free: 434.3 MiB)
[2023-01-31T07:45:12.050+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:12 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:45:12.065+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:45:12.076+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:12 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2023-01-31T07:45:12.089+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:12 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7484 bytes) taskResourceAssignments Map()
[2023-01-31T07:45:12.110+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:12 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2023-01-31T07:45:12.898+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T07:45:12.901+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T07:45:12.910+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:12 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T07:45:12.917+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T07:45:12.923+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T07:45:12.928+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T07:45:12.946+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:12 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T07:45:12.963+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:12 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T07:45:13.222+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:13 INFO ParquetOutputFormat: Parquet block size to 134217728
[2023-01-31T07:45:13.223+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:13 INFO ParquetOutputFormat: Validation is off
[2023-01-31T07:45:13.224+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:13 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-01-31T07:45:13.224+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:13 INFO ParquetOutputFormat: Parquet properties are:
[2023-01-31T07:45:13.225+0000] {spark_submit.py:495} INFO - Parquet page size to 1048576
[2023-01-31T07:45:13.232+0000] {spark_submit.py:495} INFO - Parquet dictionary page size to 1048576
[2023-01-31T07:45:13.233+0000] {spark_submit.py:495} INFO - Dictionary is true
[2023-01-31T07:45:13.236+0000] {spark_submit.py:495} INFO - Writer version is: PARQUET_1_0
[2023-01-31T07:45:13.237+0000] {spark_submit.py:495} INFO - Page size checking is: estimated
[2023-01-31T07:45:13.237+0000] {spark_submit.py:495} INFO - Min row count for page size check is: 100
[2023-01-31T07:45:13.239+0000] {spark_submit.py:495} INFO - Max row count for page size check is: 10000
[2023-01-31T07:45:13.241+0000] {spark_submit.py:495} INFO - Truncate length for column indexes is: 64
[2023-01-31T07:45:13.254+0000] {spark_submit.py:495} INFO - Truncate length for statistics min/max  is: 2147483647
[2023-01-31T07:45:13.279+0000] {spark_submit.py:495} INFO - Bloom filter enabled: false
[2023-01-31T07:45:13.282+0000] {spark_submit.py:495} INFO - Max Bloom filter size for a column is 1048576
[2023-01-31T07:45:13.284+0000] {spark_submit.py:495} INFO - Bloom filter expected number of distinct values are: null
[2023-01-31T07:45:13.296+0000] {spark_submit.py:495} INFO - Page row count limit to 20000
[2023-01-31T07:45:13.301+0000] {spark_submit.py:495} INFO - Writing page checksums is: on
[2023-01-31T07:45:13.470+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:13 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-01-31T07:45:13.470+0000] {spark_submit.py:495} INFO - {
[2023-01-31T07:45:13.471+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-31T07:45:13.471+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-31T07:45:13.471+0000] {spark_submit.py:495} INFO - "name" : "measured_at",
[2023-01-31T07:45:13.471+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T07:45:13.472+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T07:45:13.472+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:13.472+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:13.473+0000] {spark_submit.py:495} INFO - "name" : "created_at",
[2023-01-31T07:45:13.473+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T07:45:13.473+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T07:45:13.474+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:13.476+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:13.477+0000] {spark_submit.py:495} INFO - "name" : "Biomass - Actual Aggregated",
[2023-01-31T07:45:13.478+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:13.479+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:13.483+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:13.486+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:13.487+0000] {spark_submit.py:495} INFO - "name" : "Fossil Brown coal/Lignite - Actual Aggregated",
[2023-01-31T07:45:13.489+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:13.490+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:13.491+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:13.492+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:13.493+0000] {spark_submit.py:495} INFO - "name" : "Fossil Gas - Actual Aggregated",
[2023-01-31T07:45:13.493+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:13.494+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:13.495+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:13.495+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:13.495+0000] {spark_submit.py:495} INFO - "name" : "Fossil Hard coal - Actual Aggregated",
[2023-01-31T07:45:13.499+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:13.500+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:13.503+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:13.507+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:13.507+0000] {spark_submit.py:495} INFO - "name" : "Fossil Oil - Actual Aggregated",
[2023-01-31T07:45:13.508+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:13.510+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:13.511+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:13.511+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:13.513+0000] {spark_submit.py:495} INFO - "name" : "Geothermal - Actual Aggregated",
[2023-01-31T07:45:13.515+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:13.518+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:13.525+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:13.529+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:13.530+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Aggregated",
[2023-01-31T07:45:13.530+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:13.545+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:13.546+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:13.546+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:13.548+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Consumption",
[2023-01-31T07:45:13.548+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:13.549+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:13.551+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:13.552+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:13.552+0000] {spark_submit.py:495} INFO - "name" : "Hydro Run-of-river and poundage - Actual Aggregated",
[2023-01-31T07:45:13.553+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:13.553+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:13.554+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:13.555+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:13.555+0000] {spark_submit.py:495} INFO - "name" : "Hydro Water Reservoir - Actual Aggregated",
[2023-01-31T07:45:13.555+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:13.557+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:13.557+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:13.557+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:13.562+0000] {spark_submit.py:495} INFO - "name" : "Nuclear - Actual Aggregated",
[2023-01-31T07:45:13.562+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:13.562+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:13.565+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:13.567+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:13.569+0000] {spark_submit.py:495} INFO - "name" : "Other - Actual Aggregated",
[2023-01-31T07:45:13.569+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:13.569+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:13.571+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:13.571+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:13.571+0000] {spark_submit.py:495} INFO - "name" : "Other renewable - Actual Aggregated",
[2023-01-31T07:45:13.571+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:13.573+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:13.573+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:13.573+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:13.576+0000] {spark_submit.py:495} INFO - "name" : "Solar - Actual Aggregated",
[2023-01-31T07:45:13.576+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:13.576+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:13.578+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:13.581+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:13.582+0000] {spark_submit.py:495} INFO - "name" : "Waste - Actual Aggregated",
[2023-01-31T07:45:13.582+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:13.587+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:13.587+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:13.590+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:13.590+0000] {spark_submit.py:495} INFO - "name" : "Wind Offshore - Actual Aggregated",
[2023-01-31T07:45:13.591+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:13.591+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:13.591+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:13.591+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:13.592+0000] {spark_submit.py:495} INFO - "name" : "Wind Onshore - Actual Aggregated",
[2023-01-31T07:45:13.592+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:13.592+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:13.592+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:13.592+0000] {spark_submit.py:495} INFO - } ]
[2023-01-31T07:45:13.592+0000] {spark_submit.py:495} INFO - }
[2023-01-31T07:45:13.593+0000] {spark_submit.py:495} INFO - and corresponding Parquet message type:
[2023-01-31T07:45:13.593+0000] {spark_submit.py:495} INFO - message spark_schema {
[2023-01-31T07:45:13.593+0000] {spark_submit.py:495} INFO - required int96 measured_at;
[2023-01-31T07:45:13.593+0000] {spark_submit.py:495} INFO - required int96 created_at;
[2023-01-31T07:45:13.594+0000] {spark_submit.py:495} INFO - optional double Biomass - Actual Aggregated;
[2023-01-31T07:45:13.594+0000] {spark_submit.py:495} INFO - optional double Fossil Brown coal/Lignite - Actual Aggregated;
[2023-01-31T07:45:13.594+0000] {spark_submit.py:495} INFO - optional double Fossil Gas - Actual Aggregated;
[2023-01-31T07:45:13.594+0000] {spark_submit.py:495} INFO - optional double Fossil Hard coal - Actual Aggregated;
[2023-01-31T07:45:13.594+0000] {spark_submit.py:495} INFO - optional double Fossil Oil - Actual Aggregated;
[2023-01-31T07:45:13.594+0000] {spark_submit.py:495} INFO - optional double Geothermal - Actual Aggregated;
[2023-01-31T07:45:13.595+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Aggregated;
[2023-01-31T07:45:13.595+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Consumption;
[2023-01-31T07:45:13.595+0000] {spark_submit.py:495} INFO - optional double Hydro Run-of-river and poundage - Actual Aggregated;
[2023-01-31T07:45:13.595+0000] {spark_submit.py:495} INFO - optional double Hydro Water Reservoir - Actual Aggregated;
[2023-01-31T07:45:13.595+0000] {spark_submit.py:495} INFO - optional double Nuclear - Actual Aggregated;
[2023-01-31T07:45:13.596+0000] {spark_submit.py:495} INFO - optional double Other - Actual Aggregated;
[2023-01-31T07:45:13.596+0000] {spark_submit.py:495} INFO - optional double Other renewable - Actual Aggregated;
[2023-01-31T07:45:13.596+0000] {spark_submit.py:495} INFO - optional double Solar - Actual Aggregated;
[2023-01-31T07:45:13.596+0000] {spark_submit.py:495} INFO - optional double Waste - Actual Aggregated;
[2023-01-31T07:45:13.596+0000] {spark_submit.py:495} INFO - optional double Wind Offshore - Actual Aggregated;
[2023-01-31T07:45:13.596+0000] {spark_submit.py:495} INFO - optional double Wind Onshore - Actual Aggregated;
[2023-01-31T07:45:13.597+0000] {spark_submit.py:495} INFO - }
[2023-01-31T07:45:13.597+0000] {spark_submit.py:495} INFO - 
[2023-01-31T07:45:13.597+0000] {spark_submit.py:495} INFO - 
[2023-01-31T07:45:14.397+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:14 INFO CodecPool: Got brand-new compressor [.snappy]
[2023-01-31T07:45:15.132+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:15 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 81d5fcd0285b:39809 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:45:20.059+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:20 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675150846325-665136dd-3266-45e2-aa62-4ee88a6496d9/_temporary/0/_temporary/' directory.
[2023-01-31T07:45:20.068+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:20 INFO FileOutputCommitter: Saved output of task 'attempt_202301310745116804079838005475834_0008_m_000000_8' to gs://entsoe_temp_1009/.spark-bigquery-local-1675150846325-665136dd-3266-45e2-aa62-4ee88a6496d9/_temporary/0/task_202301310745116804079838005475834_0008_m_000000
[2023-01-31T07:45:20.081+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:20 INFO SparkHadoopMapRedUtil: attempt_202301310745116804079838005475834_0008_m_000000_8: Committed. Elapsed time: 1560 ms.
[2023-01-31T07:45:20.166+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:20 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2588 bytes result sent to driver
[2023-01-31T07:45:20.177+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:20 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 8094 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:45:20.178+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:20 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2023-01-31T07:45:20.180+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:20 INFO DAGScheduler: ResultStage 8 (save at BigQueryWriteHelper.java:105) finished in 8.496 s
[2023-01-31T07:45:20.190+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:20 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:45:20.195+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2023-01-31T07:45:20.195+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:20 INFO DAGScheduler: Job 8 finished: save at BigQueryWriteHelper.java:105, took 8.536486 s
[2023-01-31T07:45:20.212+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:20 INFO FileFormatWriter: Start to commit write Job 152683fa-d27a-463c-93f3-4fc408806323.
[2023-01-31T07:45:22.036+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:22 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675150846325-665136dd-3266-45e2-aa62-4ee88a6496d9/_temporary/0/task_202301310745116804079838005475834_0008_m_000000/' directory.
[2023-01-31T07:45:23.185+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:23 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675150846325-665136dd-3266-45e2-aa62-4ee88a6496d9/' directory.
[2023-01-31T07:45:23.785+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:23 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 81d5fcd0285b:39809 in memory (size: 74.2 KiB, free: 434.4 MiB)
[2023-01-31T07:45:24.661+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:24 INFO FileFormatWriter: Write Job 152683fa-d27a-463c-93f3-4fc408806323 committed. Elapsed time: 4439 ms.
[2023-01-31T07:45:24.690+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:24 INFO FileFormatWriter: Finished processing stats for write job 152683fa-d27a-463c-93f3-4fc408806323.
[2023-01-31T07:45:27.298+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:27 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=entsoe_analytics_1009, projectId=rafzul-analytics-1009, tableId=TEST_total_generation_staging}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=null, ignoreUnknownValue=null, sourceUris=[gs://entsoe_temp_1009/.spark-bigquery-local-1675150846325-665136dd-3266-45e2-aa62-4ee88a6496d9/part-00000-40a8bf58-b3f7-48b9-b2bf-f40620681f2b-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=true, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=rafzul-analytics-1009, job=35162d58-4b55-4a63-b613-b485f9ebdb33, location=US}
[2023-01-31T07:45:31.165+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:31 INFO BigQueryClient: Done loading to rafzul-analytics-1009.entsoe_analytics_1009.TEST_total_generation_staging. jobId: JobId{project=rafzul-analytics-1009, job=35162d58-4b55-4a63-b613-b485f9ebdb33, location=US}
[2023-01-31T07:45:32.118+0000] {spark_submit.py:495} INFO - MANTEP!!!!
[2023-01-31T07:45:32.982+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:32 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-31T07:45:33.218+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:33 INFO SparkUI: Stopped Spark web UI at http://81d5fcd0285b:4040
[2023-01-31T07:45:33.695+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:33 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-31T07:45:34.108+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:34 INFO MemoryStore: MemoryStore cleared
[2023-01-31T07:45:34.116+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:34 INFO BlockManager: BlockManager stopped
[2023-01-31T07:45:34.187+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:34 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-31T07:45:34.202+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:34 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-31T07:45:34.310+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:34 INFO SparkContext: Successfully stopped SparkContext
[2023-01-31T07:45:34.312+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:34 INFO ShutdownHookManager: Shutdown hook called
[2023-01-31T07:45:34.316+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-7ef585df-f3b3-4837-b64e-f28d62bfc0e4/pyspark-b5c84b22-6b98-4715-8cc5-60e3cf4ba380
[2023-01-31T07:45:34.349+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-4b0d0954-2afc-4fcc-b5a0-d880b389e580
[2023-01-31T07:45:34.393+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-7ef585df-f3b3-4837-b64e-f28d62bfc0e4
[2023-01-31T07:45:35.262+0000] {taskinstance.py:1322} INFO - Marking task as SUCCESS. dag_id=entsoe-energydata-backfill, task_id=stage_total_generation, execution_date=20210101T010000, start_date=20230131T073854, end_date=20230131T074535
[2023-01-31T07:45:35.403+0000] {local_task_job.py:159} INFO - Task exited with return code 0
[2023-01-31T07:45:35.486+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T08:21:35.418+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-31T08:21:35.454+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-31T08:21:35.454+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T08:21:35.455+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T08:21:35.455+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T08:21:35.501+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 01:00:00+00:00
[2023-01-31T08:21:35.526+0000] {standard_task_runner.py:55} INFO - Started process 2078 to run task
[2023-01-31T08:21:35.542+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '926', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpskmqi5jx']
[2023-01-31T08:21:35.550+0000] {standard_task_runner.py:83} INFO - Job 926: Subtask stage_total_generation
[2023-01-31T08:21:35.908+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 81d5fcd0285b
[2023-01-31T08:21:36.097+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-31T08:21:36.112+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T08:21:36.114+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-31T08:21:57.714+0000] {spark_submit.py:495} INFO - {{ start_date }}
[2023-01-31T08:21:57.715+0000] {spark_submit.py:495} INFO - mulai periode: {{ start_date }}
[2023-01-31T08:21:57.715+0000] {spark_submit.py:495} INFO - selesai periode: {{ end_date }}
[2023-01-31T08:21:58.258+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:58 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T08:21:58.653+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T08:21:59.438+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:59 INFO ResourceUtils: ==============================================================
[2023-01-31T08:21:59.441+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:59 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T08:21:59.447+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:59 INFO ResourceUtils: ==============================================================
[2023-01-31T08:21:59.453+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:59 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T08:21:59.643+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:59 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T08:21:59.679+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:59 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T08:21:59.691+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:59 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T08:22:00.140+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:00 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T08:22:00.140+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:00 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T08:22:00.147+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:00 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T08:22:00.147+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:00 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T08:22:00.153+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:00 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T08:22:02.842+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:02 INFO Utils: Successfully started service 'sparkDriver' on port 40003.
[2023-01-31T08:22:03.322+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:03 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T08:22:03.734+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:03 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T08:22:04.192+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:04 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T08:22:04.234+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:04 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T08:22:04.258+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:04 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T08:22:04.373+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:04 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-edb80f96-8984-4db1-81d2-21fee7a38ddc
[2023-01-31T08:22:04.477+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:04 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T08:22:04.622+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:04 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T08:22:08.222+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:08 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T08:22:08.232+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:08 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2023-01-31T08:22:08.244+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:08 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[2023-01-31T08:22:08.260+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:08 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[2023-01-31T08:22:08.265+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:08 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[2023-01-31T08:22:08.352+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:08 INFO Utils: Successfully started service 'SparkUI' on port 4045.
[2023-01-31T08:22:08.450+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:08 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://81d5fcd0285b:40003/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675153318220
[2023-01-31T08:22:08.451+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:08 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://81d5fcd0285b:40003/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675153318220
[2023-01-31T08:22:09.042+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:09 INFO Executor: Starting executor ID driver on host 81d5fcd0285b
[2023-01-31T08:22:09.069+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:09 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T08:22:09.195+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:09 INFO Executor: Fetching spark://81d5fcd0285b:40003/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675153318220
[2023-01-31T08:22:09.634+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:09 INFO TransportClientFactory: Successfully created connection to 81d5fcd0285b/172.19.0.5:40003 after 150 ms (0 ms spent in bootstraps)
[2023-01-31T08:22:09.723+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:09 INFO Utils: Fetching spark://81d5fcd0285b:40003/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-c4f1dfe7-d1e4-45bf-b9e7-b2fc2fba8cff/userFiles-a5548199-01fd-412e-9beb-8ade62e52be3/fetchFileTemp12406506412998728321.tmp
[2023-01-31T08:22:11.241+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:11 INFO Executor: Adding file:/tmp/spark-c4f1dfe7-d1e4-45bf-b9e7-b2fc2fba8cff/userFiles-a5548199-01fd-412e-9beb-8ade62e52be3/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T08:22:11.241+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:11 INFO Executor: Fetching spark://81d5fcd0285b:40003/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675153318220
[2023-01-31T08:22:11.241+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:11 INFO Utils: Fetching spark://81d5fcd0285b:40003/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-c4f1dfe7-d1e4-45bf-b9e7-b2fc2fba8cff/userFiles-a5548199-01fd-412e-9beb-8ade62e52be3/fetchFileTemp15104154119789702605.tmp
[2023-01-31T08:22:13.414+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:13 INFO Executor: Adding file:/tmp/spark-c4f1dfe7-d1e4-45bf-b9e7-b2fc2fba8cff/userFiles-a5548199-01fd-412e-9beb-8ade62e52be3/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T08:22:13.450+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:13 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46763.
[2023-01-31T08:22:13.451+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:13 INFO NettyBlockTransferService: Server created on 81d5fcd0285b:46763
[2023-01-31T08:22:13.458+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:13 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T08:22:13.480+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:13 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 46763, None)
[2023-01-31T08:22:13.487+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:13 INFO BlockManagerMasterEndpoint: Registering block manager 81d5fcd0285b:46763 with 434.4 MiB RAM, BlockManagerId(driver, 81d5fcd0285b, 46763, None)
[2023-01-31T08:22:13.493+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:13 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 46763, None)
[2023-01-31T08:22:13.500+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:13 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 81d5fcd0285b, 46763, None)
[2023-01-31T08:22:17.587+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:17 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T08:22:17.712+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:17 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T08:22:32.354+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:32 INFO InMemoryFileIndex: It took 458 ms to list leaf files for 1 paths.
[2023-01-31T08:22:33.658+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:33 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T08:22:34.075+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:34 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T08:22:34.082+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:34 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 81d5fcd0285b:46763 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T08:22:34.098+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:34 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T08:22:36.005+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:35 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T08:22:36.073+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:36 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T08:22:36.271+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:36 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T08:22:36.359+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:36 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T08:22:36.361+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:36 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T08:22:36.366+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:36 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:22:36.368+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:36 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:22:36.414+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:36 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T08:22:37.044+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:37 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T08:22:37.189+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:37 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T08:22:37.197+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:37 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 81d5fcd0285b:46763 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T08:22:37.240+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:37 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:22:37.419+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:22:37.421+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:37 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T08:22:37.940+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:37 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T08:22:38.089+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:38 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T08:22:39.076+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:39 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010100__202101010200.json:0+9369
[2023-01-31T08:22:40.870+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:40 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3437 bytes result sent to driver
[2023-01-31T08:22:40.923+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:40 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3166 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:22:40.936+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:40 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T08:22:40.980+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:40 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 4.427 s
[2023-01-31T08:22:41.028+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:41 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:22:41.030+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T08:22:41.042+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:41 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 4.765730 s
[2023-01-31T08:22:44.112+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:44 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 81d5fcd0285b:46763 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T08:22:44.218+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:44 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 81d5fcd0285b:46763 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T08:23:04.978+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:04 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:05.038+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:05 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T08:23:05.142+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:05 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:09.433+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:09 INFO CodeGenerator: Code generated in 1662.196272 ms
[2023-01-31T08:23:09.515+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:09 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T08:23:09.558+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:09 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T08:23:09.584+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:09 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 81d5fcd0285b:46763 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T08:23:09.590+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:09 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T08:23:09.819+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:11.992+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:11 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T08:23:12.025+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:12 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T08:23:12.026+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:12 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T08:23:12.028+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:12 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:12.029+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:12 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:12.051+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:12 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T08:23:12.175+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:12 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T08:23:12.229+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:12 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T08:23:12.233+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:12 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 81d5fcd0285b:46763 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T08:23:12.236+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:12 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:12.248+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:12.254+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:12 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T08:23:12.279+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:12 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:12.298+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:12 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T08:23:12.929+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:12 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T08:23:14.715+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:14 INFO CodeGenerator: Code generated in 1277.571713 ms
[2023-01-31T08:23:16.231+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:16 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1628 bytes result sent to driver
[2023-01-31T08:23:16.372+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:16 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 4101 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:16.386+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:16 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 4.262 s
[2023-01-31T08:23:16.390+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:16 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:16.395+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:16 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T08:23:16.396+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T08:23:16.406+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:16 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 4.381820 s
[2023-01-31T08:23:17.906+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:17 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 81d5fcd0285b:46763 in memory (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T08:23:18.409+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:18 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:18.410+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:18 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T08:23:18.415+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:18 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:19.407+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:19 INFO CodeGenerator: Code generated in 401.142291 ms
[2023-01-31T08:23:19.489+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:19 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T08:23:19.801+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:19 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T08:23:19.804+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:19 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 81d5fcd0285b:46763 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T08:23:19.834+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:19 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T08:23:19.846+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:19 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:20.441+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T08:23:20.446+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T08:23:20.447+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T08:23:20.447+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:20.447+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:20.462+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T08:23:20.582+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T08:23:20.616+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T08:23:20.664+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 81d5fcd0285b:46763 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T08:23:20.669+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:20.683+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:20.684+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T08:23:20.753+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:20.762+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T08:23:20.884+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T08:23:22.145+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:22 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1585 bytes result sent to driver
[2023-01-31T08:23:22.165+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:22 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1476 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:22.177+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:22 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 1.715 s
[2023-01-31T08:23:22.177+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:22 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:22.180+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:22 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T08:23:22.182+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T08:23:22.185+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:22 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 1.738207 s
[2023-01-31T08:23:24.377+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:24 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:24.378+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:24 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T08:23:24.378+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:24 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:24.396+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:24 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 81d5fcd0285b:46763 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T08:23:25.790+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO CodeGenerator: Code generated in 824.522459 ms
[2023-01-31T08:23:25.838+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T08:23:25.875+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T08:23:25.878+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 81d5fcd0285b:46763 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T08:23:25.879+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T08:23:25.881+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:26.097+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T08:23:26.099+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T08:23:26.100+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T08:23:26.100+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:26.100+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:26.108+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T08:23:26.185+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.7 MiB)
[2023-01-31T08:23:26.196+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.7 MiB)
[2023-01-31T08:23:26.200+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 81d5fcd0285b:46763 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T08:23:26.210+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:26.213+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:26.215+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T08:23:26.244+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:26.262+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T08:23:26.289+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T08:23:26.676+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-01-31T08:23:26.697+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 455 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:26.705+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 0.590 s
[2023-01-31T08:23:26.706+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:26.745+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T08:23:26.747+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T08:23:26.747+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 0.650006 s
[2023-01-31T08:23:27.028+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:27.029+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T08:23:27.031+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:27.245+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO CodeGenerator: Code generated in 184.276791 ms
[2023-01-31T08:23:27.327+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.5 MiB)
[2023-01-31T08:23:27.510+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T08:23:27.515+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 81d5fcd0285b:46763 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T08:23:27.519+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T08:23:27.541+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:27.873+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 81d5fcd0285b:46763 in memory (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T08:23:28.207+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T08:23:28.209+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T08:23:28.209+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T08:23:28.210+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:28.210+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:28.213+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T08:23:28.243+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.5 MiB)
[2023-01-31T08:23:28.291+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.5 MiB)
[2023-01-31T08:23:28.297+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 81d5fcd0285b:46763 (size: 7.6 KiB, free: 434.3 MiB)
[2023-01-31T08:23:28.312+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:28.341+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:28.341+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T08:23:28.344+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:28.345+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T08:23:28.424+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T08:23:28.775+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1586 bytes result sent to driver
[2023-01-31T08:23:28.785+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 441 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:28.790+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T08:23:28.792+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 0.572 s
[2023-01-31T08:23:28.795+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:28.796+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T08:23:28.797+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 0.581792 s
[2023-01-31T08:23:29.148+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 81d5fcd0285b:46763 in memory (size: 7.6 KiB, free: 434.3 MiB)
[2023-01-31T08:23:31.595+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:31.598+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T08:23:31.601+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:32.686+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO CodeGenerator: Code generated in 508.019092 ms
[2023-01-31T08:23:32.707+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.3 MiB)
[2023-01-31T08:23:32.722+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T08:23:32.724+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 81d5fcd0285b:46763 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T08:23:32.726+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T08:23:32.727+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:32.887+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T08:23:32.891+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) with 1 output partitions
[2023-01-31T08:23:32.892+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201)
[2023-01-31T08:23:32.892+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:32.892+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:32.903+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201), which has no missing parents
[2023-01-31T08:23:32.918+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.2 MiB)
[2023-01-31T08:23:32.920+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.2 MiB)
[2023-01-31T08:23:32.923+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 81d5fcd0285b:46763 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T08:23:32.943+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:32.945+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:32.949+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T08:23:32.954+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:32.957+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T08:23:32.975+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T08:23:33.216+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2559 bytes result sent to driver
[2023-01-31T08:23:33.219+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 266 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:33.219+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T08:23:33.229+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) finished in 0.316 s
[2023-01-31T08:23:33.230+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:33.230+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T08:23:33.230+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201, took 0.338473 s
[2023-01-31T08:23:33.375+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:33.379+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T08:23:33.380+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:33.904+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 81d5fcd0285b:46763 in memory (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T08:23:33.949+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO CodeGenerator: Code generated in 495.126423 ms
[2023-01-31T08:23:34.045+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 433.1 MiB)
[2023-01-31T08:23:34.264+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.0 MiB)
[2023-01-31T08:23:34.271+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 81d5fcd0285b:46763 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T08:23:34.279+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T08:23:34.339+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:34.762+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T08:23:34.766+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T08:23:34.769+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T08:23:34.769+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:34.778+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:34.800+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T08:23:34.833+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 433.0 MiB)
[2023-01-31T08:23:34.885+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 433.0 MiB)
[2023-01-31T08:23:34.894+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 81d5fcd0285b:46763 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T08:23:34.898+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:34.899+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:34.900+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T08:23:34.917+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:34.918+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T08:23:34.997+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T08:23:35.299+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:35 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-01-31T08:23:35.342+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:35 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 427 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:35.348+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:35 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 0.555 s
[2023-01-31T08:23:35.349+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:35 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:35.351+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:35 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T08:23:35.355+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T08:23:35.355+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:35 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 0.589304 s
[2023-01-31T08:23:35.751+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:35 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 81d5fcd0285b:46763 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T08:23:35.833+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:35 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:35.846+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:35 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T08:23:35.849+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:35 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:37.457+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:37 INFO CodeGenerator: Code generated in 1146.343592 ms
[2023-01-31T08:23:37.547+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:37 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.8 MiB)
[2023-01-31T08:23:37.793+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:37 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.8 MiB)
[2023-01-31T08:23:37.797+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:37 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 81d5fcd0285b:46763 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T08:23:37.810+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:37 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T08:23:37.865+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:37 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:38.261+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T08:23:38.262+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T08:23:38.263+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T08:23:38.264+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:38.266+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:38.272+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T08:23:38.289+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.8 MiB)
[2023-01-31T08:23:38.293+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.7 MiB)
[2023-01-31T08:23:38.330+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 81d5fcd0285b:46763 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T08:23:38.335+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:38.337+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:38.365+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T08:23:38.372+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:38.378+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T08:23:38.472+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T08:23:38.936+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1729 bytes result sent to driver
[2023-01-31T08:23:38.964+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 593 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:38.968+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T08:23:38.968+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 0.697 s
[2023-01-31T08:23:38.968+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:38.969+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T08:23:38.969+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 0.705767 s
[2023-01-31T08:23:40.409+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:40 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 81d5fcd0285b:46763 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T08:23:40.782+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:40 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 81d5fcd0285b:46763 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T08:23:44.632+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:44 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 81d5fcd0285b:46763 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:01:41.176+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-31T12:01:41.419+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-31T12:01:41.420+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T12:01:41.420+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T12:01:41.420+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T12:01:41.722+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 01:00:00+00:00
[2023-01-31T12:01:41.747+0000] {standard_task_runner.py:55} INFO - Started process 153 to run task
[2023-01-31T12:01:41.788+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '977', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpt4bdpyjw']
[2023-01-31T12:01:41.798+0000] {standard_task_runner.py:83} INFO - Job 977: Subtask stage_total_generation
[2023-01-31T12:01:42.203+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 8124f810dec3
[2023-01-31T12:01:42.640+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-31T12:01:42.698+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T12:01:42.700+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 398, in submit
    spark_submit_cmd = self._build_spark_submit_command(application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 344, in _build_spark_submit_command
    self.log.info("Spark-Submit cmd: %s", self._mask_cmd(connection_cmd))
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 256, in _mask_cmd
    " ".join(connection_cmd),
TypeError: sequence item 10: expected str instance, DateTime found
[2023-01-31T12:01:42.777+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=entsoe-energydata-backfill, task_id=stage_total_generation, execution_date=20210101T010000, start_date=20230131T120141, end_date=20230131T120142
[2023-01-31T12:01:42.857+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 977 for task stage_total_generation (sequence item 10: expected str instance, DateTime found; 153)
[2023-01-31T12:01:42.961+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-31T12:01:43.062+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T12:08:51.470+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-31T12:08:51.521+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-31T12:08:51.522+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T12:08:51.523+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T12:08:51.523+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T12:08:51.598+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 01:00:00+00:00
[2023-01-31T12:08:51.664+0000] {standard_task_runner.py:55} INFO - Started process 601 to run task
[2023-01-31T12:08:51.761+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '999', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp2wdify8v']
[2023-01-31T12:08:51.782+0000] {standard_task_runner.py:83} INFO - Job 999: Subtask stage_total_generation
[2023-01-31T12:08:52.339+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 8124f810dec3
[2023-01-31T12:08:52.805+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-31T12:08:52.986+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T12:08:52.992+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET 202101010000 202101010600
[2023-01-31T12:09:24.111+0000] {spark_submit.py:495} INFO - mulai periode: 202101010000
[2023-01-31T12:09:24.113+0000] {spark_submit.py:495} INFO - selesai periode: 202101010600
[2023-01-31T12:09:24.398+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:24 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T12:09:25.037+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T12:09:25.914+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:25 INFO ResourceUtils: ==============================================================
[2023-01-31T12:09:25.931+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:25 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T12:09:25.942+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:25 INFO ResourceUtils: ==============================================================
[2023-01-31T12:09:25.943+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:25 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T12:09:26.118+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:26 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T12:09:26.126+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:26 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T12:09:26.135+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:26 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T12:09:26.470+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:26 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T12:09:26.471+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:26 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T12:09:26.471+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:26 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T12:09:26.474+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:26 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T12:09:26.474+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:26 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T12:09:29.310+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:29 INFO Utils: Successfully started service 'sparkDriver' on port 39019.
[2023-01-31T12:09:29.712+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:29 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T12:09:30.348+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:30 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T12:09:30.670+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:30 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T12:09:30.674+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:30 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T12:09:30.872+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:30 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T12:09:31.433+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:31 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-595345c4-6a55-4723-9903-e21bfb8c1170
[2023-01-31T12:09:31.600+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:31 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T12:09:31.867+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:31 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T12:09:34.195+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:34 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T12:09:34.286+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:34 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2023-01-31T12:09:35.057+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:35 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://8124f810dec3:39019/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675166964364
[2023-01-31T12:09:35.074+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:35 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://8124f810dec3:39019/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675166964364
[2023-01-31T12:09:35.754+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:35 INFO Executor: Starting executor ID driver on host 8124f810dec3
[2023-01-31T12:09:35.961+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:35 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T12:09:36.154+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:36 INFO Executor: Fetching spark://8124f810dec3:39019/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675166964364
[2023-01-31T12:09:36.670+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:36 INFO TransportClientFactory: Successfully created connection to 8124f810dec3/172.20.0.9:39019 after 457 ms (0 ms spent in bootstraps)
[2023-01-31T12:09:36.861+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:36 INFO Utils: Fetching spark://8124f810dec3:39019/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-f6ca1b06-9738-4c87-9008-bbaa51179539/userFiles-2883a7de-955f-46ce-a549-3ed3bca83867/fetchFileTemp14310287787796561260.tmp
[2023-01-31T12:09:40.742+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:40 INFO Executor: Adding file:/tmp/spark-f6ca1b06-9738-4c87-9008-bbaa51179539/userFiles-2883a7de-955f-46ce-a549-3ed3bca83867/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T12:09:40.744+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:40 INFO Executor: Fetching spark://8124f810dec3:39019/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675166964364
[2023-01-31T12:09:40.748+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:40 INFO Utils: Fetching spark://8124f810dec3:39019/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-f6ca1b06-9738-4c87-9008-bbaa51179539/userFiles-2883a7de-955f-46ce-a549-3ed3bca83867/fetchFileTemp13491291997946156638.tmp
[2023-01-31T12:09:42.663+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:42 INFO Executor: Adding file:/tmp/spark-f6ca1b06-9738-4c87-9008-bbaa51179539/userFiles-2883a7de-955f-46ce-a549-3ed3bca83867/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T12:09:42.830+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:42 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46749.
[2023-01-31T12:09:42.831+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:42 INFO NettyBlockTransferService: Server created on 8124f810dec3:46749
[2023-01-31T12:09:42.838+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:42 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T12:09:43.119+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:43 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:09:43.142+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:43 INFO BlockManagerMasterEndpoint: Registering block manager 8124f810dec3:46749 with 434.4 MiB RAM, BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:09:43.207+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:43 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:09:43.222+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:43 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:09:46.572+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:46 INFO AsyncEventQueue: Process of event SparkListenerResourceProfileAdded(Profile: id = 0, executor resources: cores -> name: cores, amount: 1, script: , vendor: ,memory -> name: memory, amount: 1024, script: , vendor: ,offHeap -> name: offHeap, amount: 0, script: , vendor: , task resources: cpus -> name: cpus, amount: 1.0) by listener AppStatusListener took 1.413924298s.
[2023-01-31T12:09:52.266+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:52 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T12:09:52.333+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:52 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T12:10:09.526+0000] {spark_submit.py:495} INFO - 23/01/31 12:10:09 INFO InMemoryFileIndex: It took 509 ms to list leaf files for 1 paths.
[2023-01-31T12:10:12.104+0000] {spark_submit.py:495} INFO - 23/01/31 12:10:12 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T12:10:19.073+0000] {spark_submit.py:495} INFO - 23/01/31 12:10:19 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T12:10:19.236+0000] {spark_submit.py:495} INFO - 23/01/31 12:10:19 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 8124f810dec3:46749 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T12:10:19.430+0000] {spark_submit.py:495} INFO - 23/01/31 12:10:19 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T12:13:17.780+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:17 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 140470 ms exceeds timeout 120000 ms
[2023-01-31T12:13:17.882+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:17 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:13:17.883+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:17 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:13:17.883+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:17 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:13:17.888+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:17 WARN SparkContext: Killing executors is not supported by current scheduler.
[2023-01-31T12:13:17.921+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:17 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:13:17.921+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:17 INFO BlockManager: Reporting 2 blocks to the master.
[2023-01-31T12:13:18.023+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManagerInfo: Updated broadcast_0_piece0 in memory on 8124f810dec3:46749 (current size: 34.6 KiB, original size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T12:13:18.038+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:13:18.039+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:13:18.039+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:13:18.043+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:13:18.090+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManager: Reporting 2 blocks to the master.
[2023-01-31T12:13:18.091+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManagerInfo: Updated broadcast_0_piece0 in memory on 8124f810dec3:46749 (current size: 34.6 KiB, original size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T12:13:18.165+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:13:18.186+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:13:18.206+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:13:18.230+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:13:18.230+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManager: Reporting 2 blocks to the master.
[2023-01-31T12:13:18.253+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManagerInfo: Updated broadcast_0_piece0 in memory on 8124f810dec3:46749 (current size: 34.6 KiB, original size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T12:13:18.320+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:13:18.340+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:13:18.361+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:13:18.391+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:13:18.399+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManager: Reporting 2 blocks to the master.
[2023-01-31T12:13:18.433+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManagerInfo: Updated broadcast_0_piece0 in memory on 8124f810dec3:46749 (current size: 34.6 KiB, original size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T12:13:18.483+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:13:18.494+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:13:18.512+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:13:18.526+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:13:18.527+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManager: Reporting 2 blocks to the master.
[2023-01-31T12:13:18.535+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManagerInfo: Updated broadcast_0_piece0 in memory on 8124f810dec3:46749 (current size: 34.6 KiB, original size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T12:13:18.568+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:13:18.573+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:13:18.642+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:13:18.643+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:13:18.644+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManager: Reporting 2 blocks to the master.
[2023-01-31T12:13:18.651+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManagerInfo: Updated broadcast_0_piece0 in memory on 8124f810dec3:46749 (current size: 34.6 KiB, original size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T12:13:18.719+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:13:18.720+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:13:18.778+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:13:18.780+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:13:18.781+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManager: Reporting 2 blocks to the master.
[2023-01-31T12:13:18.788+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManagerInfo: Updated broadcast_0_piece0 in memory on 8124f810dec3:46749 (current size: 34.6 KiB, original size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T12:13:18.873+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:13:18.873+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:13:18.874+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:13:18.893+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:13:18.905+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManager: Reporting 2 blocks to the master.
[2023-01-31T12:13:18.922+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManagerInfo: Updated broadcast_0_piece0 in memory on 8124f810dec3:46749 (current size: 34.6 KiB, original size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T12:13:18.936+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:13:18.962+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:13:18.962+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:13:18.972+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:13:18.973+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManager: Reporting 2 blocks to the master.
[2023-01-31T12:13:19.099+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManagerInfo: Updated broadcast_0_piece0 in memory on 8124f810dec3:46749 (current size: 34.6 KiB, original size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T12:13:19.107+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:13:19.107+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:13:19.107+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:13:19.118+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:13:19.138+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO BlockManager: Reporting 2 blocks to the master.
[2023-01-31T12:13:19.162+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO BlockManagerInfo: Updated broadcast_0_piece0 in memory on 8124f810dec3:46749 (current size: 34.6 KiB, original size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T12:13:19.221+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:13:19.237+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:13:19.238+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:13:19.258+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:13:19.259+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO BlockManager: Reporting 2 blocks to the master.
[2023-01-31T12:13:19.259+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO BlockManagerInfo: Updated broadcast_0_piece0 in memory on 8124f810dec3:46749 (current size: 34.6 KiB, original size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T12:13:19.334+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:13:19.335+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:13:19.335+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:13:19.389+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:13:19.389+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO BlockManager: Reporting 2 blocks to the master.
[2023-01-31T12:13:19.421+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO BlockManagerInfo: Updated broadcast_0_piece0 in memory on 8124f810dec3:46749 (current size: 34.6 KiB, original size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T12:13:19.433+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:13:19.438+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:13:19.439+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:13:19.449+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:13:19.450+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO BlockManager: Reporting 2 blocks to the master.
[2023-01-31T12:13:19.454+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO BlockManagerInfo: Updated broadcast_0_piece0 in memory on 8124f810dec3:46749 (current size: 34.6 KiB, original size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T12:13:19.461+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:13:19.462+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:13:19.462+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:13:19.469+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:13:19.470+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO BlockManager: Reporting 2 blocks to the master.
[2023-01-31T12:13:19.472+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO BlockManagerInfo: Updated broadcast_0_piece0 in memory on 8124f810dec3:46749 (current size: 34.6 KiB, original size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T12:13:21.723+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:21 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T12:13:23.034+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:23 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T12:13:24.296+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:24 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T12:13:25.096+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:25 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T12:13:25.114+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:25 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T12:13:25.123+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:25 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T12:13:25.146+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:25 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:13:25.146+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:25 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:13:25.147+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:25 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:13:25.174+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:25 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:13:25.190+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:25 INFO BlockManager: Reporting 2 blocks to the master.
[2023-01-31T12:13:25.268+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:25 INFO DAGScheduler: Missing parents: List()
[2023-01-31T12:13:25.400+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:25 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T12:13:25.415+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:25 INFO BlockManagerInfo: Updated broadcast_0_piece0 in memory on 8124f810dec3:46749 (current size: 34.6 KiB, original size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T12:13:26.680+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:26 INFO AsyncEventQueue: Process of event SparkListenerJobStart(0,1675167205268,ArraySeq(org.apache.spark.scheduler.StageInfo@20db115a),{spark.master=local, spark.driver.port=39019, spark.submit.pyFiles=, spark.app.startTime=1675166964364, spark.rdd.compress=True, spark.executor.id=driver, spark.app.name=gcp_playground, spark.submit.deployMode=client, spark.hadoop.google.cloud.auth.service.account.json.keyfile=/.google/credentials/google_credentials.json, spark.driver.host=8124f810dec3, spark.app.id=local-1675166975348, spark.app.submitTime=1675166955548, spark.hadoop.fs.AbstractFileSystem.gs.impl=google.cloud.hadoop.fs.gcs.GoogleHadoopFS, spark.sql.session.timeZone=UTC, spark.hadoop.google.cloud.auth.service.account.enable=true, spark.app.initial.jar.urls=spark://8124f810dec3:39019/jars/gcs-connector-hadoop3-latest.jar,spark://8124f810dec3:39019/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar, spark.executor.extraJavaOptions=-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED, spark.jars=/opt/spark/jars/gcs-connector-hadoop3-latest.jar, /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar, spark.sql.warehouse.dir=file:/opt/***/spark-warehouse, spark.serializer.objectStreamReset=100, spark.driver.extraJavaOptions=-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED}) by listener AppStatusListener took 1.312576491s.
[2023-01-31T12:13:27.060+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:27 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T12:13:27.171+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:27 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T12:13:27.184+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:27 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 8124f810dec3:46749 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T12:13:27.193+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:27 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T12:13:27.266+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T12:13:27.282+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:27 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T12:13:28.310+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:28 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (8124f810dec3, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T12:13:28.866+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:28 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T12:13:32.001+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:32 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010100__202101010200.json:0+9369
[2023-01-31T12:13:35.190+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:35 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:13:35.191+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:35 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:13:35.197+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:35 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:13:35.200+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:35 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:13:35.202+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:35 INFO BlockManager: Reporting 4 blocks to the master.
[2023-01-31T12:13:35.238+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:35 INFO BlockManagerInfo: Updated broadcast_1_piece0 in memory on 8124f810dec3:46749 (current size: 4.6 KiB, original size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T12:13:35.240+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:35 INFO BlockManagerInfo: Updated broadcast_0_piece0 in memory on 8124f810dec3:46749 (current size: 34.6 KiB, original size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T12:13:36.415+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:36 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3437 bytes result sent to driver
[2023-01-31T12:13:36.548+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:36 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 8604 ms on 8124f810dec3 (executor driver) (1/1)
[2023-01-31T12:13:36.607+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:36 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T12:13:36.727+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:36 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 10.409 s
[2023-01-31T12:13:36.760+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:36 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T12:13:36.802+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T12:13:36.820+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:36 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 12.522237 s
[2023-01-31T12:13:45.641+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:45 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:13:45.681+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:45 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:13:45.682+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:45 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:13:45.683+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:45 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:13:45.683+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:45 INFO BlockManager: Reporting 4 blocks to the master.
[2023-01-31T12:13:45.683+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:45 INFO BlockManagerInfo: Updated broadcast_1_piece0 in memory on 8124f810dec3:46749 (current size: 4.6 KiB, original size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T12:13:45.684+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:45 INFO BlockManagerInfo: Updated broadcast_0_piece0 in memory on 8124f810dec3:46749 (current size: 34.6 KiB, original size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T12:14:11.100+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:11 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:14:11.296+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:11 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:14:11.297+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:11 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:14:11.297+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:11 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:14:11.298+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:11 INFO BlockManager: Reporting 4 blocks to the master.
[2023-01-31T12:14:11.916+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:11 INFO BlockManagerInfo: Updated broadcast_1_piece0 in memory on 8124f810dec3:46749 (current size: 4.6 KiB, original size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T12:14:13.090+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:13 INFO BlockManagerInfo: Updated broadcast_0_piece0 in memory on 8124f810dec3:46749 (current size: 34.6 KiB, original size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T12:14:13.420+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:13 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:14:13.421+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:13 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:14:13.431+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:13 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:14:13.457+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:13 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:14:13.458+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:13 INFO BlockManager: Reporting 4 blocks to the master.
[2023-01-31T12:14:13.494+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:13 INFO BlockManagerInfo: Updated broadcast_1_piece0 in memory on 8124f810dec3:46749 (current size: 4.6 KiB, original size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T12:14:13.498+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:13 INFO BlockManagerInfo: Updated broadcast_0_piece0 in memory on 8124f810dec3:46749 (current size: 34.6 KiB, original size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T12:14:15.161+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:15 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:14:15.186+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:15 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:14:15.187+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:15 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:14:15.233+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:15 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:14:15.234+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:15 INFO BlockManager: Reporting 4 blocks to the master.
[2023-01-31T12:14:15.305+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:15 INFO BlockManagerInfo: Updated broadcast_1_piece0 in memory on 8124f810dec3:46749 (current size: 4.6 KiB, original size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T12:14:15.414+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:15 INFO BlockManagerInfo: Updated broadcast_0_piece0 in memory on 8124f810dec3:46749 (current size: 34.6 KiB, original size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T12:14:17.659+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:17 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 8124f810dec3:46749 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T12:14:21.574+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:21 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 8124f810dec3:46749 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T12:14:25.394+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:25 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:14:25.455+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:25 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:14:25.477+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:25 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:14:25.477+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:25 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:14:25.503+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:25 INFO BlockManager: Reporting 0 blocks to the master.
[2023-01-31T12:14:39.128+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:36 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:14:40.528+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:39 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:14:48.028+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:42 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:14:59.240+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:59 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:14:59.272+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:59 INFO BlockManager: Reporting 0 blocks to the master.
[2023-01-31T12:14:59.273+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:59 INFO AsyncEventQueue: Process of event SparkListenerBlockManagerAdded(1675167293994,BlockManagerId(driver, 8124f810dec3, 46749, None),455501414,Some(455501414),Some(0)) by listener ExecutionListenerBus took 2.01608573s.
[2023-01-31T12:14:59.299+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:59 INFO AsyncEventQueue: Process of event SparkListenerBlockManagerAdded(1675167293994,BlockManagerId(driver, 8124f810dec3, 46749, None),455501414,Some(455501414),Some(0)) by listener AppStatusListener took 2.025355774s.
[2023-01-31T12:14:59.508+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:59 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:14:59.549+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:59 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:14:59.549+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:59 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:14:59.550+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:59 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:14:59.550+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:59 INFO BlockManager: Reporting 0 blocks to the master.
[2023-01-31T12:14:59.550+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:59 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:14:59.550+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:59 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:14:59.552+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:59 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:14:59.682+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:59 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:14:59.683+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:59 INFO BlockManager: Reporting 0 blocks to the master.
[2023-01-31T12:15:05.133+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:05 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:15:05.134+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:05 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:15:05.135+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:05 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:15:05.136+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:05 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:15:05.136+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:05 INFO BlockManager: Reporting 0 blocks to the master.
[2023-01-31T12:15:10.425+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:10 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T12:15:10.443+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:10 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T12:15:10.459+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:10 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T12:15:12.445+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:12 INFO CodeGenerator: Code generated in 687.837689 ms
[2023-01-31T12:15:12.505+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:12 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T12:15:12.660+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:12 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T12:15:12.662+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:12 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 8124f810dec3:46749 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T12:15:12.664+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:12 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T12:15:12.709+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T12:15:13.284+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:13 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T12:15:13.285+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:13 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T12:15:13.286+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:13 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T12:15:13.286+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:13 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T12:15:13.287+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:13 INFO DAGScheduler: Missing parents: List()
[2023-01-31T12:15:13.299+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:13 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T12:15:13.336+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:13 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T12:15:13.356+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:13 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T12:15:13.358+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:13 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 8124f810dec3:46749 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T12:15:13.360+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:13 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T12:15:13.369+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T12:15:13.374+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:13 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T12:15:13.407+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:13 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (8124f810dec3, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T12:15:13.420+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:13 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T12:15:14.044+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:14 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T12:15:14.677+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:14 INFO CodeGenerator: Code generated in 398.157688 ms
[2023-01-31T12:15:15.136+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:15:15.137+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:15:15.139+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:15:15.139+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:15:15.139+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO BlockManager: Reporting 4 blocks to the master.
[2023-01-31T12:15:15.144+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:46749 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T12:15:15.149+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T12:15:15.256+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1628 bytes result sent to driver
[2023-01-31T12:15:15.264+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1878 ms on 8124f810dec3 (executor driver) (1/1)
[2023-01-31T12:15:15.274+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 1.952 s
[2023-01-31T12:15:15.274+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T12:15:15.276+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T12:15:15.281+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T12:15:15.282+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 1.994297 s
[2023-01-31T12:15:15.717+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T12:15:15.718+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T12:15:15.719+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T12:15:16.049+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:16 INFO CodeGenerator: Code generated in 263.86428 ms
[2023-01-31T12:15:16.195+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:16 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T12:15:16.237+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:16 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T12:15:16.267+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:16 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 8124f810dec3:46749 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:15:16.272+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:16 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T12:15:16.287+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:16 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T12:15:16.502+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:16 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T12:15:16.507+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:16 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T12:15:16.517+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:16 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T12:15:16.518+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:16 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T12:15:16.518+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:16 INFO DAGScheduler: Missing parents: List()
[2023-01-31T12:15:16.546+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:16 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T12:15:16.562+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:16 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T12:15:16.584+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:16 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T12:15:16.591+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:16 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 8124f810dec3:46749 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:15:16.596+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:16 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T12:15:16.599+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T12:15:16.600+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:16 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T12:15:16.612+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:16 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (8124f810dec3, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T12:15:16.614+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:16 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T12:15:16.734+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:16 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T12:15:17.664+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:17 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1585 bytes result sent to driver
[2023-01-31T12:15:17.875+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:17 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1260 ms on 8124f810dec3 (executor driver) (1/1)
[2023-01-31T12:15:17.876+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:17 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T12:15:17.905+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:17 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 1.330 s
[2023-01-31T12:15:17.905+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:17 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T12:15:17.906+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T12:15:17.906+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:17 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 1.376246 s
[2023-01-31T12:15:20.712+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:20 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T12:15:20.777+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:20 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T12:15:20.824+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:20 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T12:15:21.693+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO CodeGenerator: Code generated in 467.145307 ms
[2023-01-31T12:15:21.722+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T12:15:21.857+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T12:15:21.863+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 8124f810dec3:46749 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:15:21.871+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T12:15:21.911+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T12:15:22.416+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:22 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T12:15:22.420+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:22 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T12:15:22.420+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:22 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T12:15:22.421+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:22 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T12:15:22.421+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:22 INFO DAGScheduler: Missing parents: List()
[2023-01-31T12:15:22.441+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:22 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T12:15:22.468+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:22 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.6 MiB)
[2023-01-31T12:15:22.481+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:22 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.6 MiB)
[2023-01-31T12:15:22.485+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:22 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 8124f810dec3:46749 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T12:15:22.487+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:22 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T12:15:22.492+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T12:15:22.493+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:22 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T12:15:22.498+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:22 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (8124f810dec3, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T12:15:22.500+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:22 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T12:15:22.585+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:22 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T12:15:23.106+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:23 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-01-31T12:15:23.129+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:23 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 628 ms on 8124f810dec3 (executor driver) (1/1)
[2023-01-31T12:15:23.130+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:23 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T12:15:23.131+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:23 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 0.682 s
[2023-01-31T12:15:23.132+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:23 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T12:15:23.138+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T12:15:23.140+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:23 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 0.722741 s
[2023-01-31T12:15:23.385+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:23 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T12:15:23.386+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:23 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T12:15:23.402+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:23 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T12:15:23.799+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:23 INFO CodeGenerator: Code generated in 298.583391 ms
[2023-01-31T12:15:23.821+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:23 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.4 MiB)
[2023-01-31T12:15:23.879+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:23 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T12:15:23.886+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:23 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 8124f810dec3:46749 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:15:23.889+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:23 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T12:15:23.894+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:23 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T12:15:23.959+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:23 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T12:15:23.969+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:23 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T12:15:23.970+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:23 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T12:15:23.971+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:23 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T12:15:23.977+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:23 INFO DAGScheduler: Missing parents: List()
[2023-01-31T12:15:23.985+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:23 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T12:15:24.005+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:24 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-31T12:15:24.029+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:24 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-01-31T12:15:24.032+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:24 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 8124f810dec3:46749 (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T12:15:24.037+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:24 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T12:15:24.043+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T12:15:24.047+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:24 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T12:15:24.051+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:24 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (8124f810dec3, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T12:15:24.052+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:24 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T12:15:24.125+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:24 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T12:15:24.382+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:24 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1586 bytes result sent to driver
[2023-01-31T12:15:24.392+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:24 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 342 ms on 8124f810dec3 (executor driver) (1/1)
[2023-01-31T12:15:24.393+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:24 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T12:15:24.394+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:24 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 0.411 s
[2023-01-31T12:15:24.399+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:24 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T12:15:24.400+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T12:15:24.404+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:24 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 0.436794 s
[2023-01-31T12:15:24.589+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:24 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 8124f810dec3:46749 in memory (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T12:15:24.671+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:24 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 8124f810dec3:46749 in memory (size: 7.7 KiB, free: 434.2 MiB)
[2023-01-31T12:15:24.716+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:24 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 8124f810dec3:46749 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:15:24.791+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:24 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 8124f810dec3:46749 in memory (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T12:15:25.145+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:25 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:15:25.160+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:25 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:15:25.162+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:25 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:15:25.171+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:25 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:15:25.172+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:25 INFO BlockManager: Reporting 8 blocks to the master.
[2023-01-31T12:15:25.205+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:25 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:15:25.224+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:25 INFO BlockManagerInfo: Updated broadcast_8_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:15:25.230+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:25 INFO BlockManagerInfo: Updated broadcast_6_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:15:25.243+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:25 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:15:26.597+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:26 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T12:15:26.607+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:26 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T12:15:26.612+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:26 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T12:15:28.133+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO CodeGenerator: Code generated in 1318.920046 ms
[2023-01-31T12:15:28.216+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.3 MiB)
[2023-01-31T12:15:28.409+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T12:15:28.419+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 8124f810dec3:46749 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:15:28.422+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T12:15:28.438+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T12:15:28.590+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T12:15:28.597+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T12:15:28.598+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T12:15:28.598+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T12:15:28.599+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO DAGScheduler: Missing parents: List()
[2023-01-31T12:15:28.602+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T12:15:28.669+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.2 MiB)
[2023-01-31T12:15:28.676+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.2 MiB)
[2023-01-31T12:15:28.685+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 8124f810dec3:46749 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T12:15:28.689+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T12:15:28.691+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T12:15:28.695+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T12:15:28.706+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (8124f810dec3, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T12:15:28.708+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T12:15:28.775+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T12:15:28.975+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2602 bytes result sent to driver
[2023-01-31T12:15:28.988+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 275 ms on 8124f810dec3 (executor driver) (1/1)
[2023-01-31T12:15:28.989+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T12:15:28.989+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 0.373 s
[2023-01-31T12:15:28.990+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T12:15:28.990+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T12:15:28.991+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 0.387535 s
[2023-01-31T12:15:29.372+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:29 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T12:15:29.388+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:29 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T12:15:29.390+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:29 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T12:15:30.843+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:30 INFO CodeGenerator: Code generated in 790.792845 ms
[2023-01-31T12:15:30.890+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:30 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 433.0 MiB)
[2023-01-31T12:15:31.049+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:31 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.0 MiB)
[2023-01-31T12:15:31.242+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:31 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 8124f810dec3:46749 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:15:31.348+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:31 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T12:15:31.390+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:31 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T12:15:31.774+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:31 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T12:15:31.795+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:31 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T12:15:31.796+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:31 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T12:15:31.796+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:31 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T12:15:31.798+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:31 INFO DAGScheduler: Missing parents: List()
[2023-01-31T12:15:32.129+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:32 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T12:15:32.795+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:32 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 432.9 MiB)
[2023-01-31T12:15:33.634+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:33 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.9 MiB)
[2023-01-31T12:15:33.703+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:33 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 8124f810dec3:46749 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T12:15:34.238+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:34 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T12:15:36.084+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T12:15:36.117+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:36 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T12:15:36.222+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:36 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (8124f810dec3, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T12:15:36.246+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:36 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T12:15:36.310+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:36 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:15:36.311+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:36 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:15:36.353+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:36 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:15:36.353+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:36 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:15:36.365+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:36 INFO BlockManager: Reporting 16 blocks to the master.
[2023-01-31T12:15:36.600+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:36 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:15:37.750+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:37 INFO BlockManagerInfo: Updated broadcast_13_piece0 in memory on 8124f810dec3:46749 (current size: 12.6 KiB, original size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T12:15:39.643+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:39 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T12:15:39.683+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:39 INFO BlockManagerInfo: Updated broadcast_10_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:15:40.902+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:40 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:15:41.394+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:40 INFO BlockManagerInfo: Updated broadcast_8_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:15:42.655+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:42 INFO BlockManagerInfo: Updated broadcast_6_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:15:45.154+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:44 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,ArraySeq(),HashMap((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@6e96bf53)) by listener AppStatusListener took 1.608754975s.
[2023-01-31T12:15:45.767+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:44 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:15:46.558+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:46 INFO BlockManagerInfo: Updated broadcast_11_piece0 in memory on 8124f810dec3:46749 (current size: 13.4 KiB, original size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T12:15:50.154+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:50 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:15:50.934+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:50 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:15:52.573+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:50 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:15:52.573+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:50 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:15:52.913+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:50 INFO BlockManager: Reporting 16 blocks to the master.
[2023-01-31T12:15:55.166+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:53 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:15:55.170+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:53 INFO BlockManagerInfo: Updated broadcast_13_piece0 in memory on 8124f810dec3:46749 (current size: 12.6 KiB, original size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T12:15:57.884+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:54 INFO BlockManagerInfo: Updated broadcast_10_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:20:18.558+0000] {spark_submit.py:495} INFO - 23/01/31 12:20:16 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,ArraySeq(),HashMap((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@2a7f8dc2)) by listener AppStatusListener took 262.153043269s.
[2023-01-31T12:20:19.601+0000] {spark_submit.py:495} INFO - 23/01/31 12:20:18 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:20:20.183+0000] {spark_submit.py:495} INFO - 23/01/31 12:20:19 INFO BlockManagerInfo: Updated broadcast_8_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:20:26.655+0000] {spark_submit.py:495} INFO - 23/01/31 12:20:26 INFO BlockManagerInfo: Updated broadcast_6_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:20:26.452+0000] {base_job.py:232} ERROR - LocalTaskJob heartbeat got an exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3361, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 325, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 888, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 491, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 271, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 386, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 684, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 210, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 680, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/jobs/base_job.py", line 228, in heartbeat
    self.heartbeat_callback(session=session)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/jobs/local_task_job.py", line 178, in heartbeat_callback
    self.task_instance.refresh_from_db()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 795, in refresh_from_db
    ti = qry.one_or_none()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2849, in one_or_none
    return self._iter().one_or_none()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2915, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1552, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3315, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3394, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3364, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2198, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 210, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3361, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 325, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 888, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 491, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 271, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 386, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 684, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 210, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 680, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-01-31T12:20:29.270+0000] {spark_submit.py:495} INFO - 23/01/31 12:20:29 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:20:30.189+0000] {spark_submit.py:495} INFO - 23/01/31 12:20:30 INFO BlockManagerInfo: Updated broadcast_11_piece0 in memory on 8124f810dec3:46749 (current size: 13.4 KiB, original size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T12:20:30.994+0000] {spark_submit.py:495} INFO - 23/01/31 12:20:30 INFO AsyncEventQueue: Process of event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(driver, 8124f810dec3, 46749, None),broadcast_6_piece0,StorageLevel(memory, 1 replicas),35288,0)) by listener AppStatusListener took 1.451304457s.
[2023-01-31T12:20:32.044+0000] {spark_submit.py:495} INFO - 23/01/31 12:20:31 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:20:32.232+0000] {spark_submit.py:495} INFO - 23/01/31 12:20:32 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:20:32.613+0000] {spark_submit.py:495} INFO - 23/01/31 12:20:32 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:20:32.689+0000] {spark_submit.py:495} INFO - 23/01/31 12:20:32 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:20:32.695+0000] {spark_submit.py:495} INFO - 23/01/31 12:20:32 INFO BlockManager: Reporting 16 blocks to the master.
[2023-01-31T12:20:33.020+0000] {spark_submit.py:495} INFO - 23/01/31 12:20:32 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:20:33.818+0000] {spark_submit.py:495} INFO - 23/01/31 12:20:33 INFO BlockManagerInfo: Updated broadcast_13_piece0 in memory on 8124f810dec3:46749 (current size: 12.6 KiB, original size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T12:20:37.965+0000] {spark_submit.py:495} INFO - 23/01/31 12:20:36 INFO BlockManagerInfo: Updated broadcast_10_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:20:50.897+0000] {spark_submit.py:495} INFO - 23/01/31 12:20:49 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:21:00.764+0000] {spark_submit.py:495} INFO - 23/01/31 12:20:57 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,ArraySeq(),HashMap((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@3e488751)) by listener AppStatusListener took 3.738137648s.
[2023-01-31T12:21:04.250+0000] {spark_submit.py:495} INFO - 23/01/31 12:20:57 INFO BlockManagerInfo: Updated broadcast_8_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:21:59.160+0000] {spark_submit.py:495} INFO - 23/01/31 12:21:58 INFO BlockManagerInfo: Updated broadcast_6_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.133+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.134+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_11_piece0 in memory on 8124f810dec3:46749 (current size: 13.4 KiB, original size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.135+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:00.135+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:22:00.135+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:00.136+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:00.137+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManager: Reporting 16 blocks to the master.
[2023-01-31T12:22:00.138+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.142+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_13_piece0 in memory on 8124f810dec3:46749 (current size: 12.6 KiB, original size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.145+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_10_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.152+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.155+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_8_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.158+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_6_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.160+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.163+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_11_piece0 in memory on 8124f810dec3:46749 (current size: 13.4 KiB, original size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.167+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:00.168+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:22:00.168+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:00.170+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:00.171+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManager: Reporting 16 blocks to the master.
[2023-01-31T12:22:00.173+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.175+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_13_piece0 in memory on 8124f810dec3:46749 (current size: 12.6 KiB, original size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.178+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_10_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.206+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.209+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_8_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.212+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_6_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.215+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.217+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_11_piece0 in memory on 8124f810dec3:46749 (current size: 13.4 KiB, original size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.222+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:00.223+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:22:00.223+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:00.225+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:00.226+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManager: Reporting 16 blocks to the master.
[2023-01-31T12:22:00.229+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.232+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_13_piece0 in memory on 8124f810dec3:46749 (current size: 12.6 KiB, original size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.235+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_10_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.237+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.240+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_8_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.242+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_6_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.245+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.248+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_11_piece0 in memory on 8124f810dec3:46749 (current size: 13.4 KiB, original size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.251+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:00.252+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:22:00.252+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:00.254+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:00.254+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManager: Reporting 16 blocks to the master.
[2023-01-31T12:22:00.256+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.259+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_13_piece0 in memory on 8124f810dec3:46749 (current size: 12.6 KiB, original size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.261+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_10_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.264+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.267+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_8_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.269+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_6_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.272+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.274+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_11_piece0 in memory on 8124f810dec3:46749 (current size: 13.4 KiB, original size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.278+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:00.279+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:22:00.279+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:00.281+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:00.281+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManager: Reporting 16 blocks to the master.
[2023-01-31T12:22:00.284+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.286+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_13_piece0 in memory on 8124f810dec3:46749 (current size: 12.6 KiB, original size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.288+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_10_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.291+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.297+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_8_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.300+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_6_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.303+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.305+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_11_piece0 in memory on 8124f810dec3:46749 (current size: 13.4 KiB, original size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.309+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:00.310+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:22:00.310+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:00.312+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:00.313+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManager: Reporting 16 blocks to the master.
[2023-01-31T12:22:00.315+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.317+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_13_piece0 in memory on 8124f810dec3:46749 (current size: 12.6 KiB, original size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.319+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_10_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.322+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.324+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_8_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.327+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_6_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.330+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.332+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_11_piece0 in memory on 8124f810dec3:46749 (current size: 13.4 KiB, original size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.336+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:00.337+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:22:00.338+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:00.340+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:00.340+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManager: Reporting 16 blocks to the master.
[2023-01-31T12:22:00.342+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.345+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_13_piece0 in memory on 8124f810dec3:46749 (current size: 12.6 KiB, original size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.351+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_10_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.354+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.357+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_8_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.360+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_6_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.363+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.366+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_11_piece0 in memory on 8124f810dec3:46749 (current size: 13.4 KiB, original size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.370+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:00.371+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:22:00.371+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:00.373+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:00.374+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManager: Reporting 16 blocks to the master.
[2023-01-31T12:22:00.376+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.378+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_13_piece0 in memory on 8124f810dec3:46749 (current size: 12.6 KiB, original size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.381+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_10_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.384+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.387+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_8_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.390+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_6_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.416+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.430+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_11_piece0 in memory on 8124f810dec3:46749 (current size: 13.4 KiB, original size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.443+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:00.444+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:22:00.445+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:00.448+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:00.448+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManager: Reporting 16 blocks to the master.
[2023-01-31T12:22:00.450+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.454+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_13_piece0 in memory on 8124f810dec3:46749 (current size: 12.6 KiB, original size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.457+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_10_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.461+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.472+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_8_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.474+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_6_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.477+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.479+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_11_piece0 in memory on 8124f810dec3:46749 (current size: 13.4 KiB, original size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.483+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:00.484+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:22:00.484+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:00.486+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:00.486+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManager: Reporting 16 blocks to the master.
[2023-01-31T12:22:00.488+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.491+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_13_piece0 in memory on 8124f810dec3:46749 (current size: 12.6 KiB, original size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.493+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_10_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.497+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.499+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_8_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.502+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_6_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.504+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.524+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_11_piece0 in memory on 8124f810dec3:46749 (current size: 13.4 KiB, original size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.536+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:00.536+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:22:00.537+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:00.539+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:00.540+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManager: Reporting 16 blocks to the master.
[2023-01-31T12:22:00.542+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.545+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_13_piece0 in memory on 8124f810dec3:46749 (current size: 12.6 KiB, original size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.547+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_10_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.550+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.553+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_8_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.556+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_6_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.558+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.561+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_11_piece0 in memory on 8124f810dec3:46749 (current size: 13.4 KiB, original size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.565+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:00.565+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:22:00.566+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:00.567+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:00.568+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManager: Reporting 16 blocks to the master.
[2023-01-31T12:22:00.570+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.572+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_13_piece0 in memory on 8124f810dec3:46749 (current size: 12.6 KiB, original size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.575+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_10_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.577+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.580+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_8_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.582+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_6_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.595+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.597+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_11_piece0 in memory on 8124f810dec3:46749 (current size: 13.4 KiB, original size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.620+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:00.622+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:22:00.638+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:00.641+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:00.641+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManager: Reporting 16 blocks to the master.
[2023-01-31T12:22:00.643+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.648+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_13_piece0 in memory on 8124f810dec3:46749 (current size: 12.6 KiB, original size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.659+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_10_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.670+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.674+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_8_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.678+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_6_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.684+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.692+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_11_piece0 in memory on 8124f810dec3:46749 (current size: 13.4 KiB, original size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.695+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:00.696+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:22:00.696+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:00.698+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:00.698+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManager: Reporting 16 blocks to the master.
[2023-01-31T12:22:00.700+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.702+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_13_piece0 in memory on 8124f810dec3:46749 (current size: 12.6 KiB, original size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.704+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_10_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.731+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.734+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_8_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.740+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_6_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.742+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.745+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_11_piece0 in memory on 8124f810dec3:46749 (current size: 13.4 KiB, original size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.753+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:00.758+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:22:00.759+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:00.778+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:00.778+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManager: Reporting 16 blocks to the master.
[2023-01-31T12:22:00.779+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.792+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_13_piece0 in memory on 8124f810dec3:46749 (current size: 12.6 KiB, original size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.795+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_10_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.864+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.870+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_8_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.885+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_6_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.911+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.925+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_11_piece0 in memory on 8124f810dec3:46749 (current size: 13.4 KiB, original size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.937+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:00.941+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:22:00.947+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:00.952+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:00.953+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManager: Reporting 16 blocks to the master.
[2023-01-31T12:22:00.955+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.964+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_13_piece0 in memory on 8124f810dec3:46749 (current size: 12.6 KiB, original size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.968+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_10_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.971+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.980+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_8_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.985+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_6_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:00.988+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.023+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_11_piece0 in memory on 8124f810dec3:46749 (current size: 13.4 KiB, original size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.040+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:01.040+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:22:01.041+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:01.044+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:01.044+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManager: Reporting 16 blocks to the master.
[2023-01-31T12:22:01.045+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.054+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_13_piece0 in memory on 8124f810dec3:46749 (current size: 12.6 KiB, original size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.067+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_10_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.080+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.095+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_8_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.104+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_6_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.115+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.119+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_11_piece0 in memory on 8124f810dec3:46749 (current size: 13.4 KiB, original size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.129+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:01.129+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:22:01.129+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:01.131+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:01.132+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManager: Reporting 16 blocks to the master.
[2023-01-31T12:22:01.133+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.136+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_13_piece0 in memory on 8124f810dec3:46749 (current size: 12.6 KiB, original size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.142+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_10_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.154+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.157+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_8_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.161+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_6_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.177+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.180+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_11_piece0 in memory on 8124f810dec3:46749 (current size: 13.4 KiB, original size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.187+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:01.188+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:22:01.188+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:01.190+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:01.191+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManager: Reporting 16 blocks to the master.
[2023-01-31T12:22:01.192+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.198+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_13_piece0 in memory on 8124f810dec3:46749 (current size: 12.6 KiB, original size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.203+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_10_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.210+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.242+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_8_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.245+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_6_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.253+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.260+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_11_piece0 in memory on 8124f810dec3:46749 (current size: 13.4 KiB, original size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.266+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:01.266+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:22:01.267+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:01.331+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:01.331+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManager: Reporting 16 blocks to the master.
[2023-01-31T12:22:01.333+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.335+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_13_piece0 in memory on 8124f810dec3:46749 (current size: 12.6 KiB, original size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.391+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_10_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.436+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.438+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_8_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.496+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_6_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.564+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.572+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_11_piece0 in memory on 8124f810dec3:46749 (current size: 13.4 KiB, original size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.576+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:01.576+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:22:01.577+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:01.588+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:01.588+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManager: Reporting 16 blocks to the master.
[2023-01-31T12:22:01.617+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.620+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_13_piece0 in memory on 8124f810dec3:46749 (current size: 12.6 KiB, original size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.676+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_10_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.686+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.871+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_8_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.874+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_6_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.908+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.916+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_11_piece0 in memory on 8124f810dec3:46749 (current size: 13.4 KiB, original size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.927+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:01.928+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:22:01.928+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:01.932+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:01.933+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManager: Reporting 16 blocks to the master.
[2023-01-31T12:22:01.933+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.935+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_13_piece0 in memory on 8124f810dec3:46749 (current size: 12.6 KiB, original size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.939+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_10_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.941+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.943+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_8_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.944+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_6_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:02.049+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:02.103+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_11_piece0 in memory on 8124f810dec3:46749 (current size: 13.4 KiB, original size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T12:22:02.107+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:02.108+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:22:02.109+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:02.121+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:02.122+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManager: Reporting 16 blocks to the master.
[2023-01-31T12:22:02.134+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:02.197+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_13_piece0 in memory on 8124f810dec3:46749 (current size: 12.6 KiB, original size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T12:22:02.202+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_10_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:02.275+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:02.277+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_8_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:02.278+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_6_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:02.326+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:02.327+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_11_piece0 in memory on 8124f810dec3:46749 (current size: 13.4 KiB, original size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T12:22:02.337+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:02.338+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:22:02.338+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:02.388+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:02.394+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManager: Reporting 16 blocks to the master.
[2023-01-31T12:22:02.402+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:02.427+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_13_piece0 in memory on 8124f810dec3:46749 (current size: 12.6 KiB, original size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T12:22:02.429+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_10_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:02.433+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:02.442+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_8_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:02.449+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_6_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:02.458+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:02.469+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_11_piece0 in memory on 8124f810dec3:46749 (current size: 13.4 KiB, original size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T12:22:02.475+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:02.476+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:22:02.477+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:02.539+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:02.540+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManager: Reporting 16 blocks to the master.
[2023-01-31T12:22:02.543+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:02.544+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_13_piece0 in memory on 8124f810dec3:46749 (current size: 12.6 KiB, original size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T12:22:02.699+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_10_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:02.913+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:03.013+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_8_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:03.014+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_6_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:03.023+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:03.164+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_11_piece0 in memory on 8124f810dec3:46749 (current size: 13.4 KiB, original size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T12:22:03.203+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:03.224+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:22:03.229+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:03.229+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:03.230+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManager: Reporting 16 blocks to the master.
[2023-01-31T12:22:03.346+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:03.352+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_13_piece0 in memory on 8124f810dec3:46749 (current size: 12.6 KiB, original size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T12:22:03.353+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_10_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:03.353+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:03.370+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_8_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:03.409+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-01-31T12:22:03.444+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 387273 ms on 8124f810dec3 (executor driver) (1/1)
[2023-01-31T12:22:03.452+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T12:22:03.454+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_6_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:03.472+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 391.238 s
[2023-01-31T12:22:03.501+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T12:22:03.517+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T12:22:03.537+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 391.652925 s
[2023-01-31T12:22:03.566+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:03.581+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_11_piece0 in memory on 8124f810dec3:46749 (current size: 13.4 KiB, original size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T12:22:03.592+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:03.605+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:22:03.639+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:03.652+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:03.675+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManager: Reporting 16 blocks to the master.
[2023-01-31T12:22:03.685+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:03.747+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_13_piece0 in memory on 8124f810dec3:46749 (current size: 12.6 KiB, original size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T12:22:03.747+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_10_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:03.748+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:03.901+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_8_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:03.902+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_6_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:03.902+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:03.903+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_11_piece0 in memory on 8124f810dec3:46749 (current size: 13.4 KiB, original size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T12:22:03.903+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:03.903+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:22:03.904+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:03.904+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:03.962+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManager: Reporting 16 blocks to the master.
[2023-01-31T12:22:03.963+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:03.963+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_13_piece0 in memory on 8124f810dec3:46749 (current size: 12.6 KiB, original size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T12:22:03.963+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_10_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:03.964+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:03.994+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_8_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:04.046+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_6_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:04.047+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:04.047+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_11_piece0 in memory on 8124f810dec3:46749 (current size: 13.4 KiB, original size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T12:22:04.047+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:04.114+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:22:04.215+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:04.216+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:04.217+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManager: Reporting 16 blocks to the master.
[2023-01-31T12:22:04.225+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:04.260+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_13_piece0 in memory on 8124f810dec3:46749 (current size: 12.6 KiB, original size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T12:22:04.261+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_10_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:04.261+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:04.262+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_8_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:04.262+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_6_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:04.268+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:04.268+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_11_piece0 in memory on 8124f810dec3:46749 (current size: 13.4 KiB, original size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T12:22:04.275+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:04.277+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:22:04.280+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:04.283+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:04.300+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManager: Reporting 16 blocks to the master.
[2023-01-31T12:22:04.343+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:04.344+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_13_piece0 in memory on 8124f810dec3:46749 (current size: 12.6 KiB, original size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T12:22:04.408+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_10_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:04.408+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:04.409+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_8_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:04.409+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_6_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:04.410+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:04.464+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_11_piece0 in memory on 8124f810dec3:46749 (current size: 13.4 KiB, original size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T12:22:04.465+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:04.465+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:22:04.465+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:05.142+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:05.150+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManager: Reporting 16 blocks to the master.
[2023-01-31T12:22:05.151+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:05.151+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_13_piece0 in memory on 8124f810dec3:46749 (current size: 12.6 KiB, original size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T12:22:05.151+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_10_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:05.152+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:05.152+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_8_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:05.153+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_6_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:05.153+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:05.153+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_11_piece0 in memory on 8124f810dec3:46749 (current size: 13.4 KiB, original size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T12:22:05.154+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:05.154+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:22:05.155+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:05.155+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:05.155+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManager: Reporting 16 blocks to the master.
[2023-01-31T12:22:05.156+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:05.156+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_13_piece0 in memory on 8124f810dec3:46749 (current size: 12.6 KiB, original size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T12:22:05.157+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_10_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:05.157+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:05.157+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_8_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:05.158+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_6_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:05.158+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:05.159+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_11_piece0 in memory on 8124f810dec3:46749 (current size: 13.4 KiB, original size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T12:22:05.159+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:05.159+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:22:05.160+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:05.160+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:05.160+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManager: Reporting 16 blocks to the master.
[2023-01-31T12:22:05.161+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:05.161+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_13_piece0 in memory on 8124f810dec3:46749 (current size: 12.6 KiB, original size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T12:22:05.162+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_10_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:05.162+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:05.162+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_8_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:05.163+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_6_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:05.163+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:05.163+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_11_piece0 in memory on 8124f810dec3:46749 (current size: 13.4 KiB, original size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T12:22:05.164+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:05.164+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:22:05.165+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:05.165+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:05.165+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManager: Reporting 16 blocks to the master.
[2023-01-31T12:22:05.166+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:05.166+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_13_piece0 in memory on 8124f810dec3:46749 (current size: 12.6 KiB, original size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T12:22:05.166+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_10_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:05.167+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:05.167+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_8_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:05.167+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_6_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:05.168+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:05.168+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_11_piece0 in memory on 8124f810dec3:46749 (current size: 13.4 KiB, original size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T12:22:05.169+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:05.169+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:22:05.169+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:05.170+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:05.170+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManager: Reporting 16 blocks to the master.
[2023-01-31T12:22:05.171+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:05.171+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_13_piece0 in memory on 8124f810dec3:46749 (current size: 12.6 KiB, original size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T12:22:05.171+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_10_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:05.172+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:05.172+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_8_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:05.172+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_6_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:05.173+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:05.173+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_11_piece0 in memory on 8124f810dec3:46749 (current size: 13.4 KiB, original size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T12:22:05.174+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:05.174+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:22:05.174+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:05.175+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:05.175+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManager: Reporting 16 blocks to the master.
[2023-01-31T12:22:05.175+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:05.176+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_13_piece0 in memory on 8124f810dec3:46749 (current size: 12.6 KiB, original size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T12:22:05.176+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_10_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:05.177+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:05.177+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_8_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:05.177+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_6_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:05.177+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:05.178+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_11_piece0 in memory on 8124f810dec3:46749 (current size: 13.4 KiB, original size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T12:22:05.178+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T12:22:05.178+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T12:22:05.179+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T12:22:05.179+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO CodeGenerator: Code generated in 91.95927 ms
[2023-01-31T12:22:05.180+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.7 MiB)
[2023-01-31T12:22:05.180+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.7 MiB)
[2023-01-31T12:22:05.180+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 8124f810dec3:46749 (size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T12:22:05.181+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:204
[2023-01-31T12:22:05.181+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T12:22:05.182+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:204
[2023-01-31T12:22:05.182+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 8124f810dec3:46749 in memory (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T12:22:05.182+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 8124f810dec3:46749 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:05.183+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:204) with 1 output partitions
[2023-01-31T12:22:05.183+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:204)
[2023-01-31T12:22:05.183+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T12:22:05.184+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO DAGScheduler: Missing parents: List()
[2023-01-31T12:22:05.184+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 8124f810dec3:46749 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:05.185+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 8124f810dec3:46749 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:05.185+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:204), which has no missing parents
[2023-01-31T12:22:05.185+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 8124f810dec3:46749 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:05.186+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 433.6 MiB)
[2023-01-31T12:22:05.186+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 433.6 MiB)
[2023-01-31T12:22:05.186+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 8124f810dec3:46749 (size: 12.6 KiB, free: 434.3 MiB)
[2023-01-31T12:22:05.187+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T12:22:05.187+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:204) (first 15 tasks are for partitions Vector(0))
[2023-01-31T12:22:05.188+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T12:22:05.188+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (8124f810dec3, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T12:22:05.188+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T12:22:05.189+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:05 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T12:22:05.189+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:05 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 8124f810dec3:46749 in memory (size: 12.6 KiB, free: 434.3 MiB)
[2023-01-31T12:22:05.678+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:05 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:05.701+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:05 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:22:05.701+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:05 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:05.704+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:05 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:05.704+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:05 INFO BlockManager: Reporting 8 blocks to the master.
[2023-01-31T12:22:05.727+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:05 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:05.770+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:05 INFO BlockManagerInfo: Updated broadcast_14_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:05.872+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:05 INFO BlockManagerInfo: Updated broadcast_10_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:05.885+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:05 INFO BlockManagerInfo: Updated broadcast_15_piece0 in memory on 8124f810dec3:46749 (current size: 12.6 KiB, original size: 12.6 KiB, free: 434.3 MiB)
[2023-01-31T12:22:06.320+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1729 bytes result sent to driver
[2023-01-31T12:22:06.330+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 1372 ms on 8124f810dec3 (executor driver) (1/1)
[2023-01-31T12:22:06.347+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T12:22:06.350+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:204) finished in 1.418 s
[2023-01-31T12:22:06.351+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T12:22:06.351+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T12:22:06.372+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:204, took 1.449603 s
[2023-01-31T12:22:15.130+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:15 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:15.132+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:15 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:22:15.133+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:15 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:15.133+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:15 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:15.134+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:15 INFO BlockManager: Reporting 8 blocks to the master.
[2023-01-31T12:22:15.134+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:15 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:15.142+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:15 INFO BlockManagerInfo: Updated broadcast_14_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:15.143+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:15 INFO BlockManagerInfo: Updated broadcast_10_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:15.146+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:15 INFO BlockManagerInfo: Updated broadcast_15_piece0 in memory on 8124f810dec3:46749 (current size: 12.6 KiB, original size: 12.6 KiB, free: 434.3 MiB)
[2023-01-31T12:22:19.029+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:19 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 8124f810dec3:46749 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:19.041+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:19 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 8124f810dec3:46749 in memory (size: 12.6 KiB, free: 434.3 MiB)
[2023-01-31T12:22:19.048+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:19 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 8124f810dec3:46749 in memory (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T12:22:25.130+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:25.132+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:22:25.132+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:25.134+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:25.136+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO BlockManager: Reporting 2 blocks to the master.
[2023-01-31T12:22:25.136+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO BlockManagerInfo: Updated broadcast_10_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T12:22:30.005+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:30 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T12:22:30.032+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T12:22:30.033+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T12:22:30.035+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T12:22:30.036+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T12:22:30.037+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T12:22:30.046+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T12:22:30.720+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:30 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-31T12:22:30.721+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:30 INFO DAGScheduler: Got job 8 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T12:22:30.726+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:30 INFO DAGScheduler: Final stage: ResultStage 8 (save at BigQueryWriteHelper.java:105)
[2023-01-31T12:22:30.727+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:30 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T12:22:30.727+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:30 INFO DAGScheduler: Missing parents: List()
[2023-01-31T12:22:30.735+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:30 INFO DAGScheduler: Submitting ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T12:22:30.834+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:30 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 209.9 KiB, free 434.0 MiB)
[2023-01-31T12:22:30.845+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:30 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 74.3 KiB, free 433.9 MiB)
[2023-01-31T12:22:30.848+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:30 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 8124f810dec3:46749 (size: 74.3 KiB, free: 434.3 MiB)
[2023-01-31T12:22:30.851+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:30 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513
[2023-01-31T12:22:30.854+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T12:22:30.856+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:30 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2023-01-31T12:22:30.869+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:30 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (8124f810dec3, executor driver, partition 0, PROCESS_LOCAL, 7484 bytes) taskResourceAssignments Map()
[2023-01-31T12:22:30.870+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:30 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2023-01-31T12:22:30.958+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T12:22:30.961+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T12:22:30.965+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T12:22:30.966+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T12:22:30.967+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T12:22:30.973+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T12:22:31.008+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:31 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T12:22:31.027+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:31 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T12:22:31.161+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:31 INFO ParquetOutputFormat: Parquet block size to 134217728
[2023-01-31T12:22:31.162+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:31 INFO ParquetOutputFormat: Validation is off
[2023-01-31T12:22:31.162+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:31 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-01-31T12:22:31.162+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:31 INFO ParquetOutputFormat: Parquet properties are:
[2023-01-31T12:22:31.162+0000] {spark_submit.py:495} INFO - Parquet page size to 1048576
[2023-01-31T12:22:31.163+0000] {spark_submit.py:495} INFO - Parquet dictionary page size to 1048576
[2023-01-31T12:22:31.163+0000] {spark_submit.py:495} INFO - Dictionary is true
[2023-01-31T12:22:31.163+0000] {spark_submit.py:495} INFO - Writer version is: PARQUET_1_0
[2023-01-31T12:22:31.163+0000] {spark_submit.py:495} INFO - Page size checking is: estimated
[2023-01-31T12:22:31.163+0000] {spark_submit.py:495} INFO - Min row count for page size check is: 100
[2023-01-31T12:22:31.164+0000] {spark_submit.py:495} INFO - Max row count for page size check is: 10000
[2023-01-31T12:22:31.164+0000] {spark_submit.py:495} INFO - Truncate length for column indexes is: 64
[2023-01-31T12:22:31.164+0000] {spark_submit.py:495} INFO - Truncate length for statistics min/max  is: 2147483647
[2023-01-31T12:22:31.164+0000] {spark_submit.py:495} INFO - Bloom filter enabled: false
[2023-01-31T12:22:31.164+0000] {spark_submit.py:495} INFO - Max Bloom filter size for a column is 1048576
[2023-01-31T12:22:31.166+0000] {spark_submit.py:495} INFO - Bloom filter expected number of distinct values are: null
[2023-01-31T12:22:31.166+0000] {spark_submit.py:495} INFO - Page row count limit to 20000
[2023-01-31T12:22:31.166+0000] {spark_submit.py:495} INFO - Writing page checksums is: on
[2023-01-31T12:22:31.290+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:31 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-01-31T12:22:31.291+0000] {spark_submit.py:495} INFO - {
[2023-01-31T12:22:31.292+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-31T12:22:31.292+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-31T12:22:31.292+0000] {spark_submit.py:495} INFO - "name" : "measured_at",
[2023-01-31T12:22:31.292+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T12:22:31.293+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T12:22:31.293+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:31.293+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:31.295+0000] {spark_submit.py:495} INFO - "name" : "created_at",
[2023-01-31T12:22:31.296+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T12:22:31.299+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T12:22:31.299+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:31.302+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:31.303+0000] {spark_submit.py:495} INFO - "name" : "Biomass - Actual Aggregated",
[2023-01-31T12:22:31.303+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:31.303+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:31.303+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:31.304+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:31.304+0000] {spark_submit.py:495} INFO - "name" : "Fossil Brown coal/Lignite - Actual Aggregated",
[2023-01-31T12:22:31.304+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:31.305+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:31.305+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:31.305+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:31.305+0000] {spark_submit.py:495} INFO - "name" : "Fossil Gas - Actual Aggregated",
[2023-01-31T12:22:31.310+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:31.310+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:31.311+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:31.311+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:31.311+0000] {spark_submit.py:495} INFO - "name" : "Fossil Hard coal - Actual Aggregated",
[2023-01-31T12:22:31.311+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:31.312+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:31.313+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:31.313+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:31.313+0000] {spark_submit.py:495} INFO - "name" : "Fossil Oil - Actual Aggregated",
[2023-01-31T12:22:31.313+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:31.314+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:31.314+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:31.314+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:31.314+0000] {spark_submit.py:495} INFO - "name" : "Geothermal - Actual Aggregated",
[2023-01-31T12:22:31.315+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:31.315+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:31.315+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:31.315+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:31.315+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Aggregated",
[2023-01-31T12:22:31.316+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:31.320+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:31.321+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:31.321+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:31.322+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Consumption",
[2023-01-31T12:22:31.322+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:31.330+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:31.331+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:31.332+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:31.332+0000] {spark_submit.py:495} INFO - "name" : "Hydro Run-of-river and poundage - Actual Aggregated",
[2023-01-31T12:22:31.333+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:31.333+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:31.334+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:31.335+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:31.335+0000] {spark_submit.py:495} INFO - "name" : "Hydro Water Reservoir - Actual Aggregated",
[2023-01-31T12:22:31.335+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:31.335+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:31.335+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:31.336+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:31.336+0000] {spark_submit.py:495} INFO - "name" : "Nuclear - Actual Aggregated",
[2023-01-31T12:22:31.336+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:31.336+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:31.336+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:31.336+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:31.337+0000] {spark_submit.py:495} INFO - "name" : "Other - Actual Aggregated",
[2023-01-31T12:22:31.337+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:31.337+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:31.337+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:31.337+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:31.337+0000] {spark_submit.py:495} INFO - "name" : "Other renewable - Actual Aggregated",
[2023-01-31T12:22:31.338+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:31.338+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:31.338+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:31.350+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:31.351+0000] {spark_submit.py:495} INFO - "name" : "Solar - Actual Aggregated",
[2023-01-31T12:22:31.351+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:31.353+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:31.358+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:31.360+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:31.360+0000] {spark_submit.py:495} INFO - "name" : "Waste - Actual Aggregated",
[2023-01-31T12:22:31.361+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:31.361+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:31.362+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:31.363+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:31.364+0000] {spark_submit.py:495} INFO - "name" : "Wind Offshore - Actual Aggregated",
[2023-01-31T12:22:31.364+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:31.365+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:31.367+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:31.367+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:31.368+0000] {spark_submit.py:495} INFO - "name" : "Wind Onshore - Actual Aggregated",
[2023-01-31T12:22:31.374+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:31.378+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:31.378+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:31.379+0000] {spark_submit.py:495} INFO - } ]
[2023-01-31T12:22:31.379+0000] {spark_submit.py:495} INFO - }
[2023-01-31T12:22:31.379+0000] {spark_submit.py:495} INFO - and corresponding Parquet message type:
[2023-01-31T12:22:31.379+0000] {spark_submit.py:495} INFO - message spark_schema {
[2023-01-31T12:22:31.379+0000] {spark_submit.py:495} INFO - required int96 measured_at;
[2023-01-31T12:22:31.380+0000] {spark_submit.py:495} INFO - required int96 created_at;
[2023-01-31T12:22:31.380+0000] {spark_submit.py:495} INFO - optional double Biomass - Actual Aggregated;
[2023-01-31T12:22:31.380+0000] {spark_submit.py:495} INFO - optional double Fossil Brown coal/Lignite - Actual Aggregated;
[2023-01-31T12:22:31.380+0000] {spark_submit.py:495} INFO - optional double Fossil Gas - Actual Aggregated;
[2023-01-31T12:22:31.380+0000] {spark_submit.py:495} INFO - optional double Fossil Hard coal - Actual Aggregated;
[2023-01-31T12:22:31.380+0000] {spark_submit.py:495} INFO - optional double Fossil Oil - Actual Aggregated;
[2023-01-31T12:22:31.380+0000] {spark_submit.py:495} INFO - optional double Geothermal - Actual Aggregated;
[2023-01-31T12:22:31.381+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Aggregated;
[2023-01-31T12:22:31.381+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Consumption;
[2023-01-31T12:22:31.381+0000] {spark_submit.py:495} INFO - optional double Hydro Run-of-river and poundage - Actual Aggregated;
[2023-01-31T12:22:31.381+0000] {spark_submit.py:495} INFO - optional double Hydro Water Reservoir - Actual Aggregated;
[2023-01-31T12:22:31.381+0000] {spark_submit.py:495} INFO - optional double Nuclear - Actual Aggregated;
[2023-01-31T12:22:31.381+0000] {spark_submit.py:495} INFO - optional double Other - Actual Aggregated;
[2023-01-31T12:22:31.382+0000] {spark_submit.py:495} INFO - optional double Other renewable - Actual Aggregated;
[2023-01-31T12:22:31.382+0000] {spark_submit.py:495} INFO - optional double Solar - Actual Aggregated;
[2023-01-31T12:22:31.382+0000] {spark_submit.py:495} INFO - optional double Waste - Actual Aggregated;
[2023-01-31T12:22:31.388+0000] {spark_submit.py:495} INFO - optional double Wind Offshore - Actual Aggregated;
[2023-01-31T12:22:31.388+0000] {spark_submit.py:495} INFO - optional double Wind Onshore - Actual Aggregated;
[2023-01-31T12:22:31.388+0000] {spark_submit.py:495} INFO - }
[2023-01-31T12:22:31.388+0000] {spark_submit.py:495} INFO - 
[2023-01-31T12:22:31.388+0000] {spark_submit.py:495} INFO - 
[2023-01-31T12:22:32.000+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:32 INFO CodecPool: Got brand-new compressor [.snappy]
[2023-01-31T12:22:35.138+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:35 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:35.139+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:35 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None) re-registering with master
[2023-01-31T12:22:35.139+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:35 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:35.139+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:35 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 46749, None)
[2023-01-31T12:22:35.139+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:35 INFO BlockManager: Reporting 4 blocks to the master.
[2023-01-31T12:22:35.142+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:35 INFO BlockManagerInfo: Updated broadcast_10_piece0 in memory on 8124f810dec3:46749 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:35.145+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:35 INFO BlockManagerInfo: Updated broadcast_16_piece0 in memory on 8124f810dec3:46749 (current size: 74.3 KiB, original size: 74.3 KiB, free: 434.3 MiB)
[2023-01-31T12:22:35.650+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:35 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675166975348-ae792d27-4da5-4d25-93f3-c6178b6547d2/_temporary/0/_temporary/' directory.
[2023-01-31T12:22:35.651+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:35 INFO FileOutputCommitter: Saved output of task 'attempt_202301311222307786220873132432672_0008_m_000000_8' to gs://entsoe_temp_1009/.spark-bigquery-local-1675166975348-ae792d27-4da5-4d25-93f3-c6178b6547d2/_temporary/0/task_202301311222307786220873132432672_0008_m_000000
[2023-01-31T12:22:35.652+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:35 INFO SparkHadoopMapRedUtil: attempt_202301311222307786220873132432672_0008_m_000000_8: Committed. Elapsed time: 658 ms.
[2023-01-31T12:22:35.676+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:35 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2588 bytes result sent to driver
[2023-01-31T12:22:35.679+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:35 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 4819 ms on 8124f810dec3 (executor driver) (1/1)
[2023-01-31T12:22:35.682+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:35 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2023-01-31T12:22:35.683+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:35 INFO DAGScheduler: ResultStage 8 (save at BigQueryWriteHelper.java:105) finished in 4.941 s
[2023-01-31T12:22:35.683+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:35 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T12:22:35.683+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2023-01-31T12:22:35.683+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:35 INFO DAGScheduler: Job 8 finished: save at BigQueryWriteHelper.java:105, took 4.960767 s
[2023-01-31T12:22:35.685+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:35 INFO FileFormatWriter: Start to commit write Job 2d51e8d6-f63d-4905-9170-1a1a19a3f80e.
[2023-01-31T12:22:36.342+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:36 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675166975348-ae792d27-4da5-4d25-93f3-c6178b6547d2/_temporary/0/task_202301311222307786220873132432672_0008_m_000000/' directory.
[2023-01-31T12:22:36.846+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:36 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675166975348-ae792d27-4da5-4d25-93f3-c6178b6547d2/' directory.
[2023-01-31T12:22:36.998+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:36 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 8124f810dec3:46749 in memory (size: 74.3 KiB, free: 434.4 MiB)
[2023-01-31T12:22:37.403+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:37 INFO FileFormatWriter: Write Job 2d51e8d6-f63d-4905-9170-1a1a19a3f80e committed. Elapsed time: 1717 ms.
[2023-01-31T12:22:37.409+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:37 INFO FileFormatWriter: Finished processing stats for write job 2d51e8d6-f63d-4905-9170-1a1a19a3f80e.
[2023-01-31T12:22:39.414+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:39 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=entsoe_analytics_1009, projectId=rafzul-analytics-1009, tableId=TEST_total_generation_staging}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=null, ignoreUnknownValue=null, sourceUris=[gs://entsoe_temp_1009/.spark-bigquery-local-1675166975348-ae792d27-4da5-4d25-93f3-c6178b6547d2/part-00000-2b011216-9fe6-40da-8bc5-980521bade11-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=true, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=rafzul-analytics-1009, job=e3874d47-cbee-4281-b0ca-838c72e3a5b1, location=US}
[2023-01-31T12:22:41.958+0000] {local_task_job.py:223} WARNING - State of this instance has been externally set to up_for_retry. Terminating instance.
[2023-01-31T12:22:41.960+0000] {process_utils.py:129} INFO - Sending Signals.SIGTERM to group 601. PIDs of all processes in the group: [642, 926, 601]
[2023-01-31T12:22:41.960+0000] {process_utils.py:84} INFO - Sending the signal Signals.SIGTERM to group 601
[2023-01-31T12:22:41.960+0000] {taskinstance.py:1483} ERROR - Received SIGTERM. Terminating subprocesses.
[2023-01-31T12:22:41.960+0000] {spark_submit.py:620} INFO - Sending kill signal to spark-submit
[2023-01-31T12:22:41.967+0000] {process_utils.py:79} INFO - Process psutil.Process(pid=926, status='terminated', started='12:09:15') (926) terminated with exit code None
[2023-01-31T12:22:41.973+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 414, in submit
    self._process_spark_submit_log(iter(self._submit_sp.stdout))  # type: ignore
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 463, in _process_spark_submit_log
    for line in itr:
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 1485, in signal_handler
    raise AirflowException("Task received SIGTERM signal")
airflow.exceptions.AirflowException: Task received SIGTERM signal
[2023-01-31T12:22:41.979+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=entsoe-energydata-backfill, task_id=stage_total_generation, execution_date=20210101T010000, start_date=20230131T120851, end_date=20230131T122241
[2023-01-31T12:22:41.995+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 999 for task stage_total_generation (Task received SIGTERM signal; 601)
[2023-01-31T12:22:42.020+0000] {process_utils.py:79} INFO - Process psutil.Process(pid=601, status='terminated', exitcode=1, started='12:08:51') (601) terminated with exit code 1
[2023-01-31T12:22:42.047+0000] {process_utils.py:79} INFO - Process psutil.Process(pid=642, status='terminated', started='12:08:52') (642) terminated with exit code None
[2023-01-31T13:03:43.610+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-31T13:03:43.663+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-31T13:03:43.667+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T13:03:43.667+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T13:03:43.668+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T13:03:43.759+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 01:00:00+00:00
[2023-01-31T13:03:43.795+0000] {standard_task_runner.py:55} INFO - Started process 311 to run task
[2023-01-31T13:03:43.815+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '1030', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpwmjx2lb3']
[2023-01-31T13:03:43.840+0000] {standard_task_runner.py:83} INFO - Job 1030: Subtask stage_total_generation
[2023-01-31T13:03:43.996+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host b37fe3cbf330
[2023-01-31T13:03:44.167+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-31T13:03:44.195+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T13:03:44.203+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET 202101010000 202101010600
[2023-01-31T13:04:16.667+0000] {spark_submit.py:495} INFO - mulai periode: 202101010000
[2023-01-31T13:04:16.668+0000] {spark_submit.py:495} INFO - selesai periode: 202101010600
[2023-01-31T13:04:17.553+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:17 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T13:04:19.443+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T13:04:21.667+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:21 INFO ResourceUtils: ==============================================================
[2023-01-31T13:04:21.672+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:21 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T13:04:21.693+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:21 INFO ResourceUtils: ==============================================================
[2023-01-31T13:04:21.702+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:21 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T13:04:22.197+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:22 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T13:04:22.311+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:22 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T13:04:22.335+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:22 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T13:04:23.221+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:23 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T13:04:23.243+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:23 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T13:04:23.259+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:23 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T13:04:23.265+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:23 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T13:04:23.297+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T13:04:29.006+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:29 INFO Utils: Successfully started service 'sparkDriver' on port 35767.
[2023-01-31T13:04:30.900+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:30 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T13:04:31.868+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:31 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T13:04:32.382+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:32 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T13:04:32.387+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:32 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T13:04:32.421+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:32 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T13:04:32.917+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:32 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-9ebfb428-41b9-4257-a7ab-446108c80ae9
[2023-01-31T13:04:33.271+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:33 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T13:04:33.711+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:33 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T13:04:37.943+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:37 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T13:04:37.956+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:37 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2023-01-31T13:04:37.974+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:37 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[2023-01-31T13:04:37.977+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:37 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[2023-01-31T13:04:38.106+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:38 INFO Utils: Successfully started service 'SparkUI' on port 4044.
[2023-01-31T13:04:38.315+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:38 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://b37fe3cbf330:35767/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675170257425
[2023-01-31T13:04:38.322+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:38 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://b37fe3cbf330:35767/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675170257425
[2023-01-31T13:04:40.056+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:40 INFO Executor: Starting executor ID driver on host b37fe3cbf330
[2023-01-31T13:04:40.124+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:40 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T13:04:40.194+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:40 INFO Executor: Fetching spark://b37fe3cbf330:35767/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675170257425
[2023-01-31T13:04:40.486+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:40 INFO TransportClientFactory: Successfully created connection to b37fe3cbf330/172.19.0.8:35767 after 198 ms (0 ms spent in bootstraps)
[2023-01-31T13:04:40.578+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:40 INFO Utils: Fetching spark://b37fe3cbf330:35767/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-4b81cb81-bfa2-4f54-b0e5-1a205cf9cae8/userFiles-927c3c60-5b52-4fa1-918a-d6a42c998e1c/fetchFileTemp7120318661208533929.tmp
[2023-01-31T13:04:43.903+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:43 INFO Executor: Adding file:/tmp/spark-4b81cb81-bfa2-4f54-b0e5-1a205cf9cae8/userFiles-927c3c60-5b52-4fa1-918a-d6a42c998e1c/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T13:04:43.904+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:43 INFO Executor: Fetching spark://b37fe3cbf330:35767/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675170257425
[2023-01-31T13:04:43.924+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:43 INFO Utils: Fetching spark://b37fe3cbf330:35767/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-4b81cb81-bfa2-4f54-b0e5-1a205cf9cae8/userFiles-927c3c60-5b52-4fa1-918a-d6a42c998e1c/fetchFileTemp7632069501593820279.tmp
[2023-01-31T13:04:46.241+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:46 INFO Executor: Adding file:/tmp/spark-4b81cb81-bfa2-4f54-b0e5-1a205cf9cae8/userFiles-927c3c60-5b52-4fa1-918a-d6a42c998e1c/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T13:04:46.397+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:46 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33981.
[2023-01-31T13:04:46.403+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:46 INFO NettyBlockTransferService: Server created on b37fe3cbf330:33981
[2023-01-31T13:04:46.433+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:46 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T13:04:46.537+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:46 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, b37fe3cbf330, 33981, None)
[2023-01-31T13:04:46.622+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:46 INFO BlockManagerMasterEndpoint: Registering block manager b37fe3cbf330:33981 with 434.4 MiB RAM, BlockManagerId(driver, b37fe3cbf330, 33981, None)
[2023-01-31T13:04:46.759+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:46 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, b37fe3cbf330, 33981, None)
[2023-01-31T13:04:46.852+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:46 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, b37fe3cbf330, 33981, None)
[2023-01-31T13:04:58.555+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:58 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T13:04:58.595+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:58 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T13:05:25.704+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:25 INFO InMemoryFileIndex: It took 402 ms to list leaf files for 1 paths.
[2023-01-31T13:05:26.463+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:26 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T13:05:26.797+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:26 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T13:05:26.805+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:26 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on b37fe3cbf330:33981 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T13:05:26.816+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:26 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T13:05:30.237+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:30 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T13:05:30.487+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:30 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T13:05:30.603+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:30 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T13:05:30.692+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:30 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T13:05:30.698+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:30 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T13:05:30.699+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:30 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T13:05:30.714+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:30 INFO DAGScheduler: Missing parents: List()
[2023-01-31T13:05:30.747+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:30 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T13:05:31.022+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:31 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T13:05:31.043+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:31 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T13:05:31.050+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:31 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on b37fe3cbf330:33981 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T13:05:31.057+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:31 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T13:05:31.138+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T13:05:31.157+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:31 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T13:05:31.862+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:31 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T13:05:31.964+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:31 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T13:05:33.041+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:33 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010100__202101010200.json:0+9369
[2023-01-31T13:05:33.977+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:33 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3437 bytes result sent to driver
[2023-01-31T13:05:34.141+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:34 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2380 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T13:05:34.176+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:34 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T13:05:34.303+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:34 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 3.431 s
[2023-01-31T13:05:34.425+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:34 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T13:05:34.448+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T13:05:34.503+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:34 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 3.898735 s
[2023-01-31T13:05:36.118+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:36 INFO BlockManagerInfo: Removed broadcast_1_piece0 on b37fe3cbf330:33981 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T13:05:36.257+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:36 INFO BlockManagerInfo: Removed broadcast_0_piece0 on b37fe3cbf330:33981 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T13:06:21.749+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:21 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T13:06:21.778+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:21 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T13:06:21.805+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:21 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T13:06:24.535+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO CodeGenerator: Code generated in 1380.490003 ms
[2023-01-31T13:06:24.560+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T13:06:24.635+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T13:06:24.638+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on b37fe3cbf330:33981 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T13:06:24.642+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T13:06:24.722+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T13:06:25.364+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:25 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T13:06:25.377+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:25 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T13:06:25.377+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:25 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T13:06:25.377+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:25 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T13:06:25.378+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:25 INFO DAGScheduler: Missing parents: List()
[2023-01-31T13:06:25.386+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:25 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T13:06:25.423+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:25 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T13:06:25.458+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:25 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T13:06:25.461+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:25 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on b37fe3cbf330:33981 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T13:06:25.467+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:25 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T13:06:25.469+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T13:06:25.475+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:25 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T13:06:25.498+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:25 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T13:06:25.501+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:25 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T13:06:25.744+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:25 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T13:06:26.194+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO CodeGenerator: Code generated in 384.212049 ms
[2023-01-31T13:06:26.604+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1628 bytes result sent to driver
[2023-01-31T13:06:26.623+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1139 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T13:06:26.623+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T13:06:26.627+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 1.220 s
[2023-01-31T13:06:26.635+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T13:06:26.637+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T13:06:26.643+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 1.266507 s
[2023-01-31T13:06:26.989+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T13:06:26.989+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T13:06:26.991+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T13:06:27.258+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO BlockManagerInfo: Removed broadcast_3_piece0 on b37fe3cbf330:33981 in memory (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T13:06:27.309+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO CodeGenerator: Code generated in 216.39227 ms
[2023-01-31T13:06:27.395+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T13:06:27.554+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T13:06:27.564+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on b37fe3cbf330:33981 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T13:06:27.564+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T13:06:27.573+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T13:06:27.818+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T13:06:27.819+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T13:06:27.819+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T13:06:27.820+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T13:06:27.820+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO DAGScheduler: Missing parents: List()
[2023-01-31T13:06:27.841+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T13:06:27.913+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T13:06:27.951+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T13:06:27.955+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on b37fe3cbf330:33981 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T13:06:27.968+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T13:06:27.986+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T13:06:27.987+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T13:06:28.001+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T13:06:28.002+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T13:06:28.085+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T13:06:28.421+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1585 bytes result sent to driver
[2023-01-31T13:06:28.440+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 441 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T13:06:28.447+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T13:06:28.453+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 0.591 s
[2023-01-31T13:06:28.453+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T13:06:28.454+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T13:06:28.454+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 0.636966 s
[2023-01-31T13:06:29.155+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T13:06:29.160+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T13:06:29.161+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T13:06:29.740+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO CodeGenerator: Code generated in 115.987556 ms
[2023-01-31T13:06:29.748+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T13:06:29.793+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T13:06:29.794+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on b37fe3cbf330:33981 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T13:06:29.796+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T13:06:29.799+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T13:06:29.833+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T13:06:29.834+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T13:06:29.835+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T13:06:29.835+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T13:06:29.835+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO DAGScheduler: Missing parents: List()
[2023-01-31T13:06:29.837+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T13:06:29.860+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.7 MiB)
[2023-01-31T13:06:29.894+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.6 MiB)
[2023-01-31T13:06:29.897+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on b37fe3cbf330:33981 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T13:06:29.901+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T13:06:29.906+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T13:06:29.907+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T13:06:29.910+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T13:06:29.911+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T13:06:29.984+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T13:06:30.264+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO BlockManagerInfo: Removed broadcast_5_piece0 on b37fe3cbf330:33981 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T13:06:30.265+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1755 bytes result sent to driver
[2023-01-31T13:06:30.344+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 436 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T13:06:30.348+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 0.501 s
[2023-01-31T13:06:30.349+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T13:06:30.349+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T13:06:30.351+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T13:06:30.356+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 0.521113 s
[2023-01-31T13:06:30.545+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T13:06:30.545+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T13:06:30.546+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T13:06:30.574+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO CodeGenerator: Code generated in 17.476528 ms
[2023-01-31T13:06:30.581+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.5 MiB)
[2023-01-31T13:06:30.599+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T13:06:30.601+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on b37fe3cbf330:33981 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T13:06:30.603+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T13:06:30.605+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T13:06:30.724+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T13:06:30.727+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T13:06:30.728+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T13:06:30.728+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T13:06:30.728+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO DAGScheduler: Missing parents: List()
[2023-01-31T13:06:30.734+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T13:06:30.765+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-31T13:06:30.786+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-01-31T13:06:30.791+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on b37fe3cbf330:33981 (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T13:06:30.794+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T13:06:30.800+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T13:06:30.804+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T13:06:30.811+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T13:06:30.812+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T13:06:30.880+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T13:06:31.104+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:31 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1585 bytes result sent to driver
[2023-01-31T13:06:31.127+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:31 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 317 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T13:06:31.127+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:31 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T13:06:31.128+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:31 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 0.390 s
[2023-01-31T13:06:31.128+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:31 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T13:06:31.128+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T13:06:31.133+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:31 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 0.403276 s
[2023-01-31T13:06:33.259+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:33 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T13:06:33.264+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:33 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T13:06:33.271+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:33 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T13:06:34.409+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO CodeGenerator: Code generated in 764.623536 ms
[2023-01-31T13:06:34.441+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.2 MiB)
[2023-01-31T13:06:34.646+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO BlockManagerInfo: Removed broadcast_7_piece0 on b37fe3cbf330:33981 in memory (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T13:06:34.746+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO BlockManagerInfo: Removed broadcast_9_piece0 on b37fe3cbf330:33981 in memory (size: 7.6 KiB, free: 434.3 MiB)
[2023-01-31T13:06:34.759+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T13:06:34.763+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on b37fe3cbf330:33981 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T13:06:34.764+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T13:06:34.775+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T13:06:34.869+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T13:06:34.872+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T13:06:34.872+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T13:06:34.873+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T13:06:34.873+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO DAGScheduler: Missing parents: List()
[2023-01-31T13:06:34.882+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T13:06:34.895+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.2 MiB)
[2023-01-31T13:06:34.910+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.2 MiB)
[2023-01-31T13:06:34.913+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on b37fe3cbf330:33981 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T13:06:34.915+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T13:06:34.921+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T13:06:34.924+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T13:06:34.931+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T13:06:34.943+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T13:06:35.055+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T13:06:35.230+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2559 bytes result sent to driver
[2023-01-31T13:06:35.233+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 302 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T13:06:35.233+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T13:06:35.234+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 0.356 s
[2023-01-31T13:06:35.235+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T13:06:35.235+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T13:06:35.236+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 0.365357 s
[2023-01-31T13:06:35.482+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T13:06:35.483+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T13:06:35.483+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T13:06:35.850+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO CodeGenerator: Code generated in 260.505154 ms
[2023-01-31T13:06:35.863+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 433.0 MiB)
[2023-01-31T13:06:35.885+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.0 MiB)
[2023-01-31T13:06:35.888+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on b37fe3cbf330:33981 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T13:06:35.890+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T13:06:35.895+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T13:06:35.927+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T13:06:35.931+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T13:06:35.931+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T13:06:35.932+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T13:06:35.932+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO DAGScheduler: Missing parents: List()
[2023-01-31T13:06:35.936+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T13:06:35.940+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 432.9 MiB)
[2023-01-31T13:06:35.948+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.9 MiB)
[2023-01-31T13:06:35.951+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on b37fe3cbf330:33981 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T13:06:35.954+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T13:06:35.955+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T13:06:35.956+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T13:06:35.960+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T13:06:35.966+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T13:06:35.977+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T13:06:36.109+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:36 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-01-31T13:06:36.116+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:36 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 155 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T13:06:36.116+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:36 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T13:06:36.117+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:36 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 0.181 s
[2023-01-31T13:06:36.117+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:36 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T13:06:36.117+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T13:06:36.117+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:36 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 0.186797 s
[2023-01-31T13:06:36.426+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:36 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T13:06:36.431+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:36 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T13:06:36.431+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:36 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T13:06:36.844+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:36 INFO BlockManagerInfo: Removed broadcast_6_piece0 on b37fe3cbf330:33981 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T13:06:36.908+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:36 INFO BlockManagerInfo: Removed broadcast_8_piece0 on b37fe3cbf330:33981 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T13:06:36.926+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:36 INFO CodeGenerator: Code generated in 263.591102 ms
[2023-01-31T13:06:36.946+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:36 INFO BlockManagerInfo: Removed broadcast_13_piece0 on b37fe3cbf330:33981 in memory (size: 12.6 KiB, free: 434.3 MiB)
[2023-01-31T13:06:36.967+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:36 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 433.2 MiB)
[2023-01-31T13:06:36.998+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:36 INFO BlockManagerInfo: Removed broadcast_11_piece0 on b37fe3cbf330:33981 in memory (size: 13.4 KiB, free: 434.3 MiB)
[2023-01-31T13:06:37.073+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T13:06:37.073+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on b37fe3cbf330:33981 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T13:06:37.078+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:204
[2023-01-31T13:06:37.094+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T13:06:37.210+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:204
[2023-01-31T13:06:37.211+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:204) with 1 output partitions
[2023-01-31T13:06:37.212+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:204)
[2023-01-31T13:06:37.212+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T13:06:37.212+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO DAGScheduler: Missing parents: List()
[2023-01-31T13:06:37.216+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:204), which has no missing parents
[2023-01-31T13:06:37.230+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 433.2 MiB)
[2023-01-31T13:06:37.240+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 433.2 MiB)
[2023-01-31T13:06:37.241+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on b37fe3cbf330:33981 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T13:06:37.245+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T13:06:37.247+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:204) (first 15 tasks are for partitions Vector(0))
[2023-01-31T13:06:37.248+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T13:06:37.251+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T13:06:37.252+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T13:06:37.285+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T13:06:37.424+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1729 bytes result sent to driver
[2023-01-31T13:06:37.425+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 175 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T13:06:37.430+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T13:06:37.431+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:204) finished in 0.209 s
[2023-01-31T13:06:37.432+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T13:06:37.433+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T13:06:37.433+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:204, took 0.219317 s
[2023-01-31T13:06:39.183+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:39 INFO BlockManagerInfo: Removed broadcast_15_piece0 on b37fe3cbf330:33981 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T13:06:39.240+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:39 INFO BlockManagerInfo: Removed broadcast_12_piece0 on b37fe3cbf330:33981 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T13:06:41.508+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:41 INFO BlockManagerInfo: Removed broadcast_14_piece0 on b37fe3cbf330:33981 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T13:06:49.552+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:49 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T13:06:49.638+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T13:06:49.638+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T13:06:49.641+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:49 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T13:06:49.647+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T13:06:49.648+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T13:06:49.668+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T13:06:50.302+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-31T13:06:50.303+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO DAGScheduler: Got job 8 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T13:06:50.303+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO DAGScheduler: Final stage: ResultStage 8 (save at BigQueryWriteHelper.java:105)
[2023-01-31T13:06:50.303+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T13:06:50.304+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO DAGScheduler: Missing parents: List()
[2023-01-31T13:06:50.306+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO DAGScheduler: Submitting ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T13:06:50.527+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 209.9 KiB, free 433.5 MiB)
[2023-01-31T13:06:50.616+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 74.3 KiB, free 433.4 MiB)
[2023-01-31T13:06:50.617+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on b37fe3cbf330:33981 (size: 74.3 KiB, free: 434.2 MiB)
[2023-01-31T13:06:50.617+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513
[2023-01-31T13:06:50.617+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T13:06:50.618+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2023-01-31T13:06:50.618+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7484 bytes) taskResourceAssignments Map()
[2023-01-31T13:06:50.638+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2023-01-31T13:06:51.223+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T13:06:51.227+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T13:06:51.235+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T13:06:51.238+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T13:06:51.245+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T13:06:51.252+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T13:06:51.306+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T13:06:51.352+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T13:06:51.449+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO ParquetOutputFormat: Parquet block size to 134217728
[2023-01-31T13:06:51.450+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO ParquetOutputFormat: Validation is off
[2023-01-31T13:06:51.450+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-01-31T13:06:51.451+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO ParquetOutputFormat: Parquet properties are:
[2023-01-31T13:06:51.451+0000] {spark_submit.py:495} INFO - Parquet page size to 1048576
[2023-01-31T13:06:51.451+0000] {spark_submit.py:495} INFO - Parquet dictionary page size to 1048576
[2023-01-31T13:06:51.451+0000] {spark_submit.py:495} INFO - Dictionary is true
[2023-01-31T13:06:51.452+0000] {spark_submit.py:495} INFO - Writer version is: PARQUET_1_0
[2023-01-31T13:06:51.452+0000] {spark_submit.py:495} INFO - Page size checking is: estimated
[2023-01-31T13:06:51.452+0000] {spark_submit.py:495} INFO - Min row count for page size check is: 100
[2023-01-31T13:06:51.452+0000] {spark_submit.py:495} INFO - Max row count for page size check is: 10000
[2023-01-31T13:06:51.453+0000] {spark_submit.py:495} INFO - Truncate length for column indexes is: 64
[2023-01-31T13:06:51.453+0000] {spark_submit.py:495} INFO - Truncate length for statistics min/max  is: 2147483647
[2023-01-31T13:06:51.454+0000] {spark_submit.py:495} INFO - Bloom filter enabled: false
[2023-01-31T13:06:51.458+0000] {spark_submit.py:495} INFO - Max Bloom filter size for a column is 1048576
[2023-01-31T13:06:51.458+0000] {spark_submit.py:495} INFO - Bloom filter expected number of distinct values are: null
[2023-01-31T13:06:51.460+0000] {spark_submit.py:495} INFO - Page row count limit to 20000
[2023-01-31T13:06:51.461+0000] {spark_submit.py:495} INFO - Writing page checksums is: on
[2023-01-31T13:06:51.616+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-01-31T13:06:51.617+0000] {spark_submit.py:495} INFO - {
[2023-01-31T13:06:51.617+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-31T13:06:51.617+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-31T13:06:51.617+0000] {spark_submit.py:495} INFO - "name" : "measured_at",
[2023-01-31T13:06:51.617+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T13:06:51.617+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T13:06:51.618+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.618+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.618+0000] {spark_submit.py:495} INFO - "name" : "created_at",
[2023-01-31T13:06:51.618+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T13:06:51.618+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T13:06:51.619+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.619+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.619+0000] {spark_submit.py:495} INFO - "name" : "Biomass - Actual Aggregated",
[2023-01-31T13:06:51.620+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.620+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.620+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.620+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.620+0000] {spark_submit.py:495} INFO - "name" : "Fossil Brown coal/Lignite - Actual Aggregated",
[2023-01-31T13:06:51.625+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.633+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.634+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.634+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.634+0000] {spark_submit.py:495} INFO - "name" : "Fossil Gas - Actual Aggregated",
[2023-01-31T13:06:51.644+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.644+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.645+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.645+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.645+0000] {spark_submit.py:495} INFO - "name" : "Fossil Hard coal - Actual Aggregated",
[2023-01-31T13:06:51.652+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.652+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.652+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.652+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.652+0000] {spark_submit.py:495} INFO - "name" : "Fossil Oil - Actual Aggregated",
[2023-01-31T13:06:51.658+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.659+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.659+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.659+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.659+0000] {spark_submit.py:495} INFO - "name" : "Geothermal - Actual Aggregated",
[2023-01-31T13:06:51.666+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.667+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.667+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.667+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.671+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Aggregated",
[2023-01-31T13:06:51.676+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.676+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.677+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.678+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.681+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Consumption",
[2023-01-31T13:06:51.682+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.684+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.686+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.688+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.690+0000] {spark_submit.py:495} INFO - "name" : "Hydro Run-of-river and poundage - Actual Aggregated",
[2023-01-31T13:06:51.691+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.692+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.693+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.694+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.695+0000] {spark_submit.py:495} INFO - "name" : "Hydro Water Reservoir - Actual Aggregated",
[2023-01-31T13:06:51.696+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.699+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.701+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.702+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.702+0000] {spark_submit.py:495} INFO - "name" : "Nuclear - Actual Aggregated",
[2023-01-31T13:06:51.705+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.713+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.714+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.714+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.717+0000] {spark_submit.py:495} INFO - "name" : "Other - Actual Aggregated",
[2023-01-31T13:06:51.717+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.717+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.718+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.720+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.721+0000] {spark_submit.py:495} INFO - "name" : "Other renewable - Actual Aggregated",
[2023-01-31T13:06:51.723+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.725+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.726+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.727+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.730+0000] {spark_submit.py:495} INFO - "name" : "Solar - Actual Aggregated",
[2023-01-31T13:06:51.730+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.732+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.734+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.735+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.736+0000] {spark_submit.py:495} INFO - "name" : "Waste - Actual Aggregated",
[2023-01-31T13:06:51.738+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.743+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.745+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.748+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.749+0000] {spark_submit.py:495} INFO - "name" : "Wind Offshore - Actual Aggregated",
[2023-01-31T13:06:51.752+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.752+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.753+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.755+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.759+0000] {spark_submit.py:495} INFO - "name" : "Wind Onshore - Actual Aggregated",
[2023-01-31T13:06:51.759+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.764+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.766+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.768+0000] {spark_submit.py:495} INFO - } ]
[2023-01-31T13:06:51.776+0000] {spark_submit.py:495} INFO - }
[2023-01-31T13:06:51.777+0000] {spark_submit.py:495} INFO - and corresponding Parquet message type:
[2023-01-31T13:06:51.778+0000] {spark_submit.py:495} INFO - message spark_schema {
[2023-01-31T13:06:51.782+0000] {spark_submit.py:495} INFO - required int96 measured_at;
[2023-01-31T13:06:51.784+0000] {spark_submit.py:495} INFO - required int96 created_at;
[2023-01-31T13:06:51.785+0000] {spark_submit.py:495} INFO - optional double Biomass - Actual Aggregated;
[2023-01-31T13:06:51.787+0000] {spark_submit.py:495} INFO - optional double Fossil Brown coal/Lignite - Actual Aggregated;
[2023-01-31T13:06:51.793+0000] {spark_submit.py:495} INFO - optional double Fossil Gas - Actual Aggregated;
[2023-01-31T13:06:51.794+0000] {spark_submit.py:495} INFO - optional double Fossil Hard coal - Actual Aggregated;
[2023-01-31T13:06:51.803+0000] {spark_submit.py:495} INFO - optional double Fossil Oil - Actual Aggregated;
[2023-01-31T13:06:51.807+0000] {spark_submit.py:495} INFO - optional double Geothermal - Actual Aggregated;
[2023-01-31T13:06:51.810+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Aggregated;
[2023-01-31T13:06:51.813+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Consumption;
[2023-01-31T13:06:51.814+0000] {spark_submit.py:495} INFO - optional double Hydro Run-of-river and poundage - Actual Aggregated;
[2023-01-31T13:06:51.819+0000] {spark_submit.py:495} INFO - optional double Hydro Water Reservoir - Actual Aggregated;
[2023-01-31T13:06:51.826+0000] {spark_submit.py:495} INFO - optional double Nuclear - Actual Aggregated;
[2023-01-31T13:06:51.828+0000] {spark_submit.py:495} INFO - optional double Other - Actual Aggregated;
[2023-01-31T13:06:51.831+0000] {spark_submit.py:495} INFO - optional double Other renewable - Actual Aggregated;
[2023-01-31T13:06:51.834+0000] {spark_submit.py:495} INFO - optional double Solar - Actual Aggregated;
[2023-01-31T13:06:51.837+0000] {spark_submit.py:495} INFO - optional double Waste - Actual Aggregated;
[2023-01-31T13:06:51.838+0000] {spark_submit.py:495} INFO - optional double Wind Offshore - Actual Aggregated;
[2023-01-31T13:06:51.841+0000] {spark_submit.py:495} INFO - optional double Wind Onshore - Actual Aggregated;
[2023-01-31T13:06:51.843+0000] {spark_submit.py:495} INFO - }
[2023-01-31T13:06:51.846+0000] {spark_submit.py:495} INFO - 
[2023-01-31T13:06:51.848+0000] {spark_submit.py:495} INFO - 
[2023-01-31T13:06:52.162+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:52 INFO CodecPool: Got brand-new compressor [.snappy]
[2023-01-31T13:06:52.956+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:52 INFO BlockManagerInfo: Removed broadcast_4_piece0 on b37fe3cbf330:33981 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T13:06:53.003+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:53 INFO BlockManagerInfo: Removed broadcast_2_piece0 on b37fe3cbf330:33981 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T13:06:55.721+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:55 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675170279206-1c91aabb-8c71-40fb-a228-edc71a469dc3/_temporary/0/_temporary/' directory.
[2023-01-31T13:06:55.721+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:55 INFO FileOutputCommitter: Saved output of task 'attempt_202301311306503380299284719496975_0008_m_000000_8' to gs://entsoe_temp_1009/.spark-bigquery-local-1675170279206-1c91aabb-8c71-40fb-a228-edc71a469dc3/_temporary/0/task_202301311306503380299284719496975_0008_m_000000
[2023-01-31T13:06:55.724+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:55 INFO SparkHadoopMapRedUtil: attempt_202301311306503380299284719496975_0008_m_000000_8: Committed. Elapsed time: 871 ms.
[2023-01-31T13:06:55.738+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:55 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2588 bytes result sent to driver
[2023-01-31T13:06:55.740+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:55 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 5139 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T13:06:55.741+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:55 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2023-01-31T13:06:55.741+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:55 INFO DAGScheduler: ResultStage 8 (save at BigQueryWriteHelper.java:105) finished in 5.434 s
[2023-01-31T13:06:55.741+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:55 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T13:06:55.742+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2023-01-31T13:06:55.743+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:55 INFO DAGScheduler: Job 8 finished: save at BigQueryWriteHelper.java:105, took 5.440193 s
[2023-01-31T13:06:55.744+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:55 INFO FileFormatWriter: Start to commit write Job 8b287203-7d02-448e-99ab-1413df81ef9a.
[2023-01-31T13:06:57.288+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:57 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675170279206-1c91aabb-8c71-40fb-a228-edc71a469dc3/_temporary/0/task_202301311306503380299284719496975_0008_m_000000/' directory.
[2023-01-31T13:06:58.350+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:58 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675170279206-1c91aabb-8c71-40fb-a228-edc71a469dc3/' directory.
[2023-01-31T13:06:58.651+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:58 INFO BlockManagerInfo: Removed broadcast_16_piece0 on b37fe3cbf330:33981 in memory (size: 74.3 KiB, free: 434.4 MiB)
[2023-01-31T13:06:58.935+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:58 INFO FileFormatWriter: Write Job 8b287203-7d02-448e-99ab-1413df81ef9a committed. Elapsed time: 3189 ms.
[2023-01-31T13:06:58.939+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:58 INFO FileFormatWriter: Finished processing stats for write job 8b287203-7d02-448e-99ab-1413df81ef9a.
[2023-01-31T13:07:01.453+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:01 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=entsoe_analytics_1009, projectId=rafzul-analytics-1009, tableId=TEST_total_generation_staging}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=null, ignoreUnknownValue=null, sourceUris=[gs://entsoe_temp_1009/.spark-bigquery-local-1675170279206-1c91aabb-8c71-40fb-a228-edc71a469dc3/part-00000-859bb4f1-48a0-4675-9166-bbec2ea2ec7f-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=true, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=rafzul-analytics-1009, job=28ae6dfa-be65-48b7-a2ef-980ecb2a8c39, location=US}
[2023-01-31T13:07:07.618+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:07 INFO BigQueryClient: Done loading to rafzul-analytics-1009.entsoe_analytics_1009.TEST_total_generation_staging. jobId: JobId{project=rafzul-analytics-1009, job=28ae6dfa-be65-48b7-a2ef-980ecb2a8c39, location=US}
[2023-01-31T13:07:09.268+0000] {spark_submit.py:495} INFO - MANTEP!!!!
[2023-01-31T13:07:09.673+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:09 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-31T13:07:09.706+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:09 INFO SparkUI: Stopped Spark web UI at http://b37fe3cbf330:4044
[2023-01-31T13:07:09.763+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:09 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-31T13:07:09.799+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:09 INFO MemoryStore: MemoryStore cleared
[2023-01-31T13:07:09.801+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:09 INFO BlockManager: BlockManager stopped
[2023-01-31T13:07:09.809+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:09 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-31T13:07:09.819+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:09 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-31T13:07:09.836+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:09 INFO SparkContext: Successfully stopped SparkContext
[2023-01-31T13:07:09.837+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:09 INFO ShutdownHookManager: Shutdown hook called
[2023-01-31T13:07:09.839+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-4b81cb81-bfa2-4f54-b0e5-1a205cf9cae8/pyspark-d2f8a8eb-4c05-4b41-a9f1-21a134057fc8
[2023-01-31T13:07:09.851+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-10afdd4a-9541-4ddf-9e82-861e09487c31
[2023-01-31T13:07:09.865+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-4b81cb81-bfa2-4f54-b0e5-1a205cf9cae8
[2023-01-31T13:07:10.188+0000] {taskinstance.py:1322} INFO - Marking task as SUCCESS. dag_id=entsoe-energydata-backfill, task_id=stage_total_generation, execution_date=20210101T010000, start_date=20230131T130343, end_date=20230131T130710
[2023-01-31T13:07:10.295+0000] {local_task_job.py:159} INFO - Task exited with return code 0
[2023-01-31T13:07:10.423+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T14:09:42.949+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-31T14:09:43.001+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-31T14:09:43.002+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T14:09:43.002+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T14:09:43.002+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T14:09:43.044+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 01:00:00+00:00
[2023-01-31T14:09:43.083+0000] {standard_task_runner.py:55} INFO - Started process 6103 to run task
[2023-01-31T14:09:43.108+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '1039', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp7g1utlcg']
[2023-01-31T14:09:43.121+0000] {standard_task_runner.py:83} INFO - Job 1039: Subtask stage_total_generation
[2023-01-31T14:09:43.455+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host b37fe3cbf330
[2023-01-31T14:09:43.827+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-31T14:09:43.894+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T14:09:43.898+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 398, in submit
    spark_submit_cmd = self._build_spark_submit_command(application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 344, in _build_spark_submit_command
    self.log.info("Spark-Submit cmd: %s", self._mask_cmd(connection_cmd))
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 256, in _mask_cmd
    " ".join(connection_cmd),
TypeError: sequence item 10: expected str instance, DateTime found
[2023-01-31T14:09:43.928+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=entsoe-energydata-backfill, task_id=stage_total_generation, execution_date=20210101T010000, start_date=20230131T140942, end_date=20230131T140943
[2023-01-31T14:09:43.980+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 1039 for task stage_total_generation (sequence item 10: expected str instance, DateTime found; 6103)
[2023-01-31T14:09:44.086+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-31T14:09:44.137+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T14:10:51.052+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-31T14:10:51.080+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-31T14:10:51.081+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T14:10:51.081+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T14:10:51.082+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T14:10:51.121+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 01:00:00+00:00
[2023-01-31T14:10:51.176+0000] {standard_task_runner.py:55} INFO - Started process 6202 to run task
[2023-01-31T14:10:51.203+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '1054', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpxb0x8kwt']
[2023-01-31T14:10:51.222+0000] {standard_task_runner.py:83} INFO - Job 1054: Subtask stage_total_generation
[2023-01-31T14:10:51.593+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host b37fe3cbf330
[2023-01-31T14:10:51.864+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-31T14:10:51.898+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T14:10:51.900+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 398, in submit
    spark_submit_cmd = self._build_spark_submit_command(application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 344, in _build_spark_submit_command
    self.log.info("Spark-Submit cmd: %s", self._mask_cmd(connection_cmd))
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 256, in _mask_cmd
    " ".join(connection_cmd),
TypeError: sequence item 10: expected str instance, DateTime found
[2023-01-31T14:10:51.929+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=entsoe-energydata-backfill, task_id=stage_total_generation, execution_date=20210101T010000, start_date=20230131T141051, end_date=20230131T141051
[2023-01-31T14:10:51.973+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 1054 for task stage_total_generation (sequence item 10: expected str instance, DateTime found; 6202)
[2023-01-31T14:10:52.033+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-31T14:10:52.092+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
