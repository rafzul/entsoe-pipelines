[2023-01-31T05:11:01.603+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-31T05:11:01.653+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-31T05:11:01.654+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T05:11:01.654+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T05:11:01.655+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T05:11:01.720+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 01:00:00+00:00
[2023-01-31T05:11:01.750+0000] {standard_task_runner.py:55} INFO - Started process 12302 to run task
[2023-01-31T05:11:01.787+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '884', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpasl9ym7u']
[2023-01-31T05:11:01.805+0000] {standard_task_runner.py:83} INFO - Job 884: Subtask stage_total_generation
[2023-01-31T05:11:02.170+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 81d5fcd0285b
[2023-01-31T05:11:02.539+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-31T05:11:02.601+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T05:11:02.617+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-31T05:11:34.116+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:34 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T05:11:34.898+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T05:11:36.244+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:36 INFO ResourceUtils: ==============================================================
[2023-01-31T05:11:36.263+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:36 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T05:11:36.273+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:36 INFO ResourceUtils: ==============================================================
[2023-01-31T05:11:36.278+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:36 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T05:11:36.638+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:36 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T05:11:36.661+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:36 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T05:11:36.679+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:36 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T05:11:37.289+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:37 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T05:11:37.292+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:37 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T05:11:37.294+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:37 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T05:11:37.297+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:37 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T05:11:37.303+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:37 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T05:11:40.902+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:40 INFO Utils: Successfully started service 'sparkDriver' on port 38427.
[2023-01-31T05:11:41.239+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:41 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T05:11:41.717+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:41 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T05:11:42.088+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:42 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T05:11:42.110+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:42 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T05:11:42.215+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:42 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T05:11:42.409+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:42 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d7fcad89-fd04-48ec-8974-6905489c76fa
[2023-01-31T05:11:42.542+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:42 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T05:11:42.736+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:42 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T05:11:45.683+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:45 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T05:11:45.694+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:45 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2023-01-31T05:11:45.776+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:45 INFO Utils: Successfully started service 'SparkUI' on port 4042.
[2023-01-31T05:11:46.328+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:46 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://81d5fcd0285b:38427/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675141894041
[2023-01-31T05:11:46.345+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:46 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://81d5fcd0285b:38427/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675141894041
[2023-01-31T05:11:47.234+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:47 INFO Executor: Starting executor ID driver on host 81d5fcd0285b
[2023-01-31T05:11:47.283+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:47 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T05:11:47.494+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:47 INFO Executor: Fetching spark://81d5fcd0285b:38427/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675141894041
[2023-01-31T05:11:48.014+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:48 INFO TransportClientFactory: Successfully created connection to 81d5fcd0285b/172.19.0.9:38427 after 343 ms (0 ms spent in bootstraps)
[2023-01-31T05:11:48.108+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:48 INFO Utils: Fetching spark://81d5fcd0285b:38427/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-a9cec05e-0ca5-44a0-b848-00ad65ff68da/userFiles-a819ed0f-1466-45e5-a3a5-5cf99fe39274/fetchFileTemp4607501923503944968.tmp
[2023-01-31T05:11:50.197+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:50 INFO Executor: Adding file:/tmp/spark-a9cec05e-0ca5-44a0-b848-00ad65ff68da/userFiles-a819ed0f-1466-45e5-a3a5-5cf99fe39274/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T05:11:50.204+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:50 INFO Executor: Fetching spark://81d5fcd0285b:38427/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675141894041
[2023-01-31T05:11:50.227+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:50 INFO Utils: Fetching spark://81d5fcd0285b:38427/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-a9cec05e-0ca5-44a0-b848-00ad65ff68da/userFiles-a819ed0f-1466-45e5-a3a5-5cf99fe39274/fetchFileTemp3453613327379139609.tmp
[2023-01-31T05:11:52.012+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:52 INFO Executor: Adding file:/tmp/spark-a9cec05e-0ca5-44a0-b848-00ad65ff68da/userFiles-a819ed0f-1466-45e5-a3a5-5cf99fe39274/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T05:11:52.096+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:52 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37421.
[2023-01-31T05:11:52.097+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:52 INFO NettyBlockTransferService: Server created on 81d5fcd0285b:37421
[2023-01-31T05:11:52.119+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:52 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T05:11:52.232+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:52 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 37421, None)
[2023-01-31T05:11:52.289+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:52 INFO BlockManagerMasterEndpoint: Registering block manager 81d5fcd0285b:37421 with 434.4 MiB RAM, BlockManagerId(driver, 81d5fcd0285b, 37421, None)
[2023-01-31T05:11:52.320+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:52 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 37421, None)
[2023-01-31T05:11:52.326+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:52 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 81d5fcd0285b, 37421, None)
[2023-01-31T05:11:59.038+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:59 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T05:11:59.100+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:59 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T05:12:21.645+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:21 INFO InMemoryFileIndex: It took 660 ms to list leaf files for 1 paths.
[2023-01-31T05:12:23.418+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:23 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T05:12:24.275+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:24 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T05:12:24.306+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:24 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 81d5fcd0285b:37421 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T05:12:24.343+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:24 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T05:12:28.897+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:28 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T05:12:28.960+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:28 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T05:12:29.193+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:29 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T05:12:29.531+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:29 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T05:12:29.559+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:29 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T05:12:29.560+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:29 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:12:29.637+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:29 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:12:29.867+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:29 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T05:12:30.560+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:30 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T05:12:30.593+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:30 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T05:12:30.599+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:30 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 81d5fcd0285b:37421 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T05:12:30.605+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:30 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:12:30.732+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:12:30.739+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:30 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T05:12:31.941+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:31 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T05:12:32.076+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:32 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T05:12:33.510+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:33 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010100__202101010200.json:0+9369
[2023-01-31T05:12:35.252+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:35 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3437 bytes result sent to driver
[2023-01-31T05:12:35.359+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:35 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3780 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:12:35.371+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:35 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T05:12:35.462+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:35 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 5.362 s
[2023-01-31T05:12:35.555+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:35 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:12:35.556+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T05:12:35.581+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:35 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 6.385868 s
[2023-01-31T05:12:37.725+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:37 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 81d5fcd0285b:37421 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T05:12:37.804+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:37 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 81d5fcd0285b:37421 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T05:13:09.769+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:09 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:09.775+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:09 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:13:09.785+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:09 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:11.690+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:11 INFO CodeGenerator: Code generated in 989.825465 ms
[2023-01-31T05:13:11.745+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:11 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T05:13:11.823+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:11 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T05:13:11.826+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:11 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 81d5fcd0285b:37421 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T05:13:11.834+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:11 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T05:13:11.877+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:11 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:12.198+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:12 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T05:13:12.202+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:12 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T05:13:12.202+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:12 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T05:13:12.202+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:12 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:12.202+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:12 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:12.205+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:12 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T05:13:12.240+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:12 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T05:13:12.245+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:12 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T05:13:12.247+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:12 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 81d5fcd0285b:37421 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T05:13:12.250+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:12 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:12.255+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:12.263+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:12 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T05:13:12.275+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:12 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:12.280+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:12 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T05:13:12.871+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:12 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T05:13:13.734+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:13 INFO CodeGenerator: Code generated in 611.448414 ms
[2023-01-31T05:13:14.699+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1628 bytes result sent to driver
[2023-01-31T05:13:14.710+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2443 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:14.711+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T05:13:14.717+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 2.489 s
[2023-01-31T05:13:14.717+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:14.718+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T05:13:14.729+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 2.524018 s
[2023-01-31T05:13:15.152+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:15.158+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:13:15.175+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:15.387+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO CodeGenerator: Code generated in 106.065205 ms
[2023-01-31T05:13:15.439+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T05:13:15.636+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T05:13:15.638+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 81d5fcd0285b:37421 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:13:15.639+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T05:13:15.651+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:15.824+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T05:13:15.826+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T05:13:15.826+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T05:13:15.827+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:15.827+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:15.830+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T05:13:15.860+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T05:13:15.912+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T05:13:15.921+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 81d5fcd0285b:37421 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T05:13:15.925+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:15.935+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:15.939+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T05:13:15.953+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:15.955+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T05:13:16.057+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:16 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T05:13:16.493+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:16 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1585 bytes result sent to driver
[2023-01-31T05:13:16.521+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:16 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 569 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:16.522+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:16 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T05:13:16.529+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:16 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 0.686 s
[2023-01-31T05:13:16.532+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:16 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:16.532+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T05:13:16.536+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:16 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 0.711563 s
[2023-01-31T05:13:17.206+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:17.218+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T05:13:17.220+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:18.184+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO CodeGenerator: Code generated in 823.230402 ms
[2023-01-31T05:13:18.222+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T05:13:18.385+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T05:13:18.385+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 81d5fcd0285b:37421 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:13:18.398+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T05:13:18.407+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:18.507+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T05:13:18.513+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T05:13:18.516+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T05:13:18.517+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:18.519+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:18.532+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T05:13:18.577+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.6 MiB)
[2023-01-31T05:13:18.660+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.6 MiB)
[2023-01-31T05:13:18.671+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 81d5fcd0285b:37421 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T05:13:18.685+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:18.700+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:18.709+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T05:13:18.722+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:18.723+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T05:13:19.137+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T05:13:19.630+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1755 bytes result sent to driver
[2023-01-31T05:13:19.677+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 958 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:19.689+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 1.139 s
[2023-01-31T05:13:19.689+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:19.692+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T05:13:19.696+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T05:13:19.740+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 1.231705 s
[2023-01-31T05:13:20.124+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:20.126+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:13:20.132+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:20.353+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO CodeGenerator: Code generated in 130.020752 ms
[2023-01-31T05:13:20.407+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.4 MiB)
[2023-01-31T05:13:20.559+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T05:13:20.561+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 81d5fcd0285b:37421 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:13:20.566+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T05:13:20.598+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:20.894+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T05:13:20.898+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T05:13:20.898+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T05:13:20.898+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:20.899+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:20.906+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T05:13:20.947+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-31T05:13:21.009+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-01-31T05:13:21.016+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 81d5fcd0285b:37421 (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T05:13:21.017+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:21.028+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:21.030+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T05:13:21.036+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:21.037+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T05:13:21.106+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T05:13:21.376+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1586 bytes result sent to driver
[2023-01-31T05:13:21.383+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 347 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:21.384+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T05:13:21.386+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 0.475 s
[2023-01-31T05:13:21.387+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:21.387+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T05:13:21.387+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 0.490803 s
[2023-01-31T05:13:23.278+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:23.280+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T05:13:23.310+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:24.750+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 81d5fcd0285b:37421 in memory (size: 7.7 KiB, free: 434.2 MiB)
[2023-01-31T05:13:24.973+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 81d5fcd0285b:37421 in memory (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T05:13:25.128+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 81d5fcd0285b:37421 in memory (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T05:13:25.205+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 81d5fcd0285b:37421 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T05:13:25.310+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO CodeGenerator: Code generated in 1607.792941 ms
[2023-01-31T05:13:25.433+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.3 MiB)
[2023-01-31T05:13:25.583+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T05:13:25.587+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 81d5fcd0285b:37421 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:13:25.593+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T05:13:25.609+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:25.863+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T05:13:25.870+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) with 1 output partitions
[2023-01-31T05:13:25.871+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201)
[2023-01-31T05:13:25.872+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:25.872+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:25.873+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201), which has no missing parents
[2023-01-31T05:13:25.918+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.2 MiB)
[2023-01-31T05:13:25.936+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.2 MiB)
[2023-01-31T05:13:25.942+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 81d5fcd0285b:37421 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T05:13:25.956+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:25.965+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:25.966+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T05:13:25.981+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:25.982+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T05:13:26.077+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T05:13:26.311+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2602 bytes result sent to driver
[2023-01-31T05:13:26.315+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 335 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:26.316+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T05:13:26.323+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) finished in 0.442 s
[2023-01-31T05:13:26.324+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:26.324+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T05:13:26.325+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201, took 0.455162 s
[2023-01-31T05:13:26.526+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:26.528+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T05:13:26.534+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:27.214+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:27 INFO CodeGenerator: Code generated in 364.276636 ms
[2023-01-31T05:13:27.413+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:27 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 433.0 MiB)
[2023-01-31T05:13:27.706+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:27 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.0 MiB)
[2023-01-31T05:13:27.715+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:27 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 81d5fcd0285b:37421 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:13:27.720+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:27 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T05:13:27.742+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:27 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:28.080+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T05:13:28.081+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T05:13:28.082+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T05:13:28.082+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:28.085+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:28.095+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T05:13:28.148+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 432.9 MiB)
[2023-01-31T05:13:28.173+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.9 MiB)
[2023-01-31T05:13:28.179+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 81d5fcd0285b:37421 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T05:13:28.190+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:28.197+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:28.201+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T05:13:28.215+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:28.216+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T05:13:28.287+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T05:13:28.615+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1827 bytes result sent to driver
[2023-01-31T05:13:28.638+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 417 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:28.642+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T05:13:28.645+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 0.530 s
[2023-01-31T05:13:28.648+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:28.652+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T05:13:28.684+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 0.563693 s
[2023-01-31T05:13:29.037+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:29.049+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T05:13:29.052+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:29.962+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO CodeGenerator: Code generated in 547.891432 ms
[2023-01-31T05:13:30.014+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.7 MiB)
[2023-01-31T05:13:30.126+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.7 MiB)
[2023-01-31T05:13:30.130+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 81d5fcd0285b:37421 (size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T05:13:30.135+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T05:13:30.141+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:30.315+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T05:13:30.320+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T05:13:30.320+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T05:13:30.321+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:30.321+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:30.326+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T05:13:30.345+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.7 MiB)
[2023-01-31T05:13:30.364+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.6 MiB)
[2023-01-31T05:13:30.369+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 81d5fcd0285b:37421 (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T05:13:30.374+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:30.378+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:30.382+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T05:13:30.388+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:30.396+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T05:13:30.458+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T05:13:30.723+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1729 bytes result sent to driver
[2023-01-31T05:13:30.727+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 340 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:30.728+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T05:13:30.730+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 0.404 s
[2023-01-31T05:13:30.730+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:30.732+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T05:13:30.732+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 0.416450 s
[2023-01-31T05:13:41.080+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:41 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 81d5fcd0285b:37421 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:13:41.156+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:41 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 81d5fcd0285b:37421 in memory (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T05:13:41.230+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:41 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 81d5fcd0285b:37421 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T05:13:41.261+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:41 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 81d5fcd0285b:37421 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:13:41.298+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:41 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 81d5fcd0285b:37421 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:13:41.427+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:41 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 81d5fcd0285b:37421 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:13:41.502+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:41 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 81d5fcd0285b:37421 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:13:41.582+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:41 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 81d5fcd0285b:37421 in memory (size: 12.6 KiB, free: 434.3 MiB)
[2023-01-31T05:13:41.679+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:41 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 81d5fcd0285b:37421 in memory (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T05:13:59.695+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:59 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:13:59.778+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:13:59.779+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:13:59.788+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:59 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:13:59.790+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:13:59.792+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:13:59.806+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:14:00.751+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:00 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-31T05:14:00.754+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:00 INFO DAGScheduler: Got job 8 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T05:14:00.754+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:00 INFO DAGScheduler: Final stage: ResultStage 8 (save at BigQueryWriteHelper.java:105)
[2023-01-31T05:14:00.755+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:00 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:14:00.755+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:00 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:14:00.760+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:00 INFO DAGScheduler: Submitting ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T05:14:00.918+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:00 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 209.9 KiB, free 434.0 MiB)
[2023-01-31T05:14:01.028+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:00 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 74.3 KiB, free 433.9 MiB)
[2023-01-31T05:14:01.033+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 81d5fcd0285b:37421 (size: 74.3 KiB, free: 434.3 MiB)
[2023-01-31T05:14:01.033+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:14:01.034+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:14:01.034+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2023-01-31T05:14:01.035+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7484 bytes) taskResourceAssignments Map()
[2023-01-31T05:14:01.048+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2023-01-31T05:14:01.233+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:14:01.236+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:14:01.256+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:14:01.260+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:14:01.261+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:14:01.271+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:14:01.311+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T05:14:01.328+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T05:14:01.576+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO ParquetOutputFormat: Parquet block size to 134217728
[2023-01-31T05:14:01.581+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO ParquetOutputFormat: Validation is off
[2023-01-31T05:14:01.583+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-01-31T05:14:01.584+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO ParquetOutputFormat: Parquet properties are:
[2023-01-31T05:14:01.585+0000] {spark_submit.py:495} INFO - Parquet page size to 1048576
[2023-01-31T05:14:01.585+0000] {spark_submit.py:495} INFO - Parquet dictionary page size to 1048576
[2023-01-31T05:14:01.586+0000] {spark_submit.py:495} INFO - Dictionary is true
[2023-01-31T05:14:01.586+0000] {spark_submit.py:495} INFO - Writer version is: PARQUET_1_0
[2023-01-31T05:14:01.587+0000] {spark_submit.py:495} INFO - Page size checking is: estimated
[2023-01-31T05:14:01.587+0000] {spark_submit.py:495} INFO - Min row count for page size check is: 100
[2023-01-31T05:14:01.588+0000] {spark_submit.py:495} INFO - Max row count for page size check is: 10000
[2023-01-31T05:14:01.588+0000] {spark_submit.py:495} INFO - Truncate length for column indexes is: 64
[2023-01-31T05:14:01.597+0000] {spark_submit.py:495} INFO - Truncate length for statistics min/max  is: 2147483647
[2023-01-31T05:14:01.603+0000] {spark_submit.py:495} INFO - Bloom filter enabled: false
[2023-01-31T05:14:01.606+0000] {spark_submit.py:495} INFO - Max Bloom filter size for a column is 1048576
[2023-01-31T05:14:01.612+0000] {spark_submit.py:495} INFO - Bloom filter expected number of distinct values are: null
[2023-01-31T05:14:01.616+0000] {spark_submit.py:495} INFO - Page row count limit to 20000
[2023-01-31T05:14:01.617+0000] {spark_submit.py:495} INFO - Writing page checksums is: on
[2023-01-31T05:14:02.252+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:02 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-01-31T05:14:02.258+0000] {spark_submit.py:495} INFO - {
[2023-01-31T05:14:02.259+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-31T05:14:02.261+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-31T05:14:02.264+0000] {spark_submit.py:495} INFO - "name" : "measured_at",
[2023-01-31T05:14:02.267+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T05:14:02.284+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T05:14:02.285+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:02.298+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:02.301+0000] {spark_submit.py:495} INFO - "name" : "created_at",
[2023-01-31T05:14:02.319+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T05:14:02.333+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T05:14:02.342+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:02.343+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:02.360+0000] {spark_submit.py:495} INFO - "name" : "Biomass - Actual Aggregated",
[2023-01-31T05:14:02.366+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:02.370+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:02.373+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:02.374+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:02.379+0000] {spark_submit.py:495} INFO - "name" : "Fossil Brown coal/Lignite - Actual Aggregated",
[2023-01-31T05:14:02.381+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:02.384+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:02.386+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:02.387+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:02.387+0000] {spark_submit.py:495} INFO - "name" : "Fossil Gas - Actual Aggregated",
[2023-01-31T05:14:02.388+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:02.388+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:02.389+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:02.390+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:02.390+0000] {spark_submit.py:495} INFO - "name" : "Fossil Hard coal - Actual Aggregated",
[2023-01-31T05:14:02.391+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:02.391+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:02.391+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:02.392+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:02.392+0000] {spark_submit.py:495} INFO - "name" : "Fossil Oil - Actual Aggregated",
[2023-01-31T05:14:02.393+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:02.393+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:02.394+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:02.394+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:02.395+0000] {spark_submit.py:495} INFO - "name" : "Geothermal - Actual Aggregated",
[2023-01-31T05:14:02.395+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:02.409+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:02.410+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:02.410+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:02.415+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Aggregated",
[2023-01-31T05:14:02.416+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:02.425+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:02.425+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:02.426+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:02.427+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Consumption",
[2023-01-31T05:14:02.427+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:02.428+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:02.428+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:02.429+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:02.429+0000] {spark_submit.py:495} INFO - "name" : "Hydro Run-of-river and poundage - Actual Aggregated",
[2023-01-31T05:14:02.429+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:02.430+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:02.431+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:02.431+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:02.432+0000] {spark_submit.py:495} INFO - "name" : "Hydro Water Reservoir - Actual Aggregated",
[2023-01-31T05:14:02.432+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:02.433+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:02.433+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:02.433+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:02.434+0000] {spark_submit.py:495} INFO - "name" : "Nuclear - Actual Aggregated",
[2023-01-31T05:14:02.434+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:02.435+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:02.435+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:02.435+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:02.436+0000] {spark_submit.py:495} INFO - "name" : "Other - Actual Aggregated",
[2023-01-31T05:14:02.436+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:02.437+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:02.442+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:02.446+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:02.447+0000] {spark_submit.py:495} INFO - "name" : "Other renewable - Actual Aggregated",
[2023-01-31T05:14:02.448+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:02.449+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:02.449+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:02.450+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:02.450+0000] {spark_submit.py:495} INFO - "name" : "Solar - Actual Aggregated",
[2023-01-31T05:14:02.451+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:02.451+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:02.452+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:02.453+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:02.453+0000] {spark_submit.py:495} INFO - "name" : "Waste - Actual Aggregated",
[2023-01-31T05:14:02.454+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:02.454+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:02.455+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:02.455+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:02.456+0000] {spark_submit.py:495} INFO - "name" : "Wind Offshore - Actual Aggregated",
[2023-01-31T05:14:02.456+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:02.457+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:02.457+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:02.458+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:02.462+0000] {spark_submit.py:495} INFO - "name" : "Wind Onshore - Actual Aggregated",
[2023-01-31T05:14:02.467+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:02.467+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:02.469+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:02.469+0000] {spark_submit.py:495} INFO - } ]
[2023-01-31T05:14:02.470+0000] {spark_submit.py:495} INFO - }
[2023-01-31T05:14:02.471+0000] {spark_submit.py:495} INFO - and corresponding Parquet message type:
[2023-01-31T05:14:02.471+0000] {spark_submit.py:495} INFO - message spark_schema {
[2023-01-31T05:14:02.472+0000] {spark_submit.py:495} INFO - required int96 measured_at;
[2023-01-31T05:14:02.472+0000] {spark_submit.py:495} INFO - required int96 created_at;
[2023-01-31T05:14:02.473+0000] {spark_submit.py:495} INFO - optional double Biomass - Actual Aggregated;
[2023-01-31T05:14:02.474+0000] {spark_submit.py:495} INFO - optional double Fossil Brown coal/Lignite - Actual Aggregated;
[2023-01-31T05:14:02.477+0000] {spark_submit.py:495} INFO - optional double Fossil Gas - Actual Aggregated;
[2023-01-31T05:14:02.477+0000] {spark_submit.py:495} INFO - optional double Fossil Hard coal - Actual Aggregated;
[2023-01-31T05:14:02.479+0000] {spark_submit.py:495} INFO - optional double Fossil Oil - Actual Aggregated;
[2023-01-31T05:14:02.479+0000] {spark_submit.py:495} INFO - optional double Geothermal - Actual Aggregated;
[2023-01-31T05:14:02.481+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Aggregated;
[2023-01-31T05:14:02.486+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Consumption;
[2023-01-31T05:14:02.486+0000] {spark_submit.py:495} INFO - optional double Hydro Run-of-river and poundage - Actual Aggregated;
[2023-01-31T05:14:02.488+0000] {spark_submit.py:495} INFO - optional double Hydro Water Reservoir - Actual Aggregated;
[2023-01-31T05:14:02.489+0000] {spark_submit.py:495} INFO - optional double Nuclear - Actual Aggregated;
[2023-01-31T05:14:02.491+0000] {spark_submit.py:495} INFO - optional double Other - Actual Aggregated;
[2023-01-31T05:14:02.492+0000] {spark_submit.py:495} INFO - optional double Other renewable - Actual Aggregated;
[2023-01-31T05:14:02.492+0000] {spark_submit.py:495} INFO - optional double Solar - Actual Aggregated;
[2023-01-31T05:14:02.493+0000] {spark_submit.py:495} INFO - optional double Waste - Actual Aggregated;
[2023-01-31T05:14:02.497+0000] {spark_submit.py:495} INFO - optional double Wind Offshore - Actual Aggregated;
[2023-01-31T05:14:02.497+0000] {spark_submit.py:495} INFO - optional double Wind Onshore - Actual Aggregated;
[2023-01-31T05:14:02.498+0000] {spark_submit.py:495} INFO - }
[2023-01-31T05:14:02.498+0000] {spark_submit.py:495} INFO - 
[2023-01-31T05:14:02.499+0000] {spark_submit.py:495} INFO - 
[2023-01-31T05:14:03.464+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:03 INFO CodecPool: Got brand-new compressor [.snappy]
[2023-01-31T05:14:10.054+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:10 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675141906725-6a38f81b-3c04-4138-aebb-a4ce57b04d53/_temporary/0/_temporary/' directory.
[2023-01-31T05:14:10.060+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:10 INFO FileOutputCommitter: Saved output of task 'attempt_20230131051400728185770031411967_0008_m_000000_8' to gs://entsoe_temp_1009/.spark-bigquery-local-1675141906725-6a38f81b-3c04-4138-aebb-a4ce57b04d53/_temporary/0/task_20230131051400728185770031411967_0008_m_000000
[2023-01-31T05:14:10.064+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:10 INFO SparkHadoopMapRedUtil: attempt_20230131051400728185770031411967_0008_m_000000_8: Committed. Elapsed time: 2444 ms.
[2023-01-31T05:14:10.217+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:10 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2588 bytes result sent to driver
[2023-01-31T05:14:10.224+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:10 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 9196 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:14:10.225+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:10 INFO DAGScheduler: ResultStage 8 (save at BigQueryWriteHelper.java:105) finished in 9.458 s
[2023-01-31T05:14:10.226+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:10 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:14:10.226+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:10 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2023-01-31T05:14:10.232+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2023-01-31T05:14:10.235+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:10 INFO DAGScheduler: Job 8 finished: save at BigQueryWriteHelper.java:105, took 9.476811 s
[2023-01-31T05:14:10.239+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:10 INFO FileFormatWriter: Start to commit write Job d1518355-9fed-4afe-9192-149892bba72f.
[2023-01-31T05:14:11.173+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:11 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675141906725-6a38f81b-3c04-4138-aebb-a4ce57b04d53/_temporary/0/task_20230131051400728185770031411967_0008_m_000000/' directory.
[2023-01-31T05:14:11.874+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:11 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675141906725-6a38f81b-3c04-4138-aebb-a4ce57b04d53/' directory.
[2023-01-31T05:14:12.294+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:12 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 81d5fcd0285b:37421 in memory (size: 74.3 KiB, free: 434.4 MiB)
[2023-01-31T05:14:13.027+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:13 INFO FileFormatWriter: Write Job d1518355-9fed-4afe-9192-149892bba72f committed. Elapsed time: 2777 ms.
[2023-01-31T05:14:13.097+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:13 INFO FileFormatWriter: Finished processing stats for write job d1518355-9fed-4afe-9192-149892bba72f.
[2023-01-31T05:14:15.591+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:15 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=entsoe_analytics_1009, projectId=rafzul-analytics-1009, tableId=TEST_total_generation_staging}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=null, ignoreUnknownValue=null, sourceUris=[gs://entsoe_temp_1009/.spark-bigquery-local-1675141906725-6a38f81b-3c04-4138-aebb-a4ce57b04d53/part-00000-2b231d04-3e0e-4030-a9aa-6708d1cd0940-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=true, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=rafzul-analytics-1009, job=d4fc32f3-e935-4736-84b4-336f360371c9, location=US}
[2023-01-31T05:14:20.764+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:20 INFO BigQueryClient: Done loading to rafzul-analytics-1009.entsoe_analytics_1009.TEST_total_generation_staging. jobId: JobId{project=rafzul-analytics-1009, job=d4fc32f3-e935-4736-84b4-336f360371c9, location=US}
[2023-01-31T05:14:21.194+0000] {spark_submit.py:495} INFO - MANTEP!!!!
[2023-01-31T05:14:21.342+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:21 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-31T05:14:21.357+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:21 INFO SparkUI: Stopped Spark web UI at http://81d5fcd0285b:4042
[2023-01-31T05:14:21.376+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:21 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-31T05:14:21.398+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:21 INFO MemoryStore: MemoryStore cleared
[2023-01-31T05:14:21.399+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:21 INFO BlockManager: BlockManager stopped
[2023-01-31T05:14:21.405+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:21 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-31T05:14:21.410+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:21 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-31T05:14:21.420+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:21 INFO SparkContext: Successfully stopped SparkContext
[2023-01-31T05:14:21.421+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:21 INFO ShutdownHookManager: Shutdown hook called
[2023-01-31T05:14:21.422+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-e8d8e790-70a3-41c3-b4be-e9c2605edcc2
[2023-01-31T05:14:21.430+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-a9cec05e-0ca5-44a0-b848-00ad65ff68da
[2023-01-31T05:14:21.437+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-a9cec05e-0ca5-44a0-b848-00ad65ff68da/pyspark-8ff7af60-bc17-4d66-9768-1a67ace79b1b
[2023-01-31T05:14:21.606+0000] {taskinstance.py:1322} INFO - Marking task as SUCCESS. dag_id=entsoe-energydata-backfill, task_id=stage_total_generation, execution_date=20210101T010000, start_date=20230131T051101, end_date=20230131T051421
[2023-01-31T05:14:21.667+0000] {local_task_job.py:159} INFO - Task exited with return code 0
[2023-01-31T05:14:21.688+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T05:30:45.864+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-31T05:30:45.952+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-31T05:30:45.957+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T05:30:45.961+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T05:30:45.968+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T05:30:46.380+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 01:00:00+00:00
[2023-01-31T05:30:46.486+0000] {standard_task_runner.py:55} INFO - Started process 15814 to run task
[2023-01-31T05:30:46.509+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '896', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmphe0p_osj']
[2023-01-31T05:30:46.521+0000] {standard_task_runner.py:83} INFO - Job 896: Subtask stage_total_generation
[2023-01-31T05:30:47.911+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 81d5fcd0285b
[2023-01-31T05:30:48.682+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-31T05:30:48.890+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T05:30:48.920+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-31T05:32:05.062+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:04 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T05:32:07.110+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T05:32:10.982+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:10 INFO ResourceUtils: ==============================================================
[2023-01-31T05:32:10.987+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:10 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T05:32:10.996+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:10 INFO ResourceUtils: ==============================================================
[2023-01-31T05:32:11.000+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:10 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T05:32:11.275+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:11 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T05:32:11.316+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:11 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T05:32:11.334+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:11 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T05:32:11.979+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:11 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T05:32:11.984+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:11 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T05:32:11.989+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:11 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T05:32:11.993+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:11 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T05:32:11.999+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:11 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T05:32:20.201+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:20 INFO Utils: Successfully started service 'sparkDriver' on port 36923.
[2023-01-31T05:32:21.318+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:21 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T05:32:21.684+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:21 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T05:32:22.859+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:22 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T05:32:22.903+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:22 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T05:32:23.165+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:23 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T05:32:23.671+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:23 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f2685689-99e4-4b87-8e5c-25d7fbdae8b0
[2023-01-31T05:32:23.998+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:23 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T05:32:25.183+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:25 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T05:32:32.023+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:32 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T05:32:32.028+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:32 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2023-01-31T05:32:32.032+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:32 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[2023-01-31T05:32:32.041+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:32 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[2023-01-31T05:32:32.122+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:32 INFO Utils: Successfully started service 'SparkUI' on port 4044.
[2023-01-31T05:32:32.837+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:32 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://81d5fcd0285b:36923/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675143124413
[2023-01-31T05:32:32.840+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:32 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://81d5fcd0285b:36923/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675143124413
[2023-01-31T05:32:34.964+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:34 INFO Executor: Starting executor ID driver on host 81d5fcd0285b
[2023-01-31T05:32:35.353+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:35 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T05:32:35.590+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:35 INFO Executor: Fetching spark://81d5fcd0285b:36923/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675143124413
[2023-01-31T05:32:36.937+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:36 INFO TransportClientFactory: Successfully created connection to 81d5fcd0285b/172.19.0.9:36923 after 894 ms (0 ms spent in bootstraps)
[2023-01-31T05:32:37.105+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:37 INFO Utils: Fetching spark://81d5fcd0285b:36923/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-8d32b5c8-46f2-48c3-88a0-537d51ae3452/userFiles-f5b1038c-7a86-46a2-a5f6-e6b26d37ec93/fetchFileTemp4997795712983897854.tmp
[2023-01-31T05:32:43.755+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:43 INFO Executor: Adding file:/tmp/spark-8d32b5c8-46f2-48c3-88a0-537d51ae3452/userFiles-f5b1038c-7a86-46a2-a5f6-e6b26d37ec93/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T05:32:43.790+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:43 INFO Executor: Fetching spark://81d5fcd0285b:36923/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675143124413
[2023-01-31T05:32:43.830+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:43 INFO Utils: Fetching spark://81d5fcd0285b:36923/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-8d32b5c8-46f2-48c3-88a0-537d51ae3452/userFiles-f5b1038c-7a86-46a2-a5f6-e6b26d37ec93/fetchFileTemp15450079157130938286.tmp
[2023-01-31T05:32:50.705+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:50 INFO Executor: Adding file:/tmp/spark-8d32b5c8-46f2-48c3-88a0-537d51ae3452/userFiles-f5b1038c-7a86-46a2-a5f6-e6b26d37ec93/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T05:32:50.849+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:50 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44295.
[2023-01-31T05:32:50.850+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:50 INFO NettyBlockTransferService: Server created on 81d5fcd0285b:44295
[2023-01-31T05:32:50.925+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:50 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T05:32:51.033+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:51 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 44295, None)
[2023-01-31T05:32:51.150+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:51 INFO BlockManagerMasterEndpoint: Registering block manager 81d5fcd0285b:44295 with 434.4 MiB RAM, BlockManagerId(driver, 81d5fcd0285b, 44295, None)
[2023-01-31T05:32:51.209+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:51 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 44295, None)
[2023-01-31T05:32:51.256+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:51 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 81d5fcd0285b, 44295, None)
[2023-01-31T05:32:52.088+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:52 INFO Executor: Told to re-register on heartbeat
[2023-01-31T05:32:52.122+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:52 INFO BlockManager: BlockManager BlockManagerId(driver, 81d5fcd0285b, 44295, None) re-registering with master
[2023-01-31T05:32:52.126+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:52 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 44295, None)
[2023-01-31T05:32:52.198+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:52 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 44295, None)
[2023-01-31T05:32:52.200+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:52 INFO BlockManager: Reporting 0 blocks to the master.
[2023-01-31T05:33:08.785+0000] {spark_submit.py:495} INFO - 23/01/31 05:33:08 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T05:33:08.904+0000] {spark_submit.py:495} INFO - 23/01/31 05:33:08 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T05:34:10.341+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:10 INFO InMemoryFileIndex: It took 1860 ms to list leaf files for 1 paths.
[2023-01-31T05:34:13.604+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:13 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T05:34:15.315+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:15 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T05:34:15.381+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:15 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 81d5fcd0285b:44295 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T05:34:15.483+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:15 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T05:34:28.531+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:28 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T05:34:28.941+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:28 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T05:34:29.763+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:29 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T05:34:30.748+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:30 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T05:34:30.773+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:30 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T05:34:30.803+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:30 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:34:30.859+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:30 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:34:31.084+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:31 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T05:34:32.344+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:32 INFO AsyncEventQueue: Process of event SparkListenerJobStart(0,1675143270860,ArraySeq(org.apache.spark.scheduler.StageInfo@40892729),{spark.master=local, spark.driver.port=36923, spark.submit.pyFiles=, spark.app.startTime=1675143124413, spark.rdd.compress=True, spark.executor.id=driver, spark.app.name=gcp_playground, spark.submit.deployMode=client, spark.hadoop.google.cloud.auth.service.account.json.keyfile=/.google/credentials/google_credentials.json, spark.driver.host=81d5fcd0285b, spark.app.id=local-1675143153611, spark.app.submitTime=1675143103441, spark.hadoop.fs.AbstractFileSystem.gs.impl=google.cloud.hadoop.fs.gcs.GoogleHadoopFS, spark.sql.session.timeZone=UTC, spark.hadoop.google.cloud.auth.service.account.enable=true, spark.app.initial.jar.urls=spark://81d5fcd0285b:36923/jars/gcs-connector-hadoop3-latest.jar,spark://81d5fcd0285b:36923/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar, spark.executor.extraJavaOptions=-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED, spark.jars=/opt/spark/jars/gcs-connector-hadoop3-latest.jar, /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar, spark.sql.warehouse.dir=file:/opt/***/spark-warehouse, spark.serializer.objectStreamReset=100, spark.driver.extraJavaOptions=-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED}) by listener AppStatusListener took 1.295189893s.
[2023-01-31T05:34:34.978+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:34 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T05:34:35.233+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:35 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T05:34:35.282+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:35 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 81d5fcd0285b:44295 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T05:34:35.372+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:35 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:34:37.047+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:34:37.111+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:37 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T05:34:39.402+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:39 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T05:34:39.750+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:39 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T05:34:45.140+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:45 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010100__202101010200.json:0+9369
[2023-01-31T05:34:51.249+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:51 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3480 bytes result sent to driver
[2023-01-31T05:34:51.765+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:51 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 12926 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:34:51.794+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:51 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T05:34:51.875+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:51 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 19.841 s
[2023-01-31T05:34:51.899+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:51 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:34:51.903+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T05:34:52.023+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:52 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 22.179234 s
[2023-01-31T05:35:17.431+0000] {spark_submit.py:495} INFO - 23/01/31 05:35:17 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 81d5fcd0285b:44295 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T05:35:17.518+0000] {spark_submit.py:495} INFO - 23/01/31 05:35:17 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 81d5fcd0285b:44295 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T05:36:19.017+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:19 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:36:19.027+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:19 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:36:19.041+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:19 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:36:29.245+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:29 INFO CodeGenerator: Code generated in 6081.401696 ms
[2023-01-31T05:36:29.541+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:29 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T05:36:30.031+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:30 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T05:36:30.048+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:30 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 81d5fcd0285b:44295 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T05:36:30.084+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:30 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T05:36:30.699+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:36:31.697+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:31 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T05:36:31.711+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:31 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T05:36:31.713+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:31 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T05:36:31.715+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:31 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:36:31.740+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:31 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:36:31.762+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:31 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T05:36:32.123+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:32 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T05:36:32.598+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:32 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T05:36:32.614+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:32 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 81d5fcd0285b:44295 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T05:36:32.631+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:32 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:36:32.652+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:36:32.655+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:32 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T05:36:32.770+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:32 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:36:32.799+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:32 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T05:36:34.485+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:34 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T05:36:39.496+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:39 INFO CodeGenerator: Code generated in 3647.8571 ms
[2023-01-31T05:36:42.851+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:42 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1671 bytes result sent to driver
[2023-01-31T05:36:42.894+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:42 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 10215 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:36:42.905+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:42 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T05:36:42.926+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:42 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 10.980 s
[2023-01-31T05:36:42.929+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:42 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:36:42.935+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T05:36:42.945+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:42 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 11.243056 s
[2023-01-31T05:36:45.257+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:45 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:36:45.265+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:45 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:36:45.265+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:45 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:36:46.553+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:46 INFO CodeGenerator: Code generated in 703.115493 ms
[2023-01-31T05:36:46.709+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:46 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T05:36:46.969+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:46 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T05:36:46.993+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:46 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 81d5fcd0285b:44295 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:36:47.019+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:47 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T05:36:47.126+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:47 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:36:47.966+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:47 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T05:36:47.991+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:47 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T05:36:48.020+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:48 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T05:36:48.025+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:48 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:36:48.053+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:48 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:36:48.129+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:48 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T05:36:48.495+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:48 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T05:36:48.783+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:48 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T05:36:48.811+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:48 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 81d5fcd0285b:44295 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T05:36:48.833+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:48 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:36:48.859+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:36:48.885+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:48 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T05:36:48.894+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:48 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:36:48.986+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:48 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T05:36:49.559+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:49 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T05:36:51.376+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:51 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1585 bytes result sent to driver
[2023-01-31T05:36:51.485+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:51 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 2541 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:36:51.497+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:51 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 3.356 s
[2023-01-31T05:36:51.500+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:51 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:36:51.532+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:51 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T05:36:51.535+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T05:36:51.549+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:51 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 3.570556 s
[2023-01-31T05:36:57.782+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:57 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:36:57.868+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:57 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T05:36:57.885+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:57 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:37:03.574+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:03 INFO CodeGenerator: Code generated in 3106.970361 ms
[2023-01-31T05:37:03.667+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:03 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T05:37:03.770+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:03 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T05:37:03.771+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:03 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 81d5fcd0285b:44295 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:37:03.801+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:03 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T05:37:03.802+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:03 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:37:04.483+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:04 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 81d5fcd0285b:44295 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T05:37:04.839+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:04 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 81d5fcd0285b:44295 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T05:37:05.036+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:05 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T05:37:05.073+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:05 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T05:37:05.082+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:05 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T05:37:05.090+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:05 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:37:05.143+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:05 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:37:05.184+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:05 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T05:37:05.345+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:05 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.7 MiB)
[2023-01-31T05:37:05.379+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:05 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.7 MiB)
[2023-01-31T05:37:05.394+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:05 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 81d5fcd0285b:44295 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T05:37:05.417+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:05 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:37:05.439+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:37:05.441+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:05 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T05:37:05.459+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:05 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:37:05.505+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:05 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T05:37:05.898+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:05 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T05:37:07.505+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:07 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-01-31T05:37:07.585+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:07 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 2120 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:37:07.586+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:07 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 2.323 s
[2023-01-31T05:37:07.586+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:07 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:37:07.637+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:07 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T05:37:07.658+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T05:37:07.659+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:07 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 2.571065 s
[2023-01-31T05:37:09.432+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:09 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:37:09.433+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:09 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:37:09.433+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:09 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:37:10.352+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:10 INFO CodeGenerator: Code generated in 439.952172 ms
[2023-01-31T05:37:10.733+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:10 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.5 MiB)
[2023-01-31T05:37:11.537+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T05:37:11.604+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 81d5fcd0285b:44295 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:37:11.620+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T05:37:11.698+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:37:13.207+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:13 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T05:37:13.237+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:13 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T05:37:13.257+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:13 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T05:37:13.277+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:13 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:37:13.303+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:13 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:37:13.381+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:13 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T05:37:13.648+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:13 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-31T05:37:13.780+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:13 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-01-31T05:37:13.788+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:13 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 81d5fcd0285b:44295 (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T05:37:13.846+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:13 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:37:13.947+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:37:13.950+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:13 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T05:37:14.061+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:14 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:37:14.067+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:14 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T05:37:14.262+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:14 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T05:37:17.275+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:17 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1585 bytes result sent to driver
[2023-01-31T05:37:17.413+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:17 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 3356 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:37:17.423+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:17 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 3.976 s
[2023-01-31T05:37:17.464+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:17 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T05:37:17.467+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:17 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:37:17.484+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T05:37:17.493+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:17 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 4.270765 s
[2023-01-31T05:37:22.858+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:22 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:37:22.862+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:22 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T05:37:22.865+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:22 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:37:24.919+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:24 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 81d5fcd0285b:44295 in memory (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T05:37:25.302+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:25 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 81d5fcd0285b:44295 in memory (size: 7.6 KiB, free: 434.3 MiB)
[2023-01-31T05:37:27.591+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:27 INFO CodeGenerator: Code generated in 2548.343717 ms
[2023-01-31T05:37:27.611+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:27 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.3 MiB)
[2023-01-31T05:37:27.993+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:27 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T05:37:28.019+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:28 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 81d5fcd0285b:44295 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:37:28.034+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:28 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T05:37:28.052+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:28 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:37:28.318+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:28 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T05:37:28.426+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:28 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) with 1 output partitions
[2023-01-31T05:37:28.430+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:28 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201)
[2023-01-31T05:37:28.431+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:28 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:37:28.495+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:28 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:37:28.510+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:28 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201), which has no missing parents
[2023-01-31T05:37:28.809+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:28 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.2 MiB)
[2023-01-31T05:37:28.882+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:28 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.2 MiB)
[2023-01-31T05:37:28.885+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:28 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 81d5fcd0285b:44295 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T05:37:28.888+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:28 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:37:28.897+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:37:28.907+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:28 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T05:37:28.924+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:28 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:37:28.926+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:28 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T05:37:28.976+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:28 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T05:37:29.410+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2559 bytes result sent to driver
[2023-01-31T05:37:29.416+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 494 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:37:29.416+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T05:37:29.419+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) finished in 0.831 s
[2023-01-31T05:37:29.420+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:37:29.421+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T05:37:29.421+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201, took 1.100595 s
[2023-01-31T05:37:30.132+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:30 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:37:30.135+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:30 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T05:37:30.139+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:30 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:37:31.953+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:31 INFO CodeGenerator: Code generated in 1317.025548 ms
[2023-01-31T05:37:32.078+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:32 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 433.0 MiB)
[2023-01-31T05:37:32.270+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:32 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 81d5fcd0285b:44295 in memory (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T05:37:32.365+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:32 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.0 MiB)
[2023-01-31T05:37:32.370+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:32 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 81d5fcd0285b:44295 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:37:32.403+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:32 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T05:37:32.464+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:32 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:37:32.916+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:32 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T05:37:32.921+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:32 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T05:37:32.924+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:32 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T05:37:32.925+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:32 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:37:32.931+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:32 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:37:32.959+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:32 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T05:37:33.106+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:33 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 433.0 MiB)
[2023-01-31T05:37:33.173+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:33 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 433.0 MiB)
[2023-01-31T05:37:33.197+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:33 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 81d5fcd0285b:44295 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T05:37:33.227+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:33 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:37:33.269+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:37:33.272+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:33 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T05:37:33.311+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:33 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:37:33.317+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:33 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T05:37:33.714+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:33 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T05:37:34.686+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:34 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-01-31T05:37:34.693+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:34 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 1383 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:37:34.695+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:34 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T05:37:34.706+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:34 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 1.714 s
[2023-01-31T05:37:34.713+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:34 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:37:34.716+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T05:37:34.737+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:34 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 1.814459 s
[2023-01-31T05:37:35.351+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:35 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:37:35.362+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:35 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T05:37:35.370+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:35 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:37:36.573+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:36 INFO CodeGenerator: Code generated in 929.771343 ms
[2023-01-31T05:37:36.597+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:36 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.8 MiB)
[2023-01-31T05:37:36.669+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:36 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.7 MiB)
[2023-01-31T05:37:36.675+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:36 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 81d5fcd0285b:44295 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:37:36.680+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:36 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T05:37:36.689+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:36 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:37:37.204+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:37 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T05:37:37.221+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:37 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T05:37:37.222+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:37 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T05:37:37.223+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:37 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:37:37.223+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:37 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:37:37.243+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:37 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T05:37:37.293+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:37 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.7 MiB)
[2023-01-31T05:37:37.463+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:37 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.7 MiB)
[2023-01-31T05:37:37.487+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:37 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 81d5fcd0285b:44295 (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T05:37:37.492+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:37 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:37:37.505+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:37:37.509+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:37 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T05:37:37.535+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:37 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:37:37.543+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:37 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T05:37:37.661+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:37 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T05:37:38.137+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:38 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1772 bytes result sent to driver
[2023-01-31T05:37:38.145+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:38 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 607 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:37:38.147+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:38 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 0.884 s
[2023-01-31T05:37:38.166+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:38 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T05:37:38.175+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:38 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:37:38.178+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T05:37:38.197+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:38 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 0.940383 s
[2023-01-31T05:37:41.532+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:41 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 81d5fcd0285b:44295 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:37:41.669+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:41 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 81d5fcd0285b:44295 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:37:41.883+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:41 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 81d5fcd0285b:44295 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T05:37:42.098+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:42 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 81d5fcd0285b:44295 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T05:37:42.242+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:42 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 81d5fcd0285b:44295 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:37:42.369+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:42 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 81d5fcd0285b:44295 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:37:42.460+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:42 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 81d5fcd0285b:44295 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:37:42.562+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:42 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 81d5fcd0285b:44295 in memory (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T05:38:45.089+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:45 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:38:45.289+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:38:45.290+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:38:45.303+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:45 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:38:45.308+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:38:45.309+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:38:45.328+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:38:49.069+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:49 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-31T05:38:49.073+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:49 INFO DAGScheduler: Got job 8 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T05:38:49.086+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:49 INFO DAGScheduler: Final stage: ResultStage 8 (save at BigQueryWriteHelper.java:105)
[2023-01-31T05:38:49.088+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:49 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:38:49.108+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:49 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:38:49.170+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:49 INFO DAGScheduler: Submitting ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T05:38:50.405+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:50 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 209.9 KiB, free 434.0 MiB)
[2023-01-31T05:38:50.411+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:50 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 74.3 KiB, free 433.9 MiB)
[2023-01-31T05:38:50.496+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:50 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 81d5fcd0285b:44295 (size: 74.3 KiB, free: 434.3 MiB)
[2023-01-31T05:38:50.497+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:50 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:38:50.498+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:38:50.498+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:50 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2023-01-31T05:38:50.563+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:50 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7484 bytes) taskResourceAssignments Map()
[2023-01-31T05:38:50.574+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:50 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2023-01-31T05:38:51.059+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:38:51.060+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:38:51.067+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:38:51.069+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:38:51.073+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:38:51.086+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:38:51.142+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:51 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T05:38:51.181+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:51 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T05:38:51.828+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:51 INFO ParquetOutputFormat: Parquet block size to 134217728
[2023-01-31T05:38:51.828+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:51 INFO ParquetOutputFormat: Validation is off
[2023-01-31T05:38:51.831+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:51 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-01-31T05:38:51.832+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:51 INFO ParquetOutputFormat: Parquet properties are:
[2023-01-31T05:38:51.832+0000] {spark_submit.py:495} INFO - Parquet page size to 1048576
[2023-01-31T05:38:51.839+0000] {spark_submit.py:495} INFO - Parquet dictionary page size to 1048576
[2023-01-31T05:38:51.839+0000] {spark_submit.py:495} INFO - Dictionary is true
[2023-01-31T05:38:51.840+0000] {spark_submit.py:495} INFO - Writer version is: PARQUET_1_0
[2023-01-31T05:38:51.840+0000] {spark_submit.py:495} INFO - Page size checking is: estimated
[2023-01-31T05:38:51.862+0000] {spark_submit.py:495} INFO - Min row count for page size check is: 100
[2023-01-31T05:38:51.865+0000] {spark_submit.py:495} INFO - Max row count for page size check is: 10000
[2023-01-31T05:38:51.866+0000] {spark_submit.py:495} INFO - Truncate length for column indexes is: 64
[2023-01-31T05:38:51.882+0000] {spark_submit.py:495} INFO - Truncate length for statistics min/max  is: 2147483647
[2023-01-31T05:38:51.890+0000] {spark_submit.py:495} INFO - Bloom filter enabled: false
[2023-01-31T05:38:51.916+0000] {spark_submit.py:495} INFO - Max Bloom filter size for a column is 1048576
[2023-01-31T05:38:51.918+0000] {spark_submit.py:495} INFO - Bloom filter expected number of distinct values are: null
[2023-01-31T05:38:51.922+0000] {spark_submit.py:495} INFO - Page row count limit to 20000
[2023-01-31T05:38:51.924+0000] {spark_submit.py:495} INFO - Writing page checksums is: on
[2023-01-31T05:38:52.783+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:52 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-01-31T05:38:52.784+0000] {spark_submit.py:495} INFO - {
[2023-01-31T05:38:52.785+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-31T05:38:52.786+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-31T05:38:52.788+0000] {spark_submit.py:495} INFO - "name" : "measured_at",
[2023-01-31T05:38:52.789+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T05:38:52.804+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T05:38:52.813+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:52.818+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:52.820+0000] {spark_submit.py:495} INFO - "name" : "created_at",
[2023-01-31T05:38:52.827+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T05:38:52.832+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T05:38:52.839+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:52.845+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:52.858+0000] {spark_submit.py:495} INFO - "name" : "Biomass - Actual Aggregated",
[2023-01-31T05:38:52.874+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:52.879+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:52.888+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:52.897+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:52.919+0000] {spark_submit.py:495} INFO - "name" : "Fossil Brown coal/Lignite - Actual Aggregated",
[2023-01-31T05:38:52.938+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:52.943+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:52.947+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:52.953+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:52.957+0000] {spark_submit.py:495} INFO - "name" : "Fossil Gas - Actual Aggregated",
[2023-01-31T05:38:52.966+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:52.975+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:52.980+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:52.982+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:52.983+0000] {spark_submit.py:495} INFO - "name" : "Fossil Hard coal - Actual Aggregated",
[2023-01-31T05:38:52.985+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:52.986+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:52.986+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:52.987+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:52.987+0000] {spark_submit.py:495} INFO - "name" : "Fossil Oil - Actual Aggregated",
[2023-01-31T05:38:52.988+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:52.988+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:52.989+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:52.989+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:52.990+0000] {spark_submit.py:495} INFO - "name" : "Geothermal - Actual Aggregated",
[2023-01-31T05:38:52.991+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:52.991+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:52.992+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:52.992+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:52.993+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Aggregated",
[2023-01-31T05:38:52.993+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:52.994+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:52.995+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:52.995+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:52.996+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Consumption",
[2023-01-31T05:38:52.996+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:52.997+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:52.997+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:52.998+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:52.998+0000] {spark_submit.py:495} INFO - "name" : "Hydro Run-of-river and poundage - Actual Aggregated",
[2023-01-31T05:38:52.999+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:52.999+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:53.000+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:53.000+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:53.001+0000] {spark_submit.py:495} INFO - "name" : "Hydro Water Reservoir - Actual Aggregated",
[2023-01-31T05:38:53.001+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:53.002+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:53.002+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:53.003+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:53.003+0000] {spark_submit.py:495} INFO - "name" : "Nuclear - Actual Aggregated",
[2023-01-31T05:38:53.004+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:53.005+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:53.006+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:53.006+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:53.007+0000] {spark_submit.py:495} INFO - "name" : "Other - Actual Aggregated",
[2023-01-31T05:38:53.007+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:53.008+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:53.008+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:53.009+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:53.009+0000] {spark_submit.py:495} INFO - "name" : "Other renewable - Actual Aggregated",
[2023-01-31T05:38:53.010+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:53.010+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:53.011+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:53.011+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:53.012+0000] {spark_submit.py:495} INFO - "name" : "Solar - Actual Aggregated",
[2023-01-31T05:38:53.012+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:53.013+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:53.013+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:53.014+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:53.015+0000] {spark_submit.py:495} INFO - "name" : "Waste - Actual Aggregated",
[2023-01-31T05:38:53.015+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:53.016+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:53.016+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:53.016+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:53.017+0000] {spark_submit.py:495} INFO - "name" : "Wind Offshore - Actual Aggregated",
[2023-01-31T05:38:53.018+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:53.018+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:53.019+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:53.019+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:53.020+0000] {spark_submit.py:495} INFO - "name" : "Wind Onshore - Actual Aggregated",
[2023-01-31T05:38:53.020+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:53.021+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:53.021+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:53.022+0000] {spark_submit.py:495} INFO - } ]
[2023-01-31T05:38:53.023+0000] {spark_submit.py:495} INFO - }
[2023-01-31T05:38:53.023+0000] {spark_submit.py:495} INFO - and corresponding Parquet message type:
[2023-01-31T05:38:53.024+0000] {spark_submit.py:495} INFO - message spark_schema {
[2023-01-31T05:38:53.024+0000] {spark_submit.py:495} INFO - required int96 measured_at;
[2023-01-31T05:38:53.025+0000] {spark_submit.py:495} INFO - required int96 created_at;
[2023-01-31T05:38:53.025+0000] {spark_submit.py:495} INFO - optional double Biomass - Actual Aggregated;
[2023-01-31T05:38:53.026+0000] {spark_submit.py:495} INFO - optional double Fossil Brown coal/Lignite - Actual Aggregated;
[2023-01-31T05:38:53.026+0000] {spark_submit.py:495} INFO - optional double Fossil Gas - Actual Aggregated;
[2023-01-31T05:38:53.027+0000] {spark_submit.py:495} INFO - optional double Fossil Hard coal - Actual Aggregated;
[2023-01-31T05:38:53.027+0000] {spark_submit.py:495} INFO - optional double Fossil Oil - Actual Aggregated;
[2023-01-31T05:38:53.028+0000] {spark_submit.py:495} INFO - optional double Geothermal - Actual Aggregated;
[2023-01-31T05:38:53.028+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Aggregated;
[2023-01-31T05:38:53.029+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Consumption;
[2023-01-31T05:38:53.029+0000] {spark_submit.py:495} INFO - optional double Hydro Run-of-river and poundage - Actual Aggregated;
[2023-01-31T05:38:53.030+0000] {spark_submit.py:495} INFO - optional double Hydro Water Reservoir - Actual Aggregated;
[2023-01-31T05:38:53.030+0000] {spark_submit.py:495} INFO - optional double Nuclear - Actual Aggregated;
[2023-01-31T05:38:53.031+0000] {spark_submit.py:495} INFO - optional double Other - Actual Aggregated;
[2023-01-31T05:38:53.032+0000] {spark_submit.py:495} INFO - optional double Other renewable - Actual Aggregated;
[2023-01-31T05:38:53.032+0000] {spark_submit.py:495} INFO - optional double Solar - Actual Aggregated;
[2023-01-31T05:38:53.033+0000] {spark_submit.py:495} INFO - optional double Waste - Actual Aggregated;
[2023-01-31T05:38:53.033+0000] {spark_submit.py:495} INFO - optional double Wind Offshore - Actual Aggregated;
[2023-01-31T05:38:53.034+0000] {spark_submit.py:495} INFO - optional double Wind Onshore - Actual Aggregated;
[2023-01-31T05:38:53.034+0000] {spark_submit.py:495} INFO - }
[2023-01-31T05:38:53.035+0000] {spark_submit.py:495} INFO - 
[2023-01-31T05:38:53.036+0000] {spark_submit.py:495} INFO - 
[2023-01-31T05:38:56.270+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:56 INFO CodecPool: Got brand-new compressor [.snappy]
[2023-01-31T05:39:14.754+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:14 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675143153611-bc4991d9-9568-4e1a-90c9-74a335f770db/_temporary/0/_temporary/' directory.
[2023-01-31T05:39:14.759+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:14 INFO FileOutputCommitter: Saved output of task 'attempt_20230131053847974473178344799884_0008_m_000000_8' to gs://entsoe_temp_1009/.spark-bigquery-local-1675143153611-bc4991d9-9568-4e1a-90c9-74a335f770db/_temporary/0/task_20230131053847974473178344799884_0008_m_000000
[2023-01-31T05:39:14.768+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:14 INFO SparkHadoopMapRedUtil: attempt_20230131053847974473178344799884_0008_m_000000_8: Committed. Elapsed time: 3049 ms.
[2023-01-31T05:39:14.891+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:14 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2588 bytes result sent to driver
[2023-01-31T05:39:14.900+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:14 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 24351 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:39:14.909+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:14 INFO DAGScheduler: ResultStage 8 (save at BigQueryWriteHelper.java:105) finished in 25.615 s
[2023-01-31T05:39:14.911+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:14 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2023-01-31T05:39:14.916+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:14 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:39:14.922+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2023-01-31T05:39:14.933+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:14 INFO DAGScheduler: Job 8 finished: save at BigQueryWriteHelper.java:105, took 25.861172 s
[2023-01-31T05:39:14.942+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:14 INFO FileFormatWriter: Start to commit write Job 932f3d8f-adb6-4b95-8074-55dae15fb2eb.
[2023-01-31T05:39:17.377+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:17 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675143153611-bc4991d9-9568-4e1a-90c9-74a335f770db/_temporary/0/task_20230131053847974473178344799884_0008_m_000000/' directory.
[2023-01-31T05:39:18.245+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:18 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675143153611-bc4991d9-9568-4e1a-90c9-74a335f770db/' directory.
[2023-01-31T05:39:19.919+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:19 INFO FileFormatWriter: Write Job 932f3d8f-adb6-4b95-8074-55dae15fb2eb committed. Elapsed time: 4949 ms.
[2023-01-31T05:39:20.103+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:20 INFO FileFormatWriter: Finished processing stats for write job 932f3d8f-adb6-4b95-8074-55dae15fb2eb.
[2023-01-31T05:39:20.457+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:20 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 81d5fcd0285b:44295 in memory (size: 74.3 KiB, free: 434.4 MiB)
[2023-01-31T05:39:24.183+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:24 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=entsoe_analytics_1009, projectId=rafzul-analytics-1009, tableId=TEST_total_generation_staging}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=null, ignoreUnknownValue=null, sourceUris=[gs://entsoe_temp_1009/.spark-bigquery-local-1675143153611-bc4991d9-9568-4e1a-90c9-74a335f770db/part-00000-158bd315-41ca-49cb-ba8f-69be59385f66-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=true, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=rafzul-analytics-1009, job=5a7b4c3b-3381-47c9-9540-31105825613d, location=US}
[2023-01-31T05:39:27.417+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:27 INFO BigQueryClient: Done loading to rafzul-analytics-1009.entsoe_analytics_1009.TEST_total_generation_staging. jobId: JobId{project=rafzul-analytics-1009, job=5a7b4c3b-3381-47c9-9540-31105825613d, location=US}
[2023-01-31T05:39:28.285+0000] {spark_submit.py:495} INFO - MANTEP!!!!
[2023-01-31T05:39:29.747+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:29 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-31T05:39:29.986+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:29 INFO SparkUI: Stopped Spark web UI at http://81d5fcd0285b:4044
[2023-01-31T05:39:30.180+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:30 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-31T05:39:30.333+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:30 INFO MemoryStore: MemoryStore cleared
[2023-01-31T05:39:30.336+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:30 INFO BlockManager: BlockManager stopped
[2023-01-31T05:39:30.357+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:30 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-31T05:39:30.392+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:30 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-31T05:39:30.479+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:30 INFO SparkContext: Successfully stopped SparkContext
[2023-01-31T05:39:30.488+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:30 INFO ShutdownHookManager: Shutdown hook called
[2023-01-31T05:39:30.499+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:30 INFO ShutdownHookManager: Deleting directory /tmp/spark-8d32b5c8-46f2-48c3-88a0-537d51ae3452
[2023-01-31T05:39:30.543+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:30 INFO ShutdownHookManager: Deleting directory /tmp/spark-8d32b5c8-46f2-48c3-88a0-537d51ae3452/pyspark-6509b6b9-30c1-4ea3-9f3e-8ef5d294c630
[2023-01-31T05:39:30.617+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:30 INFO ShutdownHookManager: Deleting directory /tmp/spark-85fa5459-4fa0-46d1-9eaf-c65bc3c7cb0b
[2023-01-31T05:39:31.334+0000] {taskinstance.py:1322} INFO - Marking task as SUCCESS. dag_id=entsoe-energydata-backfill, task_id=stage_total_generation, execution_date=20210101T010000, start_date=20230131T053045, end_date=20230131T053931
[2023-01-31T05:39:31.468+0000] {local_task_job.py:159} INFO - Task exited with return code 0
[2023-01-31T05:39:31.589+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T07:38:54.156+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-31T07:38:54.438+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-31T07:38:54.449+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T07:38:54.459+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T07:38:54.467+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T07:38:55.018+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 01:00:00+00:00
[2023-01-31T07:38:55.155+0000] {standard_task_runner.py:55} INFO - Started process 25781 to run task
[2023-01-31T07:38:55.232+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '908', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpn0hf1icj']
[2023-01-31T07:38:55.277+0000] {standard_task_runner.py:83} INFO - Job 908: Subtask stage_total_generation
[2023-01-31T07:38:55.980+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 81d5fcd0285b
[2023-01-31T07:38:57.472+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-31T07:38:57.647+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T07:38:57.659+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-31T07:40:14.910+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:14 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T07:40:16.233+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T07:40:19.162+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:19 INFO ResourceUtils: ==============================================================
[2023-01-31T07:40:19.166+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:19 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T07:40:19.178+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:19 INFO ResourceUtils: ==============================================================
[2023-01-31T07:40:19.181+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:19 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T07:40:19.534+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:19 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T07:40:19.645+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:19 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T07:40:19.660+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:19 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T07:40:20.762+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:20 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T07:40:20.777+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:20 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T07:40:20.778+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:20 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T07:40:20.785+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:20 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T07:40:20.785+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:20 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T07:40:26.948+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:26 INFO Utils: Successfully started service 'sparkDriver' on port 45255.
[2023-01-31T07:40:28.056+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:28 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T07:40:30.228+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:30 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T07:40:30.439+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:30 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T07:40:30.514+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:30 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T07:40:30.846+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:30 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T07:40:31.803+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:31 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-2bd6f9df-ea29-43d5-a0fb-583d4aec5362
[2023-01-31T07:40:32.967+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:32 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T07:40:33.714+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:33 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T07:40:42.410+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:42 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-01-31T07:40:43.968+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:43 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://81d5fcd0285b:45255/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675150814747
[2023-01-31T07:40:43.969+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:43 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://81d5fcd0285b:45255/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675150814747
[2023-01-31T07:40:49.184+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:49 INFO Executor: Starting executor ID driver on host 81d5fcd0285b
[2023-01-31T07:40:49.450+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:49 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T07:40:49.890+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:49 INFO Executor: Fetching spark://81d5fcd0285b:45255/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675150814747
[2023-01-31T07:40:51.169+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:51 INFO TransportClientFactory: Successfully created connection to 81d5fcd0285b/172.19.0.9:45255 after 700 ms (0 ms spent in bootstraps)
[2023-01-31T07:40:51.577+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:51 INFO Utils: Fetching spark://81d5fcd0285b:45255/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-7ef585df-f3b3-4837-b64e-f28d62bfc0e4/userFiles-a808d029-06d6-472a-8131-c0ccb37c1c4b/fetchFileTemp10662363882990368823.tmp
[2023-01-31T07:41:01.365+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:01 INFO Executor: Adding file:/tmp/spark-7ef585df-f3b3-4837-b64e-f28d62bfc0e4/userFiles-a808d029-06d6-472a-8131-c0ccb37c1c4b/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T07:41:01.374+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:01 INFO Executor: Fetching spark://81d5fcd0285b:45255/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675150814747
[2023-01-31T07:41:01.383+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:01 INFO Utils: Fetching spark://81d5fcd0285b:45255/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-7ef585df-f3b3-4837-b64e-f28d62bfc0e4/userFiles-a808d029-06d6-472a-8131-c0ccb37c1c4b/fetchFileTemp3400020687664991591.tmp
[2023-01-31T07:41:07.645+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:07 INFO Executor: Told to re-register on heartbeat
[2023-01-31T07:41:07.649+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:07 INFO BlockManager: BlockManager null re-registering with master
[2023-01-31T07:41:07.679+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:07 INFO BlockManagerMaster: Registering BlockManager null
[2023-01-31T07:41:07.950+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:07 ERROR Inbox: Ignoring error
[2023-01-31T07:41:07.951+0000] {spark_submit.py:495} INFO - java.lang.NullPointerException
[2023-01-31T07:41:07.952+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)
[2023-01-31T07:41:07.955+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)
[2023-01-31T07:41:07.956+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-01-31T07:41:07.965+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-01-31T07:41:07.966+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-01-31T07:41:07.967+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-01-31T07:41:07.968+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-01-31T07:41:07.969+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-01-31T07:41:07.985+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-01-31T07:41:07.986+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-31T07:41:07.987+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-31T07:41:07.994+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-31T07:41:07.995+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:07 WARN Executor: Issue communicating with driver in heartbeater
[2023-01-31T07:41:07.996+0000] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-01-31T07:41:07.996+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-01-31T07:41:08.008+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-01-31T07:41:08.009+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2023-01-31T07:41:08.011+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2023-01-31T07:41:08.011+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:79)
[2023-01-31T07:41:08.012+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:643)
[2023-01-31T07:41:08.013+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2023-01-31T07:41:08.013+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2023-01-31T07:41:08.014+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
[2023-01-31T07:41:08.015+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2023-01-31T07:41:08.015+0000] {spark_submit.py:495} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2023-01-31T07:41:08.016+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-01-31T07:41:08.017+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2023-01-31T07:41:08.018+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2023-01-31T07:41:08.019+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-31T07:41:08.019+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-31T07:41:08.020+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-31T07:41:08.029+0000] {spark_submit.py:495} INFO - Caused by: java.lang.NullPointerException
[2023-01-31T07:41:08.029+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)
[2023-01-31T07:41:08.030+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)
[2023-01-31T07:41:08.030+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-01-31T07:41:08.031+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-01-31T07:41:08.042+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-01-31T07:41:08.043+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-01-31T07:41:08.043+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-01-31T07:41:08.044+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-01-31T07:41:08.058+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-01-31T07:41:08.058+0000] {spark_submit.py:495} INFO - ... 3 more
[2023-01-31T07:41:08.064+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:07 INFO Executor: Adding file:/tmp/spark-7ef585df-f3b3-4837-b64e-f28d62bfc0e4/userFiles-a808d029-06d6-472a-8131-c0ccb37c1c4b/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T07:41:08.121+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:08 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39809.
[2023-01-31T07:41:08.122+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:08 INFO NettyBlockTransferService: Server created on 81d5fcd0285b:39809
[2023-01-31T07:41:08.148+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:08 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T07:41:08.232+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:08 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 39809, None)
[2023-01-31T07:41:08.253+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:08 INFO BlockManagerMasterEndpoint: Registering block manager 81d5fcd0285b:39809 with 434.4 MiB RAM, BlockManagerId(driver, 81d5fcd0285b, 39809, None)
[2023-01-31T07:41:08.293+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:08 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 39809, None)
[2023-01-31T07:41:08.309+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:08 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 81d5fcd0285b, 39809, None)
[2023-01-31T07:41:32.333+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:32 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T07:41:32.698+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:32 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T07:42:11.651+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:11 INFO InMemoryFileIndex: It took 1993 ms to list leaf files for 1 paths.
[2023-01-31T07:42:18.061+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:18 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T07:42:19.177+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:19 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T07:42:19.206+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:19 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 81d5fcd0285b:39809 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T07:42:19.237+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:19 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T07:42:27.404+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:27 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T07:42:27.693+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:27 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T07:42:28.717+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:28 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T07:42:29.309+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:29 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T07:42:29.311+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:29 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T07:42:29.314+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:29 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:42:29.428+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:29 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:42:29.797+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:29 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T07:42:31.227+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:31 INFO AsyncEventQueue: Process of event SparkListenerJobStart(0,1675150949423,ArraySeq(org.apache.spark.scheduler.StageInfo@2299e343),{spark.master=local, spark.driver.port=45255, spark.submit.pyFiles=, spark.app.startTime=1675150814747, spark.rdd.compress=True, spark.executor.id=driver, spark.app.name=gcp_playground, spark.submit.deployMode=client, spark.hadoop.google.cloud.auth.service.account.json.keyfile=/.google/credentials/google_credentials.json, spark.driver.host=81d5fcd0285b, spark.app.id=local-1675150846325, spark.app.submitTime=1675150801439, spark.hadoop.fs.AbstractFileSystem.gs.impl=google.cloud.hadoop.fs.gcs.GoogleHadoopFS, spark.sql.session.timeZone=UTC, spark.hadoop.google.cloud.auth.service.account.enable=true, spark.app.initial.jar.urls=spark://81d5fcd0285b:45255/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar,spark://81d5fcd0285b:45255/jars/gcs-connector-hadoop3-latest.jar, spark.executor.extraJavaOptions=-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED, spark.jars=/opt/spark/jars/gcs-connector-hadoop3-latest.jar, /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar, spark.sql.warehouse.dir=file:/opt/***/spark-warehouse, spark.serializer.objectStreamReset=100, spark.driver.extraJavaOptions=-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED}) by listener AppStatusListener took 1.643165761s.
[2023-01-31T07:42:31.615+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:31 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T07:42:31.764+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:31 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T07:42:31.774+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:31 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 81d5fcd0285b:39809 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T07:42:31.794+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:31 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:42:32.460+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:42:32.488+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:32 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T07:42:33.604+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:33 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T07:42:33.791+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:33 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T07:42:37.638+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:37 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010100__202101010200.json:0+9369
[2023-01-31T07:42:45.324+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:45 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3480 bytes result sent to driver
[2023-01-31T07:42:45.643+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:45 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 12237 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:42:45.750+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:45 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T07:42:45.761+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:45 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 15.403 s
[2023-01-31T07:42:46.133+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:46 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:42:46.156+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T07:42:46.229+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:46 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 17.504316 s
[2023-01-31T07:43:09.048+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:09 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 81d5fcd0285b:39809 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T07:43:09.100+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:09 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 81d5fcd0285b:39809 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T07:43:22.835+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:22 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:43:22.844+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:22 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T07:43:22.858+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:22 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:43:25.818+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:25 INFO CodeGenerator: Code generated in 1735.150762 ms
[2023-01-31T07:43:25.857+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:25 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T07:43:25.898+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:25 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T07:43:25.901+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:25 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 81d5fcd0285b:39809 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T07:43:25.942+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:25 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T07:43:26.057+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:26 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:43:26.421+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:26 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T07:43:26.436+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:26 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T07:43:26.437+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:26 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T07:43:26.438+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:26 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:43:26.438+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:26 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:43:26.449+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:26 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T07:43:26.484+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:26 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T07:43:26.508+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:26 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T07:43:26.513+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:26 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 81d5fcd0285b:39809 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T07:43:26.533+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:26 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:43:26.535+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:43:26.536+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:26 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T07:43:26.575+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:26 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:43:26.583+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:26 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T07:43:27.102+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T07:43:29.599+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO CodeGenerator: Code generated in 2190.022421 ms
[2023-01-31T07:43:30.569+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1628 bytes result sent to driver
[2023-01-31T07:43:30.628+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 4074 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:43:30.629+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T07:43:30.639+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 4.171 s
[2023-01-31T07:43:30.649+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:43:30.649+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T07:43:30.656+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 4.224868 s
[2023-01-31T07:43:31.462+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:31 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:43:31.463+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:31 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T07:43:31.466+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:31 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:43:32.235+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO CodeGenerator: Code generated in 298.155124 ms
[2023-01-31T07:43:32.324+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T07:43:32.572+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T07:43:32.576+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 81d5fcd0285b:39809 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:43:32.588+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T07:43:32.611+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:43:32.798+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T07:43:32.799+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T07:43:32.799+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T07:43:32.800+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:43:32.800+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:43:32.809+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T07:43:32.929+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T07:43:32.988+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T07:43:33.003+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 81d5fcd0285b:39809 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T07:43:33.010+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:43:33.020+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:43:33.024+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T07:43:33.031+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:43:33.033+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T07:43:33.208+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T07:43:33.456+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 81d5fcd0285b:39809 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T07:43:34.027+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1628 bytes result sent to driver
[2023-01-31T07:43:34.077+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1029 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:43:34.093+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 1.214 s
[2023-01-31T07:43:34.093+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:43:34.094+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T07:43:34.094+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T07:43:34.109+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 1.298404 s
[2023-01-31T07:43:35.765+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:35 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:43:35.836+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:35 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T07:43:35.856+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:35 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:43:40.065+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO CodeGenerator: Code generated in 2360.430934 ms
[2023-01-31T07:43:40.202+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T07:43:40.599+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T07:43:40.618+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 81d5fcd0285b:39809 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:43:40.662+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T07:43:40.769+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:43:41.446+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:41 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T07:43:41.453+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:41 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T07:43:41.453+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:41 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T07:43:41.466+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:41 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:43:41.468+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:41 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:43:41.501+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:41 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T07:43:41.598+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:41 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.7 MiB)
[2023-01-31T07:43:41.657+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:41 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.6 MiB)
[2023-01-31T07:43:41.679+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:41 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 81d5fcd0285b:39809 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T07:43:41.697+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:41 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:43:41.719+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:43:41.749+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:41 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T07:43:41.766+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:41 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:43:41.777+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:41 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T07:43:42.204+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:42 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T07:43:43.447+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:43 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-01-31T07:43:43.557+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:43 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 1792 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:43:43.578+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:43 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T07:43:43.596+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:43 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 2.065 s
[2023-01-31T07:43:43.598+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:43 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:43:43.599+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T07:43:43.599+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:43 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 2.147979 s
[2023-01-31T07:43:44.565+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:44 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:43:44.583+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:44 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T07:43:44.592+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:44 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:43:44.936+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:44 INFO CodeGenerator: Code generated in 103.41744 ms
[2023-01-31T07:43:45.007+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:44 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.4 MiB)
[2023-01-31T07:43:45.193+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T07:43:45.209+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 81d5fcd0285b:39809 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T07:43:45.229+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T07:43:45.247+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:43:45.713+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T07:43:45.714+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T07:43:45.746+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T07:43:45.772+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:43:45.773+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:43:45.846+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T07:43:46.164+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:46 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-31T07:43:46.225+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:46 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-01-31T07:43:46.231+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:46 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 81d5fcd0285b:39809 (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T07:43:46.246+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:46 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:43:46.252+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:43:46.257+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:46 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T07:43:46.276+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:46 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:43:46.283+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:46 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T07:43:46.614+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:46 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T07:43:47.874+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:47 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1585 bytes result sent to driver
[2023-01-31T07:43:47.911+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:47 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 1639 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:43:47.929+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:47 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 1.987 s
[2023-01-31T07:43:47.929+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:47 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:43:47.942+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:47 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T07:43:47.945+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T07:43:47.955+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:47 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 2.248024 s
[2023-01-31T07:43:53.948+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:53 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:43:53.977+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:53 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T07:43:53.979+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:53 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:43:55.688+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:55 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 81d5fcd0285b:39809 in memory (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T07:43:55.943+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:55 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 81d5fcd0285b:39809 in memory (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T07:43:56.081+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:56 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 81d5fcd0285b:39809 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T07:43:57.014+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:57 INFO CodeGenerator: Code generated in 2207.669118 ms
[2023-01-31T07:43:57.351+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:57 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.3 MiB)
[2023-01-31T07:43:58.258+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:58 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T07:43:58.273+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:58 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 81d5fcd0285b:39809 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T07:43:58.285+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:58 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T07:43:58.331+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:58 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:43:59.336+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:59 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T07:43:59.351+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:59 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) with 1 output partitions
[2023-01-31T07:43:59.352+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:59 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201)
[2023-01-31T07:43:59.355+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:59 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:43:59.355+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:59 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:43:59.367+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:59 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201), which has no missing parents
[2023-01-31T07:43:59.530+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:59 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.2 MiB)
[2023-01-31T07:43:59.764+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:59 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.2 MiB)
[2023-01-31T07:43:59.808+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:59 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 81d5fcd0285b:39809 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T07:43:59.837+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:59 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:43:59.882+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:43:59.885+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:59 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T07:43:59.906+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:59 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:43:59.912+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:59 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T07:44:00.176+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:00 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T07:44:01.801+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:01 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2559 bytes result sent to driver
[2023-01-31T07:44:01.821+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:01 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 1915 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:44:01.822+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:01 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T07:44:01.830+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:01 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) finished in 2.452 s
[2023-01-31T07:44:01.833+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:01 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:44:01.834+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T07:44:01.834+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:01 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201, took 2.490502 s
[2023-01-31T07:44:02.519+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:02 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:44:02.534+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:02 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T07:44:02.547+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:02 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:44:03.789+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:03 INFO CodeGenerator: Code generated in 896.03878 ms
[2023-01-31T07:44:03.870+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:03 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 433.0 MiB)
[2023-01-31T07:44:03.991+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:03 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.0 MiB)
[2023-01-31T07:44:03.994+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:03 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 81d5fcd0285b:39809 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T07:44:03.995+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:03 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T07:44:04.007+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:44:04.220+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T07:44:04.247+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T07:44:04.249+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T07:44:04.250+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:44:04.250+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:44:04.410+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T07:44:04.453+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 432.9 MiB)
[2023-01-31T07:44:04.467+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.9 MiB)
[2023-01-31T07:44:04.475+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 81d5fcd0285b:39809 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T07:44:04.483+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:44:04.519+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:44:04.534+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T07:44:04.558+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:44:04.561+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T07:44:04.877+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T07:44:05.577+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:05 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-01-31T07:44:05.578+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:05 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 1007 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:44:05.579+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:05 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T07:44:05.580+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:05 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 1.137 s
[2023-01-31T07:44:05.580+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:05 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:44:05.586+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T07:44:05.587+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:05 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 1.371427 s
[2023-01-31T07:44:08.579+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:08 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:44:08.669+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:08 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T07:44:08.695+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:08 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:44:11.361+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:11 INFO CodeGenerator: Code generated in 1670.063674 ms
[2023-01-31T07:44:11.781+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:11 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.7 MiB)
[2023-01-31T07:44:12.434+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:12 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.7 MiB)
[2023-01-31T07:44:12.446+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:12 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 81d5fcd0285b:39809 (size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T07:44:12.458+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:12 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T07:44:12.462+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:44:13.729+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:13 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T07:44:13.729+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:13 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T07:44:13.730+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:13 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T07:44:13.730+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:13 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:44:13.731+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:13 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:44:13.731+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:13 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T07:44:13.732+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:13 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 81d5fcd0285b:39809 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T07:44:13.732+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:13 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.7 MiB)
[2023-01-31T07:44:13.814+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:13 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.7 MiB)
[2023-01-31T07:44:13.830+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:13 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 81d5fcd0285b:39809 (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T07:44:13.836+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:13 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:44:13.856+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:44:13.871+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:13 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T07:44:13.878+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:13 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 81d5fcd0285b:39809 in memory (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T07:44:13.897+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:13 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:44:13.905+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:13 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T07:44:14.188+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:14 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T07:44:15.689+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:15 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1772 bytes result sent to driver
[2023-01-31T07:44:15.690+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:15 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 1778 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:44:15.690+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:15 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T07:44:15.691+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:15 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 1.957 s
[2023-01-31T07:44:15.691+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:15 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:44:15.691+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T07:44:15.704+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:15 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 2.041025 s
[2023-01-31T07:44:39.754+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:39 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 81d5fcd0285b:39809 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T07:44:39.986+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:39 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 81d5fcd0285b:39809 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T07:44:40.386+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:40 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 81d5fcd0285b:39809 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:44:40.534+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:40 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 81d5fcd0285b:39809 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:44:40.697+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:40 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 81d5fcd0285b:39809 in memory (size: 12.6 KiB, free: 434.3 MiB)
[2023-01-31T07:44:41.061+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:41 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 81d5fcd0285b:39809 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:45:10.541+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:10 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T07:45:10.593+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T07:45:10.595+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T07:45:10.606+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:10 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T07:45:10.608+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T07:45:10.608+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T07:45:10.623+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T07:45:11.648+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-31T07:45:11.660+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO DAGScheduler: Got job 8 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T07:45:11.665+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO DAGScheduler: Final stage: ResultStage 8 (save at BigQueryWriteHelper.java:105)
[2023-01-31T07:45:11.665+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:45:11.666+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:45:11.677+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO DAGScheduler: Submitting ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T07:45:12.004+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 209.9 KiB, free 433.7 MiB)
[2023-01-31T07:45:12.038+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:12 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 74.2 KiB, free 433.7 MiB)
[2023-01-31T07:45:12.043+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:12 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 81d5fcd0285b:39809 (size: 74.2 KiB, free: 434.3 MiB)
[2023-01-31T07:45:12.050+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:12 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:45:12.065+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:45:12.076+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:12 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2023-01-31T07:45:12.089+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:12 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7484 bytes) taskResourceAssignments Map()
[2023-01-31T07:45:12.110+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:12 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2023-01-31T07:45:12.898+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T07:45:12.901+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T07:45:12.910+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:12 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T07:45:12.917+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T07:45:12.923+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T07:45:12.928+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T07:45:12.946+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:12 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T07:45:12.963+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:12 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T07:45:13.222+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:13 INFO ParquetOutputFormat: Parquet block size to 134217728
[2023-01-31T07:45:13.223+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:13 INFO ParquetOutputFormat: Validation is off
[2023-01-31T07:45:13.224+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:13 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-01-31T07:45:13.224+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:13 INFO ParquetOutputFormat: Parquet properties are:
[2023-01-31T07:45:13.225+0000] {spark_submit.py:495} INFO - Parquet page size to 1048576
[2023-01-31T07:45:13.232+0000] {spark_submit.py:495} INFO - Parquet dictionary page size to 1048576
[2023-01-31T07:45:13.233+0000] {spark_submit.py:495} INFO - Dictionary is true
[2023-01-31T07:45:13.236+0000] {spark_submit.py:495} INFO - Writer version is: PARQUET_1_0
[2023-01-31T07:45:13.237+0000] {spark_submit.py:495} INFO - Page size checking is: estimated
[2023-01-31T07:45:13.237+0000] {spark_submit.py:495} INFO - Min row count for page size check is: 100
[2023-01-31T07:45:13.239+0000] {spark_submit.py:495} INFO - Max row count for page size check is: 10000
[2023-01-31T07:45:13.241+0000] {spark_submit.py:495} INFO - Truncate length for column indexes is: 64
[2023-01-31T07:45:13.254+0000] {spark_submit.py:495} INFO - Truncate length for statistics min/max  is: 2147483647
[2023-01-31T07:45:13.279+0000] {spark_submit.py:495} INFO - Bloom filter enabled: false
[2023-01-31T07:45:13.282+0000] {spark_submit.py:495} INFO - Max Bloom filter size for a column is 1048576
[2023-01-31T07:45:13.284+0000] {spark_submit.py:495} INFO - Bloom filter expected number of distinct values are: null
[2023-01-31T07:45:13.296+0000] {spark_submit.py:495} INFO - Page row count limit to 20000
[2023-01-31T07:45:13.301+0000] {spark_submit.py:495} INFO - Writing page checksums is: on
[2023-01-31T07:45:13.470+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:13 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-01-31T07:45:13.470+0000] {spark_submit.py:495} INFO - {
[2023-01-31T07:45:13.471+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-31T07:45:13.471+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-31T07:45:13.471+0000] {spark_submit.py:495} INFO - "name" : "measured_at",
[2023-01-31T07:45:13.471+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T07:45:13.472+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T07:45:13.472+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:13.472+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:13.473+0000] {spark_submit.py:495} INFO - "name" : "created_at",
[2023-01-31T07:45:13.473+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T07:45:13.473+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T07:45:13.474+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:13.476+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:13.477+0000] {spark_submit.py:495} INFO - "name" : "Biomass - Actual Aggregated",
[2023-01-31T07:45:13.478+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:13.479+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:13.483+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:13.486+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:13.487+0000] {spark_submit.py:495} INFO - "name" : "Fossil Brown coal/Lignite - Actual Aggregated",
[2023-01-31T07:45:13.489+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:13.490+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:13.491+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:13.492+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:13.493+0000] {spark_submit.py:495} INFO - "name" : "Fossil Gas - Actual Aggregated",
[2023-01-31T07:45:13.493+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:13.494+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:13.495+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:13.495+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:13.495+0000] {spark_submit.py:495} INFO - "name" : "Fossil Hard coal - Actual Aggregated",
[2023-01-31T07:45:13.499+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:13.500+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:13.503+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:13.507+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:13.507+0000] {spark_submit.py:495} INFO - "name" : "Fossil Oil - Actual Aggregated",
[2023-01-31T07:45:13.508+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:13.510+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:13.511+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:13.511+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:13.513+0000] {spark_submit.py:495} INFO - "name" : "Geothermal - Actual Aggregated",
[2023-01-31T07:45:13.515+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:13.518+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:13.525+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:13.529+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:13.530+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Aggregated",
[2023-01-31T07:45:13.530+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:13.545+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:13.546+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:13.546+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:13.548+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Consumption",
[2023-01-31T07:45:13.548+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:13.549+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:13.551+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:13.552+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:13.552+0000] {spark_submit.py:495} INFO - "name" : "Hydro Run-of-river and poundage - Actual Aggregated",
[2023-01-31T07:45:13.553+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:13.553+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:13.554+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:13.555+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:13.555+0000] {spark_submit.py:495} INFO - "name" : "Hydro Water Reservoir - Actual Aggregated",
[2023-01-31T07:45:13.555+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:13.557+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:13.557+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:13.557+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:13.562+0000] {spark_submit.py:495} INFO - "name" : "Nuclear - Actual Aggregated",
[2023-01-31T07:45:13.562+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:13.562+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:13.565+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:13.567+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:13.569+0000] {spark_submit.py:495} INFO - "name" : "Other - Actual Aggregated",
[2023-01-31T07:45:13.569+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:13.569+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:13.571+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:13.571+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:13.571+0000] {spark_submit.py:495} INFO - "name" : "Other renewable - Actual Aggregated",
[2023-01-31T07:45:13.571+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:13.573+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:13.573+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:13.573+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:13.576+0000] {spark_submit.py:495} INFO - "name" : "Solar - Actual Aggregated",
[2023-01-31T07:45:13.576+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:13.576+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:13.578+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:13.581+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:13.582+0000] {spark_submit.py:495} INFO - "name" : "Waste - Actual Aggregated",
[2023-01-31T07:45:13.582+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:13.587+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:13.587+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:13.590+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:13.590+0000] {spark_submit.py:495} INFO - "name" : "Wind Offshore - Actual Aggregated",
[2023-01-31T07:45:13.591+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:13.591+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:13.591+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:13.591+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:13.592+0000] {spark_submit.py:495} INFO - "name" : "Wind Onshore - Actual Aggregated",
[2023-01-31T07:45:13.592+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:13.592+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:13.592+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:13.592+0000] {spark_submit.py:495} INFO - } ]
[2023-01-31T07:45:13.592+0000] {spark_submit.py:495} INFO - }
[2023-01-31T07:45:13.593+0000] {spark_submit.py:495} INFO - and corresponding Parquet message type:
[2023-01-31T07:45:13.593+0000] {spark_submit.py:495} INFO - message spark_schema {
[2023-01-31T07:45:13.593+0000] {spark_submit.py:495} INFO - required int96 measured_at;
[2023-01-31T07:45:13.593+0000] {spark_submit.py:495} INFO - required int96 created_at;
[2023-01-31T07:45:13.594+0000] {spark_submit.py:495} INFO - optional double Biomass - Actual Aggregated;
[2023-01-31T07:45:13.594+0000] {spark_submit.py:495} INFO - optional double Fossil Brown coal/Lignite - Actual Aggregated;
[2023-01-31T07:45:13.594+0000] {spark_submit.py:495} INFO - optional double Fossil Gas - Actual Aggregated;
[2023-01-31T07:45:13.594+0000] {spark_submit.py:495} INFO - optional double Fossil Hard coal - Actual Aggregated;
[2023-01-31T07:45:13.594+0000] {spark_submit.py:495} INFO - optional double Fossil Oil - Actual Aggregated;
[2023-01-31T07:45:13.594+0000] {spark_submit.py:495} INFO - optional double Geothermal - Actual Aggregated;
[2023-01-31T07:45:13.595+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Aggregated;
[2023-01-31T07:45:13.595+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Consumption;
[2023-01-31T07:45:13.595+0000] {spark_submit.py:495} INFO - optional double Hydro Run-of-river and poundage - Actual Aggregated;
[2023-01-31T07:45:13.595+0000] {spark_submit.py:495} INFO - optional double Hydro Water Reservoir - Actual Aggregated;
[2023-01-31T07:45:13.595+0000] {spark_submit.py:495} INFO - optional double Nuclear - Actual Aggregated;
[2023-01-31T07:45:13.596+0000] {spark_submit.py:495} INFO - optional double Other - Actual Aggregated;
[2023-01-31T07:45:13.596+0000] {spark_submit.py:495} INFO - optional double Other renewable - Actual Aggregated;
[2023-01-31T07:45:13.596+0000] {spark_submit.py:495} INFO - optional double Solar - Actual Aggregated;
[2023-01-31T07:45:13.596+0000] {spark_submit.py:495} INFO - optional double Waste - Actual Aggregated;
[2023-01-31T07:45:13.596+0000] {spark_submit.py:495} INFO - optional double Wind Offshore - Actual Aggregated;
[2023-01-31T07:45:13.596+0000] {spark_submit.py:495} INFO - optional double Wind Onshore - Actual Aggregated;
[2023-01-31T07:45:13.597+0000] {spark_submit.py:495} INFO - }
[2023-01-31T07:45:13.597+0000] {spark_submit.py:495} INFO - 
[2023-01-31T07:45:13.597+0000] {spark_submit.py:495} INFO - 
[2023-01-31T07:45:14.397+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:14 INFO CodecPool: Got brand-new compressor [.snappy]
[2023-01-31T07:45:15.132+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:15 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 81d5fcd0285b:39809 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:45:20.059+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:20 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675150846325-665136dd-3266-45e2-aa62-4ee88a6496d9/_temporary/0/_temporary/' directory.
[2023-01-31T07:45:20.068+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:20 INFO FileOutputCommitter: Saved output of task 'attempt_202301310745116804079838005475834_0008_m_000000_8' to gs://entsoe_temp_1009/.spark-bigquery-local-1675150846325-665136dd-3266-45e2-aa62-4ee88a6496d9/_temporary/0/task_202301310745116804079838005475834_0008_m_000000
[2023-01-31T07:45:20.081+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:20 INFO SparkHadoopMapRedUtil: attempt_202301310745116804079838005475834_0008_m_000000_8: Committed. Elapsed time: 1560 ms.
[2023-01-31T07:45:20.166+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:20 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2588 bytes result sent to driver
[2023-01-31T07:45:20.177+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:20 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 8094 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:45:20.178+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:20 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2023-01-31T07:45:20.180+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:20 INFO DAGScheduler: ResultStage 8 (save at BigQueryWriteHelper.java:105) finished in 8.496 s
[2023-01-31T07:45:20.190+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:20 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:45:20.195+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2023-01-31T07:45:20.195+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:20 INFO DAGScheduler: Job 8 finished: save at BigQueryWriteHelper.java:105, took 8.536486 s
[2023-01-31T07:45:20.212+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:20 INFO FileFormatWriter: Start to commit write Job 152683fa-d27a-463c-93f3-4fc408806323.
[2023-01-31T07:45:22.036+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:22 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675150846325-665136dd-3266-45e2-aa62-4ee88a6496d9/_temporary/0/task_202301310745116804079838005475834_0008_m_000000/' directory.
[2023-01-31T07:45:23.185+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:23 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675150846325-665136dd-3266-45e2-aa62-4ee88a6496d9/' directory.
[2023-01-31T07:45:23.785+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:23 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 81d5fcd0285b:39809 in memory (size: 74.2 KiB, free: 434.4 MiB)
[2023-01-31T07:45:24.661+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:24 INFO FileFormatWriter: Write Job 152683fa-d27a-463c-93f3-4fc408806323 committed. Elapsed time: 4439 ms.
[2023-01-31T07:45:24.690+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:24 INFO FileFormatWriter: Finished processing stats for write job 152683fa-d27a-463c-93f3-4fc408806323.
[2023-01-31T07:45:27.298+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:27 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=entsoe_analytics_1009, projectId=rafzul-analytics-1009, tableId=TEST_total_generation_staging}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=null, ignoreUnknownValue=null, sourceUris=[gs://entsoe_temp_1009/.spark-bigquery-local-1675150846325-665136dd-3266-45e2-aa62-4ee88a6496d9/part-00000-40a8bf58-b3f7-48b9-b2bf-f40620681f2b-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=true, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=rafzul-analytics-1009, job=35162d58-4b55-4a63-b613-b485f9ebdb33, location=US}
[2023-01-31T07:45:31.165+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:31 INFO BigQueryClient: Done loading to rafzul-analytics-1009.entsoe_analytics_1009.TEST_total_generation_staging. jobId: JobId{project=rafzul-analytics-1009, job=35162d58-4b55-4a63-b613-b485f9ebdb33, location=US}
[2023-01-31T07:45:32.118+0000] {spark_submit.py:495} INFO - MANTEP!!!!
[2023-01-31T07:45:32.982+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:32 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-31T07:45:33.218+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:33 INFO SparkUI: Stopped Spark web UI at http://81d5fcd0285b:4040
[2023-01-31T07:45:33.695+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:33 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-31T07:45:34.108+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:34 INFO MemoryStore: MemoryStore cleared
[2023-01-31T07:45:34.116+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:34 INFO BlockManager: BlockManager stopped
[2023-01-31T07:45:34.187+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:34 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-31T07:45:34.202+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:34 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-31T07:45:34.310+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:34 INFO SparkContext: Successfully stopped SparkContext
[2023-01-31T07:45:34.312+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:34 INFO ShutdownHookManager: Shutdown hook called
[2023-01-31T07:45:34.316+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-7ef585df-f3b3-4837-b64e-f28d62bfc0e4/pyspark-b5c84b22-6b98-4715-8cc5-60e3cf4ba380
[2023-01-31T07:45:34.349+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-4b0d0954-2afc-4fcc-b5a0-d880b389e580
[2023-01-31T07:45:34.393+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-7ef585df-f3b3-4837-b64e-f28d62bfc0e4
[2023-01-31T07:45:35.262+0000] {taskinstance.py:1322} INFO - Marking task as SUCCESS. dag_id=entsoe-energydata-backfill, task_id=stage_total_generation, execution_date=20210101T010000, start_date=20230131T073854, end_date=20230131T074535
[2023-01-31T07:45:35.403+0000] {local_task_job.py:159} INFO - Task exited with return code 0
[2023-01-31T07:45:35.486+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T08:21:35.418+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-31T08:21:35.454+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-31T08:21:35.454+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T08:21:35.455+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T08:21:35.455+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T08:21:35.501+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 01:00:00+00:00
[2023-01-31T08:21:35.526+0000] {standard_task_runner.py:55} INFO - Started process 2078 to run task
[2023-01-31T08:21:35.542+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '926', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpskmqi5jx']
[2023-01-31T08:21:35.550+0000] {standard_task_runner.py:83} INFO - Job 926: Subtask stage_total_generation
[2023-01-31T08:21:35.908+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 81d5fcd0285b
[2023-01-31T08:21:36.097+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-31T08:21:36.112+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T08:21:36.114+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-31T08:21:57.714+0000] {spark_submit.py:495} INFO - {{ start_date }}
[2023-01-31T08:21:57.715+0000] {spark_submit.py:495} INFO - mulai periode: {{ start_date }}
[2023-01-31T08:21:57.715+0000] {spark_submit.py:495} INFO - selesai periode: {{ end_date }}
[2023-01-31T08:21:58.258+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:58 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T08:21:58.653+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T08:21:59.438+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:59 INFO ResourceUtils: ==============================================================
[2023-01-31T08:21:59.441+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:59 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T08:21:59.447+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:59 INFO ResourceUtils: ==============================================================
[2023-01-31T08:21:59.453+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:59 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T08:21:59.643+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:59 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T08:21:59.679+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:59 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T08:21:59.691+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:59 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T08:22:00.140+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:00 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T08:22:00.140+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:00 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T08:22:00.147+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:00 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T08:22:00.147+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:00 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T08:22:00.153+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:00 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T08:22:02.842+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:02 INFO Utils: Successfully started service 'sparkDriver' on port 40003.
[2023-01-31T08:22:03.322+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:03 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T08:22:03.734+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:03 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T08:22:04.192+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:04 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T08:22:04.234+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:04 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T08:22:04.258+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:04 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T08:22:04.373+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:04 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-edb80f96-8984-4db1-81d2-21fee7a38ddc
[2023-01-31T08:22:04.477+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:04 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T08:22:04.622+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:04 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T08:22:08.222+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:08 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T08:22:08.232+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:08 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2023-01-31T08:22:08.244+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:08 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[2023-01-31T08:22:08.260+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:08 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[2023-01-31T08:22:08.265+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:08 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[2023-01-31T08:22:08.352+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:08 INFO Utils: Successfully started service 'SparkUI' on port 4045.
[2023-01-31T08:22:08.450+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:08 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://81d5fcd0285b:40003/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675153318220
[2023-01-31T08:22:08.451+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:08 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://81d5fcd0285b:40003/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675153318220
[2023-01-31T08:22:09.042+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:09 INFO Executor: Starting executor ID driver on host 81d5fcd0285b
[2023-01-31T08:22:09.069+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:09 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T08:22:09.195+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:09 INFO Executor: Fetching spark://81d5fcd0285b:40003/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675153318220
[2023-01-31T08:22:09.634+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:09 INFO TransportClientFactory: Successfully created connection to 81d5fcd0285b/172.19.0.5:40003 after 150 ms (0 ms spent in bootstraps)
[2023-01-31T08:22:09.723+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:09 INFO Utils: Fetching spark://81d5fcd0285b:40003/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-c4f1dfe7-d1e4-45bf-b9e7-b2fc2fba8cff/userFiles-a5548199-01fd-412e-9beb-8ade62e52be3/fetchFileTemp12406506412998728321.tmp
[2023-01-31T08:22:11.241+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:11 INFO Executor: Adding file:/tmp/spark-c4f1dfe7-d1e4-45bf-b9e7-b2fc2fba8cff/userFiles-a5548199-01fd-412e-9beb-8ade62e52be3/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T08:22:11.241+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:11 INFO Executor: Fetching spark://81d5fcd0285b:40003/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675153318220
[2023-01-31T08:22:11.241+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:11 INFO Utils: Fetching spark://81d5fcd0285b:40003/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-c4f1dfe7-d1e4-45bf-b9e7-b2fc2fba8cff/userFiles-a5548199-01fd-412e-9beb-8ade62e52be3/fetchFileTemp15104154119789702605.tmp
[2023-01-31T08:22:13.414+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:13 INFO Executor: Adding file:/tmp/spark-c4f1dfe7-d1e4-45bf-b9e7-b2fc2fba8cff/userFiles-a5548199-01fd-412e-9beb-8ade62e52be3/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T08:22:13.450+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:13 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46763.
[2023-01-31T08:22:13.451+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:13 INFO NettyBlockTransferService: Server created on 81d5fcd0285b:46763
[2023-01-31T08:22:13.458+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:13 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T08:22:13.480+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:13 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 46763, None)
[2023-01-31T08:22:13.487+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:13 INFO BlockManagerMasterEndpoint: Registering block manager 81d5fcd0285b:46763 with 434.4 MiB RAM, BlockManagerId(driver, 81d5fcd0285b, 46763, None)
[2023-01-31T08:22:13.493+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:13 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 46763, None)
[2023-01-31T08:22:13.500+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:13 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 81d5fcd0285b, 46763, None)
[2023-01-31T08:22:17.587+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:17 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T08:22:17.712+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:17 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T08:22:32.354+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:32 INFO InMemoryFileIndex: It took 458 ms to list leaf files for 1 paths.
[2023-01-31T08:22:33.658+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:33 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T08:22:34.075+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:34 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T08:22:34.082+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:34 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 81d5fcd0285b:46763 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T08:22:34.098+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:34 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T08:22:36.005+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:35 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T08:22:36.073+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:36 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T08:22:36.271+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:36 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T08:22:36.359+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:36 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T08:22:36.361+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:36 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T08:22:36.366+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:36 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:22:36.368+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:36 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:22:36.414+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:36 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T08:22:37.044+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:37 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T08:22:37.189+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:37 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T08:22:37.197+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:37 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 81d5fcd0285b:46763 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T08:22:37.240+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:37 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:22:37.419+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:22:37.421+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:37 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T08:22:37.940+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:37 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T08:22:38.089+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:38 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T08:22:39.076+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:39 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010100__202101010200.json:0+9369
[2023-01-31T08:22:40.870+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:40 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3437 bytes result sent to driver
[2023-01-31T08:22:40.923+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:40 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3166 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:22:40.936+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:40 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T08:22:40.980+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:40 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 4.427 s
[2023-01-31T08:22:41.028+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:41 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:22:41.030+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T08:22:41.042+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:41 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 4.765730 s
[2023-01-31T08:22:44.112+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:44 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 81d5fcd0285b:46763 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T08:22:44.218+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:44 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 81d5fcd0285b:46763 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T08:23:04.978+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:04 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:05.038+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:05 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T08:23:05.142+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:05 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:09.433+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:09 INFO CodeGenerator: Code generated in 1662.196272 ms
[2023-01-31T08:23:09.515+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:09 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T08:23:09.558+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:09 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T08:23:09.584+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:09 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 81d5fcd0285b:46763 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T08:23:09.590+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:09 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T08:23:09.819+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:11.992+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:11 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T08:23:12.025+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:12 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T08:23:12.026+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:12 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T08:23:12.028+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:12 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:12.029+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:12 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:12.051+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:12 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T08:23:12.175+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:12 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T08:23:12.229+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:12 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T08:23:12.233+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:12 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 81d5fcd0285b:46763 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T08:23:12.236+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:12 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:12.248+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:12.254+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:12 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T08:23:12.279+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:12 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:12.298+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:12 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T08:23:12.929+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:12 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T08:23:14.715+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:14 INFO CodeGenerator: Code generated in 1277.571713 ms
[2023-01-31T08:23:16.231+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:16 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1628 bytes result sent to driver
[2023-01-31T08:23:16.372+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:16 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 4101 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:16.386+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:16 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 4.262 s
[2023-01-31T08:23:16.390+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:16 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:16.395+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:16 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T08:23:16.396+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T08:23:16.406+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:16 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 4.381820 s
[2023-01-31T08:23:17.906+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:17 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 81d5fcd0285b:46763 in memory (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T08:23:18.409+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:18 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:18.410+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:18 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T08:23:18.415+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:18 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:19.407+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:19 INFO CodeGenerator: Code generated in 401.142291 ms
[2023-01-31T08:23:19.489+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:19 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T08:23:19.801+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:19 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T08:23:19.804+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:19 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 81d5fcd0285b:46763 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T08:23:19.834+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:19 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T08:23:19.846+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:19 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:20.441+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T08:23:20.446+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T08:23:20.447+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T08:23:20.447+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:20.447+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:20.462+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T08:23:20.582+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T08:23:20.616+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T08:23:20.664+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 81d5fcd0285b:46763 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T08:23:20.669+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:20.683+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:20.684+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T08:23:20.753+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:20.762+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T08:23:20.884+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T08:23:22.145+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:22 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1585 bytes result sent to driver
[2023-01-31T08:23:22.165+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:22 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1476 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:22.177+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:22 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 1.715 s
[2023-01-31T08:23:22.177+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:22 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:22.180+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:22 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T08:23:22.182+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T08:23:22.185+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:22 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 1.738207 s
[2023-01-31T08:23:24.377+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:24 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:24.378+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:24 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T08:23:24.378+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:24 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:24.396+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:24 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 81d5fcd0285b:46763 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T08:23:25.790+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO CodeGenerator: Code generated in 824.522459 ms
[2023-01-31T08:23:25.838+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T08:23:25.875+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T08:23:25.878+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 81d5fcd0285b:46763 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T08:23:25.879+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T08:23:25.881+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:26.097+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T08:23:26.099+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T08:23:26.100+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T08:23:26.100+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:26.100+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:26.108+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T08:23:26.185+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.7 MiB)
[2023-01-31T08:23:26.196+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.7 MiB)
[2023-01-31T08:23:26.200+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 81d5fcd0285b:46763 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T08:23:26.210+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:26.213+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:26.215+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T08:23:26.244+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:26.262+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T08:23:26.289+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T08:23:26.676+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-01-31T08:23:26.697+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 455 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:26.705+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 0.590 s
[2023-01-31T08:23:26.706+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:26.745+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T08:23:26.747+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T08:23:26.747+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 0.650006 s
[2023-01-31T08:23:27.028+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:27.029+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T08:23:27.031+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:27.245+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO CodeGenerator: Code generated in 184.276791 ms
[2023-01-31T08:23:27.327+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.5 MiB)
[2023-01-31T08:23:27.510+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T08:23:27.515+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 81d5fcd0285b:46763 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T08:23:27.519+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T08:23:27.541+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:27.873+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 81d5fcd0285b:46763 in memory (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T08:23:28.207+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T08:23:28.209+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T08:23:28.209+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T08:23:28.210+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:28.210+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:28.213+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T08:23:28.243+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.5 MiB)
[2023-01-31T08:23:28.291+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.5 MiB)
[2023-01-31T08:23:28.297+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 81d5fcd0285b:46763 (size: 7.6 KiB, free: 434.3 MiB)
[2023-01-31T08:23:28.312+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:28.341+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:28.341+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T08:23:28.344+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:28.345+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T08:23:28.424+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T08:23:28.775+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1586 bytes result sent to driver
[2023-01-31T08:23:28.785+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 441 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:28.790+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T08:23:28.792+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 0.572 s
[2023-01-31T08:23:28.795+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:28.796+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T08:23:28.797+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 0.581792 s
[2023-01-31T08:23:29.148+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 81d5fcd0285b:46763 in memory (size: 7.6 KiB, free: 434.3 MiB)
[2023-01-31T08:23:31.595+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:31.598+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T08:23:31.601+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:32.686+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO CodeGenerator: Code generated in 508.019092 ms
[2023-01-31T08:23:32.707+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.3 MiB)
[2023-01-31T08:23:32.722+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T08:23:32.724+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 81d5fcd0285b:46763 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T08:23:32.726+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T08:23:32.727+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:32.887+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T08:23:32.891+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) with 1 output partitions
[2023-01-31T08:23:32.892+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201)
[2023-01-31T08:23:32.892+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:32.892+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:32.903+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201), which has no missing parents
[2023-01-31T08:23:32.918+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.2 MiB)
[2023-01-31T08:23:32.920+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.2 MiB)
[2023-01-31T08:23:32.923+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 81d5fcd0285b:46763 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T08:23:32.943+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:32.945+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:32.949+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T08:23:32.954+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:32.957+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T08:23:32.975+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T08:23:33.216+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2559 bytes result sent to driver
[2023-01-31T08:23:33.219+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 266 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:33.219+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T08:23:33.229+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) finished in 0.316 s
[2023-01-31T08:23:33.230+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:33.230+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T08:23:33.230+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201, took 0.338473 s
[2023-01-31T08:23:33.375+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:33.379+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T08:23:33.380+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:33.904+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 81d5fcd0285b:46763 in memory (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T08:23:33.949+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO CodeGenerator: Code generated in 495.126423 ms
[2023-01-31T08:23:34.045+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 433.1 MiB)
[2023-01-31T08:23:34.264+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.0 MiB)
[2023-01-31T08:23:34.271+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 81d5fcd0285b:46763 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T08:23:34.279+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T08:23:34.339+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:34.762+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T08:23:34.766+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T08:23:34.769+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T08:23:34.769+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:34.778+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:34.800+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T08:23:34.833+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 433.0 MiB)
[2023-01-31T08:23:34.885+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 433.0 MiB)
[2023-01-31T08:23:34.894+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 81d5fcd0285b:46763 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T08:23:34.898+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:34.899+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:34.900+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T08:23:34.917+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:34.918+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T08:23:34.997+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T08:23:35.299+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:35 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-01-31T08:23:35.342+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:35 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 427 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:35.348+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:35 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 0.555 s
[2023-01-31T08:23:35.349+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:35 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:35.351+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:35 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T08:23:35.355+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T08:23:35.355+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:35 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 0.589304 s
[2023-01-31T08:23:35.751+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:35 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 81d5fcd0285b:46763 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T08:23:35.833+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:35 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:35.846+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:35 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T08:23:35.849+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:35 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:37.457+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:37 INFO CodeGenerator: Code generated in 1146.343592 ms
[2023-01-31T08:23:37.547+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:37 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.8 MiB)
[2023-01-31T08:23:37.793+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:37 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.8 MiB)
[2023-01-31T08:23:37.797+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:37 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 81d5fcd0285b:46763 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T08:23:37.810+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:37 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T08:23:37.865+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:37 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:38.261+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T08:23:38.262+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T08:23:38.263+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T08:23:38.264+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:38.266+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:38.272+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T08:23:38.289+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.8 MiB)
[2023-01-31T08:23:38.293+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.7 MiB)
[2023-01-31T08:23:38.330+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 81d5fcd0285b:46763 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T08:23:38.335+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:38.337+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:38.365+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T08:23:38.372+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:38.378+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T08:23:38.472+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-31T08:23:38.936+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1729 bytes result sent to driver
[2023-01-31T08:23:38.964+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 593 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:38.968+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T08:23:38.968+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 0.697 s
[2023-01-31T08:23:38.968+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:38.969+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T08:23:38.969+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 0.705767 s
[2023-01-31T08:23:40.409+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:40 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 81d5fcd0285b:46763 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T08:23:40.782+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:40 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 81d5fcd0285b:46763 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T08:23:44.632+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:44 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 81d5fcd0285b:46763 in memory (size: 34.5 KiB, free: 434.2 MiB)
