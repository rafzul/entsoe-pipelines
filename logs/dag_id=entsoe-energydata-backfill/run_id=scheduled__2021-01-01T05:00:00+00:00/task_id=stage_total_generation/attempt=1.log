[2023-01-31T05:11:00.447+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T05:00:00+00:00 [queued]>
[2023-01-31T05:11:00.487+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T05:00:00+00:00 [queued]>
[2023-01-31T05:11:00.488+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T05:11:00.488+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T05:11:00.489+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T05:11:00.561+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 05:00:00+00:00
[2023-01-31T05:11:00.602+0000] {standard_task_runner.py:55} INFO - Started process 12260 to run task
[2023-01-31T05:11:00.625+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T05:00:00+00:00', '--job-id', '881', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpz9en7df0']
[2023-01-31T05:11:00.635+0000] {standard_task_runner.py:83} INFO - Job 881: Subtask stage_total_generation
[2023-01-31T05:11:01.078+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T05:00:00+00:00 [running]> on host 81d5fcd0285b
[2023-01-31T05:11:01.614+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T05:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T05:00:00+00:00
[2023-01-31T05:11:01.670+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T05:11:01.690+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010500 202101010600 DE_TENNET
[2023-01-31T05:11:35.068+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:35 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T05:11:36.262+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T05:11:38.066+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:38 INFO ResourceUtils: ==============================================================
[2023-01-31T05:11:38.114+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:38 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T05:11:38.145+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:38 INFO ResourceUtils: ==============================================================
[2023-01-31T05:11:38.174+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:38 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T05:11:38.456+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:38 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T05:11:38.507+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:38 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T05:11:38.518+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:38 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T05:11:39.003+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:39 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T05:11:39.007+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:39 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T05:11:39.010+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:39 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T05:11:39.013+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:39 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T05:11:39.015+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:39 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T05:11:42.339+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:42 INFO Utils: Successfully started service 'sparkDriver' on port 41493.
[2023-01-31T05:11:42.794+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:42 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T05:11:43.965+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:43 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T05:11:44.418+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:44 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T05:11:44.434+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:44 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T05:11:44.485+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:44 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T05:11:44.693+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:44 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-8c72a8b1-3278-4d9d-897f-9be613b8bc0d
[2023-01-31T05:11:44.863+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:44 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T05:11:45.086+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:45 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T05:11:47.566+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:47 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T05:11:47.581+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:47 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2023-01-31T05:11:47.588+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:47 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[2023-01-31T05:11:47.590+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:47 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[2023-01-31T05:11:47.604+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:47 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[2023-01-31T05:11:47.698+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:47 INFO Utils: Successfully started service 'SparkUI' on port 4045.
[2023-01-31T05:11:48.031+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:48 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://81d5fcd0285b:41493/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675141894991
[2023-01-31T05:11:48.034+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:48 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://81d5fcd0285b:41493/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675141894991
[2023-01-31T05:11:48.822+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:48 INFO Executor: Starting executor ID driver on host 81d5fcd0285b
[2023-01-31T05:11:49.029+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:49 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T05:11:49.592+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:49 INFO Executor: Fetching spark://81d5fcd0285b:41493/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675141894991
[2023-01-31T05:11:50.494+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:50 INFO TransportClientFactory: Successfully created connection to 81d5fcd0285b/172.19.0.9:41493 after 421 ms (0 ms spent in bootstraps)
[2023-01-31T05:11:50.643+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:50 INFO Utils: Fetching spark://81d5fcd0285b:41493/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-94da920d-e815-4344-8642-800cfae16f24/userFiles-e5733af0-6618-4fac-b475-3ee6f2edb7d8/fetchFileTemp16399849639890025439.tmp
[2023-01-31T05:11:53.234+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:53 INFO Executor: Adding file:/tmp/spark-94da920d-e815-4344-8642-800cfae16f24/userFiles-e5733af0-6618-4fac-b475-3ee6f2edb7d8/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T05:11:53.238+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:53 INFO Executor: Fetching spark://81d5fcd0285b:41493/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675141894991
[2023-01-31T05:11:53.252+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:53 INFO Utils: Fetching spark://81d5fcd0285b:41493/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-94da920d-e815-4344-8642-800cfae16f24/userFiles-e5733af0-6618-4fac-b475-3ee6f2edb7d8/fetchFileTemp1255801949497566479.tmp
[2023-01-31T05:11:54.280+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:54 INFO Executor: Adding file:/tmp/spark-94da920d-e815-4344-8642-800cfae16f24/userFiles-e5733af0-6618-4fac-b475-3ee6f2edb7d8/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T05:11:54.327+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:54 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34735.
[2023-01-31T05:11:54.327+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:54 INFO NettyBlockTransferService: Server created on 81d5fcd0285b:34735
[2023-01-31T05:11:54.335+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:54 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T05:11:54.377+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:54 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 34735, None)
[2023-01-31T05:11:54.407+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:54 INFO BlockManagerMasterEndpoint: Registering block manager 81d5fcd0285b:34735 with 434.4 MiB RAM, BlockManagerId(driver, 81d5fcd0285b, 34735, None)
[2023-01-31T05:11:54.452+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:54 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 34735, None)
[2023-01-31T05:11:54.459+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:54 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 81d5fcd0285b, 34735, None)
[2023-01-31T05:12:00.662+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:00 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T05:12:00.710+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:00 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T05:12:22.085+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:22 INFO InMemoryFileIndex: It took 944 ms to list leaf files for 1 paths.
[2023-01-31T05:12:23.708+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:23 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T05:12:24.307+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:24 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T05:12:24.323+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:24 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 81d5fcd0285b:34735 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T05:12:24.391+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:24 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T05:12:29.241+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:29 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T05:12:29.524+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:29 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T05:12:29.813+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:29 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T05:12:30.032+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:30 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T05:12:30.038+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:30 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T05:12:30.047+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:30 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:12:30.084+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:30 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:12:30.210+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:30 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T05:12:31.101+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:31 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T05:12:31.225+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:31 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T05:12:31.225+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:31 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 81d5fcd0285b:34735 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T05:12:31.242+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:31 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:12:31.437+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:12:31.461+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:31 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T05:12:33.897+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:33 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T05:12:34.194+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:34 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T05:12:35.253+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:35 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010500__202101010600.json:0+9367
[2023-01-31T05:12:37.892+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:37 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3480 bytes result sent to driver
[2023-01-31T05:12:38.120+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:38 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 5155 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:12:38.161+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:38 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T05:12:38.259+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:38 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 7.790 s
[2023-01-31T05:12:38.349+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:38 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:12:38.350+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T05:12:38.398+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:38 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 8.575620 s
[2023-01-31T05:12:45.901+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:45 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 81d5fcd0285b:34735 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T05:12:46.088+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:46 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 81d5fcd0285b:34735 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T05:13:12.762+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:12 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:12.765+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:12 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:13:12.779+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:12 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:15.251+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO CodeGenerator: Code generated in 1134.832668 ms
[2023-01-31T05:13:15.307+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T05:13:15.425+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T05:13:15.428+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 81d5fcd0285b:34735 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T05:13:15.434+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T05:13:15.535+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:15.884+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T05:13:15.892+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T05:13:15.892+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T05:13:15.895+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:15.896+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:15.905+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T05:13:15.948+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T05:13:15.978+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T05:13:15.995+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 81d5fcd0285b:34735 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T05:13:16.002+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:16 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:16.005+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:16.007+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:16 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T05:13:16.041+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:16 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:16.049+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:16 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T05:13:16.559+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:16 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T05:13:18.071+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO CodeGenerator: Code generated in 1202.11899 ms
[2023-01-31T05:13:19.637+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1628 bytes result sent to driver
[2023-01-31T05:13:19.655+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 3637 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:19.677+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T05:13:19.684+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 3.757 s
[2023-01-31T05:13:19.696+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:19.697+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T05:13:19.698+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 3.806432 s
[2023-01-31T05:13:20.372+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:20.378+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:13:20.379+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:20.669+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO CodeGenerator: Code generated in 128.413643 ms
[2023-01-31T05:13:20.746+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T05:13:20.925+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T05:13:20.928+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 81d5fcd0285b:34735 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:13:20.945+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T05:13:20.983+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:21.111+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T05:13:21.112+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T05:13:21.114+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T05:13:21.115+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:21.115+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:21.122+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T05:13:21.132+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T05:13:21.186+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T05:13:21.192+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 81d5fcd0285b:34735 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T05:13:21.200+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:21.212+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:21.217+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T05:13:21.225+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:21.228+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T05:13:21.296+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T05:13:21.584+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1585 bytes result sent to driver
[2023-01-31T05:13:21.609+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 385 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:21.610+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T05:13:21.615+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 0.489 s
[2023-01-31T05:13:21.616+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:21.623+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T05:13:21.625+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 0.513950 s
[2023-01-31T05:13:22.555+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:22.563+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T05:13:22.566+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:23.808+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO CodeGenerator: Code generated in 740.084069 ms
[2023-01-31T05:13:23.821+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T05:13:23.916+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T05:13:23.919+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 81d5fcd0285b:34735 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:13:23.924+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T05:13:23.935+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:24.359+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T05:13:24.367+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T05:13:24.368+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T05:13:24.370+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:24.371+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:24.380+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T05:13:24.447+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.6 MiB)
[2023-01-31T05:13:24.511+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.6 MiB)
[2023-01-31T05:13:24.522+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 81d5fcd0285b:34735 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T05:13:24.536+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:24.548+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:24.549+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T05:13:24.594+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:24.599+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T05:13:24.838+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T05:13:25.276+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-01-31T05:13:25.288+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 694 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:25.289+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T05:13:25.290+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 0.902 s
[2023-01-31T05:13:25.291+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:25.292+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T05:13:25.292+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 0.931345 s
[2023-01-31T05:13:25.541+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:25.542+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:13:25.547+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:25.741+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO CodeGenerator: Code generated in 109.76961 ms
[2023-01-31T05:13:25.783+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.4 MiB)
[2023-01-31T05:13:26.058+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T05:13:26.063+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 81d5fcd0285b:34735 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:13:26.076+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T05:13:26.101+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:26.538+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 81d5fcd0285b:34735 in memory (size: 7.7 KiB, free: 434.2 MiB)
[2023-01-31T05:13:26.609+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T05:13:26.616+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 81d5fcd0285b:34735 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T05:13:26.620+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T05:13:26.621+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T05:13:26.621+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:26.627+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:26.647+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T05:13:26.667+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.5 MiB)
[2023-01-31T05:13:26.674+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 81d5fcd0285b:34735 in memory (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T05:13:26.683+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.5 MiB)
[2023-01-31T05:13:26.696+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 81d5fcd0285b:34735 (size: 7.6 KiB, free: 434.3 MiB)
[2023-01-31T05:13:26.699+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:26.710+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:26.717+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T05:13:26.724+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:26.726+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T05:13:26.859+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T05:13:27.227+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:27 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1586 bytes result sent to driver
[2023-01-31T05:13:27.282+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:27 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 548 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:27.283+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:27 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T05:13:27.289+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:27 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 0.621 s
[2023-01-31T05:13:27.289+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:27 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:27.293+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T05:13:27.294+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:27 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 0.662104 s
[2023-01-31T05:13:29.234+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:29.235+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T05:13:29.236+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:29.955+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO CodeGenerator: Code generated in 528.815265 ms
[2023-01-31T05:13:29.997+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.3 MiB)
[2023-01-31T05:13:30.112+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T05:13:30.125+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 81d5fcd0285b:34735 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:13:30.134+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T05:13:30.170+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:30.414+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T05:13:30.416+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) with 1 output partitions
[2023-01-31T05:13:30.417+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201)
[2023-01-31T05:13:30.418+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:30.419+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:30.428+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201), which has no missing parents
[2023-01-31T05:13:30.450+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.2 MiB)
[2023-01-31T05:13:30.471+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.2 MiB)
[2023-01-31T05:13:30.478+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 81d5fcd0285b:34735 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T05:13:30.482+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:30.499+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:30.500+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T05:13:30.506+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:30.508+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T05:13:30.616+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T05:13:31.244+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:31 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2533 bytes result sent to driver
[2023-01-31T05:13:31.264+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:31 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 757 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:31.265+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:31 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T05:13:31.273+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:31 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) finished in 0.838 s
[2023-01-31T05:13:31.276+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:31 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:31.300+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T05:13:31.301+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:31 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201, took 0.879550 s
[2023-01-31T05:13:31.632+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:31 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:31.635+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:31 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T05:13:31.637+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:31 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:32.086+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:32 INFO CodeGenerator: Code generated in 374.686148 ms
[2023-01-31T05:13:32.181+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:32 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 433.0 MiB)
[2023-01-31T05:13:32.656+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:32 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.9 MiB)
[2023-01-31T05:13:32.680+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:32 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 81d5fcd0285b:34735 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:13:32.682+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:32 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T05:13:32.888+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:32 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:33.749+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T05:13:33.756+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T05:13:33.758+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T05:13:33.759+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:33.762+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:33.774+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T05:13:33.839+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 432.9 MiB)
[2023-01-31T05:13:33.871+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.9 MiB)
[2023-01-31T05:13:33.879+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 81d5fcd0285b:34735 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T05:13:33.897+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:33.913+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:33.915+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T05:13:33.931+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:33.937+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T05:13:33.970+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T05:13:34.223+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:34 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-01-31T05:13:34.231+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:34 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 300 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:34.236+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:34 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T05:13:34.237+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:34 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 0.444 s
[2023-01-31T05:13:34.238+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:34 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:34.241+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T05:13:34.244+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:34 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 0.493065 s
[2023-01-31T05:13:35.082+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:35 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:35.102+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:35 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T05:13:35.102+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:35 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:36.229+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO CodeGenerator: Code generated in 519.344909 ms
[2023-01-31T05:13:36.255+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.7 MiB)
[2023-01-31T05:13:36.361+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.7 MiB)
[2023-01-31T05:13:36.363+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 81d5fcd0285b:34735 (size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T05:13:36.368+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T05:13:36.379+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:36.724+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T05:13:36.728+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T05:13:36.729+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T05:13:36.730+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:36.731+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:36.743+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T05:13:36.802+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.6 MiB)
[2023-01-31T05:13:36.814+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.6 MiB)
[2023-01-31T05:13:36.816+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 81d5fcd0285b:34735 (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T05:13:36.818+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:36.820+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:36.822+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T05:13:36.826+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:36.828+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T05:13:36.854+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T05:13:37.205+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:37 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1729 bytes result sent to driver
[2023-01-31T05:13:37.205+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:37 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 376 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:37.222+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:37 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T05:13:37.227+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:37 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 0.457 s
[2023-01-31T05:13:37.227+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:37 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:37.228+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T05:13:37.232+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:37 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 0.500631 s
[2023-01-31T05:13:40.276+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:40 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 81d5fcd0285b:34735 in memory (size: 7.6 KiB, free: 434.1 MiB)
[2023-01-31T05:13:40.432+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:40 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 81d5fcd0285b:34735 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:13:40.500+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:40 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 81d5fcd0285b:34735 in memory (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T05:13:40.590+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:40 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 81d5fcd0285b:34735 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:13:40.646+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:40 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 81d5fcd0285b:34735 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:13:40.704+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:40 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 81d5fcd0285b:34735 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:13:40.814+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:40 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 81d5fcd0285b:34735 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:13:40.936+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:40 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 81d5fcd0285b:34735 in memory (size: 12.6 KiB, free: 434.3 MiB)
[2023-01-31T05:13:41.031+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:41 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 81d5fcd0285b:34735 in memory (size: 12.6 KiB, free: 434.3 MiB)
[2023-01-31T05:13:49.161+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:49 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 81d5fcd0285b:34735 in memory (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T05:14:04.280+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:04 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:14:04.363+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:14:04.364+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:14:04.372+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:04 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:14:04.375+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:14:04.376+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:14:04.388+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:14:05.263+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-31T05:14:05.264+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO DAGScheduler: Got job 8 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T05:14:05.265+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO DAGScheduler: Final stage: ResultStage 8 (save at BigQueryWriteHelper.java:105)
[2023-01-31T05:14:05.265+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:14:05.266+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:14:05.270+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO DAGScheduler: Submitting ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T05:14:05.432+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 209.9 KiB, free 434.0 MiB)
[2023-01-31T05:14:05.450+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 74.3 KiB, free 433.9 MiB)
[2023-01-31T05:14:05.454+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 81d5fcd0285b:34735 (size: 74.3 KiB, free: 434.3 MiB)
[2023-01-31T05:14:05.457+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:14:05.461+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:14:05.463+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2023-01-31T05:14:05.477+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7484 bytes) taskResourceAssignments Map()
[2023-01-31T05:14:05.484+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2023-01-31T05:14:05.742+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:14:05.744+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:14:05.749+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:14:05.753+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:14:05.755+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:14:05.759+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:14:05.784+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T05:14:05.798+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T05:14:05.867+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO ParquetOutputFormat: Parquet block size to 134217728
[2023-01-31T05:14:05.869+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO ParquetOutputFormat: Validation is off
[2023-01-31T05:14:05.870+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-01-31T05:14:05.871+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO ParquetOutputFormat: Parquet properties are:
[2023-01-31T05:14:05.872+0000] {spark_submit.py:495} INFO - Parquet page size to 1048576
[2023-01-31T05:14:05.872+0000] {spark_submit.py:495} INFO - Parquet dictionary page size to 1048576
[2023-01-31T05:14:05.873+0000] {spark_submit.py:495} INFO - Dictionary is true
[2023-01-31T05:14:05.873+0000] {spark_submit.py:495} INFO - Writer version is: PARQUET_1_0
[2023-01-31T05:14:05.874+0000] {spark_submit.py:495} INFO - Page size checking is: estimated
[2023-01-31T05:14:05.874+0000] {spark_submit.py:495} INFO - Min row count for page size check is: 100
[2023-01-31T05:14:05.875+0000] {spark_submit.py:495} INFO - Max row count for page size check is: 10000
[2023-01-31T05:14:05.876+0000] {spark_submit.py:495} INFO - Truncate length for column indexes is: 64
[2023-01-31T05:14:05.880+0000] {spark_submit.py:495} INFO - Truncate length for statistics min/max  is: 2147483647
[2023-01-31T05:14:05.881+0000] {spark_submit.py:495} INFO - Bloom filter enabled: false
[2023-01-31T05:14:05.884+0000] {spark_submit.py:495} INFO - Max Bloom filter size for a column is 1048576
[2023-01-31T05:14:05.884+0000] {spark_submit.py:495} INFO - Bloom filter expected number of distinct values are: null
[2023-01-31T05:14:05.887+0000] {spark_submit.py:495} INFO - Page row count limit to 20000
[2023-01-31T05:14:05.888+0000] {spark_submit.py:495} INFO - Writing page checksums is: on
[2023-01-31T05:14:06.058+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-01-31T05:14:06.059+0000] {spark_submit.py:495} INFO - {
[2023-01-31T05:14:06.059+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-31T05:14:06.060+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-31T05:14:06.060+0000] {spark_submit.py:495} INFO - "name" : "measured_at",
[2023-01-31T05:14:06.061+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T05:14:06.061+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T05:14:06.061+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:06.065+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:06.065+0000] {spark_submit.py:495} INFO - "name" : "created_at",
[2023-01-31T05:14:06.070+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T05:14:06.071+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T05:14:06.076+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:06.079+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:06.082+0000] {spark_submit.py:495} INFO - "name" : "Biomass - Actual Aggregated",
[2023-01-31T05:14:06.084+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:06.089+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:06.089+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:06.090+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:06.095+0000] {spark_submit.py:495} INFO - "name" : "Fossil Brown coal/Lignite - Actual Aggregated",
[2023-01-31T05:14:06.103+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:06.104+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:06.106+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:06.107+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:06.109+0000] {spark_submit.py:495} INFO - "name" : "Fossil Gas - Actual Aggregated",
[2023-01-31T05:14:06.110+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:06.111+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:06.112+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:06.113+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:06.114+0000] {spark_submit.py:495} INFO - "name" : "Fossil Hard coal - Actual Aggregated",
[2023-01-31T05:14:06.114+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:06.119+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:06.122+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:06.123+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:06.125+0000] {spark_submit.py:495} INFO - "name" : "Fossil Oil - Actual Aggregated",
[2023-01-31T05:14:06.126+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:06.129+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:06.130+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:06.137+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:06.137+0000] {spark_submit.py:495} INFO - "name" : "Geothermal - Actual Aggregated",
[2023-01-31T05:14:06.139+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:06.144+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:06.145+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:06.147+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:06.149+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Aggregated",
[2023-01-31T05:14:06.151+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:06.152+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:06.154+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:06.155+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:06.157+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Consumption",
[2023-01-31T05:14:06.158+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:06.160+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:06.161+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:06.163+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:06.165+0000] {spark_submit.py:495} INFO - "name" : "Hydro Run-of-river and poundage - Actual Aggregated",
[2023-01-31T05:14:06.168+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:06.168+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:06.169+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:06.176+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:06.177+0000] {spark_submit.py:495} INFO - "name" : "Hydro Water Reservoir - Actual Aggregated",
[2023-01-31T05:14:06.179+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:06.181+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:06.189+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:06.192+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:06.197+0000] {spark_submit.py:495} INFO - "name" : "Nuclear - Actual Aggregated",
[2023-01-31T05:14:06.200+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:06.200+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:06.211+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:06.211+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:06.215+0000] {spark_submit.py:495} INFO - "name" : "Other - Actual Aggregated",
[2023-01-31T05:14:06.216+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:06.219+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:06.220+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:06.232+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:06.233+0000] {spark_submit.py:495} INFO - "name" : "Other renewable - Actual Aggregated",
[2023-01-31T05:14:06.233+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:06.234+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:06.234+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:06.234+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:06.235+0000] {spark_submit.py:495} INFO - "name" : "Solar - Actual Aggregated",
[2023-01-31T05:14:06.235+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:06.235+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:06.235+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:06.236+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:06.236+0000] {spark_submit.py:495} INFO - "name" : "Waste - Actual Aggregated",
[2023-01-31T05:14:06.256+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:06.257+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:06.257+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:06.258+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:06.258+0000] {spark_submit.py:495} INFO - "name" : "Wind Offshore - Actual Aggregated",
[2023-01-31T05:14:06.288+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:06.288+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:06.288+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:06.289+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:06.289+0000] {spark_submit.py:495} INFO - "name" : "Wind Onshore - Actual Aggregated",
[2023-01-31T05:14:06.289+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:06.290+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:06.290+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:06.290+0000] {spark_submit.py:495} INFO - } ]
[2023-01-31T05:14:06.304+0000] {spark_submit.py:495} INFO - }
[2023-01-31T05:14:06.305+0000] {spark_submit.py:495} INFO - and corresponding Parquet message type:
[2023-01-31T05:14:06.312+0000] {spark_submit.py:495} INFO - message spark_schema {
[2023-01-31T05:14:06.312+0000] {spark_submit.py:495} INFO - required int96 measured_at;
[2023-01-31T05:14:06.317+0000] {spark_submit.py:495} INFO - required int96 created_at;
[2023-01-31T05:14:06.318+0000] {spark_submit.py:495} INFO - optional double Biomass - Actual Aggregated;
[2023-01-31T05:14:06.321+0000] {spark_submit.py:495} INFO - optional double Fossil Brown coal/Lignite - Actual Aggregated;
[2023-01-31T05:14:06.323+0000] {spark_submit.py:495} INFO - optional double Fossil Gas - Actual Aggregated;
[2023-01-31T05:14:06.324+0000] {spark_submit.py:495} INFO - optional double Fossil Hard coal - Actual Aggregated;
[2023-01-31T05:14:06.327+0000] {spark_submit.py:495} INFO - optional double Fossil Oil - Actual Aggregated;
[2023-01-31T05:14:06.329+0000] {spark_submit.py:495} INFO - optional double Geothermal - Actual Aggregated;
[2023-01-31T05:14:06.332+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Aggregated;
[2023-01-31T05:14:06.334+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Consumption;
[2023-01-31T05:14:06.336+0000] {spark_submit.py:495} INFO - optional double Hydro Run-of-river and poundage - Actual Aggregated;
[2023-01-31T05:14:06.338+0000] {spark_submit.py:495} INFO - optional double Hydro Water Reservoir - Actual Aggregated;
[2023-01-31T05:14:06.341+0000] {spark_submit.py:495} INFO - optional double Nuclear - Actual Aggregated;
[2023-01-31T05:14:06.342+0000] {spark_submit.py:495} INFO - optional double Other - Actual Aggregated;
[2023-01-31T05:14:06.343+0000] {spark_submit.py:495} INFO - optional double Other renewable - Actual Aggregated;
[2023-01-31T05:14:06.346+0000] {spark_submit.py:495} INFO - optional double Solar - Actual Aggregated;
[2023-01-31T05:14:06.348+0000] {spark_submit.py:495} INFO - optional double Waste - Actual Aggregated;
[2023-01-31T05:14:06.349+0000] {spark_submit.py:495} INFO - optional double Wind Offshore - Actual Aggregated;
[2023-01-31T05:14:06.351+0000] {spark_submit.py:495} INFO - optional double Wind Onshore - Actual Aggregated;
[2023-01-31T05:14:06.353+0000] {spark_submit.py:495} INFO - }
[2023-01-31T05:14:06.354+0000] {spark_submit.py:495} INFO - 
[2023-01-31T05:14:06.356+0000] {spark_submit.py:495} INFO - 
[2023-01-31T05:14:06.806+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO CodecPool: Got brand-new compressor [.snappy]
[2023-01-31T05:14:15.093+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:15 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675141908356-6653d558-7645-43d6-affb-7abe4529b5af/_temporary/0/_temporary/' directory.
[2023-01-31T05:14:15.094+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:15 INFO FileOutputCommitter: Saved output of task 'attempt_202301310514053039662798403541495_0008_m_000000_8' to gs://entsoe_temp_1009/.spark-bigquery-local-1675141908356-6653d558-7645-43d6-affb-7abe4529b5af/_temporary/0/task_202301310514053039662798403541495_0008_m_000000
[2023-01-31T05:14:15.102+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:15 INFO SparkHadoopMapRedUtil: attempt_202301310514053039662798403541495_0008_m_000000_8: Committed. Elapsed time: 2716 ms.
[2023-01-31T05:14:15.136+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:15 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2588 bytes result sent to driver
[2023-01-31T05:14:15.143+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:15 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 9674 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:14:15.153+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:15 INFO DAGScheduler: ResultStage 8 (save at BigQueryWriteHelper.java:105) finished in 9.864 s
[2023-01-31T05:14:15.153+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:15 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:14:15.159+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:15 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2023-01-31T05:14:15.162+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2023-01-31T05:14:15.170+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:15 INFO DAGScheduler: Job 8 finished: save at BigQueryWriteHelper.java:105, took 9.905144 s
[2023-01-31T05:14:15.180+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:15 INFO FileFormatWriter: Start to commit write Job 955badd9-e9e1-48ae-a74b-145d64fd29e3.
[2023-01-31T05:14:16.266+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:16 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675141908356-6653d558-7645-43d6-affb-7abe4529b5af/_temporary/0/task_202301310514053039662798403541495_0008_m_000000/' directory.
[2023-01-31T05:14:16.652+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:16 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675141908356-6653d558-7645-43d6-affb-7abe4529b5af/' directory.
[2023-01-31T05:14:17.277+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:17 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 81d5fcd0285b:34735 in memory (size: 74.3 KiB, free: 434.4 MiB)
[2023-01-31T05:14:17.852+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:17 INFO FileFormatWriter: Write Job 955badd9-e9e1-48ae-a74b-145d64fd29e3 committed. Elapsed time: 2665 ms.
[2023-01-31T05:14:17.860+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:17 INFO FileFormatWriter: Finished processing stats for write job 955badd9-e9e1-48ae-a74b-145d64fd29e3.
[2023-01-31T05:14:19.150+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:19 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=entsoe_analytics_1009, projectId=rafzul-analytics-1009, tableId=TEST_total_generation_staging}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=null, ignoreUnknownValue=null, sourceUris=[gs://entsoe_temp_1009/.spark-bigquery-local-1675141908356-6653d558-7645-43d6-affb-7abe4529b5af/part-00000-b8463702-a4ec-41eb-8d0d-fc35e7016dc4-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=true, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=rafzul-analytics-1009, job=f6a0b178-6ccf-40a8-8494-be45839c2d26, location=US}
[2023-01-31T05:14:22.960+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:22 INFO BigQueryClient: Done loading to rafzul-analytics-1009.entsoe_analytics_1009.TEST_total_generation_staging. jobId: JobId{project=rafzul-analytics-1009, job=f6a0b178-6ccf-40a8-8494-be45839c2d26, location=US}
[2023-01-31T05:14:23.456+0000] {spark_submit.py:495} INFO - MANTEP!!!!
[2023-01-31T05:14:23.544+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-31T05:14:23.554+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO SparkUI: Stopped Spark web UI at http://81d5fcd0285b:4045
[2023-01-31T05:14:23.565+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-31T05:14:23.579+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO MemoryStore: MemoryStore cleared
[2023-01-31T05:14:23.580+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO BlockManager: BlockManager stopped
[2023-01-31T05:14:23.582+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-31T05:14:23.586+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-31T05:14:23.601+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO SparkContext: Successfully stopped SparkContext
[2023-01-31T05:14:23.601+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO ShutdownHookManager: Shutdown hook called
[2023-01-31T05:14:23.602+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-94da920d-e815-4344-8642-800cfae16f24
[2023-01-31T05:14:23.607+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-6d4e36ab-06a1-4843-9fe7-d85513c8ad8b
[2023-01-31T05:14:23.611+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-94da920d-e815-4344-8642-800cfae16f24/pyspark-e508f256-75bb-47ba-bc55-ab096d2a8eaa
[2023-01-31T05:14:23.736+0000] {taskinstance.py:1322} INFO - Marking task as SUCCESS. dag_id=entsoe-energydata-backfill, task_id=stage_total_generation, execution_date=20210101T050000, start_date=20230131T051100, end_date=20230131T051423
[2023-01-31T05:14:23.799+0000] {local_task_job.py:159} INFO - Task exited with return code 0
[2023-01-31T05:14:23.819+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T05:30:46.201+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T05:00:00+00:00 [queued]>
[2023-01-31T05:30:46.283+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T05:00:00+00:00 [queued]>
[2023-01-31T05:30:46.284+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T05:30:46.285+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T05:30:46.286+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T05:30:46.376+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 05:00:00+00:00
[2023-01-31T05:30:46.462+0000] {standard_task_runner.py:55} INFO - Started process 15813 to run task
[2023-01-31T05:30:46.550+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T05:00:00+00:00', '--job-id', '898', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmphp88hj9o']
[2023-01-31T05:30:46.563+0000] {standard_task_runner.py:83} INFO - Job 898: Subtask stage_total_generation
[2023-01-31T05:30:47.536+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T05:00:00+00:00 [running]> on host 81d5fcd0285b
[2023-01-31T05:30:48.263+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T05:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T05:00:00+00:00
[2023-01-31T05:30:48.335+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T05:30:48.353+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010500 202101010600 DE_TENNET
[2023-01-31T05:32:14.957+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:14 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T05:32:16.662+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T05:32:19.545+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:19 INFO ResourceUtils: ==============================================================
[2023-01-31T05:32:19.550+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:19 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T05:32:19.559+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:19 INFO ResourceUtils: ==============================================================
[2023-01-31T05:32:19.565+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:19 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T05:32:20.244+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:20 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T05:32:20.390+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:20 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T05:32:20.473+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:20 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T05:32:21.602+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:21 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T05:32:21.612+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:21 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T05:32:21.621+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:21 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T05:32:21.631+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:21 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T05:32:21.645+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:21 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T05:32:27.699+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:27 INFO Utils: Successfully started service 'sparkDriver' on port 46605.
[2023-01-31T05:32:29.020+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:29 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T05:32:29.839+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:29 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T05:32:30.574+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:30 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T05:32:30.619+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:30 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T05:32:30.772+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:30 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T05:32:31.736+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:31 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-54c65443-8a1e-4714-a824-957ff7540c7c
[2023-01-31T05:32:32.379+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:32 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T05:32:32.928+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:32 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T05:32:39.140+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:39 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T05:32:39.180+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:39 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2023-01-31T05:32:39.184+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:39 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[2023-01-31T05:32:39.239+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:39 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[2023-01-31T05:32:39.292+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:39 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[2023-01-31T05:32:39.353+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:39 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
[2023-01-31T05:32:39.678+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:39 INFO Utils: Successfully started service 'SparkUI' on port 4046.
[2023-01-31T05:32:40.911+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:40 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://81d5fcd0285b:46605/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675143134623
[2023-01-31T05:32:40.922+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:40 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://81d5fcd0285b:46605/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675143134623
[2023-01-31T05:32:43.297+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:43 INFO Executor: Starting executor ID driver on host 81d5fcd0285b
[2023-01-31T05:32:43.714+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:43 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T05:32:43.847+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:43 INFO Executor: Fetching spark://81d5fcd0285b:46605/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675143134623
[2023-01-31T05:32:45.235+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:45 INFO TransportClientFactory: Successfully created connection to 81d5fcd0285b/172.19.0.9:46605 after 1135 ms (0 ms spent in bootstraps)
[2023-01-31T05:32:45.319+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:45 INFO Utils: Fetching spark://81d5fcd0285b:46605/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-806ea5d6-cda6-4853-a8ce-05eec2876145/userFiles-db16dbc4-d3ea-4e13-9454-f0d0d5b02054/fetchFileTemp631846256595958036.tmp
[2023-01-31T05:32:50.659+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:50 INFO Executor: Adding file:/tmp/spark-806ea5d6-cda6-4853-a8ce-05eec2876145/userFiles-db16dbc4-d3ea-4e13-9454-f0d0d5b02054/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T05:32:50.673+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:50 INFO Executor: Fetching spark://81d5fcd0285b:46605/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675143134623
[2023-01-31T05:32:50.701+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:50 INFO Utils: Fetching spark://81d5fcd0285b:46605/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-806ea5d6-cda6-4853-a8ce-05eec2876145/userFiles-db16dbc4-d3ea-4e13-9454-f0d0d5b02054/fetchFileTemp7083097174979796369.tmp
[2023-01-31T05:32:54.065+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:54 INFO Executor: Adding file:/tmp/spark-806ea5d6-cda6-4853-a8ce-05eec2876145/userFiles-db16dbc4-d3ea-4e13-9454-f0d0d5b02054/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T05:32:54.269+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:54 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43583.
[2023-01-31T05:32:54.270+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:54 INFO NettyBlockTransferService: Server created on 81d5fcd0285b:43583
[2023-01-31T05:32:54.308+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:54 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T05:32:54.582+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:54 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 43583, None)
[2023-01-31T05:32:54.689+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:54 INFO BlockManagerMasterEndpoint: Registering block manager 81d5fcd0285b:43583 with 434.4 MiB RAM, BlockManagerId(driver, 81d5fcd0285b, 43583, None)
[2023-01-31T05:32:54.727+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:54 INFO Executor: Told to re-register on heartbeat
[2023-01-31T05:32:54.731+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:54 INFO BlockManager: BlockManager null re-registering with master
[2023-01-31T05:32:54.734+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:54 INFO BlockManagerMaster: Registering BlockManager null
[2023-01-31T05:32:54.753+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:54 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 43583, None)
[2023-01-31T05:32:54.787+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:54 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 81d5fcd0285b, 43583, None)
[2023-01-31T05:32:54.802+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:54 WARN Executor: Issue communicating with driver in heartbeater
[2023-01-31T05:32:54.802+0000] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-01-31T05:32:54.803+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-01-31T05:32:54.804+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-01-31T05:32:54.804+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2023-01-31T05:32:54.809+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2023-01-31T05:32:54.810+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:79)
[2023-01-31T05:32:54.810+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:643)
[2023-01-31T05:32:54.811+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2023-01-31T05:32:54.811+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2023-01-31T05:32:54.812+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
[2023-01-31T05:32:54.828+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2023-01-31T05:32:54.832+0000] {spark_submit.py:495} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2023-01-31T05:32:54.833+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-01-31T05:32:54.834+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2023-01-31T05:32:54.834+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2023-01-31T05:32:54.835+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-31T05:32:54.836+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-31T05:32:54.836+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-31T05:32:54.853+0000] {spark_submit.py:495} INFO - Caused by: java.lang.NullPointerException
[2023-01-31T05:32:54.854+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)
[2023-01-31T05:32:54.854+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)
[2023-01-31T05:32:54.855+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-01-31T05:32:54.856+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-01-31T05:32:54.856+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-01-31T05:32:54.875+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-01-31T05:32:54.876+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-01-31T05:32:54.885+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-01-31T05:32:54.886+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-01-31T05:32:54.886+0000] {spark_submit.py:495} INFO - ... 3 more
[2023-01-31T05:32:54.887+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:54 ERROR Inbox: Ignoring error
[2023-01-31T05:32:54.887+0000] {spark_submit.py:495} INFO - java.lang.NullPointerException
[2023-01-31T05:32:54.888+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)
[2023-01-31T05:32:54.900+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)
[2023-01-31T05:32:54.901+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-01-31T05:32:54.902+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-01-31T05:32:54.902+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-01-31T05:32:54.903+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-01-31T05:32:54.903+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-01-31T05:32:54.904+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-01-31T05:32:54.904+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-01-31T05:32:54.913+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-31T05:32:54.914+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-31T05:32:54.914+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-31T05:33:16.001+0000] {spark_submit.py:495} INFO - 23/01/31 05:33:15 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T05:33:16.735+0000] {spark_submit.py:495} INFO - 23/01/31 05:33:16 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T05:34:25.637+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:25 INFO InMemoryFileIndex: It took 2320 ms to list leaf files for 1 paths.
[2023-01-31T05:34:27.899+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:27 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T05:34:31.645+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:31 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T05:34:31.797+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:31 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 81d5fcd0285b:43583 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T05:34:31.986+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:31 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T05:34:43.697+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:43 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T05:34:43.836+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:43 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T05:34:45.082+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:45 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T05:34:45.713+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:45 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T05:34:45.731+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:45 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T05:34:45.746+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:45 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:34:45.842+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:45 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:34:45.931+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:45 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T05:34:47.851+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:47 INFO AsyncEventQueue: Process of event SparkListenerJobStart(0,1675143285842,ArraySeq(org.apache.spark.scheduler.StageInfo@610802a9),{spark.master=local, spark.driver.port=46605, spark.submit.pyFiles=, spark.app.startTime=1675143134623, spark.rdd.compress=True, spark.executor.id=driver, spark.app.name=gcp_playground, spark.submit.deployMode=client, spark.hadoop.google.cloud.auth.service.account.json.keyfile=/.google/credentials/google_credentials.json, spark.driver.host=81d5fcd0285b, spark.app.id=local-1675143161913, spark.app.submitTime=1675143103130, spark.hadoop.fs.AbstractFileSystem.gs.impl=google.cloud.hadoop.fs.gcs.GoogleHadoopFS, spark.sql.session.timeZone=UTC, spark.hadoop.google.cloud.auth.service.account.enable=true, spark.app.initial.jar.urls=spark://81d5fcd0285b:46605/jars/gcs-connector-hadoop3-latest.jar,spark://81d5fcd0285b:46605/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar, spark.executor.extraJavaOptions=-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED, spark.jars=/opt/spark/jars/gcs-connector-hadoop3-latest.jar, /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar, spark.sql.warehouse.dir=file:/opt/***/spark-warehouse, spark.serializer.objectStreamReset=100, spark.driver.extraJavaOptions=-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED}) by listener AppStatusListener took 1.680388851s.
[2023-01-31T05:34:48.719+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:48 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T05:34:48.824+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:48 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T05:34:48.831+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:48 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 81d5fcd0285b:43583 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T05:34:48.885+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:48 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:34:49.360+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:34:49.396+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:49 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T05:34:51.726+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:51 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T05:34:52.430+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:52 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T05:34:56.120+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:56 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010500__202101010600.json:0+9367
[2023-01-31T05:35:01.896+0000] {spark_submit.py:495} INFO - 23/01/31 05:35:01 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3480 bytes result sent to driver
[2023-01-31T05:35:02.141+0000] {spark_submit.py:495} INFO - 23/01/31 05:35:02 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 11077 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:35:02.164+0000] {spark_submit.py:495} INFO - 23/01/31 05:35:02 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T05:35:02.302+0000] {spark_submit.py:495} INFO - 23/01/31 05:35:02 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 15.513 s
[2023-01-31T05:35:02.460+0000] {spark_submit.py:495} INFO - 23/01/31 05:35:02 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:35:02.476+0000] {spark_submit.py:495} INFO - 23/01/31 05:35:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T05:35:02.637+0000] {spark_submit.py:495} INFO - 23/01/31 05:35:02 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 17.474993 s
[2023-01-31T05:35:05.814+0000] {spark_submit.py:495} INFO - 23/01/31 05:35:05 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 81d5fcd0285b:43583 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T05:35:20.841+0000] {spark_submit.py:495} INFO - 23/01/31 05:35:20 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 81d5fcd0285b:43583 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T05:36:38.550+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:38 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:36:38.566+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:38 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:36:38.586+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:38 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:36:48.524+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:48 INFO CodeGenerator: Code generated in 5092.605556 ms
[2023-01-31T05:36:48.706+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:48 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T05:36:49.618+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:49 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T05:36:49.644+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:49 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 81d5fcd0285b:43583 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T05:36:49.660+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:49 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T05:36:49.809+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:49 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:36:51.679+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:51 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T05:36:51.721+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:51 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T05:36:51.722+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:51 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T05:36:51.723+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:51 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:36:51.728+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:51 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:36:51.757+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:51 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T05:36:52.297+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:52 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T05:36:52.732+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:52 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T05:36:52.741+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:52 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 81d5fcd0285b:43583 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T05:36:52.750+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:52 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:36:52.801+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:36:52.803+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:52 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T05:36:53.087+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:53 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:36:53.118+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:53 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T05:36:56.466+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:56 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T05:36:58.292+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:58 INFO CodeGenerator: Code generated in 1552.69915 ms
[2023-01-31T05:37:01.011+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:01 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1671 bytes result sent to driver
[2023-01-31T05:37:01.197+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:01 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 8291 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:37:01.208+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:01 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 9.277 s
[2023-01-31T05:37:01.259+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:01 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T05:37:01.260+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:01 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:37:01.298+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T05:37:01.305+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:01 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 9.582993 s
[2023-01-31T05:37:04.910+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:04 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:37:04.912+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:04 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:37:05.016+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:04 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:37:07.525+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:07 INFO CodeGenerator: Code generated in 1339.464601 ms
[2023-01-31T05:37:07.973+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:07 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T05:37:08.485+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:08 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T05:37:08.485+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:08 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 81d5fcd0285b:43583 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:37:08.569+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:08 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T05:37:08.833+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:08 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:37:09.371+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:09 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T05:37:09.377+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:09 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T05:37:09.378+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:09 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T05:37:09.378+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:09 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:37:09.388+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:09 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:37:09.415+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:09 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T05:37:09.969+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:09 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T05:37:10.065+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:10 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T05:37:10.077+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:10 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 81d5fcd0285b:43583 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T05:37:10.118+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:10 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:37:10.172+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:37:10.181+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:10 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T05:37:10.224+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:10 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:37:10.250+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:10 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T05:37:11.148+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T05:37:13.880+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:13 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1585 bytes result sent to driver
[2023-01-31T05:37:14.040+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:14 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 3819 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:37:14.043+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:14 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T05:37:14.099+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:14 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 4.586 s
[2023-01-31T05:37:14.242+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:14 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:37:14.243+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T05:37:14.301+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:14 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 4.888055 s
[2023-01-31T05:37:16.451+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:16 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:37:16.485+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:16 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T05:37:16.488+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:16 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:37:17.205+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:17 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 81d5fcd0285b:43583 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T05:37:17.363+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:17 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 81d5fcd0285b:43583 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T05:37:19.777+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:19 INFO CodeGenerator: Code generated in 1327.972105 ms
[2023-01-31T05:37:19.866+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:19 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T05:37:20.356+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:20 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T05:37:20.369+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:20 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 81d5fcd0285b:43583 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:37:20.466+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:20 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T05:37:20.492+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:20 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:37:20.793+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:20 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T05:37:20.810+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:20 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T05:37:20.813+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:20 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T05:37:20.815+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:20 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:37:20.824+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:20 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:37:20.840+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:20 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T05:37:20.865+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:20 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.7 MiB)
[2023-01-31T05:37:20.875+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:20 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.7 MiB)
[2023-01-31T05:37:20.886+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:20 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 81d5fcd0285b:43583 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T05:37:20.895+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:20 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:37:20.903+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:37:20.909+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:20 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T05:37:20.923+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:20 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:37:20.932+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:20 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T05:37:21.021+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:21 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T05:37:23.445+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:23 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-01-31T05:37:23.526+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:23 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 2587 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:37:23.606+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:23 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T05:37:23.610+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:23 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 2.706 s
[2023-01-31T05:37:23.652+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:23 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:37:23.705+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T05:37:23.764+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:23 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 2.922402 s
[2023-01-31T05:37:27.379+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:27 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:37:27.389+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:27 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:37:27.518+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:27 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:37:28.626+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:28 INFO CodeGenerator: Code generated in 690.927446 ms
[2023-01-31T05:37:28.968+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:28 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.5 MiB)
[2023-01-31T05:37:29.575+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T05:37:29.578+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 81d5fcd0285b:43583 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:37:29.586+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T05:37:29.659+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:37:30.713+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:30 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T05:37:30.713+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:30 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T05:37:30.714+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:30 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T05:37:30.714+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:30 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:37:30.725+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:30 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:37:30.727+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:30 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T05:37:30.882+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:30 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-31T05:37:31.081+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:31 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-01-31T05:37:31.083+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:31 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 81d5fcd0285b:43583 (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T05:37:31.109+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:31 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:37:31.112+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:37:31.112+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:31 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T05:37:31.158+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:31 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:37:31.159+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:31 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T05:37:31.196+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:31 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T05:37:32.100+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:32 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1586 bytes result sent to driver
[2023-01-31T05:37:32.114+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:32 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 960 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:37:32.122+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:32 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 1.353 s
[2023-01-31T05:37:32.123+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:32 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:37:32.141+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:32 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T05:37:32.149+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T05:37:32.150+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:32 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 1.467274 s
[2023-01-31T05:37:36.055+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:36 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 81d5fcd0285b:43583 in memory (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T05:37:36.363+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:36 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 81d5fcd0285b:43583 in memory (size: 7.6 KiB, free: 434.3 MiB)
[2023-01-31T05:37:38.762+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:38 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:37:38.764+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:38 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T05:37:38.765+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:38 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:37:41.452+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:41 INFO CodeGenerator: Code generated in 2292.653483 ms
[2023-01-31T05:37:41.809+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:41 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.3 MiB)
[2023-01-31T05:37:42.223+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:42 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T05:37:42.243+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:42 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 81d5fcd0285b:43583 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:37:42.249+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:42 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T05:37:42.319+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:42 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:37:43.485+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:43 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T05:37:43.491+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:43 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) with 1 output partitions
[2023-01-31T05:37:43.496+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:43 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201)
[2023-01-31T05:37:43.502+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:43 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:37:43.509+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:43 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:37:43.551+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:43 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201), which has no missing parents
[2023-01-31T05:37:43.663+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:43 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.2 MiB)
[2023-01-31T05:37:43.704+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:43 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.2 MiB)
[2023-01-31T05:37:43.712+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:43 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 81d5fcd0285b:43583 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T05:37:43.735+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:43 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:37:43.774+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:37:43.799+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:43 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T05:37:43.830+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:43 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:37:43.857+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:43 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T05:37:44.165+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:44 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T05:37:45.213+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:45 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2490 bytes result sent to driver
[2023-01-31T05:37:45.228+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:45 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 1398 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:37:45.234+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:45 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) finished in 1.662 s
[2023-01-31T05:37:45.240+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:45 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T05:37:45.261+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:45 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:37:45.269+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T05:37:45.269+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:45 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201, took 1.777712 s
[2023-01-31T05:37:46.899+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:46 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:37:46.923+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:46 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T05:37:46.946+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:46 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:37:49.944+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:49 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 81d5fcd0285b:43583 in memory (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T05:37:50.311+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:50 INFO CodeGenerator: Code generated in 1804.774566 ms
[2023-01-31T05:37:50.412+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:50 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 433.1 MiB)
[2023-01-31T05:37:51.033+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:51 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.0 MiB)
[2023-01-31T05:37:51.036+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:51 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 81d5fcd0285b:43583 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:37:51.066+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:51 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T05:37:51.111+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:51 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:37:51.606+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:51 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T05:37:51.630+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:51 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T05:37:51.630+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:51 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T05:37:51.631+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:51 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:37:51.645+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:51 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:37:51.672+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:51 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T05:37:51.819+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:51 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 433.0 MiB)
[2023-01-31T05:37:51.893+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:51 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 433.0 MiB)
[2023-01-31T05:37:51.899+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:51 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 81d5fcd0285b:43583 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T05:37:51.910+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:51 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:37:51.927+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:37:51.941+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:51 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T05:37:51.971+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:51 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:37:51.981+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:51 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T05:37:52.175+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:52 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T05:37:53.334+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:53 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-01-31T05:37:53.343+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:53 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 1372 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:37:53.349+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:53 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 1.599 s
[2023-01-31T05:37:53.350+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:53 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:37:53.351+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:53 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T05:37:53.369+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T05:37:53.370+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:53 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 1.756869 s
[2023-01-31T05:37:56.003+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:56 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:37:56.023+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:56 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T05:37:56.033+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:56 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:37:59.345+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:59 INFO CodeGenerator: Code generated in 2497.889422 ms
[2023-01-31T05:37:59.381+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:59 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.8 MiB)
[2023-01-31T05:37:59.532+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:59 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.7 MiB)
[2023-01-31T05:37:59.536+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:59 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 81d5fcd0285b:43583 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:37:59.547+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:59 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T05:37:59.594+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:59 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:37:59.790+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:59 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T05:37:59.798+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:59 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T05:37:59.804+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:59 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T05:37:59.805+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:59 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:37:59.815+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:59 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:38:00.047+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:59 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T05:38:00.387+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:00 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.7 MiB)
[2023-01-31T05:38:00.536+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:00 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.7 MiB)
[2023-01-31T05:38:00.549+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:00 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 81d5fcd0285b:43583 (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T05:38:00.556+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:00 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:38:00.675+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:38:00.677+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:00 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T05:38:00.727+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:00 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:38:00.735+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:00 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T05:38:01.028+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:01 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T05:38:01.572+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:01 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1729 bytes result sent to driver
[2023-01-31T05:38:01.578+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:01 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 851 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:38:01.579+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:01 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T05:38:01.609+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:01 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 1.555 s
[2023-01-31T05:38:01.660+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:01 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:38:01.662+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T05:38:01.681+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:01 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 1.871708 s
[2023-01-31T05:38:02.570+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:02 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 81d5fcd0285b:43583 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T05:38:02.770+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:02 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 81d5fcd0285b:43583 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T05:38:15.580+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:15 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 81d5fcd0285b:43583 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:38:15.662+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:15 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 81d5fcd0285b:43583 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:38:15.903+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:15 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 81d5fcd0285b:43583 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:38:15.973+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:15 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 81d5fcd0285b:43583 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:38:51.369+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:51 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:38:51.601+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:38:51.602+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:38:51.638+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:38:51.648+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:38:51.652+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:38:51.691+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:38:55.893+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:55 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-31T05:38:55.893+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:55 INFO DAGScheduler: Got job 8 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T05:38:55.894+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:55 INFO DAGScheduler: Final stage: ResultStage 8 (save at BigQueryWriteHelper.java:105)
[2023-01-31T05:38:55.894+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:55 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:38:55.895+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:55 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:38:55.904+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:55 INFO DAGScheduler: Submitting ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T05:38:56.713+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:56 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 209.9 KiB, free 433.5 MiB)
[2023-01-31T05:38:56.807+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:56 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 74.2 KiB, free 433.4 MiB)
[2023-01-31T05:38:56.818+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:56 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 81d5fcd0285b:43583 (size: 74.2 KiB, free: 434.2 MiB)
[2023-01-31T05:38:56.831+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:56 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:38:56.856+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:38:56.860+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:56 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2023-01-31T05:38:57.056+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:57 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7484 bytes) taskResourceAssignments Map()
[2023-01-31T05:38:57.080+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:57 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2023-01-31T05:38:57.577+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:38:57.578+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:38:57.586+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:57 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:38:57.588+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:38:57.590+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:38:57.595+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:38:57.666+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:57 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T05:38:57.701+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:57 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T05:38:58.019+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:58 INFO ParquetOutputFormat: Parquet block size to 134217728
[2023-01-31T05:38:58.020+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:58 INFO ParquetOutputFormat: Validation is off
[2023-01-31T05:38:58.021+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:58 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-01-31T05:38:58.021+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:58 INFO ParquetOutputFormat: Parquet properties are:
[2023-01-31T05:38:58.022+0000] {spark_submit.py:495} INFO - Parquet page size to 1048576
[2023-01-31T05:38:58.022+0000] {spark_submit.py:495} INFO - Parquet dictionary page size to 1048576
[2023-01-31T05:38:58.023+0000] {spark_submit.py:495} INFO - Dictionary is true
[2023-01-31T05:38:58.023+0000] {spark_submit.py:495} INFO - Writer version is: PARQUET_1_0
[2023-01-31T05:38:58.024+0000] {spark_submit.py:495} INFO - Page size checking is: estimated
[2023-01-31T05:38:58.024+0000] {spark_submit.py:495} INFO - Min row count for page size check is: 100
[2023-01-31T05:38:58.031+0000] {spark_submit.py:495} INFO - Max row count for page size check is: 10000
[2023-01-31T05:38:58.032+0000] {spark_submit.py:495} INFO - Truncate length for column indexes is: 64
[2023-01-31T05:38:58.036+0000] {spark_submit.py:495} INFO - Truncate length for statistics min/max  is: 2147483647
[2023-01-31T05:38:58.037+0000] {spark_submit.py:495} INFO - Bloom filter enabled: false
[2023-01-31T05:38:58.038+0000] {spark_submit.py:495} INFO - Max Bloom filter size for a column is 1048576
[2023-01-31T05:38:58.039+0000] {spark_submit.py:495} INFO - Bloom filter expected number of distinct values are: null
[2023-01-31T05:38:58.040+0000] {spark_submit.py:495} INFO - Page row count limit to 20000
[2023-01-31T05:38:58.040+0000] {spark_submit.py:495} INFO - Writing page checksums is: on
[2023-01-31T05:38:58.315+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:58 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-01-31T05:38:58.321+0000] {spark_submit.py:495} INFO - {
[2023-01-31T05:38:58.322+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-31T05:38:58.322+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-31T05:38:58.332+0000] {spark_submit.py:495} INFO - "name" : "measured_at",
[2023-01-31T05:38:58.342+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T05:38:58.346+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T05:38:58.368+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:58.370+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:58.383+0000] {spark_submit.py:495} INFO - "name" : "created_at",
[2023-01-31T05:38:58.384+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T05:38:58.384+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T05:38:58.407+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:58.408+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:58.428+0000] {spark_submit.py:495} INFO - "name" : "Biomass - Actual Aggregated",
[2023-01-31T05:38:58.439+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:58.439+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:58.480+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:58.481+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:58.482+0000] {spark_submit.py:495} INFO - "name" : "Fossil Brown coal/Lignite - Actual Aggregated",
[2023-01-31T05:38:58.483+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:58.542+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:58.543+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:58.544+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:58.601+0000] {spark_submit.py:495} INFO - "name" : "Fossil Gas - Actual Aggregated",
[2023-01-31T05:38:58.602+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:58.602+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:58.603+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:58.604+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:58.624+0000] {spark_submit.py:495} INFO - "name" : "Fossil Hard coal - Actual Aggregated",
[2023-01-31T05:38:58.684+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:58.685+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:58.686+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:58.686+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:58.687+0000] {spark_submit.py:495} INFO - "name" : "Fossil Oil - Actual Aggregated",
[2023-01-31T05:38:58.729+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:58.730+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:58.733+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:58.734+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:58.772+0000] {spark_submit.py:495} INFO - "name" : "Geothermal - Actual Aggregated",
[2023-01-31T05:38:58.778+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:58.779+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:58.780+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:58.826+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:58.827+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Aggregated",
[2023-01-31T05:38:58.827+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:58.828+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:58.892+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:58.992+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:59.001+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Consumption",
[2023-01-31T05:38:59.013+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:59.193+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:59.198+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:59.202+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:59.207+0000] {spark_submit.py:495} INFO - "name" : "Hydro Run-of-river and poundage - Actual Aggregated",
[2023-01-31T05:38:59.219+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:59.223+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:59.224+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:59.229+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:59.238+0000] {spark_submit.py:495} INFO - "name" : "Hydro Water Reservoir - Actual Aggregated",
[2023-01-31T05:38:59.241+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:59.273+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:59.274+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:59.310+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:59.310+0000] {spark_submit.py:495} INFO - "name" : "Nuclear - Actual Aggregated",
[2023-01-31T05:38:59.311+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:59.311+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:59.357+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:59.358+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:59.358+0000] {spark_submit.py:495} INFO - "name" : "Other - Actual Aggregated",
[2023-01-31T05:38:59.359+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:59.359+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:59.383+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:59.395+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:59.395+0000] {spark_submit.py:495} INFO - "name" : "Other renewable - Actual Aggregated",
[2023-01-31T05:38:59.396+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:59.397+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:59.397+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:59.398+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:59.399+0000] {spark_submit.py:495} INFO - "name" : "Solar - Actual Aggregated",
[2023-01-31T05:38:59.399+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:59.400+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:59.410+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:59.411+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:59.412+0000] {spark_submit.py:495} INFO - "name" : "Waste - Actual Aggregated",
[2023-01-31T05:38:59.412+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:59.421+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:59.422+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:59.422+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:59.423+0000] {spark_submit.py:495} INFO - "name" : "Wind Offshore - Actual Aggregated",
[2023-01-31T05:38:59.424+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:59.424+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:59.437+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:59.438+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:59.438+0000] {spark_submit.py:495} INFO - "name" : "Wind Onshore - Actual Aggregated",
[2023-01-31T05:38:59.450+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:59.458+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:59.459+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:59.459+0000] {spark_submit.py:495} INFO - } ]
[2023-01-31T05:38:59.460+0000] {spark_submit.py:495} INFO - }
[2023-01-31T05:38:59.460+0000] {spark_submit.py:495} INFO - and corresponding Parquet message type:
[2023-01-31T05:38:59.469+0000] {spark_submit.py:495} INFO - message spark_schema {
[2023-01-31T05:38:59.470+0000] {spark_submit.py:495} INFO - required int96 measured_at;
[2023-01-31T05:38:59.498+0000] {spark_submit.py:495} INFO - required int96 created_at;
[2023-01-31T05:38:59.499+0000] {spark_submit.py:495} INFO - optional double Biomass - Actual Aggregated;
[2023-01-31T05:38:59.500+0000] {spark_submit.py:495} INFO - optional double Fossil Brown coal/Lignite - Actual Aggregated;
[2023-01-31T05:38:59.521+0000] {spark_submit.py:495} INFO - optional double Fossil Gas - Actual Aggregated;
[2023-01-31T05:38:59.522+0000] {spark_submit.py:495} INFO - optional double Fossil Hard coal - Actual Aggregated;
[2023-01-31T05:38:59.522+0000] {spark_submit.py:495} INFO - optional double Fossil Oil - Actual Aggregated;
[2023-01-31T05:38:59.523+0000] {spark_submit.py:495} INFO - optional double Geothermal - Actual Aggregated;
[2023-01-31T05:38:59.567+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Aggregated;
[2023-01-31T05:38:59.568+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Consumption;
[2023-01-31T05:38:59.580+0000] {spark_submit.py:495} INFO - optional double Hydro Run-of-river and poundage - Actual Aggregated;
[2023-01-31T05:38:59.601+0000] {spark_submit.py:495} INFO - optional double Hydro Water Reservoir - Actual Aggregated;
[2023-01-31T05:38:59.602+0000] {spark_submit.py:495} INFO - optional double Nuclear - Actual Aggregated;
[2023-01-31T05:38:59.608+0000] {spark_submit.py:495} INFO - optional double Other - Actual Aggregated;
[2023-01-31T05:38:59.650+0000] {spark_submit.py:495} INFO - optional double Other renewable - Actual Aggregated;
[2023-01-31T05:38:59.650+0000] {spark_submit.py:495} INFO - optional double Solar - Actual Aggregated;
[2023-01-31T05:38:59.651+0000] {spark_submit.py:495} INFO - optional double Waste - Actual Aggregated;
[2023-01-31T05:38:59.652+0000] {spark_submit.py:495} INFO - optional double Wind Offshore - Actual Aggregated;
[2023-01-31T05:38:59.676+0000] {spark_submit.py:495} INFO - optional double Wind Onshore - Actual Aggregated;
[2023-01-31T05:38:59.687+0000] {spark_submit.py:495} INFO - }
[2023-01-31T05:38:59.688+0000] {spark_submit.py:495} INFO - 
[2023-01-31T05:38:59.707+0000] {spark_submit.py:495} INFO - 
[2023-01-31T05:39:02.092+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:02 INFO CodecPool: Got brand-new compressor [.snappy]
[2023-01-31T05:39:03.301+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:03 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 81d5fcd0285b:43583 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:39:19.217+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:19 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675143161913-93583e4d-3019-40fe-a3f4-a037e292c416/_temporary/0/_temporary/' directory.
[2023-01-31T05:39:19.273+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:19 INFO FileOutputCommitter: Saved output of task 'attempt_202301310538543680261019423621816_0008_m_000000_8' to gs://entsoe_temp_1009/.spark-bigquery-local-1675143161913-93583e4d-3019-40fe-a3f4-a037e292c416/_temporary/0/task_202301310538543680261019423621816_0008_m_000000
[2023-01-31T05:39:19.297+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:19 INFO SparkHadoopMapRedUtil: attempt_202301310538543680261019423621816_0008_m_000000_8: Committed. Elapsed time: 4123 ms.
[2023-01-31T05:39:19.377+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:19 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2588 bytes result sent to driver
[2023-01-31T05:39:19.408+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:19 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 22507 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:39:19.419+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:19 INFO DAGScheduler: ResultStage 8 (save at BigQueryWriteHelper.java:105) finished in 23.500 s
[2023-01-31T05:39:19.420+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:19 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:39:19.426+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:19 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2023-01-31T05:39:19.429+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2023-01-31T05:39:19.436+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:19 INFO DAGScheduler: Job 8 finished: save at BigQueryWriteHelper.java:105, took 23.545211 s
[2023-01-31T05:39:19.449+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:19 INFO FileFormatWriter: Start to commit write Job 3a5ac3b6-cb3d-4ef4-8111-a644a1a44be5.
[2023-01-31T05:39:20.903+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:20 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675143161913-93583e4d-3019-40fe-a3f4-a037e292c416/_temporary/0/task_202301310538543680261019423621816_0008_m_000000/' directory.
[2023-01-31T05:39:21.706+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:21 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675143161913-93583e4d-3019-40fe-a3f4-a037e292c416/' directory.
[2023-01-31T05:39:24.724+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:24 INFO FileFormatWriter: Write Job 3a5ac3b6-cb3d-4ef4-8111-a644a1a44be5 committed. Elapsed time: 5231 ms.
[2023-01-31T05:39:25.074+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:25 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 81d5fcd0285b:43583 in memory (size: 74.2 KiB, free: 434.3 MiB)
[2023-01-31T05:39:25.083+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:25 INFO FileFormatWriter: Finished processing stats for write job 3a5ac3b6-cb3d-4ef4-8111-a644a1a44be5.
[2023-01-31T05:39:25.160+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:25 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 81d5fcd0285b:43583 in memory (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T05:39:28.353+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:28 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=entsoe_analytics_1009, projectId=rafzul-analytics-1009, tableId=TEST_total_generation_staging}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=null, ignoreUnknownValue=null, sourceUris=[gs://entsoe_temp_1009/.spark-bigquery-local-1675143161913-93583e4d-3019-40fe-a3f4-a037e292c416/part-00000-0f4380c8-a8ed-4aa6-95e1-d84ebfec1be6-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=true, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=rafzul-analytics-1009, job=73ddc7f5-5eca-42d5-ae89-53c3e6a3538c, location=US}
[2023-01-31T05:39:31.701+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:31 INFO BigQueryClient: Done loading to rafzul-analytics-1009.entsoe_analytics_1009.TEST_total_generation_staging. jobId: JobId{project=rafzul-analytics-1009, job=73ddc7f5-5eca-42d5-ae89-53c3e6a3538c, location=US}
[2023-01-31T05:39:32.802+0000] {spark_submit.py:495} INFO - MANTEP!!!!
[2023-01-31T05:39:33.417+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:33 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-31T05:39:33.494+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:33 INFO SparkUI: Stopped Spark web UI at http://81d5fcd0285b:4046
[2023-01-31T05:39:33.580+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:33 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-31T05:39:33.635+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:33 INFO MemoryStore: MemoryStore cleared
[2023-01-31T05:39:33.638+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:33 INFO BlockManager: BlockManager stopped
[2023-01-31T05:39:33.646+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:33 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-31T05:39:33.663+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:33 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-31T05:39:33.689+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:33 INFO SparkContext: Successfully stopped SparkContext
[2023-01-31T05:39:33.690+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:33 INFO ShutdownHookManager: Shutdown hook called
[2023-01-31T05:39:33.692+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:33 INFO ShutdownHookManager: Deleting directory /tmp/spark-806ea5d6-cda6-4853-a8ce-05eec2876145
[2023-01-31T05:39:33.709+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:33 INFO ShutdownHookManager: Deleting directory /tmp/spark-f0f8d19f-4e4d-4953-b7d3-508c96284995
[2023-01-31T05:39:33.725+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:33 INFO ShutdownHookManager: Deleting directory /tmp/spark-806ea5d6-cda6-4853-a8ce-05eec2876145/pyspark-319083f8-9135-4002-a369-72af06be0934
[2023-01-31T05:39:34.342+0000] {taskinstance.py:1322} INFO - Marking task as SUCCESS. dag_id=entsoe-energydata-backfill, task_id=stage_total_generation, execution_date=20210101T050000, start_date=20230131T053046, end_date=20230131T053934
[2023-01-31T05:39:34.581+0000] {local_task_job.py:159} INFO - Task exited with return code 0
[2023-01-31T05:39:34.959+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T07:39:02.633+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T05:00:00+00:00 [queued]>
[2023-01-31T07:39:02.684+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T05:00:00+00:00 [queued]>
[2023-01-31T07:39:02.684+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T07:39:02.685+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T07:39:02.686+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T07:39:02.742+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 05:00:00+00:00
[2023-01-31T07:39:02.785+0000] {standard_task_runner.py:55} INFO - Started process 25806 to run task
[2023-01-31T07:39:02.900+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T05:00:00+00:00', '--job-id', '911', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpvm1e9pml']
[2023-01-31T07:39:02.980+0000] {standard_task_runner.py:83} INFO - Job 911: Subtask stage_total_generation
[2023-01-31T07:39:04.220+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T05:00:00+00:00 [running]> on host 81d5fcd0285b
[2023-01-31T07:39:05.130+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T05:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T05:00:00+00:00
[2023-01-31T07:39:05.300+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T07:39:05.318+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010500 202101010600 DE_TENNET
[2023-01-31T07:40:26.989+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:26 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T07:40:29.397+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T07:40:32.474+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:32 INFO ResourceUtils: ==============================================================
[2023-01-31T07:40:32.486+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:32 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T07:40:32.492+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:32 INFO ResourceUtils: ==============================================================
[2023-01-31T07:40:32.511+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:32 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T07:40:33.485+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:33 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T07:40:33.630+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:33 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T07:40:33.640+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:33 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T07:40:34.989+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:34 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T07:40:35.053+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:34 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T07:40:35.075+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:34 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T07:40:35.079+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:34 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T07:40:35.086+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:34 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T07:40:42.853+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:42 INFO Utils: Successfully started service 'sparkDriver' on port 37205.
[2023-01-31T07:40:43.579+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:43 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T07:40:44.203+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:44 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T07:40:45.407+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:45 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T07:40:45.412+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:45 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T07:40:45.452+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:45 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T07:40:45.580+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:45 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-fa4a616f-86f3-4cfd-98f6-0a8a2392eecc
[2023-01-31T07:40:46.189+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:46 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T07:40:47.356+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:47 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T07:40:55.063+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:55 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T07:40:55.095+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:55 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2023-01-31T07:40:55.101+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:55 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[2023-01-31T07:40:55.107+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:55 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[2023-01-31T07:40:55.260+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:55 INFO Utils: Successfully started service 'SparkUI' on port 4044.
[2023-01-31T07:40:56.256+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:56 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://81d5fcd0285b:37205/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675150826937
[2023-01-31T07:40:56.278+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:56 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://81d5fcd0285b:37205/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675150826937
[2023-01-31T07:40:59.810+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:59 INFO Executor: Starting executor ID driver on host 81d5fcd0285b
[2023-01-31T07:41:00.103+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:00 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T07:41:00.438+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:00 INFO Executor: Fetching spark://81d5fcd0285b:37205/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675150826937
[2023-01-31T07:41:02.026+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:02 INFO TransportClientFactory: Successfully created connection to 81d5fcd0285b/172.19.0.9:37205 after 815 ms (0 ms spent in bootstraps)
[2023-01-31T07:41:02.242+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:02 INFO Utils: Fetching spark://81d5fcd0285b:37205/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-d05c22e8-051a-4d2d-8b71-cf883b33da59/userFiles-a2c447e2-2b59-45fc-a5b7-f0a655d1140e/fetchFileTemp18186246057040144703.tmp
[2023-01-31T07:41:09.657+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:09 INFO Executor: Adding file:/tmp/spark-d05c22e8-051a-4d2d-8b71-cf883b33da59/userFiles-a2c447e2-2b59-45fc-a5b7-f0a655d1140e/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T07:41:09.658+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:09 INFO Executor: Fetching spark://81d5fcd0285b:37205/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675150826937
[2023-01-31T07:41:09.767+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:09 INFO Utils: Fetching spark://81d5fcd0285b:37205/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-d05c22e8-051a-4d2d-8b71-cf883b33da59/userFiles-a2c447e2-2b59-45fc-a5b7-f0a655d1140e/fetchFileTemp17714239307770118698.tmp
[2023-01-31T07:41:15.830+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:15 INFO Executor: Adding file:/tmp/spark-d05c22e8-051a-4d2d-8b71-cf883b33da59/userFiles-a2c447e2-2b59-45fc-a5b7-f0a655d1140e/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T07:41:16.117+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:16 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33893.
[2023-01-31T07:41:16.123+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:16 INFO NettyBlockTransferService: Server created on 81d5fcd0285b:33893
[2023-01-31T07:41:16.254+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:16 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T07:41:16.611+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:16 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 33893, None)
[2023-01-31T07:41:17.067+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:17 INFO BlockManagerMasterEndpoint: Registering block manager 81d5fcd0285b:33893 with 434.4 MiB RAM, BlockManagerId(driver, 81d5fcd0285b, 33893, None)
[2023-01-31T07:41:17.257+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:17 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 33893, None)
[2023-01-31T07:41:17.263+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:17 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 81d5fcd0285b, 33893, None)
[2023-01-31T07:41:18.858+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:18 INFO Executor: Told to re-register on heartbeat
[2023-01-31T07:41:18.966+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:18 INFO BlockManager: BlockManager BlockManagerId(driver, 81d5fcd0285b, 33893, None) re-registering with master
[2023-01-31T07:41:18.986+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:18 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 33893, None)
[2023-01-31T07:41:19.153+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:19 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 33893, None)
[2023-01-31T07:41:19.154+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:19 INFO BlockManager: Reporting 0 blocks to the master.
[2023-01-31T07:41:38.298+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:38 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T07:41:38.501+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:38 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T07:42:20.753+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:20 INFO InMemoryFileIndex: It took 1042 ms to list leaf files for 1 paths.
[2023-01-31T07:42:24.448+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:24 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T07:42:25.715+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:25 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T07:42:25.778+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:25 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 81d5fcd0285b:33893 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T07:42:26.063+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:26 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T07:42:39.182+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:39 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T07:42:39.351+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:39 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T07:42:39.805+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:39 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T07:42:39.933+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:39 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T07:42:39.942+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:39 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T07:42:39.945+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:39 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:42:39.956+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:39 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:42:39.992+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:39 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T07:42:42.836+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:42 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T07:42:43.083+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:43 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T07:42:43.087+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:43 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 81d5fcd0285b:33893 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T07:42:43.115+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:43 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:42:43.265+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:42:43.345+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:43 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T07:42:44.223+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:44 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T07:42:44.712+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:44 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T07:42:50.605+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:50 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010500__202101010600.json:0+9367
[2023-01-31T07:42:54.174+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:54 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3480 bytes result sent to driver
[2023-01-31T07:42:54.476+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:54 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 10424 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:42:54.576+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:54 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T07:42:54.783+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:54 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 14.035 s
[2023-01-31T07:42:54.959+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:54 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:42:55.011+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T07:42:55.105+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:55 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 15.291651 s
[2023-01-31T07:43:03.731+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:03 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 81d5fcd0285b:33893 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T07:43:03.890+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:03 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 81d5fcd0285b:33893 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T07:43:24.421+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:24 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:43:24.427+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:24 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T07:43:24.444+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:24 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:43:26.936+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:26 INFO CodeGenerator: Code generated in 1611.806305 ms
[2023-01-31T07:43:27.007+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T07:43:27.131+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T07:43:27.145+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 81d5fcd0285b:33893 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T07:43:27.158+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T07:43:27.269+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:43:27.829+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T07:43:27.842+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T07:43:27.842+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T07:43:27.843+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:43:27.843+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:43:27.853+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T07:43:27.945+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T07:43:27.996+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T07:43:28.004+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:28 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 81d5fcd0285b:33893 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T07:43:28.005+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:28 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:43:28.013+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:43:28.014+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:28 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T07:43:28.033+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:28 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:43:28.051+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:28 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T07:43:28.373+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:28 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T07:43:29.126+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO CodeGenerator: Code generated in 577.045751 ms
[2023-01-31T07:43:29.660+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1628 bytes result sent to driver
[2023-01-31T07:43:29.676+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1656 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:43:29.683+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T07:43:29.684+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 1.767 s
[2023-01-31T07:43:29.684+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:43:29.685+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T07:43:29.685+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 1.842805 s
[2023-01-31T07:43:29.995+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:43:29.995+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T07:43:29.998+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:43:30.211+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO CodeGenerator: Code generated in 111.945938 ms
[2023-01-31T07:43:30.225+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T07:43:30.253+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T07:43:30.256+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 81d5fcd0285b:33893 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:43:30.270+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T07:43:30.282+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:43:30.355+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T07:43:30.361+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T07:43:30.361+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T07:43:30.362+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:43:30.363+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:43:30.370+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T07:43:30.394+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T07:43:30.418+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T07:43:30.422+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 81d5fcd0285b:33893 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T07:43:30.425+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:43:30.425+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:43:30.425+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T07:43:30.433+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:43:30.441+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T07:43:30.508+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T07:43:30.899+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1585 bytes result sent to driver
[2023-01-31T07:43:30.909+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 481 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:43:30.914+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T07:43:30.926+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 0.542 s
[2023-01-31T07:43:30.927+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:43:30.927+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T07:43:30.927+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 0.565974 s
[2023-01-31T07:43:31.589+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:31 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 81d5fcd0285b:33893 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T07:43:31.808+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:31 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:43:31.813+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:31 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T07:43:31.818+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:31 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:43:31.861+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:31 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 81d5fcd0285b:33893 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T07:43:33.045+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO CodeGenerator: Code generated in 710.943105 ms
[2023-01-31T07:43:33.062+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T07:43:33.098+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T07:43:33.108+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 81d5fcd0285b:33893 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:43:33.112+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T07:43:33.124+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:43:33.212+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T07:43:33.217+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T07:43:33.217+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T07:43:33.220+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:43:33.221+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:43:33.232+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T07:43:33.254+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.7 MiB)
[2023-01-31T07:43:33.261+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.7 MiB)
[2023-01-31T07:43:33.329+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 81d5fcd0285b:33893 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T07:43:33.330+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:43:33.334+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:43:33.335+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T07:43:33.336+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:43:33.341+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T07:43:33.532+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T07:43:33.980+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-01-31T07:43:34.113+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 764 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:43:34.166+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 0.897 s
[2023-01-31T07:43:34.167+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:43:34.182+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T07:43:34.204+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T07:43:34.205+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 0.970340 s
[2023-01-31T07:43:34.622+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:43:34.623+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T07:43:34.624+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:43:34.723+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO CodeGenerator: Code generated in 46.741516 ms
[2023-01-31T07:43:34.760+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.5 MiB)
[2023-01-31T07:43:34.793+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T07:43:34.799+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 81d5fcd0285b:33893 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:43:34.822+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T07:43:34.837+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:43:34.982+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T07:43:34.987+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T07:43:34.988+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T07:43:34.988+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:43:34.992+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:43:34.998+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T07:43:35.043+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:35 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-31T07:43:35.083+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:35 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-01-31T07:43:35.091+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:35 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 81d5fcd0285b:33893 (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T07:43:35.100+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:35 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:43:35.133+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:43:35.145+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:35 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T07:43:35.159+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:35 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:43:35.165+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:35 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T07:43:35.258+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:35 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T07:43:35.863+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:35 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1586 bytes result sent to driver
[2023-01-31T07:43:35.894+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:35 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 736 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:43:35.895+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:35 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T07:43:35.895+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:35 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 0.884 s
[2023-01-31T07:43:35.899+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:35 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:43:35.905+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T07:43:35.907+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:35 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 0.916941 s
[2023-01-31T07:43:38.404+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:38 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:43:38.406+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:38 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T07:43:38.415+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:38 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:43:39.403+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:39 INFO CodeGenerator: Code generated in 451.009348 ms
[2023-01-31T07:43:39.431+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:39 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.2 MiB)
[2023-01-31T07:43:39.682+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:39 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T07:43:39.695+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:39 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 81d5fcd0285b:33893 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T07:43:39.702+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:39 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T07:43:39.730+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:39 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:43:40.080+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T07:43:40.101+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) with 1 output partitions
[2023-01-31T07:43:40.102+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201)
[2023-01-31T07:43:40.102+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:43:40.103+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:43:40.141+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201), which has no missing parents
[2023-01-31T07:43:40.181+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.1 MiB)
[2023-01-31T07:43:40.202+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.1 MiB)
[2023-01-31T07:43:40.212+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 81d5fcd0285b:33893 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T07:43:40.215+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:43:40.221+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:43:40.223+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T07:43:40.243+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:43:40.250+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T07:43:40.371+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T07:43:41.038+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:41 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2490 bytes result sent to driver
[2023-01-31T07:43:41.123+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:41 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 876 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:43:41.124+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:41 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T07:43:41.133+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:41 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) finished in 0.973 s
[2023-01-31T07:43:41.136+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:41 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:43:41.137+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T07:43:41.138+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:41 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201, took 1.040332 s
[2023-01-31T07:43:43.437+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:43 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:43:43.438+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:43 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T07:43:43.439+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:43 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:43:44.437+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:44 INFO CodeGenerator: Code generated in 566.974809 ms
[2023-01-31T07:43:44.478+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:44 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 432.9 MiB)
[2023-01-31T07:43:44.653+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:44 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.9 MiB)
[2023-01-31T07:43:44.666+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:44 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 81d5fcd0285b:33893 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T07:43:44.745+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:44 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T07:43:44.755+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:44 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:43:45.253+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T07:43:45.271+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T07:43:45.272+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T07:43:45.273+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:43:45.275+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:43:45.283+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T07:43:45.309+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 432.9 MiB)
[2023-01-31T07:43:45.337+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.8 MiB)
[2023-01-31T07:43:45.339+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 81d5fcd0285b:33893 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T07:43:45.345+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:43:45.350+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:43:45.353+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T07:43:45.371+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:43:45.378+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T07:43:45.583+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T07:43:47.442+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:47 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-01-31T07:43:47.512+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:47 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 2148 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:43:47.527+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:47 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 2.231 s
[2023-01-31T07:43:47.533+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:47 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:43:47.546+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:47 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T07:43:47.569+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T07:43:47.573+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:47 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 2.321576 s
[2023-01-31T07:43:48.870+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:48 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 81d5fcd0285b:33893 in memory (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T07:43:49.157+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:49 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 81d5fcd0285b:33893 in memory (size: 12.7 KiB, free: 434.2 MiB)
[2023-01-31T07:43:49.402+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:49 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 81d5fcd0285b:33893 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T07:43:49.641+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:49 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 81d5fcd0285b:33893 in memory (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T07:43:50.016+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:43:50.020+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T07:43:50.023+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:43:52.622+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:52 INFO CodeGenerator: Code generated in 1859.035034 ms
[2023-01-31T07:43:52.655+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:52 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.8 MiB)
[2023-01-31T07:43:52.774+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:52 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.8 MiB)
[2023-01-31T07:43:52.779+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:52 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 81d5fcd0285b:33893 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T07:43:52.785+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:52 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T07:43:52.790+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:52 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:43:52.868+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:52 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T07:43:52.890+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:52 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T07:43:52.898+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:52 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T07:43:52.907+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:52 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:43:52.938+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:52 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:43:52.979+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:52 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T07:43:53.304+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:53 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.8 MiB)
[2023-01-31T07:43:53.352+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:53 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.7 MiB)
[2023-01-31T07:43:53.359+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:53 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 81d5fcd0285b:33893 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T07:43:53.369+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:53 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:43:53.398+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:43:53.401+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:53 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T07:43:53.429+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:53 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:43:53.440+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:53 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T07:43:53.662+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:53 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T07:43:54.616+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:54 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1729 bytes result sent to driver
[2023-01-31T07:43:54.634+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:54 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 1192 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:43:54.635+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:54 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T07:43:54.636+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:54 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 1.540 s
[2023-01-31T07:43:54.637+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:54 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:43:54.638+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T07:43:54.638+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:54 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 1.761434 s
[2023-01-31T07:44:35.201+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:35 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 81d5fcd0285b:33893 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T07:44:35.457+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:35 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 81d5fcd0285b:33893 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T07:44:35.595+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:35 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 81d5fcd0285b:33893 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:44:35.904+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:35 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 81d5fcd0285b:33893 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:44:36.271+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:36 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 81d5fcd0285b:33893 in memory (size: 12.6 KiB, free: 434.3 MiB)
[2023-01-31T07:44:36.519+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:36 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 81d5fcd0285b:33893 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:44:36.786+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:36 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 81d5fcd0285b:33893 in memory (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T07:45:08.731+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:08 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T07:45:09.013+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T07:45:09.029+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T07:45:09.049+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:09 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T07:45:09.058+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T07:45:09.062+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T07:45:09.079+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T07:45:10.488+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:10 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-31T07:45:10.489+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:10 INFO DAGScheduler: Got job 8 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T07:45:10.489+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:10 INFO DAGScheduler: Final stage: ResultStage 8 (save at BigQueryWriteHelper.java:105)
[2023-01-31T07:45:10.489+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:10 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:45:10.490+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:10 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:45:10.512+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:10 INFO DAGScheduler: Submitting ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T07:45:10.976+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:10 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 209.9 KiB, free 434.0 MiB)
[2023-01-31T07:45:10.980+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:10 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 74.3 KiB, free 433.9 MiB)
[2023-01-31T07:45:10.986+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:10 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 81d5fcd0285b:33893 (size: 74.3 KiB, free: 434.3 MiB)
[2023-01-31T07:45:10.992+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:10 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:45:11.003+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:45:11.014+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2023-01-31T07:45:11.034+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7484 bytes) taskResourceAssignments Map()
[2023-01-31T07:45:11.054+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2023-01-31T07:45:11.469+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T07:45:11.469+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T07:45:11.486+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T07:45:11.492+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T07:45:11.493+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T07:45:11.502+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T07:45:11.565+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T07:45:11.613+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T07:45:11.880+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO ParquetOutputFormat: Parquet block size to 134217728
[2023-01-31T07:45:11.881+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO ParquetOutputFormat: Validation is off
[2023-01-31T07:45:11.884+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-01-31T07:45:11.889+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO ParquetOutputFormat: Parquet properties are:
[2023-01-31T07:45:11.889+0000] {spark_submit.py:495} INFO - Parquet page size to 1048576
[2023-01-31T07:45:11.889+0000] {spark_submit.py:495} INFO - Parquet dictionary page size to 1048576
[2023-01-31T07:45:11.890+0000] {spark_submit.py:495} INFO - Dictionary is true
[2023-01-31T07:45:11.890+0000] {spark_submit.py:495} INFO - Writer version is: PARQUET_1_0
[2023-01-31T07:45:11.890+0000] {spark_submit.py:495} INFO - Page size checking is: estimated
[2023-01-31T07:45:11.890+0000] {spark_submit.py:495} INFO - Min row count for page size check is: 100
[2023-01-31T07:45:11.890+0000] {spark_submit.py:495} INFO - Max row count for page size check is: 10000
[2023-01-31T07:45:11.890+0000] {spark_submit.py:495} INFO - Truncate length for column indexes is: 64
[2023-01-31T07:45:11.891+0000] {spark_submit.py:495} INFO - Truncate length for statistics min/max  is: 2147483647
[2023-01-31T07:45:11.891+0000] {spark_submit.py:495} INFO - Bloom filter enabled: false
[2023-01-31T07:45:11.891+0000] {spark_submit.py:495} INFO - Max Bloom filter size for a column is 1048576
[2023-01-31T07:45:11.891+0000] {spark_submit.py:495} INFO - Bloom filter expected number of distinct values are: null
[2023-01-31T07:45:11.891+0000] {spark_submit.py:495} INFO - Page row count limit to 20000
[2023-01-31T07:45:11.891+0000] {spark_submit.py:495} INFO - Writing page checksums is: on
[2023-01-31T07:45:12.293+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:12 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-01-31T07:45:12.293+0000] {spark_submit.py:495} INFO - {
[2023-01-31T07:45:12.293+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-31T07:45:12.294+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-31T07:45:12.294+0000] {spark_submit.py:495} INFO - "name" : "measured_at",
[2023-01-31T07:45:12.294+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T07:45:12.294+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T07:45:12.294+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.295+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.295+0000] {spark_submit.py:495} INFO - "name" : "created_at",
[2023-01-31T07:45:12.295+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T07:45:12.295+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T07:45:12.295+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.295+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.297+0000] {spark_submit.py:495} INFO - "name" : "Biomass - Actual Aggregated",
[2023-01-31T07:45:12.297+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.297+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.297+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.297+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.305+0000] {spark_submit.py:495} INFO - "name" : "Fossil Brown coal/Lignite - Actual Aggregated",
[2023-01-31T07:45:12.305+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.306+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.306+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.306+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.306+0000] {spark_submit.py:495} INFO - "name" : "Fossil Gas - Actual Aggregated",
[2023-01-31T07:45:12.306+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.306+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.306+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.307+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.307+0000] {spark_submit.py:495} INFO - "name" : "Fossil Hard coal - Actual Aggregated",
[2023-01-31T07:45:12.307+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.307+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.307+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.307+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.307+0000] {spark_submit.py:495} INFO - "name" : "Fossil Oil - Actual Aggregated",
[2023-01-31T07:45:12.308+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.308+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.308+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.308+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.308+0000] {spark_submit.py:495} INFO - "name" : "Geothermal - Actual Aggregated",
[2023-01-31T07:45:12.308+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.325+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.328+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.331+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.332+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Aggregated",
[2023-01-31T07:45:12.332+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.332+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.343+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.343+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.349+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Consumption",
[2023-01-31T07:45:12.350+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.350+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.350+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.350+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.360+0000] {spark_submit.py:495} INFO - "name" : "Hydro Run-of-river and poundage - Actual Aggregated",
[2023-01-31T07:45:12.360+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.371+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.371+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.372+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.372+0000] {spark_submit.py:495} INFO - "name" : "Hydro Water Reservoir - Actual Aggregated",
[2023-01-31T07:45:12.379+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.379+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.380+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.385+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.385+0000] {spark_submit.py:495} INFO - "name" : "Nuclear - Actual Aggregated",
[2023-01-31T07:45:12.396+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.397+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.399+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.400+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.400+0000] {spark_submit.py:495} INFO - "name" : "Other - Actual Aggregated",
[2023-01-31T07:45:12.400+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.415+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.416+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.416+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.422+0000] {spark_submit.py:495} INFO - "name" : "Other renewable - Actual Aggregated",
[2023-01-31T07:45:12.422+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.423+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.431+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.431+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.437+0000] {spark_submit.py:495} INFO - "name" : "Solar - Actual Aggregated",
[2023-01-31T07:45:12.441+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.441+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.446+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.447+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.453+0000] {spark_submit.py:495} INFO - "name" : "Waste - Actual Aggregated",
[2023-01-31T07:45:12.453+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.453+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.454+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.462+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.463+0000] {spark_submit.py:495} INFO - "name" : "Wind Offshore - Actual Aggregated",
[2023-01-31T07:45:12.471+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.471+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.483+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.484+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.484+0000] {spark_submit.py:495} INFO - "name" : "Wind Onshore - Actual Aggregated",
[2023-01-31T07:45:12.489+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.490+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.495+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.495+0000] {spark_submit.py:495} INFO - } ]
[2023-01-31T07:45:12.495+0000] {spark_submit.py:495} INFO - }
[2023-01-31T07:45:12.495+0000] {spark_submit.py:495} INFO - and corresponding Parquet message type:
[2023-01-31T07:45:12.521+0000] {spark_submit.py:495} INFO - message spark_schema {
[2023-01-31T07:45:12.521+0000] {spark_submit.py:495} INFO - required int96 measured_at;
[2023-01-31T07:45:12.521+0000] {spark_submit.py:495} INFO - required int96 created_at;
[2023-01-31T07:45:12.522+0000] {spark_submit.py:495} INFO - optional double Biomass - Actual Aggregated;
[2023-01-31T07:45:12.522+0000] {spark_submit.py:495} INFO - optional double Fossil Brown coal/Lignite - Actual Aggregated;
[2023-01-31T07:45:12.522+0000] {spark_submit.py:495} INFO - optional double Fossil Gas - Actual Aggregated;
[2023-01-31T07:45:12.522+0000] {spark_submit.py:495} INFO - optional double Fossil Hard coal - Actual Aggregated;
[2023-01-31T07:45:12.522+0000] {spark_submit.py:495} INFO - optional double Fossil Oil - Actual Aggregated;
[2023-01-31T07:45:12.522+0000] {spark_submit.py:495} INFO - optional double Geothermal - Actual Aggregated;
[2023-01-31T07:45:12.523+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Aggregated;
[2023-01-31T07:45:12.523+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Consumption;
[2023-01-31T07:45:12.523+0000] {spark_submit.py:495} INFO - optional double Hydro Run-of-river and poundage - Actual Aggregated;
[2023-01-31T07:45:12.523+0000] {spark_submit.py:495} INFO - optional double Hydro Water Reservoir - Actual Aggregated;
[2023-01-31T07:45:12.523+0000] {spark_submit.py:495} INFO - optional double Nuclear - Actual Aggregated;
[2023-01-31T07:45:12.523+0000] {spark_submit.py:495} INFO - optional double Other - Actual Aggregated;
[2023-01-31T07:45:12.524+0000] {spark_submit.py:495} INFO - optional double Other renewable - Actual Aggregated;
[2023-01-31T07:45:12.524+0000] {spark_submit.py:495} INFO - optional double Solar - Actual Aggregated;
[2023-01-31T07:45:12.524+0000] {spark_submit.py:495} INFO - optional double Waste - Actual Aggregated;
[2023-01-31T07:45:12.524+0000] {spark_submit.py:495} INFO - optional double Wind Offshore - Actual Aggregated;
[2023-01-31T07:45:12.524+0000] {spark_submit.py:495} INFO - optional double Wind Onshore - Actual Aggregated;
[2023-01-31T07:45:12.537+0000] {spark_submit.py:495} INFO - }
[2023-01-31T07:45:12.538+0000] {spark_submit.py:495} INFO - 
[2023-01-31T07:45:12.538+0000] {spark_submit.py:495} INFO - 
[2023-01-31T07:45:13.531+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:13 INFO CodecPool: Got brand-new compressor [.snappy]
[2023-01-31T07:45:21.175+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:21 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675150857871-a135a152-d5f5-4f83-9abf-62782fc2a79f/_temporary/0/_temporary/' directory.
[2023-01-31T07:45:21.177+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:21 INFO FileOutputCommitter: Saved output of task 'attempt_202301310745105451738571484448784_0008_m_000000_8' to gs://entsoe_temp_1009/.spark-bigquery-local-1675150857871-a135a152-d5f5-4f83-9abf-62782fc2a79f/_temporary/0/task_202301310745105451738571484448784_0008_m_000000
[2023-01-31T07:45:21.314+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:21 INFO SparkHadoopMapRedUtil: attempt_202301310745105451738571484448784_0008_m_000000_8: Committed. Elapsed time: 2368 ms.
[2023-01-31T07:45:21.502+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:21 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2588 bytes result sent to driver
[2023-01-31T07:45:21.528+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:21 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 10494 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:45:21.530+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:21 INFO DAGScheduler: ResultStage 8 (save at BigQueryWriteHelper.java:105) finished in 10.985 s
[2023-01-31T07:45:21.530+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:21 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:45:21.541+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:21 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2023-01-31T07:45:21.552+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2023-01-31T07:45:21.561+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:21 INFO DAGScheduler: Job 8 finished: save at BigQueryWriteHelper.java:105, took 11.071062 s
[2023-01-31T07:45:21.568+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:21 INFO FileFormatWriter: Start to commit write Job 7a663e17-67fd-413e-9434-8a3413fc768a.
[2023-01-31T07:45:23.683+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:23 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675150857871-a135a152-d5f5-4f83-9abf-62782fc2a79f/_temporary/0/task_202301310745105451738571484448784_0008_m_000000/' directory.
[2023-01-31T07:45:24.410+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:24 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675150857871-a135a152-d5f5-4f83-9abf-62782fc2a79f/' directory.
[2023-01-31T07:45:25.952+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:25 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 81d5fcd0285b:33893 in memory (size: 74.3 KiB, free: 434.4 MiB)
[2023-01-31T07:45:26.277+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:26 INFO FileFormatWriter: Write Job 7a663e17-67fd-413e-9434-8a3413fc768a committed. Elapsed time: 4214 ms.
[2023-01-31T07:45:26.417+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:26 INFO FileFormatWriter: Finished processing stats for write job 7a663e17-67fd-413e-9434-8a3413fc768a.
[2023-01-31T07:45:29.372+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:29 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=entsoe_analytics_1009, projectId=rafzul-analytics-1009, tableId=TEST_total_generation_staging}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=null, ignoreUnknownValue=null, sourceUris=[gs://entsoe_temp_1009/.spark-bigquery-local-1675150857871-a135a152-d5f5-4f83-9abf-62782fc2a79f/part-00000-e691cb9a-a657-4b06-b1c0-fcfba8bad65e-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=true, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=rafzul-analytics-1009, job=0a214997-4c0e-4c47-8e3b-661e4f12944b, location=US}
[2023-01-31T07:45:35.959+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:35 INFO BigQueryClient: Done loading to rafzul-analytics-1009.entsoe_analytics_1009.TEST_total_generation_staging. jobId: JobId{project=rafzul-analytics-1009, job=0a214997-4c0e-4c47-8e3b-661e4f12944b, location=US}
[2023-01-31T07:45:36.660+0000] {spark_submit.py:495} INFO - MANTEP!!!!
[2023-01-31T07:45:37.101+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:37 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-31T07:45:37.143+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:37 INFO SparkUI: Stopped Spark web UI at http://81d5fcd0285b:4044
[2023-01-31T07:45:37.207+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:37 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-31T07:45:37.260+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:37 INFO MemoryStore: MemoryStore cleared
[2023-01-31T07:45:37.262+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:37 INFO BlockManager: BlockManager stopped
[2023-01-31T07:45:37.274+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:37 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-31T07:45:37.285+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:37 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-31T07:45:37.309+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:37 INFO SparkContext: Successfully stopped SparkContext
[2023-01-31T07:45:37.311+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:37 INFO ShutdownHookManager: Shutdown hook called
[2023-01-31T07:45:37.313+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-d05c22e8-051a-4d2d-8b71-cf883b33da59/pyspark-0e9f5cf0-4f6d-4f79-893a-cff9aec5e2a2
[2023-01-31T07:45:37.329+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-d05c22e8-051a-4d2d-8b71-cf883b33da59
[2023-01-31T07:45:37.346+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-a1caaac2-1489-4dce-8353-ef8364971301
[2023-01-31T07:45:37.839+0000] {taskinstance.py:1322} INFO - Marking task as SUCCESS. dag_id=entsoe-energydata-backfill, task_id=stage_total_generation, execution_date=20210101T050000, start_date=20230131T073902, end_date=20230131T074537
[2023-01-31T07:45:37.999+0000] {local_task_job.py:159} INFO - Task exited with return code 0
[2023-01-31T07:45:38.147+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T08:21:35.321+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T05:00:00+00:00 [queued]>
[2023-01-31T08:21:35.354+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T05:00:00+00:00 [queued]>
[2023-01-31T08:21:35.355+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T08:21:35.355+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T08:21:35.356+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T08:21:35.399+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 05:00:00+00:00
[2023-01-31T08:21:35.424+0000] {standard_task_runner.py:55} INFO - Started process 2077 to run task
[2023-01-31T08:21:35.443+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T05:00:00+00:00', '--job-id', '925', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpje24tkx0']
[2023-01-31T08:21:35.450+0000] {standard_task_runner.py:83} INFO - Job 925: Subtask stage_total_generation
[2023-01-31T08:21:35.751+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T05:00:00+00:00 [running]> on host 81d5fcd0285b
[2023-01-31T08:21:35.998+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T05:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T05:00:00+00:00
[2023-01-31T08:21:36.020+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T08:21:36.025+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010500 202101010600 DE_TENNET
[2023-01-31T08:21:58.390+0000] {spark_submit.py:495} INFO - {{ start_date }}
[2023-01-31T08:21:58.390+0000] {spark_submit.py:495} INFO - mulai periode: {{ start_date }}
[2023-01-31T08:21:58.391+0000] {spark_submit.py:495} INFO - selesai periode: {{ end_date }}
[2023-01-31T08:21:58.854+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:58 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T08:21:59.257+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T08:21:59.973+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:59 INFO ResourceUtils: ==============================================================
[2023-01-31T08:21:59.979+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:59 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T08:21:59.982+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:59 INFO ResourceUtils: ==============================================================
[2023-01-31T08:21:59.987+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:59 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T08:22:00.118+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:00 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T08:22:00.138+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:00 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T08:22:00.141+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:00 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T08:22:00.751+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:00 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T08:22:00.752+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:00 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T08:22:00.761+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:00 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T08:22:00.767+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:00 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T08:22:00.771+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:00 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T08:22:03.188+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:03 INFO Utils: Successfully started service 'sparkDriver' on port 39461.
[2023-01-31T08:22:03.546+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:03 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T08:22:03.762+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:03 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T08:22:03.887+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:03 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T08:22:03.894+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:03 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T08:22:03.936+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:03 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T08:22:04.062+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:04 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-cdc37611-b1ff-4962-9f65-f16a25ce055f
[2023-01-31T08:22:04.165+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:04 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T08:22:04.302+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:04 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T08:22:07.270+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:07 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T08:22:07.279+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:07 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2023-01-31T08:22:07.288+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:07 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[2023-01-31T08:22:07.325+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:07 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[2023-01-31T08:22:07.413+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:07 INFO Utils: Successfully started service 'SparkUI' on port 4044.
[2023-01-31T08:22:07.698+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:07 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://81d5fcd0285b:39461/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675153318796
[2023-01-31T08:22:07.699+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:07 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://81d5fcd0285b:39461/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675153318796
[2023-01-31T08:22:08.361+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:08 INFO Executor: Starting executor ID driver on host 81d5fcd0285b
[2023-01-31T08:22:08.425+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:08 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T08:22:08.573+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:08 INFO Executor: Fetching spark://81d5fcd0285b:39461/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675153318796
[2023-01-31T08:22:09.231+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:09 INFO TransportClientFactory: Successfully created connection to 81d5fcd0285b/172.19.0.5:39461 after 369 ms (0 ms spent in bootstraps)
[2023-01-31T08:22:09.301+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:09 INFO Utils: Fetching spark://81d5fcd0285b:39461/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-4ef7e6b2-7107-489e-8064-ab7e3e15fba4/userFiles-e403c695-7469-4586-9443-edb27bb84d8c/fetchFileTemp15820119948750370236.tmp
[2023-01-31T08:22:10.764+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:10 INFO Executor: Adding file:/tmp/spark-4ef7e6b2-7107-489e-8064-ab7e3e15fba4/userFiles-e403c695-7469-4586-9443-edb27bb84d8c/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T08:22:10.765+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:10 INFO Executor: Fetching spark://81d5fcd0285b:39461/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675153318796
[2023-01-31T08:22:10.768+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:10 INFO Utils: Fetching spark://81d5fcd0285b:39461/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-4ef7e6b2-7107-489e-8064-ab7e3e15fba4/userFiles-e403c695-7469-4586-9443-edb27bb84d8c/fetchFileTemp15059128953638354008.tmp
[2023-01-31T08:22:12.393+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:12 INFO Executor: Adding file:/tmp/spark-4ef7e6b2-7107-489e-8064-ab7e3e15fba4/userFiles-e403c695-7469-4586-9443-edb27bb84d8c/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T08:22:12.499+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:12 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46657.
[2023-01-31T08:22:12.503+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:12 INFO NettyBlockTransferService: Server created on 81d5fcd0285b:46657
[2023-01-31T08:22:12.562+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:12 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T08:22:12.712+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:12 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 46657, None)
[2023-01-31T08:22:12.816+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:12 INFO BlockManagerMasterEndpoint: Registering block manager 81d5fcd0285b:46657 with 434.4 MiB RAM, BlockManagerId(driver, 81d5fcd0285b, 46657, None)
[2023-01-31T08:22:12.863+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:12 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 46657, None)
[2023-01-31T08:22:12.864+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:12 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 81d5fcd0285b, 46657, None)
[2023-01-31T08:22:16.373+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:16 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T08:22:16.428+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:16 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T08:22:30.045+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:30 INFO InMemoryFileIndex: It took 634 ms to list leaf files for 1 paths.
[2023-01-31T08:22:31.097+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:31 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T08:22:31.830+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:31 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T08:22:31.838+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:31 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 81d5fcd0285b:46657 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T08:22:31.848+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:31 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T08:22:34.924+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:34 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T08:22:35.007+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:35 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T08:22:35.172+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:35 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T08:22:35.386+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:35 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T08:22:35.401+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:35 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T08:22:35.416+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:35 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:22:35.446+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:35 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:22:35.514+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:35 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T08:22:35.818+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:35 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T08:22:35.836+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:35 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T08:22:35.843+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:35 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 81d5fcd0285b:46657 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T08:22:35.850+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:35 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:22:35.964+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:22:35.965+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:35 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T08:22:36.422+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:36 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T08:22:36.578+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:36 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T08:22:38.111+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:38 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010500__202101010600.json:0+9367
[2023-01-31T08:22:40.673+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:40 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3480 bytes result sent to driver
[2023-01-31T08:22:40.800+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:40 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 4540 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:22:40.822+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:40 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T08:22:40.841+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:40 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 5.185 s
[2023-01-31T08:22:40.877+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:40 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:22:40.880+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T08:22:40.895+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:40 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 5.719416 s
[2023-01-31T08:22:44.638+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:44 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 81d5fcd0285b:46657 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T08:22:44.778+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:44 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 81d5fcd0285b:46657 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T08:23:02.544+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:02 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:02.561+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:02 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T08:23:02.593+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:02 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:07.888+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:07 INFO CodeGenerator: Code generated in 3291.135399 ms
[2023-01-31T08:23:07.937+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:07 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T08:23:08.174+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:08 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T08:23:08.181+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:08 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 81d5fcd0285b:46657 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T08:23:08.186+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:08 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T08:23:08.310+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:08 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:10.434+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:10 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T08:23:10.480+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:10 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T08:23:10.480+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:10 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T08:23:10.480+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:10 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:10.481+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:10 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:10.481+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:10 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T08:23:10.641+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:10 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T08:23:10.690+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:10 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T08:23:10.693+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:10 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 81d5fcd0285b:46657 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T08:23:10.755+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:10 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:10.758+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:10.759+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:10 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T08:23:10.825+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:10 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:11.022+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:11 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T08:23:12.178+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:12 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T08:23:14.446+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:14 INFO CodeGenerator: Code generated in 2024.179651 ms
[2023-01-31T08:23:16.158+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:16 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1628 bytes result sent to driver
[2023-01-31T08:23:16.217+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:16 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 5399 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:16.217+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:16 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T08:23:16.222+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:16 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 5.709 s
[2023-01-31T08:23:16.223+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:16 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:16.224+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T08:23:16.237+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:16 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 5.815467 s
[2023-01-31T08:23:18.110+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:18 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:18.111+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:18 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T08:23:18.149+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:18 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:19.231+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:19 INFO CodeGenerator: Code generated in 577.977196 ms
[2023-01-31T08:23:19.409+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:19 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T08:23:19.792+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:19 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T08:23:19.868+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:19 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 81d5fcd0285b:46657 in memory (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T08:23:19.876+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:19 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 81d5fcd0285b:46657 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T08:23:19.882+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:19 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T08:23:19.892+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:19 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:20.301+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T08:23:20.309+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T08:23:20.310+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T08:23:20.310+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:20.310+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:20.326+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T08:23:20.369+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T08:23:20.413+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T08:23:20.426+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 81d5fcd0285b:46657 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T08:23:20.428+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:20.445+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:20.463+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T08:23:20.471+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:20.478+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T08:23:20.712+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T08:23:21.754+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1585 bytes result sent to driver
[2023-01-31T08:23:21.831+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1353 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:21.832+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 1.508 s
[2023-01-31T08:23:21.832+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:21.837+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T08:23:21.838+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T08:23:21.845+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 1.536029 s
[2023-01-31T08:23:24.694+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:24 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:24.699+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:24 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T08:23:24.707+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:24 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:25.948+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO CodeGenerator: Code generated in 905.388667 ms
[2023-01-31T08:23:26.061+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T08:23:26.422+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T08:23:26.425+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 81d5fcd0285b:46657 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T08:23:26.446+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T08:23:26.447+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:26.714+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 81d5fcd0285b:46657 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T08:23:26.883+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T08:23:26.888+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T08:23:26.889+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T08:23:26.889+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:26.890+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:26.891+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T08:23:26.931+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.7 MiB)
[2023-01-31T08:23:26.958+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.7 MiB)
[2023-01-31T08:23:26.958+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 81d5fcd0285b:46657 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T08:23:26.969+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:26.976+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:26.979+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T08:23:26.989+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:26.989+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T08:23:27.099+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T08:23:27.718+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-01-31T08:23:27.735+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 748 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:27.741+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 0.846 s
[2023-01-31T08:23:27.746+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:27.747+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T08:23:27.749+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T08:23:27.750+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 0.866411 s
[2023-01-31T08:23:28.242+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:28.243+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T08:23:28.244+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:28.548+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO CodeGenerator: Code generated in 252.794258 ms
[2023-01-31T08:23:28.706+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.5 MiB)
[2023-01-31T08:23:28.962+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T08:23:28.966+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 81d5fcd0285b:46657 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T08:23:29.024+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T08:23:29.055+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:29.560+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T08:23:29.564+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T08:23:29.565+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T08:23:29.565+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:29.566+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:29.577+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T08:23:29.602+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-31T08:23:29.652+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-01-31T08:23:29.654+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 81d5fcd0285b:46657 (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T08:23:29.655+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:29.663+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:29.667+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T08:23:29.670+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:29.671+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T08:23:29.778+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T08:23:30.156+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:30 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1586 bytes result sent to driver
[2023-01-31T08:23:30.161+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:30 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 493 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:30.168+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:30 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T08:23:30.175+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:30 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 0.588 s
[2023-01-31T08:23:30.176+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:30 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:30.176+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T08:23:30.176+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:30 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 0.603351 s
[2023-01-31T08:23:34.106+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:34.108+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T08:23:34.109+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:34.792+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 81d5fcd0285b:46657 in memory (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T08:23:34.936+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 81d5fcd0285b:46657 in memory (size: 7.6 KiB, free: 434.3 MiB)
[2023-01-31T08:23:35.413+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:35 INFO CodeGenerator: Code generated in 767.707327 ms
[2023-01-31T08:23:35.519+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:35 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.3 MiB)
[2023-01-31T08:23:35.790+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:35 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T08:23:35.794+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:35 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 81d5fcd0285b:46657 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T08:23:35.797+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:35 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T08:23:35.834+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:35 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:36.330+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T08:23:36.332+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) with 1 output partitions
[2023-01-31T08:23:36.333+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201)
[2023-01-31T08:23:36.333+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:36.334+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:36.366+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201), which has no missing parents
[2023-01-31T08:23:36.376+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.2 MiB)
[2023-01-31T08:23:36.432+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.2 MiB)
[2023-01-31T08:23:36.434+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 81d5fcd0285b:46657 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T08:23:36.438+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:36.492+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:36.498+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T08:23:36.499+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:36.506+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T08:23:36.596+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T08:23:37.043+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:37 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2490 bytes result sent to driver
[2023-01-31T08:23:37.045+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:37 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 549 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:37.045+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:37 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T08:23:37.046+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:37 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) finished in 0.682 s
[2023-01-31T08:23:37.047+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:37 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:37.047+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T08:23:37.047+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:37 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201, took 0.717209 s
[2023-01-31T08:23:37.477+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:37 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:37.478+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:37 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T08:23:37.479+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:37 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:38.298+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO CodeGenerator: Code generated in 355.567121 ms
[2023-01-31T08:23:38.306+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 433.0 MiB)
[2023-01-31T08:23:38.421+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.0 MiB)
[2023-01-31T08:23:38.442+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 81d5fcd0285b:46657 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T08:23:38.445+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T08:23:38.468+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:39.062+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T08:23:39.065+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T08:23:39.065+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T08:23:39.066+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:39.066+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:39.070+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T08:23:39.098+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 432.9 MiB)
[2023-01-31T08:23:39.207+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 432.9 MiB)
[2023-01-31T08:23:39.210+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 81d5fcd0285b:46657 (size: 12.7 KiB, free: 434.2 MiB)
[2023-01-31T08:23:39.327+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:39.334+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:39.336+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T08:23:39.339+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:39.383+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T08:23:39.460+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T08:23:39.688+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 81d5fcd0285b:46657 in memory (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T08:23:40.026+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:40 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1827 bytes result sent to driver
[2023-01-31T08:23:40.039+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:40 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 694 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:40.040+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:40 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T08:23:40.040+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:40 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 0.939 s
[2023-01-31T08:23:40.041+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:40 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:40.041+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T08:23:40.041+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:40 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 0.972277 s
[2023-01-31T08:23:40.626+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:40 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:40.754+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:40 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T08:23:40.755+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:40 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:42.246+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:42 INFO CodeGenerator: Code generated in 708.834774 ms
[2023-01-31T08:23:42.254+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:42 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.8 MiB)
[2023-01-31T08:23:42.277+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:42 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.7 MiB)
[2023-01-31T08:23:42.279+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:42 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 81d5fcd0285b:46657 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T08:23:42.282+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:42 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T08:23:42.291+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:42 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:42.572+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:42 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T08:23:42.580+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:42 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T08:23:42.582+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:42 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T08:23:42.582+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:42 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:42.582+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:42 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:42.593+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:42 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T08:23:42.594+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:42 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.7 MiB)
[2023-01-31T08:23:42.594+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:42 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 432.7 MiB)
[2023-01-31T08:23:42.594+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:42 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 81d5fcd0285b:46657 (size: 12.7 KiB, free: 434.1 MiB)
[2023-01-31T08:23:42.625+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:42 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:42.626+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:42.626+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:42 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T08:23:42.626+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:42 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:42.627+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:42 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T08:23:42.660+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:42 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T08:23:43.336+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:43 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1729 bytes result sent to driver
[2023-01-31T08:23:43.388+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:43 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 762 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:43.410+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:43 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T08:23:43.411+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:43 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 0.805 s
[2023-01-31T08:23:43.411+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:43 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:43.412+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T08:23:43.412+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:43 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 0.828203 s
[2023-01-31T08:23:47.041+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:47 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 81d5fcd0285b:46657 in memory (size: 12.7 KiB, free: 434.2 MiB)
[2023-01-31T08:23:47.210+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:47 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 81d5fcd0285b:46657 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T08:23:47.300+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:47 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 81d5fcd0285b:46657 in memory (size: 12.7 KiB, free: 434.2 MiB)
[2023-01-31T08:23:47.452+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:47 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 81d5fcd0285b:46657 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T08:23:47.697+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:47 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 81d5fcd0285b:46657 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T08:23:47.867+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:47 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 81d5fcd0285b:46657 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:01:43.085+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T05:00:00+00:00 [queued]>
[2023-01-31T12:01:43.181+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T05:00:00+00:00 [queued]>
[2023-01-31T12:01:43.183+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T12:01:43.184+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T12:01:43.185+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T12:01:43.293+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 05:00:00+00:00
[2023-01-31T12:01:43.339+0000] {standard_task_runner.py:55} INFO - Started process 157 to run task
[2023-01-31T12:01:43.399+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T05:00:00+00:00', '--job-id', '980', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmplqjo03ng']
[2023-01-31T12:01:43.427+0000] {standard_task_runner.py:83} INFO - Job 980: Subtask stage_total_generation
[2023-01-31T12:01:43.716+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T05:00:00+00:00 [running]> on host 8124f810dec3
[2023-01-31T12:01:44.114+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T05:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T05:00:00+00:00
[2023-01-31T12:01:44.148+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T12:01:44.151+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 398, in submit
    spark_submit_cmd = self._build_spark_submit_command(application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 344, in _build_spark_submit_command
    self.log.info("Spark-Submit cmd: %s", self._mask_cmd(connection_cmd))
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 256, in _mask_cmd
    " ".join(connection_cmd),
TypeError: sequence item 10: expected str instance, DateTime found
[2023-01-31T12:01:44.181+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=entsoe-energydata-backfill, task_id=stage_total_generation, execution_date=20210101T050000, start_date=20230131T120143, end_date=20230131T120144
[2023-01-31T12:01:44.223+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 980 for task stage_total_generation (sequence item 10: expected str instance, DateTime found; 157)
[2023-01-31T12:01:44.275+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-31T12:01:44.327+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T12:08:56.713+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T05:00:00+00:00 [queued]>
[2023-01-31T12:08:56.735+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T05:00:00+00:00 [queued]>
[2023-01-31T12:08:56.735+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T12:08:56.736+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T12:08:56.736+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T12:08:56.772+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 05:00:00+00:00
[2023-01-31T12:08:56.834+0000] {standard_task_runner.py:55} INFO - Started process 757 to run task
[2023-01-31T12:08:56.888+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T05:00:00+00:00', '--job-id', '1003', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp3ndf8ai2']
[2023-01-31T12:08:56.901+0000] {standard_task_runner.py:83} INFO - Job 1003: Subtask stage_total_generation
[2023-01-31T12:08:57.996+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T05:00:00+00:00 [running]> on host 8124f810dec3
[2023-01-31T12:08:58.329+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T05:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T05:00:00+00:00
[2023-01-31T12:08:58.363+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T12:08:58.372+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010500 202101010600 DE_TENNET 202101010000 202101010600
[2023-01-31T12:09:25.656+0000] {spark_submit.py:495} INFO - mulai periode: 202101010000
[2023-01-31T12:09:25.662+0000] {spark_submit.py:495} INFO - selesai periode: 202101010600
[2023-01-31T12:09:26.100+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:26 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T12:09:26.763+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T12:09:27.546+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:27 INFO ResourceUtils: ==============================================================
[2023-01-31T12:09:27.555+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:27 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T12:09:27.555+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:27 INFO ResourceUtils: ==============================================================
[2023-01-31T12:09:27.556+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:27 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T12:09:27.714+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:27 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T12:09:27.727+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:27 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T12:09:27.734+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:27 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T12:09:27.976+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:27 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T12:09:27.978+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:27 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T12:09:27.980+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:27 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T12:09:27.985+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:27 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T12:09:27.986+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:27 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T12:09:29.224+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:29 INFO Utils: Successfully started service 'sparkDriver' on port 34881.
[2023-01-31T12:09:29.563+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:29 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T12:09:30.074+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:30 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T12:09:30.850+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:30 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T12:09:30.854+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:30 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T12:09:30.943+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:30 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T12:09:31.130+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:31 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-7ee3c442-ed3e-4704-9f8f-a292e9d02b4a
[2023-01-31T12:09:31.450+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:31 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T12:09:31.697+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:31 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T12:09:33.790+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:33 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-01-31T12:09:34.381+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:34 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://8124f810dec3:34881/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675166966048
[2023-01-31T12:09:34.381+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:34 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://8124f810dec3:34881/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675166966048
[2023-01-31T12:09:35.861+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:35 INFO Executor: Starting executor ID driver on host 8124f810dec3
[2023-01-31T12:09:35.998+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:35 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T12:09:36.357+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:36 INFO Executor: Fetching spark://8124f810dec3:34881/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675166966048
[2023-01-31T12:09:36.708+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:36 INFO TransportClientFactory: Successfully created connection to 8124f810dec3/172.20.0.9:34881 after 133 ms (0 ms spent in bootstraps)
[2023-01-31T12:09:36.818+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:36 INFO Utils: Fetching spark://8124f810dec3:34881/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-02c88ee4-0c55-4579-a0fa-8c38e22fba36/userFiles-5d25b1e9-ed11-44a5-89a0-c23f56fdbe4e/fetchFileTemp4868915508861918852.tmp
[2023-01-31T12:09:40.078+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:40 INFO Executor: Adding file:/tmp/spark-02c88ee4-0c55-4579-a0fa-8c38e22fba36/userFiles-5d25b1e9-ed11-44a5-89a0-c23f56fdbe4e/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T12:09:40.082+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:40 INFO Executor: Fetching spark://8124f810dec3:34881/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675166966048
[2023-01-31T12:09:40.085+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:40 INFO Utils: Fetching spark://8124f810dec3:34881/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-02c88ee4-0c55-4579-a0fa-8c38e22fba36/userFiles-5d25b1e9-ed11-44a5-89a0-c23f56fdbe4e/fetchFileTemp4720005450728909919.tmp
[2023-01-31T12:09:41.638+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:41 INFO Executor: Adding file:/tmp/spark-02c88ee4-0c55-4579-a0fa-8c38e22fba36/userFiles-5d25b1e9-ed11-44a5-89a0-c23f56fdbe4e/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T12:09:41.854+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:41 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40669.
[2023-01-31T12:09:41.855+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:41 INFO NettyBlockTransferService: Server created on 8124f810dec3:40669
[2023-01-31T12:09:41.887+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:41 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T12:09:42.209+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:42 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:09:42.331+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:42 INFO BlockManagerMasterEndpoint: Registering block manager 8124f810dec3:40669 with 434.4 MiB RAM, BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:09:42.415+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:42 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:09:42.433+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:42 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:09:52.153+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:52 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T12:09:52.252+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:52 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T12:10:09.946+0000] {spark_submit.py:495} INFO - 23/01/31 12:10:09 INFO InMemoryFileIndex: It took 1252 ms to list leaf files for 1 paths.
[2023-01-31T12:10:18.226+0000] {spark_submit.py:495} INFO - 23/01/31 12:10:18 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T12:10:29.245+0000] {spark_submit.py:495} INFO - 23/01/31 12:10:29 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T12:10:29.958+0000] {spark_submit.py:495} INFO - 23/01/31 12:10:29 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 8124f810dec3:40669 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T12:10:30.737+0000] {spark_submit.py:495} INFO - 23/01/31 12:10:30 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T12:13:17.384+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:17 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 143711 ms exceeds timeout 120000 ms
[2023-01-31T12:13:17.483+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:17 WARN SparkContext: Killing executors is not supported by current scheduler.
[2023-01-31T12:13:17.566+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:17 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:13:17.572+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:17 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:13:17.572+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:17 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:13:16.907+0000] {base_job.py:232} ERROR - LocalTaskJob heartbeat got an exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3361, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 325, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 888, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 491, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 271, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 386, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 684, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 210, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 680, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/jobs/base_job.py", line 222, in heartbeat
    session.merge(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in merge
    return self._merge(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3131, in _merge
    merged = self.get(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2848, in get
    return self._get_impl(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2970, in _get_impl
    return db_load_fn(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/loading.py", line 530, in load_on_pk_identity
    session.execute(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1552, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3315, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3394, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3364, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2198, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 210, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3361, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 325, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 888, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 491, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 271, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 386, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 684, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 210, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 680, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-01-31T12:13:17.851+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:17 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:13:17.852+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:17 INFO BlockManager: Reporting 2 blocks to the master.
[2023-01-31T12:13:17.958+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:17 INFO BlockManagerInfo: Updated broadcast_0_piece0 in memory on 8124f810dec3:40669 (current size: 34.6 KiB, original size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T12:13:17.971+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:17 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:13:17.971+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:17 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:13:17.972+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:17 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:13:17.974+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:17 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:13:17.975+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:17 INFO BlockManager: Reporting 2 blocks to the master.
[2023-01-31T12:13:17.980+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:17 INFO BlockManagerInfo: Updated broadcast_0_piece0 in memory on 8124f810dec3:40669 (current size: 34.6 KiB, original size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T12:13:18.082+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:13:18.083+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:13:18.116+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:13:18.123+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:13:18.124+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManager: Reporting 2 blocks to the master.
[2023-01-31T12:13:18.187+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManagerInfo: Updated broadcast_0_piece0 in memory on 8124f810dec3:40669 (current size: 34.6 KiB, original size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T12:13:18.227+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:13:18.228+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:13:18.261+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:13:18.287+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:13:18.287+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManager: Reporting 2 blocks to the master.
[2023-01-31T12:13:18.291+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManagerInfo: Updated broadcast_0_piece0 in memory on 8124f810dec3:40669 (current size: 34.6 KiB, original size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T12:13:18.379+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:13:18.380+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:13:18.380+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:13:18.382+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:13:18.383+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManager: Reporting 2 blocks to the master.
[2023-01-31T12:13:18.386+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManagerInfo: Updated broadcast_0_piece0 in memory on 8124f810dec3:40669 (current size: 34.6 KiB, original size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T12:13:18.456+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:13:18.472+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:13:18.475+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:13:18.487+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:13:18.490+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManager: Reporting 2 blocks to the master.
[2023-01-31T12:13:18.498+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManagerInfo: Updated broadcast_0_piece0 in memory on 8124f810dec3:40669 (current size: 34.6 KiB, original size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T12:13:18.845+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:13:18.846+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:13:18.848+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:13:18.852+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:13:18.853+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManager: Reporting 2 blocks to the master.
[2023-01-31T12:13:18.866+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManagerInfo: Updated broadcast_0_piece0 in memory on 8124f810dec3:40669 (current size: 34.6 KiB, original size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T12:13:18.878+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:13:18.881+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:13:18.884+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:13:18.887+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:13:18.891+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManager: Reporting 2 blocks to the master.
[2023-01-31T12:13:18.898+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManagerInfo: Updated broadcast_0_piece0 in memory on 8124f810dec3:40669 (current size: 34.6 KiB, original size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T12:13:18.970+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:13:18.971+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:13:18.971+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:13:18.981+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:13:18.991+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManager: Reporting 2 blocks to the master.
[2023-01-31T12:13:19.022+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO BlockManagerInfo: Updated broadcast_0_piece0 in memory on 8124f810dec3:40669 (current size: 34.6 KiB, original size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T12:13:19.025+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:13:19.027+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:13:19.028+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:13:19.030+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:13:19.030+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO BlockManager: Reporting 2 blocks to the master.
[2023-01-31T12:13:19.033+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO BlockManagerInfo: Updated broadcast_0_piece0 in memory on 8124f810dec3:40669 (current size: 34.6 KiB, original size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T12:13:19.039+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:13:19.039+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:13:19.040+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:13:19.045+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:13:19.046+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO BlockManager: Reporting 2 blocks to the master.
[2023-01-31T12:13:19.049+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO BlockManagerInfo: Updated broadcast_0_piece0 in memory on 8124f810dec3:40669 (current size: 34.6 KiB, original size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T12:13:19.058+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:13:19.059+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:13:19.060+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:13:19.061+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:13:19.063+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO BlockManager: Reporting 2 blocks to the master.
[2023-01-31T12:13:19.076+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO BlockManagerInfo: Updated broadcast_0_piece0 in memory on 8124f810dec3:40669 (current size: 34.6 KiB, original size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T12:13:19.083+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:13:19.085+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:13:19.085+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:13:19.091+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:13:19.092+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO BlockManager: Reporting 2 blocks to the master.
[2023-01-31T12:13:19.092+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO BlockManagerInfo: Updated broadcast_0_piece0 in memory on 8124f810dec3:40669 (current size: 34.6 KiB, original size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T12:13:19.114+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:13:19.118+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:13:19.118+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:13:19.129+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:13:19.129+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO BlockManager: Reporting 2 blocks to the master.
[2023-01-31T12:13:19.158+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO BlockManagerInfo: Updated broadcast_0_piece0 in memory on 8124f810dec3:40669 (current size: 34.6 KiB, original size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T12:13:19.179+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:13:19.203+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:13:19.209+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:13:19.223+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:13:19.245+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO BlockManager: Reporting 2 blocks to the master.
[2023-01-31T12:13:19.266+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:19 INFO BlockManagerInfo: Updated broadcast_0_piece0 in memory on 8124f810dec3:40669 (current size: 34.6 KiB, original size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T12:13:28.276+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:28 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:13:28.282+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:28 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:13:28.283+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:28 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:13:28.283+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:28 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:13:28.283+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:28 INFO BlockManager: Reporting 2 blocks to the master.
[2023-01-31T12:13:28.316+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:28 INFO BlockManagerInfo: Updated broadcast_0_piece0 in memory on 8124f810dec3:40669 (current size: 34.6 KiB, original size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T12:13:31.194+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:31 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T12:13:31.458+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:31 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T12:13:31.640+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:31 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T12:13:31.730+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:31 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T12:13:31.731+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:31 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T12:13:31.731+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:31 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T12:13:31.733+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:31 INFO DAGScheduler: Missing parents: List()
[2023-01-31T12:13:31.757+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:31 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T12:13:32.705+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:32 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T12:13:32.752+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:32 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T12:13:32.771+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:32 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 8124f810dec3:40669 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T12:13:32.786+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:32 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T12:13:33.325+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T12:13:33.338+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:33 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T12:13:34.418+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:34 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (8124f810dec3, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T12:13:34.790+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:34 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T12:13:36.440+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:36 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010500__202101010600.json:0+9367
[2023-01-31T12:13:38.722+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:38 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:13:38.742+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:38 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:13:38.750+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:38 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:13:38.896+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:38 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:13:38.904+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:38 INFO BlockManager: Reporting 4 blocks to the master.
[2023-01-31T12:13:39.975+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:39 INFO BlockManagerInfo: Updated broadcast_1_piece0 in memory on 8124f810dec3:40669 (current size: 4.6 KiB, original size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T12:13:39.991+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:39 INFO BlockManagerInfo: Updated broadcast_0_piece0 in memory on 8124f810dec3:40669 (current size: 34.6 KiB, original size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T12:13:49.980+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:49 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:13:50.217+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:50 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,ArraySeq(),HashMap((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@112dd88)) by listener AppStatusListener took 1.828678901s.
[2023-01-31T12:13:50.222+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:49 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:13:50.303+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:50 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:13:50.311+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:50 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:13:50.394+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:50 INFO BlockManager: Reporting 4 blocks to the master.
[2023-01-31T12:14:08.082+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:50 INFO BlockManagerInfo: Updated broadcast_1_piece0 in memory on 8124f810dec3:40669 (current size: 4.6 KiB, original size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T12:14:11.405+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:11 INFO BlockManagerInfo: Updated broadcast_0_piece0 in memory on 8124f810dec3:40669 (current size: 34.6 KiB, original size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T12:14:11.568+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:11 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:14:11.568+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:11 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:14:11.607+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:11 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:14:11.608+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:11 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:14:11.608+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:11 INFO BlockManager: Reporting 4 blocks to the master.
[2023-01-31T12:14:11.609+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:11 INFO BlockManagerInfo: Updated broadcast_1_piece0 in memory on 8124f810dec3:40669 (current size: 4.6 KiB, original size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T12:14:11.609+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:11 INFO BlockManagerInfo: Updated broadcast_0_piece0 in memory on 8124f810dec3:40669 (current size: 34.6 KiB, original size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T12:14:11.682+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:11 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:14:11.690+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:11 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:14:11.690+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:11 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:14:11.708+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:11 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:14:11.709+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:11 INFO BlockManager: Reporting 4 blocks to the master.
[2023-01-31T12:14:11.886+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:11 INFO BlockManagerInfo: Updated broadcast_1_piece0 in memory on 8124f810dec3:40669 (current size: 4.6 KiB, original size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T12:14:13.522+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:13 INFO BlockManagerInfo: Updated broadcast_0_piece0 in memory on 8124f810dec3:40669 (current size: 34.6 KiB, original size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T12:14:14.245+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:14 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3480 bytes result sent to driver
[2023-01-31T12:14:14.371+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:14 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 40526 ms on 8124f810dec3 (executor driver) (1/1)
[2023-01-31T12:14:14.386+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:14 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T12:14:14.858+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:14 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 42.797 s
[2023-01-31T12:14:14.949+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:14 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T12:14:14.958+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T12:14:14.974+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:14 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 43.329898 s
[2023-01-31T12:14:18.130+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:18 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:14:18.139+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:18 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:14:18.147+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:18 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:14:18.187+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:18 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:14:18.188+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:18 INFO BlockManager: Reporting 4 blocks to the master.
[2023-01-31T12:14:18.224+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:18 INFO BlockManagerInfo: Updated broadcast_1_piece0 in memory on 8124f810dec3:40669 (current size: 4.6 KiB, original size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T12:14:18.276+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:18 INFO BlockManagerInfo: Updated broadcast_0_piece0 in memory on 8124f810dec3:40669 (current size: 34.6 KiB, original size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T12:14:35.962+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:35 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:14:36.075+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:35 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:14:36.134+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:35 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:14:36.147+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:36 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:14:36.148+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:36 INFO BlockManager: Reporting 4 blocks to the master.
[2023-01-31T12:14:59.381+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:59 INFO BlockManagerInfo: Updated broadcast_1_piece0 in memory on 8124f810dec3:40669 (current size: 4.6 KiB, original size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T12:14:59.518+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:59 INFO BlockManagerInfo: Updated broadcast_0_piece0 in memory on 8124f810dec3:40669 (current size: 34.6 KiB, original size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T12:14:59.519+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:59 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:14:59.519+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:59 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:14:59.519+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:59 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:14:59.533+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:59 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:14:59.564+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:59 INFO BlockManager: Reporting 4 blocks to the master.
[2023-01-31T12:14:59.607+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:59 INFO BlockManagerInfo: Updated broadcast_1_piece0 in memory on 8124f810dec3:40669 (current size: 4.6 KiB, original size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T12:14:59.607+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:59 INFO BlockManagerInfo: Updated broadcast_0_piece0 in memory on 8124f810dec3:40669 (current size: 34.6 KiB, original size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T12:14:59.615+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:59 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:14:59.616+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:59 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:14:59.616+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:59 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:14:59.637+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:59 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:14:59.638+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:59 INFO BlockManager: Reporting 4 blocks to the master.
[2023-01-31T12:14:59.639+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:59 INFO BlockManagerInfo: Updated broadcast_1_piece0 in memory on 8124f810dec3:40669 (current size: 4.6 KiB, original size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T12:14:59.642+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:59 INFO BlockManagerInfo: Updated broadcast_0_piece0 in memory on 8124f810dec3:40669 (current size: 34.6 KiB, original size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T12:14:59.651+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:59 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:14:59.652+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:59 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:14:59.655+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:59 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:14:59.691+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:59 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:14:59.691+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:59 INFO BlockManager: Reporting 4 blocks to the master.
[2023-01-31T12:14:59.714+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:59 INFO BlockManagerInfo: Updated broadcast_1_piece0 in memory on 8124f810dec3:40669 (current size: 4.6 KiB, original size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T12:14:59.746+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:59 INFO BlockManagerInfo: Updated broadcast_0_piece0 in memory on 8124f810dec3:40669 (current size: 34.6 KiB, original size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T12:15:00.556+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:00 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 8124f810dec3:40669 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T12:15:00.645+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:00 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 8124f810dec3:40669 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T12:15:08.099+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:08 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:15:08.100+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:08 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:15:08.100+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:08 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:15:08.102+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:08 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:15:08.103+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:08 INFO BlockManager: Reporting 0 blocks to the master.
[2023-01-31T12:15:10.834+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:10 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T12:15:10.835+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:10 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T12:15:10.844+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:10 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T12:15:11.912+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:11 INFO CodeGenerator: Code generated in 633.842367 ms
[2023-01-31T12:15:11.965+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:11 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T12:15:12.038+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:12 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T12:15:12.042+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:12 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 8124f810dec3:40669 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T12:15:12.051+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:12 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T12:15:12.093+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T12:15:12.450+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:12 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T12:15:12.460+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:12 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T12:15:12.461+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:12 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T12:15:12.461+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:12 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T12:15:12.461+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:12 INFO DAGScheduler: Missing parents: List()
[2023-01-31T12:15:12.471+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:12 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T12:15:12.530+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:12 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T12:15:12.552+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:12 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T12:15:12.554+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:12 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 8124f810dec3:40669 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T12:15:12.557+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:12 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T12:15:12.559+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T12:15:12.560+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:12 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T12:15:12.580+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:12 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (8124f810dec3, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T12:15:12.594+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:12 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T12:15:12.967+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:12 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T12:15:13.877+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:13 INFO CodeGenerator: Code generated in 745.838747 ms
[2023-01-31T12:15:14.363+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:14 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1628 bytes result sent to driver
[2023-01-31T12:15:14.450+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:14 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1883 ms on 8124f810dec3 (executor driver) (1/1)
[2023-01-31T12:15:14.451+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:14 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 1.936 s
[2023-01-31T12:15:14.452+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:14 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T12:15:14.456+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:14 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T12:15:14.464+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T12:15:14.468+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:14 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 2.008118 s
[2023-01-31T12:15:14.857+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:14 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T12:15:14.858+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:14 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T12:15:14.866+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:14 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T12:15:15.050+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO CodeGenerator: Code generated in 91.989892 ms
[2023-01-31T12:15:15.095+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T12:15:15.130+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T12:15:15.132+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 8124f810dec3:40669 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:15:15.143+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T12:15:15.146+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T12:15:15.297+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T12:15:15.300+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T12:15:15.300+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T12:15:15.301+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T12:15:15.301+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO DAGScheduler: Missing parents: List()
[2023-01-31T12:15:15.307+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T12:15:15.331+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T12:15:15.335+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T12:15:15.340+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 8124f810dec3:40669 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:15:15.342+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T12:15:15.344+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T12:15:15.345+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T12:15:15.354+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (8124f810dec3, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T12:15:15.354+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T12:15:15.436+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:15 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T12:15:16.222+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:16 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1585 bytes result sent to driver
[2023-01-31T12:15:16.309+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:16 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 955 ms on 8124f810dec3 (executor driver) (1/1)
[2023-01-31T12:15:16.313+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:16 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 1.007 s
[2023-01-31T12:15:16.313+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:16 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T12:15:16.317+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:16 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T12:15:16.318+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T12:15:16.321+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:16 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 1.020955 s
[2023-01-31T12:15:17.887+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:17 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T12:15:17.890+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:17 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T12:15:17.891+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:17 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T12:15:18.173+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:18 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:15:18.184+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:18 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:15:18.184+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:18 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:15:18.185+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:18 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:15:18.185+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:18 INFO BlockManager: Reporting 8 blocks to the master.
[2023-01-31T12:15:18.258+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:18 INFO BlockManagerInfo: Updated broadcast_5_piece0 in memory on 8124f810dec3:40669 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:15:18.265+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:18 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:15:18.271+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:18 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:40669 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:15:18.285+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:18 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:15:20.186+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:20 INFO CodeGenerator: Code generated in 1332.445891 ms
[2023-01-31T12:15:20.198+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:20 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T12:15:20.617+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:20 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T12:15:20.628+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:20 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 8124f810dec3:40669 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:15:20.642+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:20 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T12:15:20.868+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:20 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T12:15:21.091+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T12:15:21.096+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T12:15:21.096+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T12:15:21.097+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T12:15:21.097+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO DAGScheduler: Missing parents: List()
[2023-01-31T12:15:21.101+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T12:15:21.134+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.6 MiB)
[2023-01-31T12:15:21.163+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.6 MiB)
[2023-01-31T12:15:21.166+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 8124f810dec3:40669 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T12:15:21.167+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T12:15:21.179+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T12:15:21.180+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T12:15:21.197+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (8124f810dec3, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T12:15:21.200+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T12:15:21.250+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T12:15:21.670+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-01-31T12:15:21.721+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 524 ms on 8124f810dec3 (executor driver) (1/1)
[2023-01-31T12:15:21.723+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T12:15:21.727+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 0.617 s
[2023-01-31T12:15:21.727+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T12:15:21.728+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T12:15:21.729+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 0.635514 s
[2023-01-31T12:15:22.175+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:22 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T12:15:22.193+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:22 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T12:15:22.197+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:22 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T12:15:22.504+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:22 INFO CodeGenerator: Code generated in 93.643254 ms
[2023-01-31T12:15:22.519+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:22 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.4 MiB)
[2023-01-31T12:15:22.615+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:22 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T12:15:22.618+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:22 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 8124f810dec3:40669 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:15:22.621+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:22 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T12:15:22.632+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:22 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T12:15:22.830+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:22 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T12:15:22.833+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:22 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T12:15:22.834+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:22 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T12:15:22.834+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:22 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T12:15:22.835+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:22 INFO DAGScheduler: Missing parents: List()
[2023-01-31T12:15:22.841+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:22 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T12:15:22.865+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:22 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-31T12:15:22.915+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:22 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-01-31T12:15:22.920+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:22 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 8124f810dec3:40669 (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T12:15:22.924+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:22 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T12:15:22.931+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T12:15:22.934+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:22 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T12:15:22.943+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:22 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (8124f810dec3, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T12:15:22.944+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:22 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T12:15:23.070+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:23 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T12:15:23.483+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:23 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1629 bytes result sent to driver
[2023-01-31T12:15:23.487+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:23 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 546 ms on 8124f810dec3 (executor driver) (1/1)
[2023-01-31T12:15:23.489+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:23 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 0.644 s
[2023-01-31T12:15:23.490+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:23 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T12:15:23.494+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:23 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T12:15:23.494+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T12:15:23.496+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:23 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 0.665009 s
[2023-01-31T12:15:25.054+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:25 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 8124f810dec3:40669 in memory (size: 7.7 KiB, free: 434.2 MiB)
[2023-01-31T12:15:25.121+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:25 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 8124f810dec3:40669 in memory (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T12:15:25.156+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:25 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 8124f810dec3:40669 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:15:25.216+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:25 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 8124f810dec3:40669 in memory (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T12:15:25.424+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:25 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T12:15:25.426+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:25 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T12:15:25.428+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:25 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T12:15:26.378+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:26 INFO CodeGenerator: Code generated in 749.898167 ms
[2023-01-31T12:15:26.399+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:26 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.3 MiB)
[2023-01-31T12:15:26.434+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:26 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T12:15:26.436+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:26 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 8124f810dec3:40669 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:15:26.439+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:26 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T12:15:26.443+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:26 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T12:15:26.603+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:26 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T12:15:26.607+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:26 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T12:15:26.608+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:26 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T12:15:26.609+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:26 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T12:15:26.609+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:26 INFO DAGScheduler: Missing parents: List()
[2023-01-31T12:15:26.617+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:26 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T12:15:26.650+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:26 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.2 MiB)
[2023-01-31T12:15:26.658+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:26 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.2 MiB)
[2023-01-31T12:15:26.665+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:26 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 8124f810dec3:40669 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T12:15:26.672+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:26 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T12:15:26.677+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T12:15:26.680+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:26 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T12:15:26.690+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:26 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (8124f810dec3, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T12:15:26.695+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:26 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T12:15:26.966+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:26 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T12:15:27.288+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:27 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2490 bytes result sent to driver
[2023-01-31T12:15:27.300+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:27 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 598 ms on 8124f810dec3 (executor driver) (1/1)
[2023-01-31T12:15:27.312+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:27 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T12:15:27.316+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:27 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 0.678 s
[2023-01-31T12:15:27.324+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:27 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T12:15:27.324+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T12:15:27.350+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:27 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 0.738420 s
[2023-01-31T12:15:27.854+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:27 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T12:15:27.856+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:27 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T12:15:27.857+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:27 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T12:15:28.069+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:15:28.072+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:15:28.074+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:15:28.079+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:15:28.080+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO BlockManager: Reporting 12 blocks to the master.
[2023-01-31T12:15:28.095+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO BlockManagerInfo: Updated broadcast_10_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:15:28.116+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:15:28.125+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO BlockManagerInfo: Updated broadcast_8_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:15:28.151+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO BlockManagerInfo: Updated broadcast_6_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:15:28.156+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:15:28.163+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO BlockManagerInfo: Updated broadcast_11_piece0 in memory on 8124f810dec3:40669 (current size: 13.4 KiB, original size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T12:15:28.523+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO CodeGenerator: Code generated in 345.625932 ms
[2023-01-31T12:15:28.570+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 433.0 MiB)
[2023-01-31T12:15:28.637+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.0 MiB)
[2023-01-31T12:15:28.640+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 8124f810dec3:40669 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:15:28.644+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T12:15:28.651+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T12:15:28.754+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T12:15:28.758+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T12:15:28.759+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T12:15:28.759+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T12:15:28.760+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO DAGScheduler: Missing parents: List()
[2023-01-31T12:15:28.768+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T12:15:28.787+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 432.9 MiB)
[2023-01-31T12:15:28.792+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.9 MiB)
[2023-01-31T12:15:28.795+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 8124f810dec3:40669 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T12:15:28.797+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T12:15:28.801+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T12:15:28.803+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T12:15:28.808+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (8124f810dec3, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T12:15:28.812+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T12:15:29.014+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:29 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T12:15:29.335+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:29 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-01-31T12:15:29.340+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:29 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 532 ms on 8124f810dec3 (executor driver) (1/1)
[2023-01-31T12:15:29.340+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:29 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T12:15:29.348+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:29 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 0.567 s
[2023-01-31T12:15:29.349+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:29 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T12:15:29.349+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T12:15:29.349+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:29 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 0.586504 s
[2023-01-31T12:15:29.936+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:29 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T12:15:29.938+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:29 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T12:15:29.939+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:29 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T12:15:33.258+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:33 INFO CodeGenerator: Code generated in 2684.443236 ms
[2023-01-31T12:15:33.523+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:33 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.7 MiB)
[2023-01-31T12:15:35.714+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:35 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.7 MiB)
[2023-01-31T12:15:36.002+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:36 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 8124f810dec3:40669 (size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T12:15:36.049+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:36 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:204
[2023-01-31T12:15:36.083+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:36 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T12:15:39.902+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:39 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:15:39.943+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:39 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:204
[2023-01-31T12:15:39.944+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:39 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:15:39.944+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:39 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:15:39.945+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:39 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:15:39.945+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:39 INFO BlockManager: Reporting 18 blocks to the master.
[2023-01-31T12:15:40.723+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:40 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:204) with 1 output partitions
[2023-01-31T12:15:40.874+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:40 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:204)
[2023-01-31T12:15:41.919+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:40 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T12:15:42.524+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:40 INFO DAGScheduler: Missing parents: List()
[2023-01-31T12:15:42.525+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:42 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:204), which has no missing parents
[2023-01-31T12:15:42.736+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:42 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T12:15:44.420+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:44 INFO BlockManagerInfo: Updated broadcast_13_piece0 in memory on 8124f810dec3:40669 (current size: 12.6 KiB, original size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T12:15:46.564+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:46 INFO BlockManagerInfo: Updated broadcast_14_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T12:15:46.625+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:46 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.7 MiB)
[2023-01-31T12:15:53.573+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:53 INFO BlockManagerInfo: Updated broadcast_10_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T12:15:53.704+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:53 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 432.6 MiB)
[2023-01-31T12:15:54.799+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:54 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 8124f810dec3:40669 (size: 12.7 KiB, free: 434.1 MiB)
[2023-01-31T12:16:22.525+0000] {spark_submit.py:495} INFO - 23/01/31 12:16:22 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T12:16:19.287+0000] {base_job.py:232} ERROR - LocalTaskJob heartbeat got an exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3361, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 325, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 888, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 491, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 271, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 386, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 684, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 210, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 680, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/jobs/base_job.py", line 204, in heartbeat
    session.merge(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in merge
    return self._merge(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3131, in _merge
    merged = self.get(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2848, in get
    return self._get_impl(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2970, in _get_impl
    return db_load_fn(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/loading.py", line 530, in load_on_pk_identity
    session.execute(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1552, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3315, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3394, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3364, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2198, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 210, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3361, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 325, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 888, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 491, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 271, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 386, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 684, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 210, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 680, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-01-31T12:17:27.612+0000] {spark_submit.py:495} INFO - 23/01/31 12:17:14 INFO BlockManagerInfo: Updated broadcast_15_piece0 in memory on 8124f810dec3:40669 (current size: 12.7 KiB, original size: 12.7 KiB, free: 434.1 MiB)
[2023-01-31T12:20:11.870+0000] {base_job.py:232} ERROR - LocalTaskJob heartbeat got an exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3361, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 325, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 888, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 491, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 271, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 386, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 684, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 210, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 680, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/jobs/base_job.py", line 204, in heartbeat
    session.merge(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in merge
    return self._merge(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3131, in _merge
    merged = self.get(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2848, in get
    return self._get_impl(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2970, in _get_impl
    return db_load_fn(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/loading.py", line 530, in load_on_pk_identity
    session.execute(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1552, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3315, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3394, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3364, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2198, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 210, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3361, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 325, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 888, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 491, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 271, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 386, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 684, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 210, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 680, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-01-31T12:20:27.933+0000] {spark_submit.py:495} INFO - 23/01/31 12:20:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:204) (first 15 tasks are for partitions Vector(0))
[2023-01-31T12:20:28.182+0000] {spark_submit.py:495} INFO - 23/01/31 12:20:27 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T12:20:30.013+0000] {spark_submit.py:495} INFO - 23/01/31 12:20:29 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T12:20:31.051+0000] {spark_submit.py:495} INFO - 23/01/31 12:20:31 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (8124f810dec3, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T12:20:31.198+0000] {spark_submit.py:495} INFO - 23/01/31 12:20:31 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.TimeoutException: Cannot receive any reply from 8124f810dec3:34881 in 120 seconds
[2023-01-31T12:20:31.266+0000] {spark_submit.py:495} INFO - 23/01/31 12:20:31 INFO BlockManagerInfo: Updated broadcast_8_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T12:20:31.267+0000] {spark_submit.py:495} INFO - 23/01/31 12:20:31 INFO BlockManagerInfo: Updated broadcast_6_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T12:20:31.718+0000] {spark_submit.py:495} INFO - 23/01/31 12:20:31 INFO AsyncEventQueue: Process of event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(driver, 8124f810dec3, 40669, None),broadcast_15_piece0,StorageLevel(memory, 1 replicas),12955,0)) by listener HeartbeatReceiver took 179.044102665s.
[2023-01-31T12:20:32.929+0000] {spark_submit.py:495} INFO - 23/01/31 12:20:32 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T12:20:33.042+0000] {spark_submit.py:495} INFO - 23/01/31 12:20:32 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T12:20:33.084+0000] {spark_submit.py:495} INFO - 23/01/31 12:20:33 INFO BlockManagerInfo: Updated broadcast_11_piece0 in memory on 8124f810dec3:40669 (current size: 13.4 KiB, original size: 13.4 KiB, free: 434.1 MiB)
[2023-01-31T12:20:34.940+0000] {spark_submit.py:495} INFO - 23/01/31 12:20:34 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:20:35.192+0000] {spark_submit.py:495} INFO - 23/01/31 12:20:34 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:20:35.565+0000] {spark_submit.py:495} INFO - 23/01/31 12:20:34 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:20:36.004+0000] {spark_submit.py:495} INFO - 23/01/31 12:20:35 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:20:36.751+0000] {spark_submit.py:495} INFO - 23/01/31 12:20:35 INFO BlockManager: Reporting 20 blocks to the master.
[2023-01-31T12:20:41.038+0000] {spark_submit.py:495} INFO - 23/01/31 12:20:41 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T12:20:49.705+0000] {spark_submit.py:495} INFO - 23/01/31 12:20:49 INFO BlockManagerInfo: Updated broadcast_13_piece0 in memory on 8124f810dec3:40669 (current size: 12.6 KiB, original size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T12:20:58.825+0000] {spark_submit.py:495} INFO - 23/01/31 12:20:57 INFO BlockManagerInfo: Updated broadcast_14_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T12:21:57.848+0000] {spark_submit.py:495} INFO - 23/01/31 12:21:57 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,ArraySeq(),HashMap((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@270df45b)) by listener AppStatusListener took 39.178134927s.
[2023-01-31T12:21:58.481+0000] {spark_submit.py:495} INFO - 23/01/31 12:21:58 INFO BlockManagerInfo: Updated broadcast_10_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T12:21:59.668+0000] {spark_submit.py:495} INFO - 23/01/31 12:21:59 INFO AsyncEventQueue: Process of event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(driver, 8124f810dec3, 40669, None),broadcast_10_piece0,StorageLevel(memory, 1 replicas),35281,0)) by listener AppStatusListener took 1.109219629s.
[2023-01-31T12:21:59.766+0000] {spark_submit.py:495} INFO - 23/01/31 12:21:59 INFO BlockManagerInfo: Updated broadcast_15_piece0 in memory on 8124f810dec3:40669 (current size: 12.7 KiB, original size: 12.7 KiB, free: 434.1 MiB)
[2023-01-31T12:22:00.080+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T12:22:00.092+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_8_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T12:22:00.136+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_6_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T12:22:00.265+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T12:22:00.272+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_11_piece0 in memory on 8124f810dec3:40669 (current size: 13.4 KiB, original size: 13.4 KiB, free: 434.1 MiB)
[2023-01-31T12:22:00.294+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:00.367+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:22:00.400+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:00.453+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:00.484+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManager: Reporting 20 blocks to the master.
[2023-01-31T12:22:00.490+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T12:22:00.514+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T12:22:00.515+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_13_piece0 in memory on 8124f810dec3:40669 (current size: 12.6 KiB, original size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T12:22:00.526+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_14_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T12:22:00.642+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerInfo: Updated broadcast_10_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T12:22:01.199+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_15_piece0 in memory on 8124f810dec3:40669 (current size: 12.7 KiB, original size: 12.7 KiB, free: 434.1 MiB)
[2023-01-31T12:22:01.285+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T12:22:01.292+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_8_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T12:22:01.336+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 8124f810dec3:40669 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.339+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_6_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.374+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.457+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_11_piece0 in memory on 8124f810dec3:40669 (current size: 13.4 KiB, original size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.472+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:01.473+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:22:01.473+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:01.498+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:01.499+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManager: Reporting 18 blocks to the master.
[2023-01-31T12:22:01.501+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.515+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_13_piece0 in memory on 8124f810dec3:40669 (current size: 12.6 KiB, original size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.534+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_14_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.584+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_10_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.589+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_15_piece0 in memory on 8124f810dec3:40669 (current size: 12.7 KiB, original size: 12.7 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.632+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 8124f810dec3:40669 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.654+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.657+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.668+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_11_piece0 in memory on 8124f810dec3:40669 (current size: 13.4 KiB, original size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.674+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:01.675+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:22:01.675+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:01.697+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:01.697+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManager: Reporting 16 blocks to the master.
[2023-01-31T12:22:01.706+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.724+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_13_piece0 in memory on 8124f810dec3:40669 (current size: 12.6 KiB, original size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.814+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_14_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.834+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_10_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.848+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_15_piece0 in memory on 8124f810dec3:40669 (current size: 12.7 KiB, original size: 12.7 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.920+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:01.932+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:02.055+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_11_piece0 in memory on 8124f810dec3:40669 (current size: 13.4 KiB, original size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T12:22:02.077+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:02.079+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:22:02.080+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:02.258+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 8124f810dec3:40669 in memory (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T12:22:02.277+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:02.278+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManager: Reporting 14 blocks to the master.
[2023-01-31T12:22:02.282+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:02.287+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_13_piece0 in memory on 8124f810dec3:40669 (current size: 12.6 KiB, original size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T12:22:02.294+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_14_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:02.304+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_10_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:02.321+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_15_piece0 in memory on 8124f810dec3:40669 (current size: 12.7 KiB, original size: 12.7 KiB, free: 434.2 MiB)
[2023-01-31T12:22:02.330+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:02.349+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:02.357+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:02.360+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:22:02.361+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:02.371+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:02.372+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManager: Reporting 14 blocks to the master.
[2023-01-31T12:22:02.373+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:02.382+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_13_piece0 in memory on 8124f810dec3:40669 (current size: 12.6 KiB, original size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T12:22:02.390+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_14_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:02.405+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1772 bytes result sent to driver
[2023-01-31T12:22:02.421+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 93110 ms on 8124f810dec3 (executor driver) (1/1)
[2023-01-31T12:22:02.422+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T12:22:02.422+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:204) finished in 379.762 s
[2023-01-31T12:22:02.422+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T12:22:02.423+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T12:22:02.423+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:204, took 382.516188 s
[2023-01-31T12:22:02.441+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 8124f810dec3:40669 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:02.442+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_10_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:02.447+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_15_piece0 in memory on 8124f810dec3:40669 (current size: 12.7 KiB, original size: 12.7 KiB, free: 434.2 MiB)
[2023-01-31T12:22:02.458+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:02.461+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:02.462+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:22:02.466+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:02.469+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:02.477+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManager: Reporting 12 blocks to the master.
[2023-01-31T12:22:02.481+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:02.495+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_13_piece0 in memory on 8124f810dec3:40669 (current size: 12.6 KiB, original size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T12:22:02.499+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_14_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:02.507+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_10_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:02.522+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_15_piece0 in memory on 8124f810dec3:40669 (current size: 12.7 KiB, original size: 12.7 KiB, free: 434.2 MiB)
[2023-01-31T12:22:02.526+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:02.538+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:02.539+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:22:02.540+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:02.559+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:02.563+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManager: Reporting 12 blocks to the master.
[2023-01-31T12:22:02.573+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:02.586+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_13_piece0 in memory on 8124f810dec3:40669 (current size: 12.6 KiB, original size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T12:22:02.597+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_14_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:02.609+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_10_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:02.652+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_15_piece0 in memory on 8124f810dec3:40669 (current size: 12.7 KiB, original size: 12.7 KiB, free: 434.2 MiB)
[2023-01-31T12:22:02.655+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:02.671+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:02.671+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:22:02.674+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:02.741+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:02.750+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManager: Reporting 11 blocks to the master.
[2023-01-31T12:22:02.751+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 8124f810dec3:40669 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:02.751+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:02.779+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_13_piece0 in memory on 8124f810dec3:40669 (current size: 12.6 KiB, original size: 12.6 KiB, free: 434.3 MiB)
[2023-01-31T12:22:02.804+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_14_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:02.866+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_15_piece0 in memory on 8124f810dec3:40669 (current size: 12.7 KiB, original size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:02.916+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:02.923+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 8124f810dec3:40669 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:02.944+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:02.945+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:22:02.945+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:02.951+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:02.951+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManager: Reporting 8 blocks to the master.
[2023-01-31T12:22:02.954+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:02.964+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_13_piece0 in memory on 8124f810dec3:40669 (current size: 12.6 KiB, original size: 12.6 KiB, free: 434.3 MiB)
[2023-01-31T12:22:02.982+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_14_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:03.053+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_15_piece0 in memory on 8124f810dec3:40669 (current size: 12.7 KiB, original size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:03.161+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:03.163+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:22:03.165+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:03.168+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:03.185+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManager: Reporting 8 blocks to the master.
[2023-01-31T12:22:03.195+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:03.243+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_13_piece0 in memory on 8124f810dec3:40669 (current size: 12.6 KiB, original size: 12.6 KiB, free: 434.3 MiB)
[2023-01-31T12:22:03.245+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_14_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:03.248+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 8124f810dec3:40669 in memory (size: 12.6 KiB, free: 434.3 MiB)
[2023-01-31T12:22:03.249+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_15_piece0 in memory on 8124f810dec3:40669 (current size: 12.7 KiB, original size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:03.256+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:03.272+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:22:03.273+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:03.282+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:03.318+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManager: Reporting 7 blocks to the master.
[2023-01-31T12:22:03.319+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:03.362+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_14_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:03.386+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_15_piece0 in memory on 8124f810dec3:40669 (current size: 12.7 KiB, original size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:03.388+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:03.389+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:22:03.389+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:03.392+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:03.393+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManager: Reporting 6 blocks to the master.
[2023-01-31T12:22:03.475+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:03.480+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_14_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:03.486+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_15_piece0 in memory on 8124f810dec3:40669 (current size: 12.7 KiB, original size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:03.487+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:03.487+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:22:03.497+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:03.516+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:03.531+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManager: Reporting 6 blocks to the master.
[2023-01-31T12:22:03.560+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:03.576+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_14_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:03.611+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_15_piece0 in memory on 8124f810dec3:40669 (current size: 12.7 KiB, original size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:03.615+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:03.640+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:22:03.671+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:03.685+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:03.803+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManager: Reporting 6 blocks to the master.
[2023-01-31T12:22:03.804+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:03.893+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_14_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:03.928+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_15_piece0 in memory on 8124f810dec3:40669 (current size: 12.7 KiB, original size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:03.928+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:03.929+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:22:03.929+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:03.929+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:03.974+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManager: Reporting 6 blocks to the master.
[2023-01-31T12:22:03.975+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:03.975+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_14_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:03.976+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_15_piece0 in memory on 8124f810dec3:40669 (current size: 12.7 KiB, original size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:03.976+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:04.047+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:22:04.048+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:04.048+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:04.052+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManager: Reporting 6 blocks to the master.
[2023-01-31T12:22:04.052+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:04.056+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_14_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:04.057+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_15_piece0 in memory on 8124f810dec3:40669 (current size: 12.7 KiB, original size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:04.058+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:04.061+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:22:04.061+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:04.064+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:04.064+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManager: Reporting 6 blocks to the master.
[2023-01-31T12:22:04.065+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:04.220+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_14_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:04.247+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_15_piece0 in memory on 8124f810dec3:40669 (current size: 12.7 KiB, original size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:04.250+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:04.251+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:22:04.251+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:04.252+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:04.258+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManager: Reporting 6 blocks to the master.
[2023-01-31T12:22:04.269+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:04.288+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_14_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:04.294+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_15_piece0 in memory on 8124f810dec3:40669 (current size: 12.7 KiB, original size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:04.340+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:04.386+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:22:04.387+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:04.387+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:04.388+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManager: Reporting 6 blocks to the master.
[2023-01-31T12:22:04.434+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:04.449+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_14_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:04.449+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_15_piece0 in memory on 8124f810dec3:40669 (current size: 12.7 KiB, original size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:04.491+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:04.601+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:22:04.601+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:04.602+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:04.602+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManager: Reporting 6 blocks to the master.
[2023-01-31T12:22:04.603+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:05.199+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:05 INFO BlockManagerInfo: Updated broadcast_14_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:05.346+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:05 INFO BlockManagerInfo: Updated broadcast_15_piece0 in memory on 8124f810dec3:40669 (current size: 12.7 KiB, original size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:05.373+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:05 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:05.374+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:05 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:22:05.380+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:05 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:05.409+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:05 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:05.421+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:05 INFO BlockManager: Reporting 6 blocks to the master.
[2023-01-31T12:22:05.571+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:05 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:05.684+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:05 INFO BlockManagerInfo: Updated broadcast_14_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:05.788+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:05 INFO BlockManagerInfo: Updated broadcast_15_piece0 in memory on 8124f810dec3:40669 (current size: 12.7 KiB, original size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:05.793+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:05 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:05.793+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:05 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:22:05.794+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:05 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:05.865+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:05 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:05.865+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:05 INFO BlockManager: Reporting 6 blocks to the master.
[2023-01-31T12:22:05.884+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:05 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:06.012+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO BlockManagerInfo: Updated broadcast_14_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:06.128+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO BlockManagerInfo: Updated broadcast_15_piece0 in memory on 8124f810dec3:40669 (current size: 12.7 KiB, original size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:06.133+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:06.133+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:22:06.134+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:06.217+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:06.219+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO BlockManager: Reporting 6 blocks to the master.
[2023-01-31T12:22:06.246+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:06.330+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO BlockManagerInfo: Updated broadcast_14_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:06.408+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO BlockManagerInfo: Updated broadcast_15_piece0 in memory on 8124f810dec3:40669 (current size: 12.7 KiB, original size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:06.479+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:06.480+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:22:06.480+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:06.482+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:06.483+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO BlockManager: Reporting 6 blocks to the master.
[2023-01-31T12:22:06.599+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:06.647+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO BlockManagerInfo: Updated broadcast_14_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:06.648+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO BlockManagerInfo: Updated broadcast_15_piece0 in memory on 8124f810dec3:40669 (current size: 12.7 KiB, original size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:06.680+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:06.680+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:22:06.680+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:06.831+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:06.831+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO BlockManager: Reporting 6 blocks to the master.
[2023-01-31T12:22:06.860+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:06.941+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO BlockManagerInfo: Updated broadcast_14_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:07.032+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerInfo: Updated broadcast_15_piece0 in memory on 8124f810dec3:40669 (current size: 12.7 KiB, original size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:07.044+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:07.044+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:22:07.045+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:07.102+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:07.102+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManager: Reporting 6 blocks to the master.
[2023-01-31T12:22:07.132+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:07.189+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerInfo: Updated broadcast_14_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:07.190+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerInfo: Updated broadcast_15_piece0 in memory on 8124f810dec3:40669 (current size: 12.7 KiB, original size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:07.298+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:07.299+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:22:07.300+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:07.313+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:07.316+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManager: Reporting 6 blocks to the master.
[2023-01-31T12:22:07.341+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:07.377+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerInfo: Updated broadcast_14_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:07.483+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerInfo: Updated broadcast_15_piece0 in memory on 8124f810dec3:40669 (current size: 12.7 KiB, original size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:07.563+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:07.564+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:22:07.564+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:07.602+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:07.603+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManager: Reporting 6 blocks to the master.
[2023-01-31T12:22:07.654+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:07.679+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerInfo: Updated broadcast_14_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:07.732+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerInfo: Updated broadcast_15_piece0 in memory on 8124f810dec3:40669 (current size: 12.7 KiB, original size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:07.769+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:07.770+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:22:07.771+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:07.785+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:07.786+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManager: Reporting 6 blocks to the master.
[2023-01-31T12:22:07.817+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:07.927+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerInfo: Updated broadcast_14_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:07.931+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerInfo: Updated broadcast_15_piece0 in memory on 8124f810dec3:40669 (current size: 12.7 KiB, original size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.018+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:08.019+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:22:08.043+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:08.051+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:08.054+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManager: Reporting 6 blocks to the master.
[2023-01-31T12:22:08.063+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.110+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_14_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.142+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_15_piece0 in memory on 8124f810dec3:40669 (current size: 12.7 KiB, original size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.230+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:08.318+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:22:08.321+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:08.329+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:08.332+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManager: Reporting 6 blocks to the master.
[2023-01-31T12:22:08.382+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.404+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_14_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.468+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_15_piece0 in memory on 8124f810dec3:40669 (current size: 12.7 KiB, original size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.535+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:08.536+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:22:08.536+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:08.563+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:08.564+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManager: Reporting 6 blocks to the master.
[2023-01-31T12:22:08.578+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.579+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_14_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.610+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_15_piece0 in memory on 8124f810dec3:40669 (current size: 12.7 KiB, original size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.613+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:08.614+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:22:08.616+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:08.618+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:08.621+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManager: Reporting 6 blocks to the master.
[2023-01-31T12:22:08.622+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.624+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_14_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.627+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_15_piece0 in memory on 8124f810dec3:40669 (current size: 12.7 KiB, original size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.628+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:08.631+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:22:08.631+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:08.632+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:08.632+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManager: Reporting 6 blocks to the master.
[2023-01-31T12:22:08.632+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.633+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_14_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.634+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_15_piece0 in memory on 8124f810dec3:40669 (current size: 12.7 KiB, original size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.637+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:08.651+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:22:08.666+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:08.667+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:08.667+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManager: Reporting 6 blocks to the master.
[2023-01-31T12:22:08.671+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.674+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_14_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.675+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_15_piece0 in memory on 8124f810dec3:40669 (current size: 12.7 KiB, original size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.676+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:08.677+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:22:08.678+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:08.682+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:08.684+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManager: Reporting 6 blocks to the master.
[2023-01-31T12:22:08.693+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.694+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_14_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.698+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_15_piece0 in memory on 8124f810dec3:40669 (current size: 12.7 KiB, original size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.702+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:08.706+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:22:08.708+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:08.711+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:08.717+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManager: Reporting 6 blocks to the master.
[2023-01-31T12:22:08.717+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.721+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_14_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.724+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_15_piece0 in memory on 8124f810dec3:40669 (current size: 12.7 KiB, original size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.727+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:08.729+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:22:08.734+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:08.734+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:08.742+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManager: Reporting 6 blocks to the master.
[2023-01-31T12:22:08.744+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.746+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_14_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.750+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_15_piece0 in memory on 8124f810dec3:40669 (current size: 12.7 KiB, original size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:12.442+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:12 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 8124f810dec3:40669 in memory (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T12:22:12.504+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:12 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 8124f810dec3:40669 in memory (size: 12.7 KiB, free: 434.4 MiB)
[2023-01-31T12:22:18.066+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:18 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:18.068+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:18 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:22:18.069+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:18 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:18.072+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:18 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:18.073+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:18 INFO BlockManager: Reporting 2 blocks to the master.
[2023-01-31T12:22:18.074+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:18 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T12:22:28.061+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:28 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:28.062+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:28 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:22:28.062+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:28 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:28.064+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:28 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:28.065+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:28 INFO BlockManager: Reporting 2 blocks to the master.
[2023-01-31T12:22:28.065+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:28 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T12:22:30.286+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:30 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T12:22:30.311+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T12:22:30.311+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T12:22:30.314+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T12:22:30.315+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T12:22:30.316+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T12:22:30.318+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T12:22:31.059+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:31 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-31T12:22:31.061+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:31 INFO DAGScheduler: Got job 8 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T12:22:31.061+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:31 INFO DAGScheduler: Final stage: ResultStage 8 (save at BigQueryWriteHelper.java:105)
[2023-01-31T12:22:31.062+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:31 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T12:22:31.063+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:31 INFO DAGScheduler: Missing parents: List()
[2023-01-31T12:22:31.063+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:31 INFO DAGScheduler: Submitting ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T12:22:31.209+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:31 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 209.9 KiB, free 434.0 MiB)
[2023-01-31T12:22:31.225+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:31 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 74.3 KiB, free 433.9 MiB)
[2023-01-31T12:22:31.228+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:31 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 8124f810dec3:40669 (size: 74.3 KiB, free: 434.3 MiB)
[2023-01-31T12:22:31.230+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:31 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513
[2023-01-31T12:22:31.232+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T12:22:31.233+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:31 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2023-01-31T12:22:31.246+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:31 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (8124f810dec3, executor driver, partition 0, PROCESS_LOCAL, 7484 bytes) taskResourceAssignments Map()
[2023-01-31T12:22:31.252+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:31 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2023-01-31T12:22:31.505+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T12:22:31.512+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T12:22:31.517+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:31 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T12:22:31.519+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T12:22:31.520+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T12:22:31.548+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T12:22:31.578+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:31 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T12:22:31.595+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:31 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T12:22:31.802+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:31 INFO ParquetOutputFormat: Parquet block size to 134217728
[2023-01-31T12:22:31.803+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:31 INFO ParquetOutputFormat: Validation is off
[2023-01-31T12:22:31.806+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:31 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-01-31T12:22:31.810+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:31 INFO ParquetOutputFormat: Parquet properties are:
[2023-01-31T12:22:31.811+0000] {spark_submit.py:495} INFO - Parquet page size to 1048576
[2023-01-31T12:22:31.811+0000] {spark_submit.py:495} INFO - Parquet dictionary page size to 1048576
[2023-01-31T12:22:31.811+0000] {spark_submit.py:495} INFO - Dictionary is true
[2023-01-31T12:22:31.812+0000] {spark_submit.py:495} INFO - Writer version is: PARQUET_1_0
[2023-01-31T12:22:31.812+0000] {spark_submit.py:495} INFO - Page size checking is: estimated
[2023-01-31T12:22:31.813+0000] {spark_submit.py:495} INFO - Min row count for page size check is: 100
[2023-01-31T12:22:31.813+0000] {spark_submit.py:495} INFO - Max row count for page size check is: 10000
[2023-01-31T12:22:31.814+0000] {spark_submit.py:495} INFO - Truncate length for column indexes is: 64
[2023-01-31T12:22:31.815+0000] {spark_submit.py:495} INFO - Truncate length for statistics min/max  is: 2147483647
[2023-01-31T12:22:31.819+0000] {spark_submit.py:495} INFO - Bloom filter enabled: false
[2023-01-31T12:22:31.822+0000] {spark_submit.py:495} INFO - Max Bloom filter size for a column is 1048576
[2023-01-31T12:22:31.823+0000] {spark_submit.py:495} INFO - Bloom filter expected number of distinct values are: null
[2023-01-31T12:22:31.824+0000] {spark_submit.py:495} INFO - Page row count limit to 20000
[2023-01-31T12:22:31.834+0000] {spark_submit.py:495} INFO - Writing page checksums is: on
[2023-01-31T12:22:32.017+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:32 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-01-31T12:22:32.017+0000] {spark_submit.py:495} INFO - {
[2023-01-31T12:22:32.017+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-31T12:22:32.018+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-31T12:22:32.018+0000] {spark_submit.py:495} INFO - "name" : "measured_at",
[2023-01-31T12:22:32.018+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T12:22:32.018+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T12:22:32.018+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:32.019+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:32.019+0000] {spark_submit.py:495} INFO - "name" : "created_at",
[2023-01-31T12:22:32.019+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T12:22:32.019+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T12:22:32.019+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:32.020+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:32.020+0000] {spark_submit.py:495} INFO - "name" : "Biomass - Actual Aggregated",
[2023-01-31T12:22:32.020+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:32.020+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:32.020+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:32.020+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:32.021+0000] {spark_submit.py:495} INFO - "name" : "Fossil Brown coal/Lignite - Actual Aggregated",
[2023-01-31T12:22:32.021+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:32.021+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:32.021+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:32.021+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:32.022+0000] {spark_submit.py:495} INFO - "name" : "Fossil Gas - Actual Aggregated",
[2023-01-31T12:22:32.022+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:32.022+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:32.022+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:32.022+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:32.023+0000] {spark_submit.py:495} INFO - "name" : "Fossil Hard coal - Actual Aggregated",
[2023-01-31T12:22:32.023+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:32.023+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:32.023+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:32.023+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:32.023+0000] {spark_submit.py:495} INFO - "name" : "Fossil Oil - Actual Aggregated",
[2023-01-31T12:22:32.024+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:32.024+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:32.024+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:32.024+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:32.024+0000] {spark_submit.py:495} INFO - "name" : "Geothermal - Actual Aggregated",
[2023-01-31T12:22:32.025+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:32.025+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:32.025+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:32.025+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:32.025+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Aggregated",
[2023-01-31T12:22:32.025+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:32.026+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:32.026+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:32.038+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:32.039+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Consumption",
[2023-01-31T12:22:32.039+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:32.039+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:32.040+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:32.040+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:32.040+0000] {spark_submit.py:495} INFO - "name" : "Hydro Run-of-river and poundage - Actual Aggregated",
[2023-01-31T12:22:32.041+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:32.041+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:32.041+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:32.041+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:32.042+0000] {spark_submit.py:495} INFO - "name" : "Hydro Water Reservoir - Actual Aggregated",
[2023-01-31T12:22:32.042+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:32.042+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:32.042+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:32.043+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:32.043+0000] {spark_submit.py:495} INFO - "name" : "Nuclear - Actual Aggregated",
[2023-01-31T12:22:32.043+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:32.043+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:32.044+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:32.044+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:32.044+0000] {spark_submit.py:495} INFO - "name" : "Other - Actual Aggregated",
[2023-01-31T12:22:32.045+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:32.045+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:32.050+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:32.051+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:32.051+0000] {spark_submit.py:495} INFO - "name" : "Other renewable - Actual Aggregated",
[2023-01-31T12:22:32.051+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:32.051+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:32.052+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:32.052+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:32.052+0000] {spark_submit.py:495} INFO - "name" : "Solar - Actual Aggregated",
[2023-01-31T12:22:32.053+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:32.053+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:32.053+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:32.053+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:32.054+0000] {spark_submit.py:495} INFO - "name" : "Waste - Actual Aggregated",
[2023-01-31T12:22:32.054+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:32.054+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:32.055+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:32.055+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:32.055+0000] {spark_submit.py:495} INFO - "name" : "Wind Offshore - Actual Aggregated",
[2023-01-31T12:22:32.055+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:32.056+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:32.056+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:32.056+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:32.058+0000] {spark_submit.py:495} INFO - "name" : "Wind Onshore - Actual Aggregated",
[2023-01-31T12:22:32.059+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:32.059+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:32.059+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:32.059+0000] {spark_submit.py:495} INFO - } ]
[2023-01-31T12:22:32.060+0000] {spark_submit.py:495} INFO - }
[2023-01-31T12:22:32.062+0000] {spark_submit.py:495} INFO - and corresponding Parquet message type:
[2023-01-31T12:22:32.062+0000] {spark_submit.py:495} INFO - message spark_schema {
[2023-01-31T12:22:32.063+0000] {spark_submit.py:495} INFO - required int96 measured_at;
[2023-01-31T12:22:32.063+0000] {spark_submit.py:495} INFO - required int96 created_at;
[2023-01-31T12:22:32.063+0000] {spark_submit.py:495} INFO - optional double Biomass - Actual Aggregated;
[2023-01-31T12:22:32.064+0000] {spark_submit.py:495} INFO - optional double Fossil Brown coal/Lignite - Actual Aggregated;
[2023-01-31T12:22:32.064+0000] {spark_submit.py:495} INFO - optional double Fossil Gas - Actual Aggregated;
[2023-01-31T12:22:32.066+0000] {spark_submit.py:495} INFO - optional double Fossil Hard coal - Actual Aggregated;
[2023-01-31T12:22:32.067+0000] {spark_submit.py:495} INFO - optional double Fossil Oil - Actual Aggregated;
[2023-01-31T12:22:32.067+0000] {spark_submit.py:495} INFO - optional double Geothermal - Actual Aggregated;
[2023-01-31T12:22:32.067+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Aggregated;
[2023-01-31T12:22:32.067+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Consumption;
[2023-01-31T12:22:32.068+0000] {spark_submit.py:495} INFO - optional double Hydro Run-of-river and poundage - Actual Aggregated;
[2023-01-31T12:22:32.068+0000] {spark_submit.py:495} INFO - optional double Hydro Water Reservoir - Actual Aggregated;
[2023-01-31T12:22:32.068+0000] {spark_submit.py:495} INFO - optional double Nuclear - Actual Aggregated;
[2023-01-31T12:22:32.074+0000] {spark_submit.py:495} INFO - optional double Other - Actual Aggregated;
[2023-01-31T12:22:32.075+0000] {spark_submit.py:495} INFO - optional double Other renewable - Actual Aggregated;
[2023-01-31T12:22:32.075+0000] {spark_submit.py:495} INFO - optional double Solar - Actual Aggregated;
[2023-01-31T12:22:32.075+0000] {spark_submit.py:495} INFO - optional double Waste - Actual Aggregated;
[2023-01-31T12:22:32.076+0000] {spark_submit.py:495} INFO - optional double Wind Offshore - Actual Aggregated;
[2023-01-31T12:22:32.076+0000] {spark_submit.py:495} INFO - optional double Wind Onshore - Actual Aggregated;
[2023-01-31T12:22:32.076+0000] {spark_submit.py:495} INFO - }
[2023-01-31T12:22:32.076+0000] {spark_submit.py:495} INFO - 
[2023-01-31T12:22:32.077+0000] {spark_submit.py:495} INFO - 
[2023-01-31T12:22:33.462+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:33 INFO CodecPool: Got brand-new compressor [.snappy]
[2023-01-31T12:22:35.855+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:35 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675166975279-f7798342-0a40-4e77-821a-db8388f61a76/_temporary/0/_temporary/' directory.
[2023-01-31T12:22:35.855+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:35 INFO FileOutputCommitter: Saved output of task 'attempt_202301311222304092566937084239905_0008_m_000000_8' to gs://entsoe_temp_1009/.spark-bigquery-local-1675166975279-f7798342-0a40-4e77-821a-db8388f61a76/_temporary/0/task_202301311222304092566937084239905_0008_m_000000
[2023-01-31T12:22:35.856+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:35 INFO SparkHadoopMapRedUtil: attempt_202301311222304092566937084239905_0008_m_000000_8: Committed. Elapsed time: 771 ms.
[2023-01-31T12:22:35.867+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:35 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2588 bytes result sent to driver
[2023-01-31T12:22:35.869+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:35 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 4634 ms on 8124f810dec3 (executor driver) (1/1)
[2023-01-31T12:22:35.870+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:35 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2023-01-31T12:22:35.870+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:35 INFO DAGScheduler: ResultStage 8 (save at BigQueryWriteHelper.java:105) finished in 4.806 s
[2023-01-31T12:22:35.871+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:35 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T12:22:35.871+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2023-01-31T12:22:35.871+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:35 INFO DAGScheduler: Job 8 finished: save at BigQueryWriteHelper.java:105, took 4.811239 s
[2023-01-31T12:22:35.874+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:35 INFO FileFormatWriter: Start to commit write Job ba22b39f-50d0-4de7-9fb9-43f8ec7bb43a.
[2023-01-31T12:22:36.579+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:36 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675166975279-f7798342-0a40-4e77-821a-db8388f61a76/_temporary/0/task_202301311222304092566937084239905_0008_m_000000/' directory.
[2023-01-31T12:22:37.672+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:37 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675166975279-f7798342-0a40-4e77-821a-db8388f61a76/' directory.
[2023-01-31T12:22:38.063+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:38 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:38.063+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:38 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None) re-registering with master
[2023-01-31T12:22:38.064+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:38 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:38.064+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:38 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 40669, None)
[2023-01-31T12:22:38.064+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:38 INFO BlockManager: Reporting 4 blocks to the master.
[2023-01-31T12:22:38.064+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:38 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on 8124f810dec3:40669 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:38.065+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:38 INFO BlockManagerInfo: Updated broadcast_16_piece0 in memory on 8124f810dec3:40669 (current size: 74.3 KiB, original size: 74.3 KiB, free: 434.3 MiB)
[2023-01-31T12:22:38.301+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:38 INFO FileFormatWriter: Write Job ba22b39f-50d0-4de7-9fb9-43f8ec7bb43a committed. Elapsed time: 2425 ms.
[2023-01-31T12:22:38.311+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:38 INFO FileFormatWriter: Finished processing stats for write job ba22b39f-50d0-4de7-9fb9-43f8ec7bb43a.
[2023-01-31T12:22:40.284+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:40 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=entsoe_analytics_1009, projectId=rafzul-analytics-1009, tableId=TEST_total_generation_staging}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=null, ignoreUnknownValue=null, sourceUris=[gs://entsoe_temp_1009/.spark-bigquery-local-1675166975279-f7798342-0a40-4e77-821a-db8388f61a76/part-00000-fb572f7f-1e32-438c-b6d5-3ce0d46d0abd-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=true, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=rafzul-analytics-1009, job=4efb08e6-27ff-42f2-9038-39de1e00cc24, location=US}
[2023-01-31T12:22:42.153+0000] {local_task_job.py:223} WARNING - State of this instance has been externally set to up_for_retry. Terminating instance.
[2023-01-31T12:22:42.154+0000] {process_utils.py:129} INFO - Sending Signals.SIGTERM to group 757. PIDs of all processes in the group: [799, 1000, 757]
[2023-01-31T12:22:42.154+0000] {process_utils.py:84} INFO - Sending the signal Signals.SIGTERM to group 757
[2023-01-31T12:22:42.154+0000] {taskinstance.py:1483} ERROR - Received SIGTERM. Terminating subprocesses.
[2023-01-31T12:22:42.155+0000] {spark_submit.py:620} INFO - Sending kill signal to spark-submit
[2023-01-31T12:22:42.165+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 414, in submit
    self._process_spark_submit_log(iter(self._submit_sp.stdout))  # type: ignore
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 463, in _process_spark_submit_log
    for line in itr:
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 1485, in signal_handler
    raise AirflowException("Task received SIGTERM signal")
airflow.exceptions.AirflowException: Task received SIGTERM signal
[2023-01-31T12:22:42.169+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=entsoe-energydata-backfill, task_id=stage_total_generation, execution_date=20210101T050000, start_date=20230131T120856, end_date=20230131T122242
[2023-01-31T12:22:42.180+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 1003 for task stage_total_generation (Task received SIGTERM signal; 757)
[2023-01-31T12:22:42.206+0000] {process_utils.py:79} INFO - Process psutil.Process(pid=757, status='terminated', exitcode=1, started='12:08:56') (757) terminated with exit code 1
[2023-01-31T12:22:42.207+0000] {process_utils.py:79} INFO - Process psutil.Process(pid=1000, status='terminated', started='12:09:18') (1000) terminated with exit code None
[2023-01-31T12:22:42.259+0000] {process_utils.py:79} INFO - Process psutil.Process(pid=799, status='terminated', started='12:08:57') (799) terminated with exit code None
[2023-01-31T13:03:41.265+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T05:00:00+00:00 [queued]>
[2023-01-31T13:03:41.311+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T05:00:00+00:00 [queued]>
[2023-01-31T13:03:41.311+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T13:03:41.312+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T13:03:41.312+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T13:03:41.429+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 05:00:00+00:00
[2023-01-31T13:03:41.512+0000] {standard_task_runner.py:55} INFO - Started process 202 to run task
[2023-01-31T13:03:41.540+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T05:00:00+00:00', '--job-id', '1024', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpniio4lah']
[2023-01-31T13:03:41.561+0000] {standard_task_runner.py:83} INFO - Job 1024: Subtask stage_total_generation
[2023-01-31T13:03:41.806+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T05:00:00+00:00 [running]> on host b37fe3cbf330
[2023-01-31T13:03:41.943+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T05:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T05:00:00+00:00
[2023-01-31T13:03:41.976+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T13:03:41.978+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010500 202101010600 DE_TENNET 202101010000 202101010600
[2023-01-31T13:04:16.741+0000] {spark_submit.py:495} INFO - mulai periode: 202101010000
[2023-01-31T13:04:16.741+0000] {spark_submit.py:495} INFO - selesai periode: 202101010600
[2023-01-31T13:04:17.856+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:17 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T13:04:18.619+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T13:04:21.433+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:21 INFO ResourceUtils: ==============================================================
[2023-01-31T13:04:21.436+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:21 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T13:04:21.490+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:21 INFO ResourceUtils: ==============================================================
[2023-01-31T13:04:21.493+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:21 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T13:04:22.140+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:22 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T13:04:22.261+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:22 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T13:04:22.286+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:22 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T13:04:23.092+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:23 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T13:04:23.137+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:23 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T13:04:23.140+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:23 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T13:04:23.154+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:23 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T13:04:23.160+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T13:04:30.593+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:30 INFO Utils: Successfully started service 'sparkDriver' on port 35945.
[2023-01-31T13:04:31.542+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:31 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T13:04:32.596+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:32 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T13:04:32.846+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:32 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T13:04:32.860+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:32 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T13:04:32.911+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:32 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T13:04:33.408+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:33 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-6a821b66-e12a-4164-8452-164ee31f0716
[2023-01-31T13:04:33.582+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:33 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T13:04:33.807+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:33 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T13:04:38.959+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:38 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T13:04:38.969+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:38 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2023-01-31T13:04:39.016+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:39 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[2023-01-31T13:04:39.055+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:39 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[2023-01-31T13:04:39.057+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:39 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[2023-01-31T13:04:39.259+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:39 INFO Utils: Successfully started service 'SparkUI' on port 4045.
[2023-01-31T13:04:39.762+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:39 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://b37fe3cbf330:35945/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675170257753
[2023-01-31T13:04:39.765+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:39 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://b37fe3cbf330:35945/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675170257753
[2023-01-31T13:04:40.833+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:40 INFO Executor: Starting executor ID driver on host b37fe3cbf330
[2023-01-31T13:04:40.937+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:40 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T13:04:41.144+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:41 INFO Executor: Fetching spark://b37fe3cbf330:35945/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675170257753
[2023-01-31T13:04:42.428+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:42 INFO TransportClientFactory: Successfully created connection to b37fe3cbf330/172.19.0.8:35945 after 606 ms (0 ms spent in bootstraps)
[2023-01-31T13:04:42.894+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:42 INFO Utils: Fetching spark://b37fe3cbf330:35945/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-8a4a865e-9caf-451d-997e-eea5477196b1/userFiles-2073678f-1657-414e-ba2f-01099bc40f8d/fetchFileTemp12574494246425753022.tmp
[2023-01-31T13:04:46.778+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:46 INFO Executor: Adding file:/tmp/spark-8a4a865e-9caf-451d-997e-eea5477196b1/userFiles-2073678f-1657-414e-ba2f-01099bc40f8d/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T13:04:46.782+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:46 INFO Executor: Fetching spark://b37fe3cbf330:35945/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675170257753
[2023-01-31T13:04:46.834+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:46 INFO Utils: Fetching spark://b37fe3cbf330:35945/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-8a4a865e-9caf-451d-997e-eea5477196b1/userFiles-2073678f-1657-414e-ba2f-01099bc40f8d/fetchFileTemp2770581691180376061.tmp
[2023-01-31T13:04:50.206+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:50 INFO Executor: Adding file:/tmp/spark-8a4a865e-9caf-451d-997e-eea5477196b1/userFiles-2073678f-1657-414e-ba2f-01099bc40f8d/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T13:04:50.334+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:50 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37989.
[2023-01-31T13:04:50.335+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:50 INFO NettyBlockTransferService: Server created on b37fe3cbf330:37989
[2023-01-31T13:04:50.358+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:50 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T13:04:50.385+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:50 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, b37fe3cbf330, 37989, None)
[2023-01-31T13:04:50.402+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:50 INFO BlockManagerMasterEndpoint: Registering block manager b37fe3cbf330:37989 with 434.4 MiB RAM, BlockManagerId(driver, b37fe3cbf330, 37989, None)
[2023-01-31T13:04:50.449+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:50 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, b37fe3cbf330, 37989, None)
[2023-01-31T13:04:50.464+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:50 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, b37fe3cbf330, 37989, None)
[2023-01-31T13:05:04.898+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:04 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T13:05:05.032+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:05 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T13:05:28.066+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:28 INFO InMemoryFileIndex: It took 438 ms to list leaf files for 1 paths.
[2023-01-31T13:05:29.200+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:29 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T13:05:29.501+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:29 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T13:05:29.509+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:29 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on b37fe3cbf330:37989 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T13:05:29.521+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:29 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T13:05:31.427+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:31 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T13:05:31.519+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:31 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T13:05:31.613+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:31 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T13:05:31.841+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:31 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T13:05:31.854+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:31 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T13:05:31.870+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:31 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T13:05:31.880+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:31 INFO DAGScheduler: Missing parents: List()
[2023-01-31T13:05:31.915+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:31 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T13:05:32.244+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:32 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T13:05:32.257+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:32 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T13:05:32.261+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:32 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on b37fe3cbf330:37989 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T13:05:32.266+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:32 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T13:05:32.346+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T13:05:32.352+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:32 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T13:05:32.628+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:32 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T13:05:32.812+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:32 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T13:05:33.771+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:33 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010500__202101010600.json:0+9367
[2023-01-31T13:05:37.987+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:37 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3480 bytes result sent to driver
[2023-01-31T13:05:38.054+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:38 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 5560 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T13:05:38.066+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:38 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T13:05:38.116+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:38 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 6.050 s
[2023-01-31T13:05:38.234+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:38 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T13:05:38.237+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T13:05:38.283+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:38 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 6.667578 s
[2023-01-31T13:05:48.827+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:48 INFO BlockManagerInfo: Removed broadcast_0_piece0 on b37fe3cbf330:37989 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T13:05:48.924+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:48 INFO BlockManagerInfo: Removed broadcast_1_piece0 on b37fe3cbf330:37989 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T13:06:19.482+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:19 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T13:06:19.486+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:19 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T13:06:19.502+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:19 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T13:06:21.802+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:21 INFO CodeGenerator: Code generated in 1578.433404 ms
[2023-01-31T13:06:21.843+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:21 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T13:06:21.972+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:21 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T13:06:21.975+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:21 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on b37fe3cbf330:37989 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T13:06:21.978+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:21 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T13:06:22.083+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:22 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T13:06:22.520+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:22 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T13:06:22.522+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:22 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T13:06:22.523+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:22 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T13:06:22.523+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:22 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T13:06:22.524+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:22 INFO DAGScheduler: Missing parents: List()
[2023-01-31T13:06:22.532+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:22 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T13:06:22.566+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:22 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T13:06:22.605+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:22 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T13:06:22.612+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:22 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on b37fe3cbf330:37989 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T13:06:22.614+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:22 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T13:06:22.625+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T13:06:22.626+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:22 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T13:06:22.671+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:22 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T13:06:22.676+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:22 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T13:06:23.578+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:23 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T13:06:24.712+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO CodeGenerator: Code generated in 840.408903 ms
[2023-01-31T13:06:25.539+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:25 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1628 bytes result sent to driver
[2023-01-31T13:06:25.565+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:25 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2927 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T13:06:25.566+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:25 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T13:06:25.574+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:25 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 3.021 s
[2023-01-31T13:06:25.574+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:25 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T13:06:25.575+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T13:06:25.577+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:25 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 3.055549 s
[2023-01-31T13:06:25.846+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:25 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T13:06:25.847+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:25 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T13:06:25.847+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:25 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T13:06:25.980+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:25 INFO CodeGenerator: Code generated in 83.747966 ms
[2023-01-31T13:06:26.012+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T13:06:26.178+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T13:06:26.187+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on b37fe3cbf330:37989 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T13:06:26.188+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T13:06:26.197+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T13:06:26.368+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T13:06:26.373+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T13:06:26.373+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T13:06:26.373+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T13:06:26.373+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO DAGScheduler: Missing parents: List()
[2023-01-31T13:06:26.380+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T13:06:26.416+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T13:06:26.443+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T13:06:26.446+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on b37fe3cbf330:37989 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T13:06:26.447+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T13:06:26.453+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T13:06:26.454+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T13:06:26.460+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T13:06:26.462+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T13:06:26.512+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T13:06:26.861+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1585 bytes result sent to driver
[2023-01-31T13:06:26.873+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 415 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T13:06:26.876+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T13:06:26.878+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 0.495 s
[2023-01-31T13:06:26.881+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T13:06:26.881+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T13:06:26.882+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 0.511583 s
[2023-01-31T13:06:27.930+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T13:06:27.939+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T13:06:27.940+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T13:06:28.646+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO CodeGenerator: Code generated in 444.102238 ms
[2023-01-31T13:06:28.682+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T13:06:28.744+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T13:06:28.747+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on b37fe3cbf330:37989 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T13:06:28.752+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T13:06:28.767+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T13:06:28.863+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T13:06:28.867+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T13:06:28.868+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T13:06:28.868+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T13:06:28.868+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO DAGScheduler: Missing parents: List()
[2023-01-31T13:06:28.870+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T13:06:28.900+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.6 MiB)
[2023-01-31T13:06:28.929+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.6 MiB)
[2023-01-31T13:06:28.931+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on b37fe3cbf330:37989 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T13:06:28.940+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T13:06:28.945+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T13:06:28.946+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T13:06:28.951+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T13:06:28.954+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T13:06:29.039+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T13:06:29.258+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-01-31T13:06:29.285+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 334 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T13:06:29.291+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 0.409 s
[2023-01-31T13:06:29.294+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T13:06:29.300+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T13:06:29.305+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T13:06:29.305+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 0.437532 s
[2023-01-31T13:06:29.620+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO BlockManagerInfo: Removed broadcast_7_piece0 on b37fe3cbf330:37989 in memory (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T13:06:29.666+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T13:06:29.667+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T13:06:29.673+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T13:06:29.678+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO BlockManagerInfo: Removed broadcast_3_piece0 on b37fe3cbf330:37989 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T13:06:29.733+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO BlockManagerInfo: Removed broadcast_5_piece0 on b37fe3cbf330:37989 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T13:06:29.817+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO CodeGenerator: Code generated in 76.621426 ms
[2023-01-31T13:06:29.850+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.5 MiB)
[2023-01-31T13:06:29.919+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.5 MiB)
[2023-01-31T13:06:29.926+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on b37fe3cbf330:37989 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T13:06:29.929+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T13:06:29.947+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T13:06:30.097+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T13:06:30.098+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T13:06:30.098+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T13:06:30.099+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T13:06:30.099+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO DAGScheduler: Missing parents: List()
[2023-01-31T13:06:30.103+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T13:06:30.126+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.5 MiB)
[2023-01-31T13:06:30.129+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.5 MiB)
[2023-01-31T13:06:30.130+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on b37fe3cbf330:37989 (size: 7.6 KiB, free: 434.3 MiB)
[2023-01-31T13:06:30.139+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T13:06:30.142+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T13:06:30.143+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T13:06:30.146+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T13:06:30.148+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T13:06:30.200+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T13:06:30.459+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1585 bytes result sent to driver
[2023-01-31T13:06:30.473+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 316 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T13:06:30.473+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T13:06:30.478+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 0.362 s
[2023-01-31T13:06:30.478+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T13:06:30.478+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T13:06:30.478+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 0.378029 s
[2023-01-31T13:06:31.320+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:31 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T13:06:31.322+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:31 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T13:06:31.324+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:31 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T13:06:31.725+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:31 INFO CodeGenerator: Code generated in 264.175167 ms
[2023-01-31T13:06:31.732+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:31 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.3 MiB)
[2023-01-31T13:06:31.767+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:31 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T13:06:31.768+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:31 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on b37fe3cbf330:37989 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T13:06:31.770+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:31 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T13:06:31.775+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:31 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T13:06:31.834+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:31 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T13:06:31.837+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:31 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T13:06:31.838+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:31 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T13:06:31.838+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:31 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T13:06:31.838+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:31 INFO DAGScheduler: Missing parents: List()
[2023-01-31T13:06:31.844+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:31 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T13:06:31.866+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:31 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.2 MiB)
[2023-01-31T13:06:31.875+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:31 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.2 MiB)
[2023-01-31T13:06:31.878+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:31 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on b37fe3cbf330:37989 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T13:06:31.884+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:31 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T13:06:31.885+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T13:06:31.887+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:31 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T13:06:31.900+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:31 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T13:06:31.902+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:31 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T13:06:31.953+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:31 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T13:06:32.191+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:32 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2490 bytes result sent to driver
[2023-01-31T13:06:32.205+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:32 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 296 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T13:06:32.206+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:32 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T13:06:32.206+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:32 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 0.353 s
[2023-01-31T13:06:32.206+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:32 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T13:06:32.207+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T13:06:32.208+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:32 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 0.368136 s
[2023-01-31T13:06:32.548+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:32 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T13:06:32.555+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:32 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T13:06:32.562+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:32 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T13:06:32.925+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:32 INFO CodeGenerator: Code generated in 243.526888 ms
[2023-01-31T13:06:32.970+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:32 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 433.0 MiB)
[2023-01-31T13:06:33.128+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:33 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.9 MiB)
[2023-01-31T13:06:33.131+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:33 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on b37fe3cbf330:37989 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T13:06:33.139+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:33 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T13:06:33.148+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T13:06:33.348+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:33 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T13:06:33.355+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:33 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T13:06:33.356+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:33 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T13:06:33.356+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:33 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T13:06:33.356+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:33 INFO DAGScheduler: Missing parents: List()
[2023-01-31T13:06:33.370+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:33 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T13:06:33.382+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:33 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 432.9 MiB)
[2023-01-31T13:06:33.394+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:33 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.9 MiB)
[2023-01-31T13:06:33.411+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:33 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on b37fe3cbf330:37989 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T13:06:33.413+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:33 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T13:06:33.416+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T13:06:33.419+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:33 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T13:06:33.426+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:33 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T13:06:33.469+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:33 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T13:06:33.500+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:33 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T13:06:33.687+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:33 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-01-31T13:06:33.742+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:33 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 302 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T13:06:33.743+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:33 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T13:06:33.743+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:33 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 0.363 s
[2023-01-31T13:06:33.743+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:33 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T13:06:33.744+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T13:06:33.744+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:33 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 0.382024 s
[2023-01-31T13:06:34.020+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T13:06:34.022+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T13:06:34.027+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T13:06:34.189+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO CodeGenerator: Code generated in 90.137002 ms
[2023-01-31T13:06:34.196+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.7 MiB)
[2023-01-31T13:06:34.330+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.7 MiB)
[2023-01-31T13:06:34.331+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on b37fe3cbf330:37989 (size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T13:06:34.331+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:204
[2023-01-31T13:06:34.331+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T13:06:34.336+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO BlockManagerInfo: Removed broadcast_4_piece0 on b37fe3cbf330:37989 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T13:06:34.401+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO BlockManagerInfo: Removed broadcast_9_piece0 on b37fe3cbf330:37989 in memory (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T13:06:34.429+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:204
[2023-01-31T13:06:34.433+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:204) with 1 output partitions
[2023-01-31T13:06:34.433+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:204)
[2023-01-31T13:06:34.433+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T13:06:34.434+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO DAGScheduler: Missing parents: List()
[2023-01-31T13:06:34.440+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:204), which has no missing parents
[2023-01-31T13:06:34.449+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO BlockManagerInfo: Removed broadcast_2_piece0 on b37fe3cbf330:37989 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T13:06:34.452+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 433.1 MiB)
[2023-01-31T13:06:34.459+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 433.1 MiB)
[2023-01-31T13:06:34.464+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on b37fe3cbf330:37989 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T13:06:34.465+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T13:06:34.471+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:204) (first 15 tasks are for partitions Vector(0))
[2023-01-31T13:06:34.472+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T13:06:34.474+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T13:06:34.475+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T13:06:34.490+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO BlockManagerInfo: Removed broadcast_11_piece0 on b37fe3cbf330:37989 in memory (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T13:06:34.534+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO BlockManagerInfo: Removed broadcast_13_piece0 on b37fe3cbf330:37989 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T13:06:34.537+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T13:06:34.650+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO BlockManagerInfo: Removed broadcast_6_piece0 on b37fe3cbf330:37989 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T13:06:34.702+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1729 bytes result sent to driver
[2023-01-31T13:06:34.710+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 236 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T13:06:34.710+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T13:06:34.710+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:204) finished in 0.272 s
[2023-01-31T13:06:34.711+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T13:06:34.711+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T13:06:34.711+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:204, took 0.282110 s
[2023-01-31T13:06:34.739+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO BlockManagerInfo: Removed broadcast_8_piece0 on b37fe3cbf330:37989 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T13:06:39.932+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:39 INFO BlockManagerInfo: Removed broadcast_15_piece0 on b37fe3cbf330:37989 in memory (size: 12.6 KiB, free: 434.3 MiB)
[2023-01-31T13:06:39.989+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:39 INFO BlockManagerInfo: Removed broadcast_12_piece0 on b37fe3cbf330:37989 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T13:06:40.114+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:40 INFO BlockManagerInfo: Removed broadcast_14_piece0 on b37fe3cbf330:37989 in memory (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T13:06:49.263+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:49 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T13:06:49.369+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T13:06:49.372+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T13:06:49.378+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:49 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T13:06:49.379+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T13:06:49.381+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T13:06:49.390+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T13:06:50.262+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-31T13:06:50.263+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO DAGScheduler: Got job 8 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T13:06:50.263+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO DAGScheduler: Final stage: ResultStage 8 (save at BigQueryWriteHelper.java:105)
[2023-01-31T13:06:50.263+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T13:06:50.263+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO DAGScheduler: Missing parents: List()
[2023-01-31T13:06:50.263+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO DAGScheduler: Submitting ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T13:06:50.383+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 209.9 KiB, free 434.0 MiB)
[2023-01-31T13:06:50.394+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 74.3 KiB, free 433.9 MiB)
[2023-01-31T13:06:50.399+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on b37fe3cbf330:37989 (size: 74.3 KiB, free: 434.3 MiB)
[2023-01-31T13:06:50.402+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513
[2023-01-31T13:06:50.412+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T13:06:50.419+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2023-01-31T13:06:50.444+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7484 bytes) taskResourceAssignments Map()
[2023-01-31T13:06:50.457+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2023-01-31T13:06:51.198+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T13:06:51.205+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T13:06:51.218+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T13:06:51.220+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T13:06:51.224+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T13:06:51.233+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T13:06:51.287+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T13:06:51.316+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T13:06:51.484+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO ParquetOutputFormat: Parquet block size to 134217728
[2023-01-31T13:06:51.486+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO ParquetOutputFormat: Validation is off
[2023-01-31T13:06:51.487+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-01-31T13:06:51.489+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO ParquetOutputFormat: Parquet properties are:
[2023-01-31T13:06:51.490+0000] {spark_submit.py:495} INFO - Parquet page size to 1048576
[2023-01-31T13:06:51.490+0000] {spark_submit.py:495} INFO - Parquet dictionary page size to 1048576
[2023-01-31T13:06:51.490+0000] {spark_submit.py:495} INFO - Dictionary is true
[2023-01-31T13:06:51.491+0000] {spark_submit.py:495} INFO - Writer version is: PARQUET_1_0
[2023-01-31T13:06:51.491+0000] {spark_submit.py:495} INFO - Page size checking is: estimated
[2023-01-31T13:06:51.491+0000] {spark_submit.py:495} INFO - Min row count for page size check is: 100
[2023-01-31T13:06:51.491+0000] {spark_submit.py:495} INFO - Max row count for page size check is: 10000
[2023-01-31T13:06:51.491+0000] {spark_submit.py:495} INFO - Truncate length for column indexes is: 64
[2023-01-31T13:06:51.492+0000] {spark_submit.py:495} INFO - Truncate length for statistics min/max  is: 2147483647
[2023-01-31T13:06:51.492+0000] {spark_submit.py:495} INFO - Bloom filter enabled: false
[2023-01-31T13:06:51.492+0000] {spark_submit.py:495} INFO - Max Bloom filter size for a column is 1048576
[2023-01-31T13:06:51.492+0000] {spark_submit.py:495} INFO - Bloom filter expected number of distinct values are: null
[2023-01-31T13:06:51.492+0000] {spark_submit.py:495} INFO - Page row count limit to 20000
[2023-01-31T13:06:51.492+0000] {spark_submit.py:495} INFO - Writing page checksums is: on
[2023-01-31T13:06:51.760+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-01-31T13:06:51.761+0000] {spark_submit.py:495} INFO - {
[2023-01-31T13:06:51.761+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-31T13:06:51.761+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-31T13:06:51.762+0000] {spark_submit.py:495} INFO - "name" : "measured_at",
[2023-01-31T13:06:51.762+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T13:06:51.762+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T13:06:51.762+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.763+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.763+0000] {spark_submit.py:495} INFO - "name" : "created_at",
[2023-01-31T13:06:51.763+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T13:06:51.763+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T13:06:51.764+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.764+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.766+0000] {spark_submit.py:495} INFO - "name" : "Biomass - Actual Aggregated",
[2023-01-31T13:06:51.767+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.775+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.776+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.778+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.779+0000] {spark_submit.py:495} INFO - "name" : "Fossil Brown coal/Lignite - Actual Aggregated",
[2023-01-31T13:06:51.782+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.785+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.786+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.788+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.792+0000] {spark_submit.py:495} INFO - "name" : "Fossil Gas - Actual Aggregated",
[2023-01-31T13:06:51.795+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.803+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.807+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.810+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.814+0000] {spark_submit.py:495} INFO - "name" : "Fossil Hard coal - Actual Aggregated",
[2023-01-31T13:06:51.814+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.819+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.827+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.827+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.827+0000] {spark_submit.py:495} INFO - "name" : "Fossil Oil - Actual Aggregated",
[2023-01-31T13:06:51.827+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.828+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.836+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.837+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.838+0000] {spark_submit.py:495} INFO - "name" : "Geothermal - Actual Aggregated",
[2023-01-31T13:06:51.841+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.844+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.845+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.848+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.850+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Aggregated",
[2023-01-31T13:06:51.851+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.851+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.853+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.854+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.856+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Consumption",
[2023-01-31T13:06:51.857+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.857+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.857+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.857+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.858+0000] {spark_submit.py:495} INFO - "name" : "Hydro Run-of-river and poundage - Actual Aggregated",
[2023-01-31T13:06:51.858+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.858+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.859+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.859+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.859+0000] {spark_submit.py:495} INFO - "name" : "Hydro Water Reservoir - Actual Aggregated",
[2023-01-31T13:06:51.859+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.860+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.860+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.860+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.861+0000] {spark_submit.py:495} INFO - "name" : "Nuclear - Actual Aggregated",
[2023-01-31T13:06:51.861+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.861+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.862+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.862+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.862+0000] {spark_submit.py:495} INFO - "name" : "Other - Actual Aggregated",
[2023-01-31T13:06:51.864+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.864+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.865+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.866+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.867+0000] {spark_submit.py:495} INFO - "name" : "Other renewable - Actual Aggregated",
[2023-01-31T13:06:51.867+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.867+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.868+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.868+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.869+0000] {spark_submit.py:495} INFO - "name" : "Solar - Actual Aggregated",
[2023-01-31T13:06:51.876+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.877+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.877+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.877+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.878+0000] {spark_submit.py:495} INFO - "name" : "Waste - Actual Aggregated",
[2023-01-31T13:06:51.879+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.879+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.879+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.879+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.881+0000] {spark_submit.py:495} INFO - "name" : "Wind Offshore - Actual Aggregated",
[2023-01-31T13:06:51.881+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.881+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.881+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.881+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.881+0000] {spark_submit.py:495} INFO - "name" : "Wind Onshore - Actual Aggregated",
[2023-01-31T13:06:51.883+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.883+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.883+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.884+0000] {spark_submit.py:495} INFO - } ]
[2023-01-31T13:06:51.885+0000] {spark_submit.py:495} INFO - }
[2023-01-31T13:06:51.885+0000] {spark_submit.py:495} INFO - and corresponding Parquet message type:
[2023-01-31T13:06:51.885+0000] {spark_submit.py:495} INFO - message spark_schema {
[2023-01-31T13:06:51.886+0000] {spark_submit.py:495} INFO - required int96 measured_at;
[2023-01-31T13:06:51.886+0000] {spark_submit.py:495} INFO - required int96 created_at;
[2023-01-31T13:06:51.887+0000] {spark_submit.py:495} INFO - optional double Biomass - Actual Aggregated;
[2023-01-31T13:06:51.887+0000] {spark_submit.py:495} INFO - optional double Fossil Brown coal/Lignite - Actual Aggregated;
[2023-01-31T13:06:51.887+0000] {spark_submit.py:495} INFO - optional double Fossil Gas - Actual Aggregated;
[2023-01-31T13:06:51.888+0000] {spark_submit.py:495} INFO - optional double Fossil Hard coal - Actual Aggregated;
[2023-01-31T13:06:51.888+0000] {spark_submit.py:495} INFO - optional double Fossil Oil - Actual Aggregated;
[2023-01-31T13:06:51.888+0000] {spark_submit.py:495} INFO - optional double Geothermal - Actual Aggregated;
[2023-01-31T13:06:51.889+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Aggregated;
[2023-01-31T13:06:51.889+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Consumption;
[2023-01-31T13:06:51.890+0000] {spark_submit.py:495} INFO - optional double Hydro Run-of-river and poundage - Actual Aggregated;
[2023-01-31T13:06:51.890+0000] {spark_submit.py:495} INFO - optional double Hydro Water Reservoir - Actual Aggregated;
[2023-01-31T13:06:51.891+0000] {spark_submit.py:495} INFO - optional double Nuclear - Actual Aggregated;
[2023-01-31T13:06:51.891+0000] {spark_submit.py:495} INFO - optional double Other - Actual Aggregated;
[2023-01-31T13:06:51.892+0000] {spark_submit.py:495} INFO - optional double Other renewable - Actual Aggregated;
[2023-01-31T13:06:51.892+0000] {spark_submit.py:495} INFO - optional double Solar - Actual Aggregated;
[2023-01-31T13:06:51.892+0000] {spark_submit.py:495} INFO - optional double Waste - Actual Aggregated;
[2023-01-31T13:06:51.892+0000] {spark_submit.py:495} INFO - optional double Wind Offshore - Actual Aggregated;
[2023-01-31T13:06:51.893+0000] {spark_submit.py:495} INFO - optional double Wind Onshore - Actual Aggregated;
[2023-01-31T13:06:51.894+0000] {spark_submit.py:495} INFO - }
[2023-01-31T13:06:51.894+0000] {spark_submit.py:495} INFO - 
[2023-01-31T13:06:51.894+0000] {spark_submit.py:495} INFO - 
[2023-01-31T13:06:52.523+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:52 INFO CodecPool: Got brand-new compressor [.snappy]
[2023-01-31T13:06:55.845+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:55 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675170280153-45cd716b-866c-42c0-9d81-2230a1867b0f/_temporary/0/_temporary/' directory.
[2023-01-31T13:06:55.845+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:55 INFO FileOutputCommitter: Saved output of task 'attempt_202301311306494810535723708885447_0008_m_000000_8' to gs://entsoe_temp_1009/.spark-bigquery-local-1675170280153-45cd716b-866c-42c0-9d81-2230a1867b0f/_temporary/0/task_202301311306494810535723708885447_0008_m_000000
[2023-01-31T13:06:55.846+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:55 INFO SparkHadoopMapRedUtil: attempt_202301311306494810535723708885447_0008_m_000000_8: Committed. Elapsed time: 818 ms.
[2023-01-31T13:06:55.858+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:55 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2545 bytes result sent to driver
[2023-01-31T13:06:55.861+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:55 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 5439 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T13:06:55.861+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:55 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2023-01-31T13:06:55.862+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:55 INFO DAGScheduler: ResultStage 8 (save at BigQueryWriteHelper.java:105) finished in 5.597 s
[2023-01-31T13:06:55.862+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:55 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T13:06:55.862+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2023-01-31T13:06:55.863+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:55 INFO DAGScheduler: Job 8 finished: save at BigQueryWriteHelper.java:105, took 5.603768 s
[2023-01-31T13:06:55.865+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:55 INFO FileFormatWriter: Start to commit write Job 02c94ec0-a1e2-4d05-b551-6def68f169f2.
[2023-01-31T13:06:56.724+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:56 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675170280153-45cd716b-866c-42c0-9d81-2230a1867b0f/_temporary/0/task_202301311306494810535723708885447_0008_m_000000/' directory.
[2023-01-31T13:06:57.188+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:57 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675170280153-45cd716b-866c-42c0-9d81-2230a1867b0f/' directory.
[2023-01-31T13:06:57.345+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:57 INFO BlockManagerInfo: Removed broadcast_16_piece0 on b37fe3cbf330:37989 in memory (size: 74.3 KiB, free: 434.4 MiB)
[2023-01-31T13:06:57.836+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:57 INFO FileFormatWriter: Write Job 02c94ec0-a1e2-4d05-b551-6def68f169f2 committed. Elapsed time: 1969 ms.
[2023-01-31T13:06:57.875+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:57 INFO FileFormatWriter: Finished processing stats for write job 02c94ec0-a1e2-4d05-b551-6def68f169f2.
[2023-01-31T13:06:59.892+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:59 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=entsoe_analytics_1009, projectId=rafzul-analytics-1009, tableId=TEST_total_generation_staging}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=null, ignoreUnknownValue=null, sourceUris=[gs://entsoe_temp_1009/.spark-bigquery-local-1675170280153-45cd716b-866c-42c0-9d81-2230a1867b0f/part-00000-924d36f6-45f0-43b2-bbb2-2f628990766b-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=true, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=rafzul-analytics-1009, job=aea14d2d-1a6a-4529-a648-354a355f81db, location=US}
[2023-01-31T13:07:05.570+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:05 INFO BigQueryClient: Done loading to rafzul-analytics-1009.entsoe_analytics_1009.TEST_total_generation_staging. jobId: JobId{project=rafzul-analytics-1009, job=aea14d2d-1a6a-4529-a648-354a355f81db, location=US}
[2023-01-31T13:07:06.162+0000] {spark_submit.py:495} INFO - MANTEP!!!!
[2023-01-31T13:07:06.584+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:06 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-31T13:07:06.663+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:06 INFO SparkUI: Stopped Spark web UI at http://b37fe3cbf330:4045
[2023-01-31T13:07:06.712+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:06 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-31T13:07:06.772+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:06 INFO MemoryStore: MemoryStore cleared
[2023-01-31T13:07:06.780+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:06 INFO BlockManager: BlockManager stopped
[2023-01-31T13:07:06.807+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:06 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-31T13:07:06.825+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:06 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-31T13:07:06.874+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:06 INFO SparkContext: Successfully stopped SparkContext
[2023-01-31T13:07:06.876+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:06 INFO ShutdownHookManager: Shutdown hook called
[2023-01-31T13:07:06.883+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:06 INFO ShutdownHookManager: Deleting directory /tmp/spark-8a4a865e-9caf-451d-997e-eea5477196b1/pyspark-17a9529c-fc1c-4fdf-be1b-3b0cc7dd3e43
[2023-01-31T13:07:06.947+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:06 INFO ShutdownHookManager: Deleting directory /tmp/spark-8a4a865e-9caf-451d-997e-eea5477196b1
[2023-01-31T13:07:07.001+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:06 INFO ShutdownHookManager: Deleting directory /tmp/spark-4439021b-bb80-407b-81ed-cb8ba595c921
[2023-01-31T13:07:07.560+0000] {taskinstance.py:1322} INFO - Marking task as SUCCESS. dag_id=entsoe-energydata-backfill, task_id=stage_total_generation, execution_date=20210101T050000, start_date=20230131T130341, end_date=20230131T130707
[2023-01-31T13:07:07.819+0000] {local_task_job.py:159} INFO - Task exited with return code 0
[2023-01-31T13:07:08.053+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T14:09:42.868+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T05:00:00+00:00 [queued]>
[2023-01-31T14:09:42.935+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T05:00:00+00:00 [queued]>
[2023-01-31T14:09:42.937+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T14:09:42.943+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T14:09:42.943+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T14:09:43.007+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 05:00:00+00:00
[2023-01-31T14:09:43.047+0000] {standard_task_runner.py:55} INFO - Started process 6102 to run task
[2023-01-31T14:09:43.088+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T05:00:00+00:00', '--job-id', '1038', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpas86_vmd']
[2023-01-31T14:09:43.112+0000] {standard_task_runner.py:83} INFO - Job 1038: Subtask stage_total_generation
[2023-01-31T14:09:43.497+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T05:00:00+00:00 [running]> on host b37fe3cbf330
[2023-01-31T14:09:43.848+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T05:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T05:00:00+00:00
[2023-01-31T14:09:43.886+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T14:09:43.894+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 398, in submit
    spark_submit_cmd = self._build_spark_submit_command(application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 344, in _build_spark_submit_command
    self.log.info("Spark-Submit cmd: %s", self._mask_cmd(connection_cmd))
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 256, in _mask_cmd
    " ".join(connection_cmd),
TypeError: sequence item 10: expected str instance, DateTime found
[2023-01-31T14:09:43.923+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=entsoe-energydata-backfill, task_id=stage_total_generation, execution_date=20210101T050000, start_date=20230131T140942, end_date=20230131T140943
[2023-01-31T14:09:43.963+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 1038 for task stage_total_generation (sequence item 10: expected str instance, DateTime found; 6102)
[2023-01-31T14:09:44.051+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-31T14:09:44.111+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T14:10:53.918+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T05:00:00+00:00 [queued]>
[2023-01-31T14:10:53.974+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T05:00:00+00:00 [queued]>
[2023-01-31T14:10:53.975+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T14:10:53.976+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T14:10:53.977+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T14:10:54.032+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 05:00:00+00:00
[2023-01-31T14:10:54.099+0000] {standard_task_runner.py:55} INFO - Started process 6211 to run task
[2023-01-31T14:10:54.161+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T05:00:00+00:00', '--job-id', '1057', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpld1ggfqy']
[2023-01-31T14:10:54.173+0000] {standard_task_runner.py:83} INFO - Job 1057: Subtask stage_total_generation
[2023-01-31T14:10:54.676+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T05:00:00+00:00 [running]> on host b37fe3cbf330
[2023-01-31T14:10:54.942+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T05:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T05:00:00+00:00
[2023-01-31T14:10:54.980+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T14:10:54.983+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 398, in submit
    spark_submit_cmd = self._build_spark_submit_command(application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 344, in _build_spark_submit_command
    self.log.info("Spark-Submit cmd: %s", self._mask_cmd(connection_cmd))
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 256, in _mask_cmd
    " ".join(connection_cmd),
TypeError: sequence item 10: expected str instance, DateTime found
[2023-01-31T14:10:55.017+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=entsoe-energydata-backfill, task_id=stage_total_generation, execution_date=20210101T050000, start_date=20230131T141053, end_date=20230131T141055
[2023-01-31T14:10:55.061+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 1057 for task stage_total_generation (sequence item 10: expected str instance, DateTime found; 6211)
[2023-01-31T14:10:55.133+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-31T14:10:55.239+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
