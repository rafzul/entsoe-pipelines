[2023-01-31T05:11:00.447+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T05:00:00+00:00 [queued]>
[2023-01-31T05:11:00.487+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T05:00:00+00:00 [queued]>
[2023-01-31T05:11:00.488+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T05:11:00.488+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T05:11:00.489+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T05:11:00.561+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 05:00:00+00:00
[2023-01-31T05:11:00.602+0000] {standard_task_runner.py:55} INFO - Started process 12260 to run task
[2023-01-31T05:11:00.625+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T05:00:00+00:00', '--job-id', '881', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpz9en7df0']
[2023-01-31T05:11:00.635+0000] {standard_task_runner.py:83} INFO - Job 881: Subtask stage_total_generation
[2023-01-31T05:11:01.078+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T05:00:00+00:00 [running]> on host 81d5fcd0285b
[2023-01-31T05:11:01.614+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T05:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T05:00:00+00:00
[2023-01-31T05:11:01.670+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T05:11:01.690+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010500 202101010600 DE_TENNET
[2023-01-31T05:11:35.068+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:35 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T05:11:36.262+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T05:11:38.066+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:38 INFO ResourceUtils: ==============================================================
[2023-01-31T05:11:38.114+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:38 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T05:11:38.145+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:38 INFO ResourceUtils: ==============================================================
[2023-01-31T05:11:38.174+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:38 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T05:11:38.456+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:38 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T05:11:38.507+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:38 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T05:11:38.518+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:38 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T05:11:39.003+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:39 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T05:11:39.007+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:39 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T05:11:39.010+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:39 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T05:11:39.013+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:39 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T05:11:39.015+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:39 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T05:11:42.339+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:42 INFO Utils: Successfully started service 'sparkDriver' on port 41493.
[2023-01-31T05:11:42.794+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:42 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T05:11:43.965+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:43 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T05:11:44.418+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:44 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T05:11:44.434+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:44 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T05:11:44.485+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:44 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T05:11:44.693+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:44 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-8c72a8b1-3278-4d9d-897f-9be613b8bc0d
[2023-01-31T05:11:44.863+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:44 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T05:11:45.086+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:45 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T05:11:47.566+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:47 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T05:11:47.581+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:47 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2023-01-31T05:11:47.588+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:47 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[2023-01-31T05:11:47.590+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:47 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[2023-01-31T05:11:47.604+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:47 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[2023-01-31T05:11:47.698+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:47 INFO Utils: Successfully started service 'SparkUI' on port 4045.
[2023-01-31T05:11:48.031+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:48 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://81d5fcd0285b:41493/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675141894991
[2023-01-31T05:11:48.034+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:48 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://81d5fcd0285b:41493/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675141894991
[2023-01-31T05:11:48.822+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:48 INFO Executor: Starting executor ID driver on host 81d5fcd0285b
[2023-01-31T05:11:49.029+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:49 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T05:11:49.592+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:49 INFO Executor: Fetching spark://81d5fcd0285b:41493/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675141894991
[2023-01-31T05:11:50.494+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:50 INFO TransportClientFactory: Successfully created connection to 81d5fcd0285b/172.19.0.9:41493 after 421 ms (0 ms spent in bootstraps)
[2023-01-31T05:11:50.643+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:50 INFO Utils: Fetching spark://81d5fcd0285b:41493/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-94da920d-e815-4344-8642-800cfae16f24/userFiles-e5733af0-6618-4fac-b475-3ee6f2edb7d8/fetchFileTemp16399849639890025439.tmp
[2023-01-31T05:11:53.234+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:53 INFO Executor: Adding file:/tmp/spark-94da920d-e815-4344-8642-800cfae16f24/userFiles-e5733af0-6618-4fac-b475-3ee6f2edb7d8/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T05:11:53.238+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:53 INFO Executor: Fetching spark://81d5fcd0285b:41493/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675141894991
[2023-01-31T05:11:53.252+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:53 INFO Utils: Fetching spark://81d5fcd0285b:41493/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-94da920d-e815-4344-8642-800cfae16f24/userFiles-e5733af0-6618-4fac-b475-3ee6f2edb7d8/fetchFileTemp1255801949497566479.tmp
[2023-01-31T05:11:54.280+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:54 INFO Executor: Adding file:/tmp/spark-94da920d-e815-4344-8642-800cfae16f24/userFiles-e5733af0-6618-4fac-b475-3ee6f2edb7d8/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T05:11:54.327+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:54 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34735.
[2023-01-31T05:11:54.327+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:54 INFO NettyBlockTransferService: Server created on 81d5fcd0285b:34735
[2023-01-31T05:11:54.335+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:54 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T05:11:54.377+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:54 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 34735, None)
[2023-01-31T05:11:54.407+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:54 INFO BlockManagerMasterEndpoint: Registering block manager 81d5fcd0285b:34735 with 434.4 MiB RAM, BlockManagerId(driver, 81d5fcd0285b, 34735, None)
[2023-01-31T05:11:54.452+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:54 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 34735, None)
[2023-01-31T05:11:54.459+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:54 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 81d5fcd0285b, 34735, None)
[2023-01-31T05:12:00.662+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:00 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T05:12:00.710+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:00 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T05:12:22.085+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:22 INFO InMemoryFileIndex: It took 944 ms to list leaf files for 1 paths.
[2023-01-31T05:12:23.708+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:23 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T05:12:24.307+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:24 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T05:12:24.323+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:24 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 81d5fcd0285b:34735 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T05:12:24.391+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:24 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T05:12:29.241+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:29 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T05:12:29.524+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:29 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T05:12:29.813+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:29 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T05:12:30.032+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:30 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T05:12:30.038+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:30 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T05:12:30.047+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:30 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:12:30.084+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:30 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:12:30.210+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:30 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T05:12:31.101+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:31 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T05:12:31.225+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:31 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T05:12:31.225+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:31 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 81d5fcd0285b:34735 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T05:12:31.242+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:31 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:12:31.437+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:12:31.461+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:31 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T05:12:33.897+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:33 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T05:12:34.194+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:34 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T05:12:35.253+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:35 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010500__202101010600.json:0+9367
[2023-01-31T05:12:37.892+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:37 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3480 bytes result sent to driver
[2023-01-31T05:12:38.120+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:38 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 5155 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:12:38.161+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:38 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T05:12:38.259+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:38 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 7.790 s
[2023-01-31T05:12:38.349+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:38 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:12:38.350+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T05:12:38.398+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:38 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 8.575620 s
[2023-01-31T05:12:45.901+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:45 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 81d5fcd0285b:34735 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T05:12:46.088+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:46 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 81d5fcd0285b:34735 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T05:13:12.762+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:12 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:12.765+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:12 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:13:12.779+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:12 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:15.251+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO CodeGenerator: Code generated in 1134.832668 ms
[2023-01-31T05:13:15.307+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T05:13:15.425+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T05:13:15.428+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 81d5fcd0285b:34735 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T05:13:15.434+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T05:13:15.535+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:15.884+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T05:13:15.892+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T05:13:15.892+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T05:13:15.895+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:15.896+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:15.905+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T05:13:15.948+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T05:13:15.978+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T05:13:15.995+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 81d5fcd0285b:34735 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T05:13:16.002+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:16 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:16.005+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:16.007+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:16 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T05:13:16.041+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:16 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:16.049+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:16 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T05:13:16.559+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:16 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T05:13:18.071+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO CodeGenerator: Code generated in 1202.11899 ms
[2023-01-31T05:13:19.637+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1628 bytes result sent to driver
[2023-01-31T05:13:19.655+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 3637 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:19.677+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T05:13:19.684+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 3.757 s
[2023-01-31T05:13:19.696+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:19.697+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T05:13:19.698+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 3.806432 s
[2023-01-31T05:13:20.372+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:20.378+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:13:20.379+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:20.669+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO CodeGenerator: Code generated in 128.413643 ms
[2023-01-31T05:13:20.746+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T05:13:20.925+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T05:13:20.928+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 81d5fcd0285b:34735 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:13:20.945+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T05:13:20.983+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:21.111+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T05:13:21.112+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T05:13:21.114+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T05:13:21.115+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:21.115+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:21.122+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T05:13:21.132+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T05:13:21.186+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T05:13:21.192+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 81d5fcd0285b:34735 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T05:13:21.200+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:21.212+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:21.217+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T05:13:21.225+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:21.228+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T05:13:21.296+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T05:13:21.584+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1585 bytes result sent to driver
[2023-01-31T05:13:21.609+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 385 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:21.610+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T05:13:21.615+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 0.489 s
[2023-01-31T05:13:21.616+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:21.623+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T05:13:21.625+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 0.513950 s
[2023-01-31T05:13:22.555+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:22.563+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T05:13:22.566+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:23.808+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO CodeGenerator: Code generated in 740.084069 ms
[2023-01-31T05:13:23.821+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T05:13:23.916+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T05:13:23.919+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 81d5fcd0285b:34735 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:13:23.924+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T05:13:23.935+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:24.359+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T05:13:24.367+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T05:13:24.368+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T05:13:24.370+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:24.371+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:24.380+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T05:13:24.447+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.6 MiB)
[2023-01-31T05:13:24.511+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.6 MiB)
[2023-01-31T05:13:24.522+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 81d5fcd0285b:34735 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T05:13:24.536+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:24.548+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:24.549+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T05:13:24.594+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:24.599+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T05:13:24.838+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T05:13:25.276+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-01-31T05:13:25.288+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 694 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:25.289+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T05:13:25.290+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 0.902 s
[2023-01-31T05:13:25.291+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:25.292+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T05:13:25.292+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 0.931345 s
[2023-01-31T05:13:25.541+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:25.542+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:13:25.547+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:25.741+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO CodeGenerator: Code generated in 109.76961 ms
[2023-01-31T05:13:25.783+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.4 MiB)
[2023-01-31T05:13:26.058+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T05:13:26.063+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 81d5fcd0285b:34735 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:13:26.076+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T05:13:26.101+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:26.538+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 81d5fcd0285b:34735 in memory (size: 7.7 KiB, free: 434.2 MiB)
[2023-01-31T05:13:26.609+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T05:13:26.616+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 81d5fcd0285b:34735 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T05:13:26.620+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T05:13:26.621+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T05:13:26.621+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:26.627+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:26.647+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T05:13:26.667+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.5 MiB)
[2023-01-31T05:13:26.674+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 81d5fcd0285b:34735 in memory (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T05:13:26.683+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.5 MiB)
[2023-01-31T05:13:26.696+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 81d5fcd0285b:34735 (size: 7.6 KiB, free: 434.3 MiB)
[2023-01-31T05:13:26.699+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:26.710+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:26.717+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T05:13:26.724+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:26.726+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T05:13:26.859+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T05:13:27.227+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:27 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1586 bytes result sent to driver
[2023-01-31T05:13:27.282+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:27 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 548 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:27.283+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:27 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T05:13:27.289+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:27 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 0.621 s
[2023-01-31T05:13:27.289+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:27 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:27.293+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T05:13:27.294+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:27 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 0.662104 s
[2023-01-31T05:13:29.234+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:29.235+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T05:13:29.236+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:29.955+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO CodeGenerator: Code generated in 528.815265 ms
[2023-01-31T05:13:29.997+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.3 MiB)
[2023-01-31T05:13:30.112+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T05:13:30.125+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 81d5fcd0285b:34735 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:13:30.134+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T05:13:30.170+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:30.414+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T05:13:30.416+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) with 1 output partitions
[2023-01-31T05:13:30.417+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201)
[2023-01-31T05:13:30.418+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:30.419+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:30.428+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201), which has no missing parents
[2023-01-31T05:13:30.450+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.2 MiB)
[2023-01-31T05:13:30.471+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.2 MiB)
[2023-01-31T05:13:30.478+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 81d5fcd0285b:34735 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T05:13:30.482+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:30.499+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:30.500+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T05:13:30.506+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:30.508+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T05:13:30.616+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T05:13:31.244+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:31 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2533 bytes result sent to driver
[2023-01-31T05:13:31.264+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:31 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 757 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:31.265+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:31 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T05:13:31.273+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:31 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) finished in 0.838 s
[2023-01-31T05:13:31.276+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:31 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:31.300+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T05:13:31.301+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:31 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201, took 0.879550 s
[2023-01-31T05:13:31.632+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:31 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:31.635+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:31 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T05:13:31.637+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:31 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:32.086+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:32 INFO CodeGenerator: Code generated in 374.686148 ms
[2023-01-31T05:13:32.181+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:32 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 433.0 MiB)
[2023-01-31T05:13:32.656+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:32 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.9 MiB)
[2023-01-31T05:13:32.680+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:32 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 81d5fcd0285b:34735 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:13:32.682+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:32 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T05:13:32.888+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:32 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:33.749+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T05:13:33.756+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T05:13:33.758+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T05:13:33.759+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:33.762+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:33.774+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T05:13:33.839+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 432.9 MiB)
[2023-01-31T05:13:33.871+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.9 MiB)
[2023-01-31T05:13:33.879+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 81d5fcd0285b:34735 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T05:13:33.897+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:33.913+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:33.915+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T05:13:33.931+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:33.937+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T05:13:33.970+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T05:13:34.223+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:34 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-01-31T05:13:34.231+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:34 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 300 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:34.236+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:34 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T05:13:34.237+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:34 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 0.444 s
[2023-01-31T05:13:34.238+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:34 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:34.241+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T05:13:34.244+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:34 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 0.493065 s
[2023-01-31T05:13:35.082+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:35 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:35.102+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:35 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T05:13:35.102+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:35 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:36.229+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO CodeGenerator: Code generated in 519.344909 ms
[2023-01-31T05:13:36.255+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.7 MiB)
[2023-01-31T05:13:36.361+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.7 MiB)
[2023-01-31T05:13:36.363+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 81d5fcd0285b:34735 (size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T05:13:36.368+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T05:13:36.379+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:36.724+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T05:13:36.728+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T05:13:36.729+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T05:13:36.730+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:36.731+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:36.743+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T05:13:36.802+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.6 MiB)
[2023-01-31T05:13:36.814+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.6 MiB)
[2023-01-31T05:13:36.816+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 81d5fcd0285b:34735 (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T05:13:36.818+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:36.820+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:36.822+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T05:13:36.826+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:36.828+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T05:13:36.854+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T05:13:37.205+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:37 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1729 bytes result sent to driver
[2023-01-31T05:13:37.205+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:37 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 376 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:37.222+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:37 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T05:13:37.227+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:37 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 0.457 s
[2023-01-31T05:13:37.227+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:37 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:37.228+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T05:13:37.232+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:37 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 0.500631 s
[2023-01-31T05:13:40.276+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:40 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 81d5fcd0285b:34735 in memory (size: 7.6 KiB, free: 434.1 MiB)
[2023-01-31T05:13:40.432+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:40 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 81d5fcd0285b:34735 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:13:40.500+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:40 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 81d5fcd0285b:34735 in memory (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T05:13:40.590+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:40 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 81d5fcd0285b:34735 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:13:40.646+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:40 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 81d5fcd0285b:34735 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:13:40.704+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:40 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 81d5fcd0285b:34735 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:13:40.814+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:40 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 81d5fcd0285b:34735 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:13:40.936+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:40 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 81d5fcd0285b:34735 in memory (size: 12.6 KiB, free: 434.3 MiB)
[2023-01-31T05:13:41.031+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:41 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 81d5fcd0285b:34735 in memory (size: 12.6 KiB, free: 434.3 MiB)
[2023-01-31T05:13:49.161+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:49 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 81d5fcd0285b:34735 in memory (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T05:14:04.280+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:04 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:14:04.363+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:14:04.364+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:14:04.372+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:04 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:14:04.375+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:14:04.376+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:14:04.388+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:14:05.263+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-31T05:14:05.264+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO DAGScheduler: Got job 8 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T05:14:05.265+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO DAGScheduler: Final stage: ResultStage 8 (save at BigQueryWriteHelper.java:105)
[2023-01-31T05:14:05.265+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:14:05.266+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:14:05.270+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO DAGScheduler: Submitting ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T05:14:05.432+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 209.9 KiB, free 434.0 MiB)
[2023-01-31T05:14:05.450+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 74.3 KiB, free 433.9 MiB)
[2023-01-31T05:14:05.454+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 81d5fcd0285b:34735 (size: 74.3 KiB, free: 434.3 MiB)
[2023-01-31T05:14:05.457+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:14:05.461+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:14:05.463+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2023-01-31T05:14:05.477+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7484 bytes) taskResourceAssignments Map()
[2023-01-31T05:14:05.484+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2023-01-31T05:14:05.742+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:14:05.744+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:14:05.749+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:14:05.753+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:14:05.755+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:14:05.759+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:14:05.784+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T05:14:05.798+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T05:14:05.867+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO ParquetOutputFormat: Parquet block size to 134217728
[2023-01-31T05:14:05.869+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO ParquetOutputFormat: Validation is off
[2023-01-31T05:14:05.870+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-01-31T05:14:05.871+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO ParquetOutputFormat: Parquet properties are:
[2023-01-31T05:14:05.872+0000] {spark_submit.py:495} INFO - Parquet page size to 1048576
[2023-01-31T05:14:05.872+0000] {spark_submit.py:495} INFO - Parquet dictionary page size to 1048576
[2023-01-31T05:14:05.873+0000] {spark_submit.py:495} INFO - Dictionary is true
[2023-01-31T05:14:05.873+0000] {spark_submit.py:495} INFO - Writer version is: PARQUET_1_0
[2023-01-31T05:14:05.874+0000] {spark_submit.py:495} INFO - Page size checking is: estimated
[2023-01-31T05:14:05.874+0000] {spark_submit.py:495} INFO - Min row count for page size check is: 100
[2023-01-31T05:14:05.875+0000] {spark_submit.py:495} INFO - Max row count for page size check is: 10000
[2023-01-31T05:14:05.876+0000] {spark_submit.py:495} INFO - Truncate length for column indexes is: 64
[2023-01-31T05:14:05.880+0000] {spark_submit.py:495} INFO - Truncate length for statistics min/max  is: 2147483647
[2023-01-31T05:14:05.881+0000] {spark_submit.py:495} INFO - Bloom filter enabled: false
[2023-01-31T05:14:05.884+0000] {spark_submit.py:495} INFO - Max Bloom filter size for a column is 1048576
[2023-01-31T05:14:05.884+0000] {spark_submit.py:495} INFO - Bloom filter expected number of distinct values are: null
[2023-01-31T05:14:05.887+0000] {spark_submit.py:495} INFO - Page row count limit to 20000
[2023-01-31T05:14:05.888+0000] {spark_submit.py:495} INFO - Writing page checksums is: on
[2023-01-31T05:14:06.058+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-01-31T05:14:06.059+0000] {spark_submit.py:495} INFO - {
[2023-01-31T05:14:06.059+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-31T05:14:06.060+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-31T05:14:06.060+0000] {spark_submit.py:495} INFO - "name" : "measured_at",
[2023-01-31T05:14:06.061+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T05:14:06.061+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T05:14:06.061+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:06.065+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:06.065+0000] {spark_submit.py:495} INFO - "name" : "created_at",
[2023-01-31T05:14:06.070+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T05:14:06.071+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T05:14:06.076+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:06.079+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:06.082+0000] {spark_submit.py:495} INFO - "name" : "Biomass - Actual Aggregated",
[2023-01-31T05:14:06.084+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:06.089+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:06.089+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:06.090+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:06.095+0000] {spark_submit.py:495} INFO - "name" : "Fossil Brown coal/Lignite - Actual Aggregated",
[2023-01-31T05:14:06.103+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:06.104+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:06.106+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:06.107+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:06.109+0000] {spark_submit.py:495} INFO - "name" : "Fossil Gas - Actual Aggregated",
[2023-01-31T05:14:06.110+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:06.111+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:06.112+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:06.113+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:06.114+0000] {spark_submit.py:495} INFO - "name" : "Fossil Hard coal - Actual Aggregated",
[2023-01-31T05:14:06.114+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:06.119+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:06.122+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:06.123+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:06.125+0000] {spark_submit.py:495} INFO - "name" : "Fossil Oil - Actual Aggregated",
[2023-01-31T05:14:06.126+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:06.129+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:06.130+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:06.137+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:06.137+0000] {spark_submit.py:495} INFO - "name" : "Geothermal - Actual Aggregated",
[2023-01-31T05:14:06.139+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:06.144+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:06.145+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:06.147+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:06.149+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Aggregated",
[2023-01-31T05:14:06.151+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:06.152+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:06.154+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:06.155+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:06.157+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Consumption",
[2023-01-31T05:14:06.158+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:06.160+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:06.161+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:06.163+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:06.165+0000] {spark_submit.py:495} INFO - "name" : "Hydro Run-of-river and poundage - Actual Aggregated",
[2023-01-31T05:14:06.168+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:06.168+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:06.169+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:06.176+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:06.177+0000] {spark_submit.py:495} INFO - "name" : "Hydro Water Reservoir - Actual Aggregated",
[2023-01-31T05:14:06.179+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:06.181+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:06.189+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:06.192+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:06.197+0000] {spark_submit.py:495} INFO - "name" : "Nuclear - Actual Aggregated",
[2023-01-31T05:14:06.200+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:06.200+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:06.211+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:06.211+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:06.215+0000] {spark_submit.py:495} INFO - "name" : "Other - Actual Aggregated",
[2023-01-31T05:14:06.216+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:06.219+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:06.220+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:06.232+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:06.233+0000] {spark_submit.py:495} INFO - "name" : "Other renewable - Actual Aggregated",
[2023-01-31T05:14:06.233+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:06.234+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:06.234+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:06.234+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:06.235+0000] {spark_submit.py:495} INFO - "name" : "Solar - Actual Aggregated",
[2023-01-31T05:14:06.235+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:06.235+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:06.235+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:06.236+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:06.236+0000] {spark_submit.py:495} INFO - "name" : "Waste - Actual Aggregated",
[2023-01-31T05:14:06.256+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:06.257+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:06.257+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:06.258+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:06.258+0000] {spark_submit.py:495} INFO - "name" : "Wind Offshore - Actual Aggregated",
[2023-01-31T05:14:06.288+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:06.288+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:06.288+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:06.289+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:06.289+0000] {spark_submit.py:495} INFO - "name" : "Wind Onshore - Actual Aggregated",
[2023-01-31T05:14:06.289+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:06.290+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:06.290+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:06.290+0000] {spark_submit.py:495} INFO - } ]
[2023-01-31T05:14:06.304+0000] {spark_submit.py:495} INFO - }
[2023-01-31T05:14:06.305+0000] {spark_submit.py:495} INFO - and corresponding Parquet message type:
[2023-01-31T05:14:06.312+0000] {spark_submit.py:495} INFO - message spark_schema {
[2023-01-31T05:14:06.312+0000] {spark_submit.py:495} INFO - required int96 measured_at;
[2023-01-31T05:14:06.317+0000] {spark_submit.py:495} INFO - required int96 created_at;
[2023-01-31T05:14:06.318+0000] {spark_submit.py:495} INFO - optional double Biomass - Actual Aggregated;
[2023-01-31T05:14:06.321+0000] {spark_submit.py:495} INFO - optional double Fossil Brown coal/Lignite - Actual Aggregated;
[2023-01-31T05:14:06.323+0000] {spark_submit.py:495} INFO - optional double Fossil Gas - Actual Aggregated;
[2023-01-31T05:14:06.324+0000] {spark_submit.py:495} INFO - optional double Fossil Hard coal - Actual Aggregated;
[2023-01-31T05:14:06.327+0000] {spark_submit.py:495} INFO - optional double Fossil Oil - Actual Aggregated;
[2023-01-31T05:14:06.329+0000] {spark_submit.py:495} INFO - optional double Geothermal - Actual Aggregated;
[2023-01-31T05:14:06.332+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Aggregated;
[2023-01-31T05:14:06.334+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Consumption;
[2023-01-31T05:14:06.336+0000] {spark_submit.py:495} INFO - optional double Hydro Run-of-river and poundage - Actual Aggregated;
[2023-01-31T05:14:06.338+0000] {spark_submit.py:495} INFO - optional double Hydro Water Reservoir - Actual Aggregated;
[2023-01-31T05:14:06.341+0000] {spark_submit.py:495} INFO - optional double Nuclear - Actual Aggregated;
[2023-01-31T05:14:06.342+0000] {spark_submit.py:495} INFO - optional double Other - Actual Aggregated;
[2023-01-31T05:14:06.343+0000] {spark_submit.py:495} INFO - optional double Other renewable - Actual Aggregated;
[2023-01-31T05:14:06.346+0000] {spark_submit.py:495} INFO - optional double Solar - Actual Aggregated;
[2023-01-31T05:14:06.348+0000] {spark_submit.py:495} INFO - optional double Waste - Actual Aggregated;
[2023-01-31T05:14:06.349+0000] {spark_submit.py:495} INFO - optional double Wind Offshore - Actual Aggregated;
[2023-01-31T05:14:06.351+0000] {spark_submit.py:495} INFO - optional double Wind Onshore - Actual Aggregated;
[2023-01-31T05:14:06.353+0000] {spark_submit.py:495} INFO - }
[2023-01-31T05:14:06.354+0000] {spark_submit.py:495} INFO - 
[2023-01-31T05:14:06.356+0000] {spark_submit.py:495} INFO - 
[2023-01-31T05:14:06.806+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO CodecPool: Got brand-new compressor [.snappy]
[2023-01-31T05:14:15.093+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:15 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675141908356-6653d558-7645-43d6-affb-7abe4529b5af/_temporary/0/_temporary/' directory.
[2023-01-31T05:14:15.094+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:15 INFO FileOutputCommitter: Saved output of task 'attempt_202301310514053039662798403541495_0008_m_000000_8' to gs://entsoe_temp_1009/.spark-bigquery-local-1675141908356-6653d558-7645-43d6-affb-7abe4529b5af/_temporary/0/task_202301310514053039662798403541495_0008_m_000000
[2023-01-31T05:14:15.102+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:15 INFO SparkHadoopMapRedUtil: attempt_202301310514053039662798403541495_0008_m_000000_8: Committed. Elapsed time: 2716 ms.
[2023-01-31T05:14:15.136+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:15 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2588 bytes result sent to driver
[2023-01-31T05:14:15.143+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:15 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 9674 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:14:15.153+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:15 INFO DAGScheduler: ResultStage 8 (save at BigQueryWriteHelper.java:105) finished in 9.864 s
[2023-01-31T05:14:15.153+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:15 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:14:15.159+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:15 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2023-01-31T05:14:15.162+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2023-01-31T05:14:15.170+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:15 INFO DAGScheduler: Job 8 finished: save at BigQueryWriteHelper.java:105, took 9.905144 s
[2023-01-31T05:14:15.180+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:15 INFO FileFormatWriter: Start to commit write Job 955badd9-e9e1-48ae-a74b-145d64fd29e3.
[2023-01-31T05:14:16.266+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:16 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675141908356-6653d558-7645-43d6-affb-7abe4529b5af/_temporary/0/task_202301310514053039662798403541495_0008_m_000000/' directory.
[2023-01-31T05:14:16.652+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:16 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675141908356-6653d558-7645-43d6-affb-7abe4529b5af/' directory.
[2023-01-31T05:14:17.277+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:17 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 81d5fcd0285b:34735 in memory (size: 74.3 KiB, free: 434.4 MiB)
[2023-01-31T05:14:17.852+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:17 INFO FileFormatWriter: Write Job 955badd9-e9e1-48ae-a74b-145d64fd29e3 committed. Elapsed time: 2665 ms.
[2023-01-31T05:14:17.860+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:17 INFO FileFormatWriter: Finished processing stats for write job 955badd9-e9e1-48ae-a74b-145d64fd29e3.
[2023-01-31T05:14:19.150+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:19 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=entsoe_analytics_1009, projectId=rafzul-analytics-1009, tableId=TEST_total_generation_staging}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=null, ignoreUnknownValue=null, sourceUris=[gs://entsoe_temp_1009/.spark-bigquery-local-1675141908356-6653d558-7645-43d6-affb-7abe4529b5af/part-00000-b8463702-a4ec-41eb-8d0d-fc35e7016dc4-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=true, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=rafzul-analytics-1009, job=f6a0b178-6ccf-40a8-8494-be45839c2d26, location=US}
[2023-01-31T05:14:22.960+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:22 INFO BigQueryClient: Done loading to rafzul-analytics-1009.entsoe_analytics_1009.TEST_total_generation_staging. jobId: JobId{project=rafzul-analytics-1009, job=f6a0b178-6ccf-40a8-8494-be45839c2d26, location=US}
[2023-01-31T05:14:23.456+0000] {spark_submit.py:495} INFO - MANTEP!!!!
[2023-01-31T05:14:23.544+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-31T05:14:23.554+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO SparkUI: Stopped Spark web UI at http://81d5fcd0285b:4045
[2023-01-31T05:14:23.565+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-31T05:14:23.579+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO MemoryStore: MemoryStore cleared
[2023-01-31T05:14:23.580+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO BlockManager: BlockManager stopped
[2023-01-31T05:14:23.582+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-31T05:14:23.586+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-31T05:14:23.601+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO SparkContext: Successfully stopped SparkContext
[2023-01-31T05:14:23.601+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO ShutdownHookManager: Shutdown hook called
[2023-01-31T05:14:23.602+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-94da920d-e815-4344-8642-800cfae16f24
[2023-01-31T05:14:23.607+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-6d4e36ab-06a1-4843-9fe7-d85513c8ad8b
[2023-01-31T05:14:23.611+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-94da920d-e815-4344-8642-800cfae16f24/pyspark-e508f256-75bb-47ba-bc55-ab096d2a8eaa
[2023-01-31T05:14:23.736+0000] {taskinstance.py:1322} INFO - Marking task as SUCCESS. dag_id=entsoe-energydata-backfill, task_id=stage_total_generation, execution_date=20210101T050000, start_date=20230131T051100, end_date=20230131T051423
[2023-01-31T05:14:23.799+0000] {local_task_job.py:159} INFO - Task exited with return code 0
[2023-01-31T05:14:23.819+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T05:30:46.201+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T05:00:00+00:00 [queued]>
[2023-01-31T05:30:46.283+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T05:00:00+00:00 [queued]>
[2023-01-31T05:30:46.284+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T05:30:46.285+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T05:30:46.286+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T05:30:46.376+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 05:00:00+00:00
[2023-01-31T05:30:46.462+0000] {standard_task_runner.py:55} INFO - Started process 15813 to run task
[2023-01-31T05:30:46.550+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T05:00:00+00:00', '--job-id', '898', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmphp88hj9o']
[2023-01-31T05:30:46.563+0000] {standard_task_runner.py:83} INFO - Job 898: Subtask stage_total_generation
[2023-01-31T05:30:47.536+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T05:00:00+00:00 [running]> on host 81d5fcd0285b
[2023-01-31T05:30:48.263+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T05:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T05:00:00+00:00
[2023-01-31T05:30:48.335+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T05:30:48.353+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010500 202101010600 DE_TENNET
[2023-01-31T05:32:14.957+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:14 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T05:32:16.662+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T05:32:19.545+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:19 INFO ResourceUtils: ==============================================================
[2023-01-31T05:32:19.550+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:19 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T05:32:19.559+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:19 INFO ResourceUtils: ==============================================================
[2023-01-31T05:32:19.565+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:19 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T05:32:20.244+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:20 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T05:32:20.390+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:20 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T05:32:20.473+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:20 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T05:32:21.602+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:21 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T05:32:21.612+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:21 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T05:32:21.621+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:21 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T05:32:21.631+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:21 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T05:32:21.645+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:21 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T05:32:27.699+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:27 INFO Utils: Successfully started service 'sparkDriver' on port 46605.
[2023-01-31T05:32:29.020+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:29 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T05:32:29.839+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:29 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T05:32:30.574+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:30 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T05:32:30.619+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:30 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T05:32:30.772+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:30 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T05:32:31.736+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:31 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-54c65443-8a1e-4714-a824-957ff7540c7c
[2023-01-31T05:32:32.379+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:32 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T05:32:32.928+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:32 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T05:32:39.140+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:39 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T05:32:39.180+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:39 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2023-01-31T05:32:39.184+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:39 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[2023-01-31T05:32:39.239+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:39 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[2023-01-31T05:32:39.292+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:39 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[2023-01-31T05:32:39.353+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:39 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
[2023-01-31T05:32:39.678+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:39 INFO Utils: Successfully started service 'SparkUI' on port 4046.
[2023-01-31T05:32:40.911+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:40 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://81d5fcd0285b:46605/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675143134623
[2023-01-31T05:32:40.922+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:40 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://81d5fcd0285b:46605/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675143134623
[2023-01-31T05:32:43.297+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:43 INFO Executor: Starting executor ID driver on host 81d5fcd0285b
[2023-01-31T05:32:43.714+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:43 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T05:32:43.847+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:43 INFO Executor: Fetching spark://81d5fcd0285b:46605/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675143134623
[2023-01-31T05:32:45.235+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:45 INFO TransportClientFactory: Successfully created connection to 81d5fcd0285b/172.19.0.9:46605 after 1135 ms (0 ms spent in bootstraps)
[2023-01-31T05:32:45.319+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:45 INFO Utils: Fetching spark://81d5fcd0285b:46605/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-806ea5d6-cda6-4853-a8ce-05eec2876145/userFiles-db16dbc4-d3ea-4e13-9454-f0d0d5b02054/fetchFileTemp631846256595958036.tmp
[2023-01-31T05:32:50.659+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:50 INFO Executor: Adding file:/tmp/spark-806ea5d6-cda6-4853-a8ce-05eec2876145/userFiles-db16dbc4-d3ea-4e13-9454-f0d0d5b02054/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T05:32:50.673+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:50 INFO Executor: Fetching spark://81d5fcd0285b:46605/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675143134623
[2023-01-31T05:32:50.701+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:50 INFO Utils: Fetching spark://81d5fcd0285b:46605/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-806ea5d6-cda6-4853-a8ce-05eec2876145/userFiles-db16dbc4-d3ea-4e13-9454-f0d0d5b02054/fetchFileTemp7083097174979796369.tmp
[2023-01-31T05:32:54.065+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:54 INFO Executor: Adding file:/tmp/spark-806ea5d6-cda6-4853-a8ce-05eec2876145/userFiles-db16dbc4-d3ea-4e13-9454-f0d0d5b02054/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T05:32:54.269+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:54 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43583.
[2023-01-31T05:32:54.270+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:54 INFO NettyBlockTransferService: Server created on 81d5fcd0285b:43583
[2023-01-31T05:32:54.308+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:54 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T05:32:54.582+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:54 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 43583, None)
[2023-01-31T05:32:54.689+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:54 INFO BlockManagerMasterEndpoint: Registering block manager 81d5fcd0285b:43583 with 434.4 MiB RAM, BlockManagerId(driver, 81d5fcd0285b, 43583, None)
[2023-01-31T05:32:54.727+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:54 INFO Executor: Told to re-register on heartbeat
[2023-01-31T05:32:54.731+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:54 INFO BlockManager: BlockManager null re-registering with master
[2023-01-31T05:32:54.734+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:54 INFO BlockManagerMaster: Registering BlockManager null
[2023-01-31T05:32:54.753+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:54 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 43583, None)
[2023-01-31T05:32:54.787+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:54 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 81d5fcd0285b, 43583, None)
[2023-01-31T05:32:54.802+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:54 WARN Executor: Issue communicating with driver in heartbeater
[2023-01-31T05:32:54.802+0000] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-01-31T05:32:54.803+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-01-31T05:32:54.804+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-01-31T05:32:54.804+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2023-01-31T05:32:54.809+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2023-01-31T05:32:54.810+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:79)
[2023-01-31T05:32:54.810+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:643)
[2023-01-31T05:32:54.811+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2023-01-31T05:32:54.811+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2023-01-31T05:32:54.812+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
[2023-01-31T05:32:54.828+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2023-01-31T05:32:54.832+0000] {spark_submit.py:495} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2023-01-31T05:32:54.833+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-01-31T05:32:54.834+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2023-01-31T05:32:54.834+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2023-01-31T05:32:54.835+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-31T05:32:54.836+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-31T05:32:54.836+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-31T05:32:54.853+0000] {spark_submit.py:495} INFO - Caused by: java.lang.NullPointerException
[2023-01-31T05:32:54.854+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)
[2023-01-31T05:32:54.854+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)
[2023-01-31T05:32:54.855+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-01-31T05:32:54.856+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-01-31T05:32:54.856+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-01-31T05:32:54.875+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-01-31T05:32:54.876+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-01-31T05:32:54.885+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-01-31T05:32:54.886+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-01-31T05:32:54.886+0000] {spark_submit.py:495} INFO - ... 3 more
[2023-01-31T05:32:54.887+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:54 ERROR Inbox: Ignoring error
[2023-01-31T05:32:54.887+0000] {spark_submit.py:495} INFO - java.lang.NullPointerException
[2023-01-31T05:32:54.888+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)
[2023-01-31T05:32:54.900+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)
[2023-01-31T05:32:54.901+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-01-31T05:32:54.902+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-01-31T05:32:54.902+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-01-31T05:32:54.903+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-01-31T05:32:54.903+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-01-31T05:32:54.904+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-01-31T05:32:54.904+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-01-31T05:32:54.913+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-31T05:32:54.914+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-31T05:32:54.914+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-31T05:33:16.001+0000] {spark_submit.py:495} INFO - 23/01/31 05:33:15 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T05:33:16.735+0000] {spark_submit.py:495} INFO - 23/01/31 05:33:16 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T05:34:25.637+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:25 INFO InMemoryFileIndex: It took 2320 ms to list leaf files for 1 paths.
[2023-01-31T05:34:27.899+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:27 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T05:34:31.645+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:31 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T05:34:31.797+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:31 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 81d5fcd0285b:43583 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T05:34:31.986+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:31 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T05:34:43.697+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:43 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T05:34:43.836+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:43 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T05:34:45.082+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:45 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T05:34:45.713+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:45 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T05:34:45.731+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:45 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T05:34:45.746+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:45 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:34:45.842+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:45 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:34:45.931+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:45 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T05:34:47.851+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:47 INFO AsyncEventQueue: Process of event SparkListenerJobStart(0,1675143285842,ArraySeq(org.apache.spark.scheduler.StageInfo@610802a9),{spark.master=local, spark.driver.port=46605, spark.submit.pyFiles=, spark.app.startTime=1675143134623, spark.rdd.compress=True, spark.executor.id=driver, spark.app.name=gcp_playground, spark.submit.deployMode=client, spark.hadoop.google.cloud.auth.service.account.json.keyfile=/.google/credentials/google_credentials.json, spark.driver.host=81d5fcd0285b, spark.app.id=local-1675143161913, spark.app.submitTime=1675143103130, spark.hadoop.fs.AbstractFileSystem.gs.impl=google.cloud.hadoop.fs.gcs.GoogleHadoopFS, spark.sql.session.timeZone=UTC, spark.hadoop.google.cloud.auth.service.account.enable=true, spark.app.initial.jar.urls=spark://81d5fcd0285b:46605/jars/gcs-connector-hadoop3-latest.jar,spark://81d5fcd0285b:46605/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar, spark.executor.extraJavaOptions=-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED, spark.jars=/opt/spark/jars/gcs-connector-hadoop3-latest.jar, /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar, spark.sql.warehouse.dir=file:/opt/***/spark-warehouse, spark.serializer.objectStreamReset=100, spark.driver.extraJavaOptions=-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED}) by listener AppStatusListener took 1.680388851s.
[2023-01-31T05:34:48.719+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:48 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T05:34:48.824+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:48 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T05:34:48.831+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:48 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 81d5fcd0285b:43583 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T05:34:48.885+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:48 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:34:49.360+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:34:49.396+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:49 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T05:34:51.726+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:51 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T05:34:52.430+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:52 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T05:34:56.120+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:56 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010500__202101010600.json:0+9367
[2023-01-31T05:35:01.896+0000] {spark_submit.py:495} INFO - 23/01/31 05:35:01 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3480 bytes result sent to driver
[2023-01-31T05:35:02.141+0000] {spark_submit.py:495} INFO - 23/01/31 05:35:02 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 11077 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:35:02.164+0000] {spark_submit.py:495} INFO - 23/01/31 05:35:02 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T05:35:02.302+0000] {spark_submit.py:495} INFO - 23/01/31 05:35:02 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 15.513 s
[2023-01-31T05:35:02.460+0000] {spark_submit.py:495} INFO - 23/01/31 05:35:02 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:35:02.476+0000] {spark_submit.py:495} INFO - 23/01/31 05:35:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T05:35:02.637+0000] {spark_submit.py:495} INFO - 23/01/31 05:35:02 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 17.474993 s
[2023-01-31T05:35:05.814+0000] {spark_submit.py:495} INFO - 23/01/31 05:35:05 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 81d5fcd0285b:43583 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T05:35:20.841+0000] {spark_submit.py:495} INFO - 23/01/31 05:35:20 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 81d5fcd0285b:43583 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T05:36:38.550+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:38 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:36:38.566+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:38 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:36:38.586+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:38 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:36:48.524+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:48 INFO CodeGenerator: Code generated in 5092.605556 ms
[2023-01-31T05:36:48.706+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:48 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T05:36:49.618+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:49 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T05:36:49.644+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:49 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 81d5fcd0285b:43583 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T05:36:49.660+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:49 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T05:36:49.809+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:49 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:36:51.679+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:51 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T05:36:51.721+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:51 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T05:36:51.722+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:51 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T05:36:51.723+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:51 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:36:51.728+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:51 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:36:51.757+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:51 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T05:36:52.297+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:52 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T05:36:52.732+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:52 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T05:36:52.741+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:52 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 81d5fcd0285b:43583 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T05:36:52.750+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:52 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:36:52.801+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:36:52.803+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:52 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T05:36:53.087+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:53 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:36:53.118+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:53 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T05:36:56.466+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:56 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T05:36:58.292+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:58 INFO CodeGenerator: Code generated in 1552.69915 ms
[2023-01-31T05:37:01.011+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:01 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1671 bytes result sent to driver
[2023-01-31T05:37:01.197+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:01 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 8291 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:37:01.208+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:01 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 9.277 s
[2023-01-31T05:37:01.259+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:01 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T05:37:01.260+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:01 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:37:01.298+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T05:37:01.305+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:01 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 9.582993 s
[2023-01-31T05:37:04.910+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:04 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:37:04.912+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:04 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:37:05.016+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:04 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:37:07.525+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:07 INFO CodeGenerator: Code generated in 1339.464601 ms
[2023-01-31T05:37:07.973+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:07 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T05:37:08.485+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:08 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T05:37:08.485+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:08 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 81d5fcd0285b:43583 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:37:08.569+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:08 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T05:37:08.833+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:08 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:37:09.371+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:09 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T05:37:09.377+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:09 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T05:37:09.378+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:09 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T05:37:09.378+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:09 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:37:09.388+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:09 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:37:09.415+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:09 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T05:37:09.969+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:09 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T05:37:10.065+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:10 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T05:37:10.077+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:10 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 81d5fcd0285b:43583 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T05:37:10.118+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:10 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:37:10.172+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:37:10.181+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:10 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T05:37:10.224+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:10 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:37:10.250+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:10 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T05:37:11.148+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T05:37:13.880+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:13 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1585 bytes result sent to driver
[2023-01-31T05:37:14.040+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:14 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 3819 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:37:14.043+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:14 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T05:37:14.099+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:14 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 4.586 s
[2023-01-31T05:37:14.242+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:14 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:37:14.243+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T05:37:14.301+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:14 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 4.888055 s
[2023-01-31T05:37:16.451+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:16 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:37:16.485+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:16 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T05:37:16.488+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:16 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:37:17.205+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:17 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 81d5fcd0285b:43583 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T05:37:17.363+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:17 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 81d5fcd0285b:43583 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T05:37:19.777+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:19 INFO CodeGenerator: Code generated in 1327.972105 ms
[2023-01-31T05:37:19.866+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:19 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T05:37:20.356+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:20 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T05:37:20.369+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:20 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 81d5fcd0285b:43583 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:37:20.466+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:20 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T05:37:20.492+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:20 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:37:20.793+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:20 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T05:37:20.810+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:20 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T05:37:20.813+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:20 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T05:37:20.815+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:20 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:37:20.824+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:20 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:37:20.840+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:20 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T05:37:20.865+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:20 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.7 MiB)
[2023-01-31T05:37:20.875+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:20 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.7 MiB)
[2023-01-31T05:37:20.886+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:20 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 81d5fcd0285b:43583 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T05:37:20.895+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:20 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:37:20.903+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:37:20.909+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:20 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T05:37:20.923+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:20 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:37:20.932+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:20 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T05:37:21.021+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:21 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T05:37:23.445+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:23 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-01-31T05:37:23.526+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:23 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 2587 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:37:23.606+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:23 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T05:37:23.610+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:23 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 2.706 s
[2023-01-31T05:37:23.652+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:23 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:37:23.705+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T05:37:23.764+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:23 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 2.922402 s
[2023-01-31T05:37:27.379+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:27 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:37:27.389+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:27 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:37:27.518+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:27 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:37:28.626+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:28 INFO CodeGenerator: Code generated in 690.927446 ms
[2023-01-31T05:37:28.968+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:28 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.5 MiB)
[2023-01-31T05:37:29.575+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T05:37:29.578+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 81d5fcd0285b:43583 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:37:29.586+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T05:37:29.659+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:37:30.713+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:30 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T05:37:30.713+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:30 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T05:37:30.714+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:30 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T05:37:30.714+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:30 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:37:30.725+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:30 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:37:30.727+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:30 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T05:37:30.882+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:30 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-31T05:37:31.081+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:31 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-01-31T05:37:31.083+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:31 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 81d5fcd0285b:43583 (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T05:37:31.109+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:31 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:37:31.112+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:37:31.112+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:31 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T05:37:31.158+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:31 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:37:31.159+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:31 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T05:37:31.196+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:31 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T05:37:32.100+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:32 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1586 bytes result sent to driver
[2023-01-31T05:37:32.114+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:32 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 960 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:37:32.122+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:32 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 1.353 s
[2023-01-31T05:37:32.123+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:32 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:37:32.141+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:32 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T05:37:32.149+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T05:37:32.150+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:32 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 1.467274 s
[2023-01-31T05:37:36.055+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:36 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 81d5fcd0285b:43583 in memory (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T05:37:36.363+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:36 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 81d5fcd0285b:43583 in memory (size: 7.6 KiB, free: 434.3 MiB)
[2023-01-31T05:37:38.762+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:38 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:37:38.764+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:38 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T05:37:38.765+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:38 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:37:41.452+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:41 INFO CodeGenerator: Code generated in 2292.653483 ms
[2023-01-31T05:37:41.809+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:41 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.3 MiB)
[2023-01-31T05:37:42.223+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:42 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T05:37:42.243+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:42 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 81d5fcd0285b:43583 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:37:42.249+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:42 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T05:37:42.319+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:42 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:37:43.485+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:43 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T05:37:43.491+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:43 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) with 1 output partitions
[2023-01-31T05:37:43.496+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:43 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201)
[2023-01-31T05:37:43.502+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:43 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:37:43.509+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:43 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:37:43.551+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:43 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201), which has no missing parents
[2023-01-31T05:37:43.663+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:43 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.2 MiB)
[2023-01-31T05:37:43.704+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:43 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.2 MiB)
[2023-01-31T05:37:43.712+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:43 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 81d5fcd0285b:43583 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T05:37:43.735+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:43 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:37:43.774+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:37:43.799+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:43 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T05:37:43.830+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:43 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:37:43.857+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:43 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T05:37:44.165+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:44 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T05:37:45.213+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:45 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2490 bytes result sent to driver
[2023-01-31T05:37:45.228+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:45 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 1398 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:37:45.234+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:45 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) finished in 1.662 s
[2023-01-31T05:37:45.240+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:45 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T05:37:45.261+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:45 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:37:45.269+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T05:37:45.269+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:45 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201, took 1.777712 s
[2023-01-31T05:37:46.899+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:46 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:37:46.923+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:46 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T05:37:46.946+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:46 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:37:49.944+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:49 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 81d5fcd0285b:43583 in memory (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T05:37:50.311+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:50 INFO CodeGenerator: Code generated in 1804.774566 ms
[2023-01-31T05:37:50.412+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:50 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 433.1 MiB)
[2023-01-31T05:37:51.033+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:51 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.0 MiB)
[2023-01-31T05:37:51.036+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:51 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 81d5fcd0285b:43583 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:37:51.066+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:51 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T05:37:51.111+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:51 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:37:51.606+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:51 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T05:37:51.630+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:51 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T05:37:51.630+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:51 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T05:37:51.631+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:51 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:37:51.645+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:51 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:37:51.672+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:51 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T05:37:51.819+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:51 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 433.0 MiB)
[2023-01-31T05:37:51.893+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:51 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 433.0 MiB)
[2023-01-31T05:37:51.899+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:51 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 81d5fcd0285b:43583 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T05:37:51.910+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:51 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:37:51.927+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:37:51.941+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:51 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T05:37:51.971+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:51 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:37:51.981+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:51 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T05:37:52.175+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:52 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T05:37:53.334+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:53 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-01-31T05:37:53.343+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:53 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 1372 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:37:53.349+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:53 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 1.599 s
[2023-01-31T05:37:53.350+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:53 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:37:53.351+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:53 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T05:37:53.369+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T05:37:53.370+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:53 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 1.756869 s
[2023-01-31T05:37:56.003+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:56 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:37:56.023+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:56 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T05:37:56.033+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:56 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:37:59.345+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:59 INFO CodeGenerator: Code generated in 2497.889422 ms
[2023-01-31T05:37:59.381+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:59 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.8 MiB)
[2023-01-31T05:37:59.532+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:59 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.7 MiB)
[2023-01-31T05:37:59.536+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:59 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 81d5fcd0285b:43583 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:37:59.547+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:59 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T05:37:59.594+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:59 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:37:59.790+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:59 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T05:37:59.798+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:59 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T05:37:59.804+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:59 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T05:37:59.805+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:59 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:37:59.815+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:59 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:38:00.047+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:59 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T05:38:00.387+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:00 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.7 MiB)
[2023-01-31T05:38:00.536+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:00 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.7 MiB)
[2023-01-31T05:38:00.549+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:00 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 81d5fcd0285b:43583 (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T05:38:00.556+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:00 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:38:00.675+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:38:00.677+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:00 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T05:38:00.727+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:00 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:38:00.735+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:00 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T05:38:01.028+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:01 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T05:38:01.572+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:01 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1729 bytes result sent to driver
[2023-01-31T05:38:01.578+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:01 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 851 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:38:01.579+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:01 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T05:38:01.609+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:01 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 1.555 s
[2023-01-31T05:38:01.660+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:01 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:38:01.662+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T05:38:01.681+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:01 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 1.871708 s
[2023-01-31T05:38:02.570+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:02 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 81d5fcd0285b:43583 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T05:38:02.770+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:02 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 81d5fcd0285b:43583 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T05:38:15.580+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:15 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 81d5fcd0285b:43583 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:38:15.662+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:15 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 81d5fcd0285b:43583 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:38:15.903+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:15 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 81d5fcd0285b:43583 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:38:15.973+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:15 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 81d5fcd0285b:43583 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:38:51.369+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:51 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:38:51.601+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:38:51.602+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:38:51.638+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:38:51.648+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:38:51.652+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:38:51.691+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:38:55.893+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:55 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-31T05:38:55.893+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:55 INFO DAGScheduler: Got job 8 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T05:38:55.894+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:55 INFO DAGScheduler: Final stage: ResultStage 8 (save at BigQueryWriteHelper.java:105)
[2023-01-31T05:38:55.894+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:55 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:38:55.895+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:55 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:38:55.904+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:55 INFO DAGScheduler: Submitting ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T05:38:56.713+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:56 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 209.9 KiB, free 433.5 MiB)
[2023-01-31T05:38:56.807+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:56 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 74.2 KiB, free 433.4 MiB)
[2023-01-31T05:38:56.818+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:56 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 81d5fcd0285b:43583 (size: 74.2 KiB, free: 434.2 MiB)
[2023-01-31T05:38:56.831+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:56 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:38:56.856+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:38:56.860+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:56 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2023-01-31T05:38:57.056+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:57 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7484 bytes) taskResourceAssignments Map()
[2023-01-31T05:38:57.080+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:57 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2023-01-31T05:38:57.577+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:38:57.578+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:38:57.586+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:57 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:38:57.588+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:38:57.590+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:38:57.595+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:38:57.666+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:57 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T05:38:57.701+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:57 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T05:38:58.019+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:58 INFO ParquetOutputFormat: Parquet block size to 134217728
[2023-01-31T05:38:58.020+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:58 INFO ParquetOutputFormat: Validation is off
[2023-01-31T05:38:58.021+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:58 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-01-31T05:38:58.021+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:58 INFO ParquetOutputFormat: Parquet properties are:
[2023-01-31T05:38:58.022+0000] {spark_submit.py:495} INFO - Parquet page size to 1048576
[2023-01-31T05:38:58.022+0000] {spark_submit.py:495} INFO - Parquet dictionary page size to 1048576
[2023-01-31T05:38:58.023+0000] {spark_submit.py:495} INFO - Dictionary is true
[2023-01-31T05:38:58.023+0000] {spark_submit.py:495} INFO - Writer version is: PARQUET_1_0
[2023-01-31T05:38:58.024+0000] {spark_submit.py:495} INFO - Page size checking is: estimated
[2023-01-31T05:38:58.024+0000] {spark_submit.py:495} INFO - Min row count for page size check is: 100
[2023-01-31T05:38:58.031+0000] {spark_submit.py:495} INFO - Max row count for page size check is: 10000
[2023-01-31T05:38:58.032+0000] {spark_submit.py:495} INFO - Truncate length for column indexes is: 64
[2023-01-31T05:38:58.036+0000] {spark_submit.py:495} INFO - Truncate length for statistics min/max  is: 2147483647
[2023-01-31T05:38:58.037+0000] {spark_submit.py:495} INFO - Bloom filter enabled: false
[2023-01-31T05:38:58.038+0000] {spark_submit.py:495} INFO - Max Bloom filter size for a column is 1048576
[2023-01-31T05:38:58.039+0000] {spark_submit.py:495} INFO - Bloom filter expected number of distinct values are: null
[2023-01-31T05:38:58.040+0000] {spark_submit.py:495} INFO - Page row count limit to 20000
[2023-01-31T05:38:58.040+0000] {spark_submit.py:495} INFO - Writing page checksums is: on
[2023-01-31T05:38:58.315+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:58 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-01-31T05:38:58.321+0000] {spark_submit.py:495} INFO - {
[2023-01-31T05:38:58.322+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-31T05:38:58.322+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-31T05:38:58.332+0000] {spark_submit.py:495} INFO - "name" : "measured_at",
[2023-01-31T05:38:58.342+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T05:38:58.346+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T05:38:58.368+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:58.370+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:58.383+0000] {spark_submit.py:495} INFO - "name" : "created_at",
[2023-01-31T05:38:58.384+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T05:38:58.384+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T05:38:58.407+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:58.408+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:58.428+0000] {spark_submit.py:495} INFO - "name" : "Biomass - Actual Aggregated",
[2023-01-31T05:38:58.439+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:58.439+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:58.480+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:58.481+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:58.482+0000] {spark_submit.py:495} INFO - "name" : "Fossil Brown coal/Lignite - Actual Aggregated",
[2023-01-31T05:38:58.483+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:58.542+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:58.543+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:58.544+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:58.601+0000] {spark_submit.py:495} INFO - "name" : "Fossil Gas - Actual Aggregated",
[2023-01-31T05:38:58.602+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:58.602+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:58.603+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:58.604+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:58.624+0000] {spark_submit.py:495} INFO - "name" : "Fossil Hard coal - Actual Aggregated",
[2023-01-31T05:38:58.684+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:58.685+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:58.686+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:58.686+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:58.687+0000] {spark_submit.py:495} INFO - "name" : "Fossil Oil - Actual Aggregated",
[2023-01-31T05:38:58.729+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:58.730+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:58.733+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:58.734+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:58.772+0000] {spark_submit.py:495} INFO - "name" : "Geothermal - Actual Aggregated",
[2023-01-31T05:38:58.778+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:58.779+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:58.780+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:58.826+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:58.827+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Aggregated",
[2023-01-31T05:38:58.827+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:58.828+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:58.892+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:58.992+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:59.001+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Consumption",
[2023-01-31T05:38:59.013+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:59.193+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:59.198+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:59.202+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:59.207+0000] {spark_submit.py:495} INFO - "name" : "Hydro Run-of-river and poundage - Actual Aggregated",
[2023-01-31T05:38:59.219+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:59.223+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:59.224+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:59.229+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:59.238+0000] {spark_submit.py:495} INFO - "name" : "Hydro Water Reservoir - Actual Aggregated",
[2023-01-31T05:38:59.241+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:59.273+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:59.274+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:59.310+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:59.310+0000] {spark_submit.py:495} INFO - "name" : "Nuclear - Actual Aggregated",
[2023-01-31T05:38:59.311+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:59.311+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:59.357+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:59.358+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:59.358+0000] {spark_submit.py:495} INFO - "name" : "Other - Actual Aggregated",
[2023-01-31T05:38:59.359+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:59.359+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:59.383+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:59.395+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:59.395+0000] {spark_submit.py:495} INFO - "name" : "Other renewable - Actual Aggregated",
[2023-01-31T05:38:59.396+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:59.397+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:59.397+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:59.398+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:59.399+0000] {spark_submit.py:495} INFO - "name" : "Solar - Actual Aggregated",
[2023-01-31T05:38:59.399+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:59.400+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:59.410+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:59.411+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:59.412+0000] {spark_submit.py:495} INFO - "name" : "Waste - Actual Aggregated",
[2023-01-31T05:38:59.412+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:59.421+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:59.422+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:59.422+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:59.423+0000] {spark_submit.py:495} INFO - "name" : "Wind Offshore - Actual Aggregated",
[2023-01-31T05:38:59.424+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:59.424+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:59.437+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:59.438+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:59.438+0000] {spark_submit.py:495} INFO - "name" : "Wind Onshore - Actual Aggregated",
[2023-01-31T05:38:59.450+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:59.458+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:59.459+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:59.459+0000] {spark_submit.py:495} INFO - } ]
[2023-01-31T05:38:59.460+0000] {spark_submit.py:495} INFO - }
[2023-01-31T05:38:59.460+0000] {spark_submit.py:495} INFO - and corresponding Parquet message type:
[2023-01-31T05:38:59.469+0000] {spark_submit.py:495} INFO - message spark_schema {
[2023-01-31T05:38:59.470+0000] {spark_submit.py:495} INFO - required int96 measured_at;
[2023-01-31T05:38:59.498+0000] {spark_submit.py:495} INFO - required int96 created_at;
[2023-01-31T05:38:59.499+0000] {spark_submit.py:495} INFO - optional double Biomass - Actual Aggregated;
[2023-01-31T05:38:59.500+0000] {spark_submit.py:495} INFO - optional double Fossil Brown coal/Lignite - Actual Aggregated;
[2023-01-31T05:38:59.521+0000] {spark_submit.py:495} INFO - optional double Fossil Gas - Actual Aggregated;
[2023-01-31T05:38:59.522+0000] {spark_submit.py:495} INFO - optional double Fossil Hard coal - Actual Aggregated;
[2023-01-31T05:38:59.522+0000] {spark_submit.py:495} INFO - optional double Fossil Oil - Actual Aggregated;
[2023-01-31T05:38:59.523+0000] {spark_submit.py:495} INFO - optional double Geothermal - Actual Aggregated;
[2023-01-31T05:38:59.567+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Aggregated;
[2023-01-31T05:38:59.568+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Consumption;
[2023-01-31T05:38:59.580+0000] {spark_submit.py:495} INFO - optional double Hydro Run-of-river and poundage - Actual Aggregated;
[2023-01-31T05:38:59.601+0000] {spark_submit.py:495} INFO - optional double Hydro Water Reservoir - Actual Aggregated;
[2023-01-31T05:38:59.602+0000] {spark_submit.py:495} INFO - optional double Nuclear - Actual Aggregated;
[2023-01-31T05:38:59.608+0000] {spark_submit.py:495} INFO - optional double Other - Actual Aggregated;
[2023-01-31T05:38:59.650+0000] {spark_submit.py:495} INFO - optional double Other renewable - Actual Aggregated;
[2023-01-31T05:38:59.650+0000] {spark_submit.py:495} INFO - optional double Solar - Actual Aggregated;
[2023-01-31T05:38:59.651+0000] {spark_submit.py:495} INFO - optional double Waste - Actual Aggregated;
[2023-01-31T05:38:59.652+0000] {spark_submit.py:495} INFO - optional double Wind Offshore - Actual Aggregated;
[2023-01-31T05:38:59.676+0000] {spark_submit.py:495} INFO - optional double Wind Onshore - Actual Aggregated;
[2023-01-31T05:38:59.687+0000] {spark_submit.py:495} INFO - }
[2023-01-31T05:38:59.688+0000] {spark_submit.py:495} INFO - 
[2023-01-31T05:38:59.707+0000] {spark_submit.py:495} INFO - 
[2023-01-31T05:39:02.092+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:02 INFO CodecPool: Got brand-new compressor [.snappy]
[2023-01-31T05:39:03.301+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:03 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 81d5fcd0285b:43583 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:39:19.217+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:19 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675143161913-93583e4d-3019-40fe-a3f4-a037e292c416/_temporary/0/_temporary/' directory.
[2023-01-31T05:39:19.273+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:19 INFO FileOutputCommitter: Saved output of task 'attempt_202301310538543680261019423621816_0008_m_000000_8' to gs://entsoe_temp_1009/.spark-bigquery-local-1675143161913-93583e4d-3019-40fe-a3f4-a037e292c416/_temporary/0/task_202301310538543680261019423621816_0008_m_000000
[2023-01-31T05:39:19.297+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:19 INFO SparkHadoopMapRedUtil: attempt_202301310538543680261019423621816_0008_m_000000_8: Committed. Elapsed time: 4123 ms.
[2023-01-31T05:39:19.377+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:19 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2588 bytes result sent to driver
[2023-01-31T05:39:19.408+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:19 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 22507 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:39:19.419+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:19 INFO DAGScheduler: ResultStage 8 (save at BigQueryWriteHelper.java:105) finished in 23.500 s
[2023-01-31T05:39:19.420+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:19 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:39:19.426+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:19 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2023-01-31T05:39:19.429+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2023-01-31T05:39:19.436+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:19 INFO DAGScheduler: Job 8 finished: save at BigQueryWriteHelper.java:105, took 23.545211 s
[2023-01-31T05:39:19.449+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:19 INFO FileFormatWriter: Start to commit write Job 3a5ac3b6-cb3d-4ef4-8111-a644a1a44be5.
[2023-01-31T05:39:20.903+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:20 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675143161913-93583e4d-3019-40fe-a3f4-a037e292c416/_temporary/0/task_202301310538543680261019423621816_0008_m_000000/' directory.
[2023-01-31T05:39:21.706+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:21 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675143161913-93583e4d-3019-40fe-a3f4-a037e292c416/' directory.
[2023-01-31T05:39:24.724+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:24 INFO FileFormatWriter: Write Job 3a5ac3b6-cb3d-4ef4-8111-a644a1a44be5 committed. Elapsed time: 5231 ms.
[2023-01-31T05:39:25.074+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:25 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 81d5fcd0285b:43583 in memory (size: 74.2 KiB, free: 434.3 MiB)
[2023-01-31T05:39:25.083+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:25 INFO FileFormatWriter: Finished processing stats for write job 3a5ac3b6-cb3d-4ef4-8111-a644a1a44be5.
[2023-01-31T05:39:25.160+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:25 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 81d5fcd0285b:43583 in memory (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T05:39:28.353+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:28 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=entsoe_analytics_1009, projectId=rafzul-analytics-1009, tableId=TEST_total_generation_staging}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=null, ignoreUnknownValue=null, sourceUris=[gs://entsoe_temp_1009/.spark-bigquery-local-1675143161913-93583e4d-3019-40fe-a3f4-a037e292c416/part-00000-0f4380c8-a8ed-4aa6-95e1-d84ebfec1be6-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=true, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=rafzul-analytics-1009, job=73ddc7f5-5eca-42d5-ae89-53c3e6a3538c, location=US}
[2023-01-31T05:39:31.701+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:31 INFO BigQueryClient: Done loading to rafzul-analytics-1009.entsoe_analytics_1009.TEST_total_generation_staging. jobId: JobId{project=rafzul-analytics-1009, job=73ddc7f5-5eca-42d5-ae89-53c3e6a3538c, location=US}
[2023-01-31T05:39:32.802+0000] {spark_submit.py:495} INFO - MANTEP!!!!
[2023-01-31T05:39:33.417+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:33 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-31T05:39:33.494+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:33 INFO SparkUI: Stopped Spark web UI at http://81d5fcd0285b:4046
[2023-01-31T05:39:33.580+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:33 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-31T05:39:33.635+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:33 INFO MemoryStore: MemoryStore cleared
[2023-01-31T05:39:33.638+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:33 INFO BlockManager: BlockManager stopped
[2023-01-31T05:39:33.646+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:33 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-31T05:39:33.663+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:33 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-31T05:39:33.689+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:33 INFO SparkContext: Successfully stopped SparkContext
[2023-01-31T05:39:33.690+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:33 INFO ShutdownHookManager: Shutdown hook called
[2023-01-31T05:39:33.692+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:33 INFO ShutdownHookManager: Deleting directory /tmp/spark-806ea5d6-cda6-4853-a8ce-05eec2876145
[2023-01-31T05:39:33.709+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:33 INFO ShutdownHookManager: Deleting directory /tmp/spark-f0f8d19f-4e4d-4953-b7d3-508c96284995
[2023-01-31T05:39:33.725+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:33 INFO ShutdownHookManager: Deleting directory /tmp/spark-806ea5d6-cda6-4853-a8ce-05eec2876145/pyspark-319083f8-9135-4002-a369-72af06be0934
[2023-01-31T05:39:34.342+0000] {taskinstance.py:1322} INFO - Marking task as SUCCESS. dag_id=entsoe-energydata-backfill, task_id=stage_total_generation, execution_date=20210101T050000, start_date=20230131T053046, end_date=20230131T053934
[2023-01-31T05:39:34.581+0000] {local_task_job.py:159} INFO - Task exited with return code 0
[2023-01-31T05:39:34.959+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T07:39:02.633+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T05:00:00+00:00 [queued]>
[2023-01-31T07:39:02.684+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T05:00:00+00:00 [queued]>
[2023-01-31T07:39:02.684+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T07:39:02.685+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T07:39:02.686+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T07:39:02.742+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 05:00:00+00:00
[2023-01-31T07:39:02.785+0000] {standard_task_runner.py:55} INFO - Started process 25806 to run task
[2023-01-31T07:39:02.900+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T05:00:00+00:00', '--job-id', '911', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpvm1e9pml']
[2023-01-31T07:39:02.980+0000] {standard_task_runner.py:83} INFO - Job 911: Subtask stage_total_generation
[2023-01-31T07:39:04.220+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T05:00:00+00:00 [running]> on host 81d5fcd0285b
[2023-01-31T07:39:05.130+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T05:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T05:00:00+00:00
[2023-01-31T07:39:05.300+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T07:39:05.318+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010500 202101010600 DE_TENNET
[2023-01-31T07:40:26.989+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:26 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T07:40:29.397+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T07:40:32.474+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:32 INFO ResourceUtils: ==============================================================
[2023-01-31T07:40:32.486+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:32 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T07:40:32.492+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:32 INFO ResourceUtils: ==============================================================
[2023-01-31T07:40:32.511+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:32 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T07:40:33.485+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:33 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T07:40:33.630+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:33 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T07:40:33.640+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:33 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T07:40:34.989+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:34 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T07:40:35.053+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:34 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T07:40:35.075+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:34 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T07:40:35.079+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:34 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T07:40:35.086+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:34 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T07:40:42.853+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:42 INFO Utils: Successfully started service 'sparkDriver' on port 37205.
[2023-01-31T07:40:43.579+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:43 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T07:40:44.203+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:44 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T07:40:45.407+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:45 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T07:40:45.412+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:45 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T07:40:45.452+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:45 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T07:40:45.580+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:45 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-fa4a616f-86f3-4cfd-98f6-0a8a2392eecc
[2023-01-31T07:40:46.189+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:46 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T07:40:47.356+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:47 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T07:40:55.063+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:55 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T07:40:55.095+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:55 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2023-01-31T07:40:55.101+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:55 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[2023-01-31T07:40:55.107+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:55 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[2023-01-31T07:40:55.260+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:55 INFO Utils: Successfully started service 'SparkUI' on port 4044.
[2023-01-31T07:40:56.256+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:56 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://81d5fcd0285b:37205/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675150826937
[2023-01-31T07:40:56.278+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:56 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://81d5fcd0285b:37205/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675150826937
[2023-01-31T07:40:59.810+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:59 INFO Executor: Starting executor ID driver on host 81d5fcd0285b
[2023-01-31T07:41:00.103+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:00 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T07:41:00.438+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:00 INFO Executor: Fetching spark://81d5fcd0285b:37205/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675150826937
[2023-01-31T07:41:02.026+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:02 INFO TransportClientFactory: Successfully created connection to 81d5fcd0285b/172.19.0.9:37205 after 815 ms (0 ms spent in bootstraps)
[2023-01-31T07:41:02.242+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:02 INFO Utils: Fetching spark://81d5fcd0285b:37205/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-d05c22e8-051a-4d2d-8b71-cf883b33da59/userFiles-a2c447e2-2b59-45fc-a5b7-f0a655d1140e/fetchFileTemp18186246057040144703.tmp
[2023-01-31T07:41:09.657+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:09 INFO Executor: Adding file:/tmp/spark-d05c22e8-051a-4d2d-8b71-cf883b33da59/userFiles-a2c447e2-2b59-45fc-a5b7-f0a655d1140e/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T07:41:09.658+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:09 INFO Executor: Fetching spark://81d5fcd0285b:37205/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675150826937
[2023-01-31T07:41:09.767+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:09 INFO Utils: Fetching spark://81d5fcd0285b:37205/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-d05c22e8-051a-4d2d-8b71-cf883b33da59/userFiles-a2c447e2-2b59-45fc-a5b7-f0a655d1140e/fetchFileTemp17714239307770118698.tmp
[2023-01-31T07:41:15.830+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:15 INFO Executor: Adding file:/tmp/spark-d05c22e8-051a-4d2d-8b71-cf883b33da59/userFiles-a2c447e2-2b59-45fc-a5b7-f0a655d1140e/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T07:41:16.117+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:16 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33893.
[2023-01-31T07:41:16.123+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:16 INFO NettyBlockTransferService: Server created on 81d5fcd0285b:33893
[2023-01-31T07:41:16.254+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:16 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T07:41:16.611+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:16 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 33893, None)
[2023-01-31T07:41:17.067+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:17 INFO BlockManagerMasterEndpoint: Registering block manager 81d5fcd0285b:33893 with 434.4 MiB RAM, BlockManagerId(driver, 81d5fcd0285b, 33893, None)
[2023-01-31T07:41:17.257+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:17 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 33893, None)
[2023-01-31T07:41:17.263+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:17 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 81d5fcd0285b, 33893, None)
[2023-01-31T07:41:18.858+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:18 INFO Executor: Told to re-register on heartbeat
[2023-01-31T07:41:18.966+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:18 INFO BlockManager: BlockManager BlockManagerId(driver, 81d5fcd0285b, 33893, None) re-registering with master
[2023-01-31T07:41:18.986+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:18 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 33893, None)
[2023-01-31T07:41:19.153+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:19 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 33893, None)
[2023-01-31T07:41:19.154+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:19 INFO BlockManager: Reporting 0 blocks to the master.
[2023-01-31T07:41:38.298+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:38 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T07:41:38.501+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:38 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T07:42:20.753+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:20 INFO InMemoryFileIndex: It took 1042 ms to list leaf files for 1 paths.
[2023-01-31T07:42:24.448+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:24 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T07:42:25.715+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:25 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T07:42:25.778+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:25 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 81d5fcd0285b:33893 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T07:42:26.063+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:26 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T07:42:39.182+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:39 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T07:42:39.351+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:39 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T07:42:39.805+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:39 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T07:42:39.933+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:39 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T07:42:39.942+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:39 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T07:42:39.945+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:39 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:42:39.956+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:39 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:42:39.992+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:39 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T07:42:42.836+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:42 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T07:42:43.083+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:43 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T07:42:43.087+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:43 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 81d5fcd0285b:33893 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T07:42:43.115+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:43 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:42:43.265+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:42:43.345+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:43 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T07:42:44.223+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:44 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T07:42:44.712+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:44 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T07:42:50.605+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:50 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010500__202101010600.json:0+9367
[2023-01-31T07:42:54.174+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:54 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3480 bytes result sent to driver
[2023-01-31T07:42:54.476+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:54 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 10424 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:42:54.576+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:54 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T07:42:54.783+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:54 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 14.035 s
[2023-01-31T07:42:54.959+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:54 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:42:55.011+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T07:42:55.105+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:55 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 15.291651 s
[2023-01-31T07:43:03.731+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:03 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 81d5fcd0285b:33893 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T07:43:03.890+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:03 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 81d5fcd0285b:33893 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T07:43:24.421+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:24 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:43:24.427+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:24 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T07:43:24.444+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:24 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:43:26.936+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:26 INFO CodeGenerator: Code generated in 1611.806305 ms
[2023-01-31T07:43:27.007+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T07:43:27.131+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T07:43:27.145+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 81d5fcd0285b:33893 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T07:43:27.158+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T07:43:27.269+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:43:27.829+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T07:43:27.842+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T07:43:27.842+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T07:43:27.843+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:43:27.843+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:43:27.853+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T07:43:27.945+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T07:43:27.996+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T07:43:28.004+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:28 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 81d5fcd0285b:33893 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T07:43:28.005+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:28 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:43:28.013+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:43:28.014+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:28 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T07:43:28.033+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:28 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:43:28.051+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:28 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T07:43:28.373+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:28 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T07:43:29.126+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO CodeGenerator: Code generated in 577.045751 ms
[2023-01-31T07:43:29.660+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1628 bytes result sent to driver
[2023-01-31T07:43:29.676+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1656 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:43:29.683+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T07:43:29.684+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 1.767 s
[2023-01-31T07:43:29.684+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:43:29.685+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T07:43:29.685+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 1.842805 s
[2023-01-31T07:43:29.995+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:43:29.995+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T07:43:29.998+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:43:30.211+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO CodeGenerator: Code generated in 111.945938 ms
[2023-01-31T07:43:30.225+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T07:43:30.253+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T07:43:30.256+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 81d5fcd0285b:33893 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:43:30.270+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T07:43:30.282+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:43:30.355+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T07:43:30.361+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T07:43:30.361+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T07:43:30.362+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:43:30.363+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:43:30.370+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T07:43:30.394+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T07:43:30.418+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T07:43:30.422+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 81d5fcd0285b:33893 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T07:43:30.425+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:43:30.425+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:43:30.425+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T07:43:30.433+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:43:30.441+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T07:43:30.508+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T07:43:30.899+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1585 bytes result sent to driver
[2023-01-31T07:43:30.909+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 481 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:43:30.914+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T07:43:30.926+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 0.542 s
[2023-01-31T07:43:30.927+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:43:30.927+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T07:43:30.927+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 0.565974 s
[2023-01-31T07:43:31.589+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:31 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 81d5fcd0285b:33893 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T07:43:31.808+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:31 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:43:31.813+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:31 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T07:43:31.818+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:31 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:43:31.861+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:31 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 81d5fcd0285b:33893 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T07:43:33.045+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO CodeGenerator: Code generated in 710.943105 ms
[2023-01-31T07:43:33.062+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T07:43:33.098+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T07:43:33.108+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 81d5fcd0285b:33893 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:43:33.112+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T07:43:33.124+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:43:33.212+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T07:43:33.217+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T07:43:33.217+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T07:43:33.220+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:43:33.221+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:43:33.232+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T07:43:33.254+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.7 MiB)
[2023-01-31T07:43:33.261+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.7 MiB)
[2023-01-31T07:43:33.329+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 81d5fcd0285b:33893 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T07:43:33.330+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:43:33.334+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:43:33.335+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T07:43:33.336+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:43:33.341+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T07:43:33.532+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T07:43:33.980+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-01-31T07:43:34.113+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 764 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:43:34.166+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 0.897 s
[2023-01-31T07:43:34.167+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:43:34.182+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T07:43:34.204+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T07:43:34.205+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 0.970340 s
[2023-01-31T07:43:34.622+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:43:34.623+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T07:43:34.624+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:43:34.723+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO CodeGenerator: Code generated in 46.741516 ms
[2023-01-31T07:43:34.760+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.5 MiB)
[2023-01-31T07:43:34.793+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T07:43:34.799+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 81d5fcd0285b:33893 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:43:34.822+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T07:43:34.837+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:43:34.982+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T07:43:34.987+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T07:43:34.988+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T07:43:34.988+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:43:34.992+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:43:34.998+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T07:43:35.043+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:35 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-31T07:43:35.083+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:35 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-01-31T07:43:35.091+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:35 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 81d5fcd0285b:33893 (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T07:43:35.100+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:35 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:43:35.133+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:43:35.145+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:35 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T07:43:35.159+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:35 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:43:35.165+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:35 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T07:43:35.258+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:35 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T07:43:35.863+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:35 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1586 bytes result sent to driver
[2023-01-31T07:43:35.894+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:35 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 736 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:43:35.895+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:35 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T07:43:35.895+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:35 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 0.884 s
[2023-01-31T07:43:35.899+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:35 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:43:35.905+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T07:43:35.907+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:35 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 0.916941 s
[2023-01-31T07:43:38.404+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:38 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:43:38.406+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:38 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T07:43:38.415+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:38 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:43:39.403+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:39 INFO CodeGenerator: Code generated in 451.009348 ms
[2023-01-31T07:43:39.431+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:39 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.2 MiB)
[2023-01-31T07:43:39.682+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:39 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T07:43:39.695+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:39 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 81d5fcd0285b:33893 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T07:43:39.702+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:39 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T07:43:39.730+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:39 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:43:40.080+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T07:43:40.101+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) with 1 output partitions
[2023-01-31T07:43:40.102+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201)
[2023-01-31T07:43:40.102+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:43:40.103+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:43:40.141+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201), which has no missing parents
[2023-01-31T07:43:40.181+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.1 MiB)
[2023-01-31T07:43:40.202+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.1 MiB)
[2023-01-31T07:43:40.212+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 81d5fcd0285b:33893 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T07:43:40.215+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:43:40.221+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:43:40.223+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T07:43:40.243+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:43:40.250+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T07:43:40.371+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T07:43:41.038+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:41 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2490 bytes result sent to driver
[2023-01-31T07:43:41.123+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:41 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 876 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:43:41.124+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:41 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T07:43:41.133+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:41 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) finished in 0.973 s
[2023-01-31T07:43:41.136+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:41 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:43:41.137+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T07:43:41.138+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:41 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201, took 1.040332 s
[2023-01-31T07:43:43.437+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:43 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:43:43.438+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:43 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T07:43:43.439+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:43 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:43:44.437+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:44 INFO CodeGenerator: Code generated in 566.974809 ms
[2023-01-31T07:43:44.478+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:44 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 432.9 MiB)
[2023-01-31T07:43:44.653+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:44 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.9 MiB)
[2023-01-31T07:43:44.666+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:44 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 81d5fcd0285b:33893 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T07:43:44.745+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:44 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T07:43:44.755+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:44 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:43:45.253+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T07:43:45.271+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T07:43:45.272+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T07:43:45.273+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:43:45.275+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:43:45.283+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T07:43:45.309+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 432.9 MiB)
[2023-01-31T07:43:45.337+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.8 MiB)
[2023-01-31T07:43:45.339+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 81d5fcd0285b:33893 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T07:43:45.345+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:43:45.350+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:43:45.353+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T07:43:45.371+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:43:45.378+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T07:43:45.583+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T07:43:47.442+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:47 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-01-31T07:43:47.512+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:47 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 2148 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:43:47.527+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:47 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 2.231 s
[2023-01-31T07:43:47.533+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:47 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:43:47.546+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:47 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T07:43:47.569+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T07:43:47.573+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:47 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 2.321576 s
[2023-01-31T07:43:48.870+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:48 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 81d5fcd0285b:33893 in memory (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T07:43:49.157+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:49 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 81d5fcd0285b:33893 in memory (size: 12.7 KiB, free: 434.2 MiB)
[2023-01-31T07:43:49.402+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:49 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 81d5fcd0285b:33893 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T07:43:49.641+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:49 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 81d5fcd0285b:33893 in memory (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T07:43:50.016+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:43:50.020+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T07:43:50.023+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:43:52.622+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:52 INFO CodeGenerator: Code generated in 1859.035034 ms
[2023-01-31T07:43:52.655+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:52 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.8 MiB)
[2023-01-31T07:43:52.774+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:52 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.8 MiB)
[2023-01-31T07:43:52.779+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:52 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 81d5fcd0285b:33893 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T07:43:52.785+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:52 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T07:43:52.790+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:52 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:43:52.868+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:52 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T07:43:52.890+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:52 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T07:43:52.898+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:52 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T07:43:52.907+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:52 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:43:52.938+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:52 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:43:52.979+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:52 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T07:43:53.304+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:53 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.8 MiB)
[2023-01-31T07:43:53.352+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:53 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.7 MiB)
[2023-01-31T07:43:53.359+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:53 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 81d5fcd0285b:33893 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T07:43:53.369+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:53 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:43:53.398+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:43:53.401+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:53 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T07:43:53.429+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:53 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:43:53.440+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:53 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T07:43:53.662+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:53 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T07:43:54.616+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:54 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1729 bytes result sent to driver
[2023-01-31T07:43:54.634+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:54 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 1192 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:43:54.635+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:54 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T07:43:54.636+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:54 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 1.540 s
[2023-01-31T07:43:54.637+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:54 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:43:54.638+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T07:43:54.638+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:54 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 1.761434 s
[2023-01-31T07:44:35.201+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:35 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 81d5fcd0285b:33893 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T07:44:35.457+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:35 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 81d5fcd0285b:33893 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T07:44:35.595+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:35 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 81d5fcd0285b:33893 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:44:35.904+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:35 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 81d5fcd0285b:33893 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:44:36.271+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:36 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 81d5fcd0285b:33893 in memory (size: 12.6 KiB, free: 434.3 MiB)
[2023-01-31T07:44:36.519+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:36 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 81d5fcd0285b:33893 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:44:36.786+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:36 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 81d5fcd0285b:33893 in memory (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T07:45:08.731+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:08 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T07:45:09.013+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T07:45:09.029+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T07:45:09.049+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:09 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T07:45:09.058+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T07:45:09.062+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T07:45:09.079+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T07:45:10.488+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:10 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-31T07:45:10.489+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:10 INFO DAGScheduler: Got job 8 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T07:45:10.489+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:10 INFO DAGScheduler: Final stage: ResultStage 8 (save at BigQueryWriteHelper.java:105)
[2023-01-31T07:45:10.489+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:10 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:45:10.490+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:10 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:45:10.512+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:10 INFO DAGScheduler: Submitting ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T07:45:10.976+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:10 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 209.9 KiB, free 434.0 MiB)
[2023-01-31T07:45:10.980+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:10 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 74.3 KiB, free 433.9 MiB)
[2023-01-31T07:45:10.986+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:10 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 81d5fcd0285b:33893 (size: 74.3 KiB, free: 434.3 MiB)
[2023-01-31T07:45:10.992+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:10 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:45:11.003+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:45:11.014+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2023-01-31T07:45:11.034+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7484 bytes) taskResourceAssignments Map()
[2023-01-31T07:45:11.054+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2023-01-31T07:45:11.469+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T07:45:11.469+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T07:45:11.486+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T07:45:11.492+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T07:45:11.493+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T07:45:11.502+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T07:45:11.565+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T07:45:11.613+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T07:45:11.880+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO ParquetOutputFormat: Parquet block size to 134217728
[2023-01-31T07:45:11.881+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO ParquetOutputFormat: Validation is off
[2023-01-31T07:45:11.884+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-01-31T07:45:11.889+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO ParquetOutputFormat: Parquet properties are:
[2023-01-31T07:45:11.889+0000] {spark_submit.py:495} INFO - Parquet page size to 1048576
[2023-01-31T07:45:11.889+0000] {spark_submit.py:495} INFO - Parquet dictionary page size to 1048576
[2023-01-31T07:45:11.890+0000] {spark_submit.py:495} INFO - Dictionary is true
[2023-01-31T07:45:11.890+0000] {spark_submit.py:495} INFO - Writer version is: PARQUET_1_0
[2023-01-31T07:45:11.890+0000] {spark_submit.py:495} INFO - Page size checking is: estimated
[2023-01-31T07:45:11.890+0000] {spark_submit.py:495} INFO - Min row count for page size check is: 100
[2023-01-31T07:45:11.890+0000] {spark_submit.py:495} INFO - Max row count for page size check is: 10000
[2023-01-31T07:45:11.890+0000] {spark_submit.py:495} INFO - Truncate length for column indexes is: 64
[2023-01-31T07:45:11.891+0000] {spark_submit.py:495} INFO - Truncate length for statistics min/max  is: 2147483647
[2023-01-31T07:45:11.891+0000] {spark_submit.py:495} INFO - Bloom filter enabled: false
[2023-01-31T07:45:11.891+0000] {spark_submit.py:495} INFO - Max Bloom filter size for a column is 1048576
[2023-01-31T07:45:11.891+0000] {spark_submit.py:495} INFO - Bloom filter expected number of distinct values are: null
[2023-01-31T07:45:11.891+0000] {spark_submit.py:495} INFO - Page row count limit to 20000
[2023-01-31T07:45:11.891+0000] {spark_submit.py:495} INFO - Writing page checksums is: on
[2023-01-31T07:45:12.293+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:12 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-01-31T07:45:12.293+0000] {spark_submit.py:495} INFO - {
[2023-01-31T07:45:12.293+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-31T07:45:12.294+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-31T07:45:12.294+0000] {spark_submit.py:495} INFO - "name" : "measured_at",
[2023-01-31T07:45:12.294+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T07:45:12.294+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T07:45:12.294+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.295+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.295+0000] {spark_submit.py:495} INFO - "name" : "created_at",
[2023-01-31T07:45:12.295+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T07:45:12.295+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T07:45:12.295+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.295+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.297+0000] {spark_submit.py:495} INFO - "name" : "Biomass - Actual Aggregated",
[2023-01-31T07:45:12.297+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.297+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.297+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.297+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.305+0000] {spark_submit.py:495} INFO - "name" : "Fossil Brown coal/Lignite - Actual Aggregated",
[2023-01-31T07:45:12.305+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.306+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.306+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.306+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.306+0000] {spark_submit.py:495} INFO - "name" : "Fossil Gas - Actual Aggregated",
[2023-01-31T07:45:12.306+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.306+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.306+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.307+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.307+0000] {spark_submit.py:495} INFO - "name" : "Fossil Hard coal - Actual Aggregated",
[2023-01-31T07:45:12.307+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.307+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.307+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.307+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.307+0000] {spark_submit.py:495} INFO - "name" : "Fossil Oil - Actual Aggregated",
[2023-01-31T07:45:12.308+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.308+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.308+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.308+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.308+0000] {spark_submit.py:495} INFO - "name" : "Geothermal - Actual Aggregated",
[2023-01-31T07:45:12.308+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.325+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.328+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.331+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.332+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Aggregated",
[2023-01-31T07:45:12.332+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.332+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.343+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.343+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.349+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Consumption",
[2023-01-31T07:45:12.350+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.350+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.350+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.350+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.360+0000] {spark_submit.py:495} INFO - "name" : "Hydro Run-of-river and poundage - Actual Aggregated",
[2023-01-31T07:45:12.360+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.371+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.371+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.372+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.372+0000] {spark_submit.py:495} INFO - "name" : "Hydro Water Reservoir - Actual Aggregated",
[2023-01-31T07:45:12.379+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.379+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.380+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.385+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.385+0000] {spark_submit.py:495} INFO - "name" : "Nuclear - Actual Aggregated",
[2023-01-31T07:45:12.396+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.397+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.399+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.400+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.400+0000] {spark_submit.py:495} INFO - "name" : "Other - Actual Aggregated",
[2023-01-31T07:45:12.400+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.415+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.416+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.416+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.422+0000] {spark_submit.py:495} INFO - "name" : "Other renewable - Actual Aggregated",
[2023-01-31T07:45:12.422+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.423+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.431+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.431+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.437+0000] {spark_submit.py:495} INFO - "name" : "Solar - Actual Aggregated",
[2023-01-31T07:45:12.441+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.441+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.446+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.447+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.453+0000] {spark_submit.py:495} INFO - "name" : "Waste - Actual Aggregated",
[2023-01-31T07:45:12.453+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.453+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.454+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.462+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.463+0000] {spark_submit.py:495} INFO - "name" : "Wind Offshore - Actual Aggregated",
[2023-01-31T07:45:12.471+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.471+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.483+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.484+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.484+0000] {spark_submit.py:495} INFO - "name" : "Wind Onshore - Actual Aggregated",
[2023-01-31T07:45:12.489+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.490+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.495+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.495+0000] {spark_submit.py:495} INFO - } ]
[2023-01-31T07:45:12.495+0000] {spark_submit.py:495} INFO - }
[2023-01-31T07:45:12.495+0000] {spark_submit.py:495} INFO - and corresponding Parquet message type:
[2023-01-31T07:45:12.521+0000] {spark_submit.py:495} INFO - message spark_schema {
[2023-01-31T07:45:12.521+0000] {spark_submit.py:495} INFO - required int96 measured_at;
[2023-01-31T07:45:12.521+0000] {spark_submit.py:495} INFO - required int96 created_at;
[2023-01-31T07:45:12.522+0000] {spark_submit.py:495} INFO - optional double Biomass - Actual Aggregated;
[2023-01-31T07:45:12.522+0000] {spark_submit.py:495} INFO - optional double Fossil Brown coal/Lignite - Actual Aggregated;
[2023-01-31T07:45:12.522+0000] {spark_submit.py:495} INFO - optional double Fossil Gas - Actual Aggregated;
[2023-01-31T07:45:12.522+0000] {spark_submit.py:495} INFO - optional double Fossil Hard coal - Actual Aggregated;
[2023-01-31T07:45:12.522+0000] {spark_submit.py:495} INFO - optional double Fossil Oil - Actual Aggregated;
[2023-01-31T07:45:12.522+0000] {spark_submit.py:495} INFO - optional double Geothermal - Actual Aggregated;
[2023-01-31T07:45:12.523+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Aggregated;
[2023-01-31T07:45:12.523+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Consumption;
[2023-01-31T07:45:12.523+0000] {spark_submit.py:495} INFO - optional double Hydro Run-of-river and poundage - Actual Aggregated;
[2023-01-31T07:45:12.523+0000] {spark_submit.py:495} INFO - optional double Hydro Water Reservoir - Actual Aggregated;
[2023-01-31T07:45:12.523+0000] {spark_submit.py:495} INFO - optional double Nuclear - Actual Aggregated;
[2023-01-31T07:45:12.523+0000] {spark_submit.py:495} INFO - optional double Other - Actual Aggregated;
[2023-01-31T07:45:12.524+0000] {spark_submit.py:495} INFO - optional double Other renewable - Actual Aggregated;
[2023-01-31T07:45:12.524+0000] {spark_submit.py:495} INFO - optional double Solar - Actual Aggregated;
[2023-01-31T07:45:12.524+0000] {spark_submit.py:495} INFO - optional double Waste - Actual Aggregated;
[2023-01-31T07:45:12.524+0000] {spark_submit.py:495} INFO - optional double Wind Offshore - Actual Aggregated;
[2023-01-31T07:45:12.524+0000] {spark_submit.py:495} INFO - optional double Wind Onshore - Actual Aggregated;
[2023-01-31T07:45:12.537+0000] {spark_submit.py:495} INFO - }
[2023-01-31T07:45:12.538+0000] {spark_submit.py:495} INFO - 
[2023-01-31T07:45:12.538+0000] {spark_submit.py:495} INFO - 
[2023-01-31T07:45:13.531+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:13 INFO CodecPool: Got brand-new compressor [.snappy]
[2023-01-31T07:45:21.175+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:21 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675150857871-a135a152-d5f5-4f83-9abf-62782fc2a79f/_temporary/0/_temporary/' directory.
[2023-01-31T07:45:21.177+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:21 INFO FileOutputCommitter: Saved output of task 'attempt_202301310745105451738571484448784_0008_m_000000_8' to gs://entsoe_temp_1009/.spark-bigquery-local-1675150857871-a135a152-d5f5-4f83-9abf-62782fc2a79f/_temporary/0/task_202301310745105451738571484448784_0008_m_000000
[2023-01-31T07:45:21.314+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:21 INFO SparkHadoopMapRedUtil: attempt_202301310745105451738571484448784_0008_m_000000_8: Committed. Elapsed time: 2368 ms.
[2023-01-31T07:45:21.502+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:21 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2588 bytes result sent to driver
[2023-01-31T07:45:21.528+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:21 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 10494 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:45:21.530+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:21 INFO DAGScheduler: ResultStage 8 (save at BigQueryWriteHelper.java:105) finished in 10.985 s
[2023-01-31T07:45:21.530+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:21 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:45:21.541+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:21 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2023-01-31T07:45:21.552+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2023-01-31T07:45:21.561+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:21 INFO DAGScheduler: Job 8 finished: save at BigQueryWriteHelper.java:105, took 11.071062 s
[2023-01-31T07:45:21.568+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:21 INFO FileFormatWriter: Start to commit write Job 7a663e17-67fd-413e-9434-8a3413fc768a.
[2023-01-31T07:45:23.683+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:23 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675150857871-a135a152-d5f5-4f83-9abf-62782fc2a79f/_temporary/0/task_202301310745105451738571484448784_0008_m_000000/' directory.
[2023-01-31T07:45:24.410+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:24 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675150857871-a135a152-d5f5-4f83-9abf-62782fc2a79f/' directory.
[2023-01-31T07:45:25.952+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:25 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 81d5fcd0285b:33893 in memory (size: 74.3 KiB, free: 434.4 MiB)
[2023-01-31T07:45:26.277+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:26 INFO FileFormatWriter: Write Job 7a663e17-67fd-413e-9434-8a3413fc768a committed. Elapsed time: 4214 ms.
[2023-01-31T07:45:26.417+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:26 INFO FileFormatWriter: Finished processing stats for write job 7a663e17-67fd-413e-9434-8a3413fc768a.
[2023-01-31T07:45:29.372+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:29 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=entsoe_analytics_1009, projectId=rafzul-analytics-1009, tableId=TEST_total_generation_staging}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=null, ignoreUnknownValue=null, sourceUris=[gs://entsoe_temp_1009/.spark-bigquery-local-1675150857871-a135a152-d5f5-4f83-9abf-62782fc2a79f/part-00000-e691cb9a-a657-4b06-b1c0-fcfba8bad65e-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=true, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=rafzul-analytics-1009, job=0a214997-4c0e-4c47-8e3b-661e4f12944b, location=US}
[2023-01-31T07:45:35.959+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:35 INFO BigQueryClient: Done loading to rafzul-analytics-1009.entsoe_analytics_1009.TEST_total_generation_staging. jobId: JobId{project=rafzul-analytics-1009, job=0a214997-4c0e-4c47-8e3b-661e4f12944b, location=US}
[2023-01-31T07:45:36.660+0000] {spark_submit.py:495} INFO - MANTEP!!!!
[2023-01-31T07:45:37.101+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:37 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-31T07:45:37.143+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:37 INFO SparkUI: Stopped Spark web UI at http://81d5fcd0285b:4044
[2023-01-31T07:45:37.207+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:37 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-31T07:45:37.260+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:37 INFO MemoryStore: MemoryStore cleared
[2023-01-31T07:45:37.262+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:37 INFO BlockManager: BlockManager stopped
[2023-01-31T07:45:37.274+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:37 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-31T07:45:37.285+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:37 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-31T07:45:37.309+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:37 INFO SparkContext: Successfully stopped SparkContext
[2023-01-31T07:45:37.311+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:37 INFO ShutdownHookManager: Shutdown hook called
[2023-01-31T07:45:37.313+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-d05c22e8-051a-4d2d-8b71-cf883b33da59/pyspark-0e9f5cf0-4f6d-4f79-893a-cff9aec5e2a2
[2023-01-31T07:45:37.329+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-d05c22e8-051a-4d2d-8b71-cf883b33da59
[2023-01-31T07:45:37.346+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-a1caaac2-1489-4dce-8353-ef8364971301
[2023-01-31T07:45:37.839+0000] {taskinstance.py:1322} INFO - Marking task as SUCCESS. dag_id=entsoe-energydata-backfill, task_id=stage_total_generation, execution_date=20210101T050000, start_date=20230131T073902, end_date=20230131T074537
[2023-01-31T07:45:37.999+0000] {local_task_job.py:159} INFO - Task exited with return code 0
[2023-01-31T07:45:38.147+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T08:21:35.321+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T05:00:00+00:00 [queued]>
[2023-01-31T08:21:35.354+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T05:00:00+00:00 [queued]>
[2023-01-31T08:21:35.355+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T08:21:35.355+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T08:21:35.356+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T08:21:35.399+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 05:00:00+00:00
[2023-01-31T08:21:35.424+0000] {standard_task_runner.py:55} INFO - Started process 2077 to run task
[2023-01-31T08:21:35.443+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T05:00:00+00:00', '--job-id', '925', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpje24tkx0']
[2023-01-31T08:21:35.450+0000] {standard_task_runner.py:83} INFO - Job 925: Subtask stage_total_generation
[2023-01-31T08:21:35.751+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T05:00:00+00:00 [running]> on host 81d5fcd0285b
[2023-01-31T08:21:35.998+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T05:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T05:00:00+00:00
[2023-01-31T08:21:36.020+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T08:21:36.025+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010500 202101010600 DE_TENNET
[2023-01-31T08:21:58.390+0000] {spark_submit.py:495} INFO - {{ start_date }}
[2023-01-31T08:21:58.390+0000] {spark_submit.py:495} INFO - mulai periode: {{ start_date }}
[2023-01-31T08:21:58.391+0000] {spark_submit.py:495} INFO - selesai periode: {{ end_date }}
[2023-01-31T08:21:58.854+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:58 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T08:21:59.257+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T08:21:59.973+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:59 INFO ResourceUtils: ==============================================================
[2023-01-31T08:21:59.979+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:59 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T08:21:59.982+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:59 INFO ResourceUtils: ==============================================================
[2023-01-31T08:21:59.987+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:59 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T08:22:00.118+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:00 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T08:22:00.138+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:00 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T08:22:00.141+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:00 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T08:22:00.751+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:00 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T08:22:00.752+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:00 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T08:22:00.761+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:00 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T08:22:00.767+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:00 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T08:22:00.771+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:00 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T08:22:03.188+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:03 INFO Utils: Successfully started service 'sparkDriver' on port 39461.
[2023-01-31T08:22:03.546+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:03 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T08:22:03.762+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:03 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T08:22:03.887+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:03 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T08:22:03.894+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:03 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T08:22:03.936+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:03 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T08:22:04.062+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:04 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-cdc37611-b1ff-4962-9f65-f16a25ce055f
[2023-01-31T08:22:04.165+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:04 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T08:22:04.302+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:04 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T08:22:07.270+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:07 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T08:22:07.279+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:07 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2023-01-31T08:22:07.288+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:07 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[2023-01-31T08:22:07.325+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:07 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[2023-01-31T08:22:07.413+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:07 INFO Utils: Successfully started service 'SparkUI' on port 4044.
[2023-01-31T08:22:07.698+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:07 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://81d5fcd0285b:39461/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675153318796
[2023-01-31T08:22:07.699+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:07 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://81d5fcd0285b:39461/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675153318796
[2023-01-31T08:22:08.361+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:08 INFO Executor: Starting executor ID driver on host 81d5fcd0285b
[2023-01-31T08:22:08.425+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:08 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T08:22:08.573+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:08 INFO Executor: Fetching spark://81d5fcd0285b:39461/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675153318796
[2023-01-31T08:22:09.231+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:09 INFO TransportClientFactory: Successfully created connection to 81d5fcd0285b/172.19.0.5:39461 after 369 ms (0 ms spent in bootstraps)
[2023-01-31T08:22:09.301+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:09 INFO Utils: Fetching spark://81d5fcd0285b:39461/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-4ef7e6b2-7107-489e-8064-ab7e3e15fba4/userFiles-e403c695-7469-4586-9443-edb27bb84d8c/fetchFileTemp15820119948750370236.tmp
[2023-01-31T08:22:10.764+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:10 INFO Executor: Adding file:/tmp/spark-4ef7e6b2-7107-489e-8064-ab7e3e15fba4/userFiles-e403c695-7469-4586-9443-edb27bb84d8c/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T08:22:10.765+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:10 INFO Executor: Fetching spark://81d5fcd0285b:39461/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675153318796
[2023-01-31T08:22:10.768+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:10 INFO Utils: Fetching spark://81d5fcd0285b:39461/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-4ef7e6b2-7107-489e-8064-ab7e3e15fba4/userFiles-e403c695-7469-4586-9443-edb27bb84d8c/fetchFileTemp15059128953638354008.tmp
[2023-01-31T08:22:12.393+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:12 INFO Executor: Adding file:/tmp/spark-4ef7e6b2-7107-489e-8064-ab7e3e15fba4/userFiles-e403c695-7469-4586-9443-edb27bb84d8c/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T08:22:12.499+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:12 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46657.
[2023-01-31T08:22:12.503+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:12 INFO NettyBlockTransferService: Server created on 81d5fcd0285b:46657
[2023-01-31T08:22:12.562+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:12 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T08:22:12.712+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:12 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 46657, None)
[2023-01-31T08:22:12.816+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:12 INFO BlockManagerMasterEndpoint: Registering block manager 81d5fcd0285b:46657 with 434.4 MiB RAM, BlockManagerId(driver, 81d5fcd0285b, 46657, None)
[2023-01-31T08:22:12.863+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:12 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 46657, None)
[2023-01-31T08:22:12.864+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:12 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 81d5fcd0285b, 46657, None)
[2023-01-31T08:22:16.373+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:16 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T08:22:16.428+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:16 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T08:22:30.045+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:30 INFO InMemoryFileIndex: It took 634 ms to list leaf files for 1 paths.
[2023-01-31T08:22:31.097+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:31 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T08:22:31.830+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:31 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T08:22:31.838+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:31 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 81d5fcd0285b:46657 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T08:22:31.848+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:31 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T08:22:34.924+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:34 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T08:22:35.007+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:35 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T08:22:35.172+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:35 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T08:22:35.386+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:35 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T08:22:35.401+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:35 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T08:22:35.416+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:35 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:22:35.446+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:35 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:22:35.514+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:35 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T08:22:35.818+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:35 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T08:22:35.836+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:35 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T08:22:35.843+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:35 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 81d5fcd0285b:46657 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T08:22:35.850+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:35 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:22:35.964+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:22:35.965+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:35 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T08:22:36.422+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:36 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T08:22:36.578+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:36 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T08:22:38.111+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:38 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010500__202101010600.json:0+9367
[2023-01-31T08:22:40.673+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:40 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3480 bytes result sent to driver
[2023-01-31T08:22:40.800+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:40 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 4540 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:22:40.822+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:40 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T08:22:40.841+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:40 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 5.185 s
[2023-01-31T08:22:40.877+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:40 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:22:40.880+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T08:22:40.895+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:40 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 5.719416 s
[2023-01-31T08:22:44.638+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:44 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 81d5fcd0285b:46657 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T08:22:44.778+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:44 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 81d5fcd0285b:46657 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T08:23:02.544+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:02 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:02.561+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:02 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T08:23:02.593+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:02 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:07.888+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:07 INFO CodeGenerator: Code generated in 3291.135399 ms
[2023-01-31T08:23:07.937+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:07 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T08:23:08.174+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:08 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T08:23:08.181+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:08 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 81d5fcd0285b:46657 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T08:23:08.186+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:08 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T08:23:08.310+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:08 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:10.434+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:10 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T08:23:10.480+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:10 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T08:23:10.480+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:10 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T08:23:10.480+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:10 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:10.481+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:10 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:10.481+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:10 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T08:23:10.641+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:10 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T08:23:10.690+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:10 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T08:23:10.693+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:10 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 81d5fcd0285b:46657 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T08:23:10.755+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:10 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:10.758+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:10.759+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:10 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T08:23:10.825+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:10 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:11.022+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:11 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T08:23:12.178+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:12 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T08:23:14.446+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:14 INFO CodeGenerator: Code generated in 2024.179651 ms
[2023-01-31T08:23:16.158+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:16 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1628 bytes result sent to driver
[2023-01-31T08:23:16.217+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:16 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 5399 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:16.217+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:16 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T08:23:16.222+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:16 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 5.709 s
[2023-01-31T08:23:16.223+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:16 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:16.224+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T08:23:16.237+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:16 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 5.815467 s
[2023-01-31T08:23:18.110+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:18 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:18.111+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:18 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T08:23:18.149+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:18 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:19.231+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:19 INFO CodeGenerator: Code generated in 577.977196 ms
[2023-01-31T08:23:19.409+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:19 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T08:23:19.792+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:19 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T08:23:19.868+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:19 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 81d5fcd0285b:46657 in memory (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T08:23:19.876+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:19 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 81d5fcd0285b:46657 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T08:23:19.882+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:19 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T08:23:19.892+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:19 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:20.301+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T08:23:20.309+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T08:23:20.310+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T08:23:20.310+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:20.310+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:20.326+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T08:23:20.369+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T08:23:20.413+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T08:23:20.426+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 81d5fcd0285b:46657 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T08:23:20.428+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:20.445+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:20.463+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T08:23:20.471+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:20.478+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T08:23:20.712+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T08:23:21.754+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1585 bytes result sent to driver
[2023-01-31T08:23:21.831+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1353 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:21.832+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 1.508 s
[2023-01-31T08:23:21.832+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:21.837+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T08:23:21.838+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T08:23:21.845+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 1.536029 s
[2023-01-31T08:23:24.694+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:24 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:24.699+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:24 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T08:23:24.707+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:24 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:25.948+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO CodeGenerator: Code generated in 905.388667 ms
[2023-01-31T08:23:26.061+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T08:23:26.422+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T08:23:26.425+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 81d5fcd0285b:46657 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T08:23:26.446+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T08:23:26.447+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:26.714+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 81d5fcd0285b:46657 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T08:23:26.883+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T08:23:26.888+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T08:23:26.889+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T08:23:26.889+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:26.890+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:26.891+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T08:23:26.931+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.7 MiB)
[2023-01-31T08:23:26.958+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.7 MiB)
[2023-01-31T08:23:26.958+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 81d5fcd0285b:46657 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T08:23:26.969+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:26.976+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:26.979+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T08:23:26.989+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:26.989+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T08:23:27.099+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T08:23:27.718+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-01-31T08:23:27.735+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 748 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:27.741+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 0.846 s
[2023-01-31T08:23:27.746+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:27.747+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T08:23:27.749+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T08:23:27.750+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 0.866411 s
[2023-01-31T08:23:28.242+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:28.243+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T08:23:28.244+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:28.548+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO CodeGenerator: Code generated in 252.794258 ms
[2023-01-31T08:23:28.706+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.5 MiB)
[2023-01-31T08:23:28.962+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T08:23:28.966+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 81d5fcd0285b:46657 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T08:23:29.024+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T08:23:29.055+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:29.560+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T08:23:29.564+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T08:23:29.565+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T08:23:29.565+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:29.566+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:29.577+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T08:23:29.602+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-31T08:23:29.652+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-01-31T08:23:29.654+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 81d5fcd0285b:46657 (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T08:23:29.655+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:29.663+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:29.667+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T08:23:29.670+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:29.671+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T08:23:29.778+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T08:23:30.156+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:30 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1586 bytes result sent to driver
[2023-01-31T08:23:30.161+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:30 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 493 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:30.168+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:30 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T08:23:30.175+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:30 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 0.588 s
[2023-01-31T08:23:30.176+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:30 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:30.176+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T08:23:30.176+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:30 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 0.603351 s
[2023-01-31T08:23:34.106+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:34.108+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T08:23:34.109+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:34.792+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 81d5fcd0285b:46657 in memory (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T08:23:34.936+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 81d5fcd0285b:46657 in memory (size: 7.6 KiB, free: 434.3 MiB)
[2023-01-31T08:23:35.413+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:35 INFO CodeGenerator: Code generated in 767.707327 ms
[2023-01-31T08:23:35.519+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:35 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.3 MiB)
[2023-01-31T08:23:35.790+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:35 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T08:23:35.794+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:35 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 81d5fcd0285b:46657 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T08:23:35.797+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:35 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T08:23:35.834+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:35 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:36.330+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T08:23:36.332+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) with 1 output partitions
[2023-01-31T08:23:36.333+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201)
[2023-01-31T08:23:36.333+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:36.334+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:36.366+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201), which has no missing parents
[2023-01-31T08:23:36.376+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.2 MiB)
[2023-01-31T08:23:36.432+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.2 MiB)
[2023-01-31T08:23:36.434+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 81d5fcd0285b:46657 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T08:23:36.438+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:36.492+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:36.498+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T08:23:36.499+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:36.506+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T08:23:36.596+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T08:23:37.043+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:37 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2490 bytes result sent to driver
[2023-01-31T08:23:37.045+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:37 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 549 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:37.045+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:37 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T08:23:37.046+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:37 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) finished in 0.682 s
[2023-01-31T08:23:37.047+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:37 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:37.047+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T08:23:37.047+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:37 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201, took 0.717209 s
[2023-01-31T08:23:37.477+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:37 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:37.478+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:37 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T08:23:37.479+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:37 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:38.298+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO CodeGenerator: Code generated in 355.567121 ms
[2023-01-31T08:23:38.306+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 433.0 MiB)
[2023-01-31T08:23:38.421+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.0 MiB)
[2023-01-31T08:23:38.442+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 81d5fcd0285b:46657 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T08:23:38.445+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T08:23:38.468+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:39.062+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T08:23:39.065+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T08:23:39.065+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T08:23:39.066+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:39.066+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:39.070+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T08:23:39.098+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 432.9 MiB)
[2023-01-31T08:23:39.207+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 432.9 MiB)
[2023-01-31T08:23:39.210+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 81d5fcd0285b:46657 (size: 12.7 KiB, free: 434.2 MiB)
[2023-01-31T08:23:39.327+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:39.334+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:39.336+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T08:23:39.339+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:39.383+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T08:23:39.460+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T08:23:39.688+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 81d5fcd0285b:46657 in memory (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T08:23:40.026+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:40 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1827 bytes result sent to driver
[2023-01-31T08:23:40.039+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:40 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 694 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:40.040+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:40 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T08:23:40.040+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:40 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 0.939 s
[2023-01-31T08:23:40.041+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:40 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:40.041+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T08:23:40.041+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:40 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 0.972277 s
[2023-01-31T08:23:40.626+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:40 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:40.754+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:40 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T08:23:40.755+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:40 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:42.246+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:42 INFO CodeGenerator: Code generated in 708.834774 ms
[2023-01-31T08:23:42.254+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:42 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.8 MiB)
[2023-01-31T08:23:42.277+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:42 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.7 MiB)
[2023-01-31T08:23:42.279+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:42 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 81d5fcd0285b:46657 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T08:23:42.282+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:42 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T08:23:42.291+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:42 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203671 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:42.572+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:42 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T08:23:42.580+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:42 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T08:23:42.582+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:42 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T08:23:42.582+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:42 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:42.582+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:42 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:42.593+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:42 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T08:23:42.594+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:42 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.7 MiB)
[2023-01-31T08:23:42.594+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:42 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 432.7 MiB)
[2023-01-31T08:23:42.594+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:42 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 81d5fcd0285b:46657 (size: 12.7 KiB, free: 434.1 MiB)
[2023-01-31T08:23:42.625+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:42 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:42.626+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:42.626+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:42 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T08:23:42.626+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:42 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:42.627+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:42 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T08:23:42.660+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:42 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010500__202101010600.json, range: 0-9367, partition values: [empty row]
[2023-01-31T08:23:43.336+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:43 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1729 bytes result sent to driver
[2023-01-31T08:23:43.388+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:43 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 762 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:43.410+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:43 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T08:23:43.411+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:43 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 0.805 s
[2023-01-31T08:23:43.411+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:43 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:43.412+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T08:23:43.412+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:43 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 0.828203 s
[2023-01-31T08:23:47.041+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:47 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 81d5fcd0285b:46657 in memory (size: 12.7 KiB, free: 434.2 MiB)
[2023-01-31T08:23:47.210+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:47 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 81d5fcd0285b:46657 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T08:23:47.300+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:47 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 81d5fcd0285b:46657 in memory (size: 12.7 KiB, free: 434.2 MiB)
[2023-01-31T08:23:47.452+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:47 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 81d5fcd0285b:46657 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T08:23:47.697+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:47 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 81d5fcd0285b:46657 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T08:23:47.867+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:47 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 81d5fcd0285b:46657 in memory (size: 34.5 KiB, free: 434.3 MiB)
