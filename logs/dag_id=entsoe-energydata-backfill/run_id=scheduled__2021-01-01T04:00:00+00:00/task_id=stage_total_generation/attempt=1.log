[2023-01-31T05:11:00.470+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T04:00:00+00:00 [queued]>
[2023-01-31T05:11:00.507+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T04:00:00+00:00 [queued]>
[2023-01-31T05:11:00.508+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T05:11:00.508+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T05:11:00.509+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T05:11:00.604+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 04:00:00+00:00
[2023-01-31T05:11:00.649+0000] {standard_task_runner.py:55} INFO - Started process 12261 to run task
[2023-01-31T05:11:00.686+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T04:00:00+00:00', '--job-id', '882', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpcj3cw9fq']
[2023-01-31T05:11:00.714+0000] {standard_task_runner.py:83} INFO - Job 882: Subtask stage_total_generation
[2023-01-31T05:11:01.381+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T04:00:00+00:00 [running]> on host 81d5fcd0285b
[2023-01-31T05:11:01.887+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T04:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T04:00:00+00:00
[2023-01-31T05:11:01.936+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T05:11:01.943+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010400 202101010500 DE_TENNET
[2023-01-31T05:11:33.943+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:33 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T05:11:34.559+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T05:11:35.488+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:35 INFO ResourceUtils: ==============================================================
[2023-01-31T05:11:35.496+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:35 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T05:11:35.502+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:35 INFO ResourceUtils: ==============================================================
[2023-01-31T05:11:35.509+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:35 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T05:11:35.861+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:35 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T05:11:35.976+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:35 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T05:11:36.008+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:36 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T05:11:36.841+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:36 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T05:11:36.857+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:36 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T05:11:36.874+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:36 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T05:11:36.887+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:36 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T05:11:36.901+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:36 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T05:11:40.470+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:40 INFO Utils: Successfully started service 'sparkDriver' on port 38201.
[2023-01-31T05:11:40.890+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:40 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T05:11:41.218+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:41 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T05:11:41.751+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:41 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T05:11:41.766+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:41 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T05:11:41.814+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:41 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T05:11:41.984+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:41 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-cc45ca08-2bcc-4a1e-a58a-0662724c5539
[2023-01-31T05:11:42.164+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:42 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T05:11:42.371+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:42 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T05:11:45.313+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:45 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T05:11:45.426+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:45 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2023-01-31T05:11:45.955+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:45 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://81d5fcd0285b:38201/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675141893858
[2023-01-31T05:11:45.962+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:45 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://81d5fcd0285b:38201/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675141893858
[2023-01-31T05:11:46.572+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:46 INFO Executor: Starting executor ID driver on host 81d5fcd0285b
[2023-01-31T05:11:46.676+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:46 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T05:11:46.889+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:46 INFO Executor: Fetching spark://81d5fcd0285b:38201/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675141893858
[2023-01-31T05:11:47.390+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:47 INFO TransportClientFactory: Successfully created connection to 81d5fcd0285b/172.19.0.9:38201 after 267 ms (0 ms spent in bootstraps)
[2023-01-31T05:11:47.451+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:47 INFO Utils: Fetching spark://81d5fcd0285b:38201/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-2e902125-9b42-4774-ad20-edb52d9066a4/userFiles-88b57cc0-930e-4d2f-824b-4d63fbe50569/fetchFileTemp14800369511174346054.tmp
[2023-01-31T05:11:50.256+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:50 INFO Executor: Adding file:/tmp/spark-2e902125-9b42-4774-ad20-edb52d9066a4/userFiles-88b57cc0-930e-4d2f-824b-4d63fbe50569/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T05:11:50.263+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:50 INFO Executor: Fetching spark://81d5fcd0285b:38201/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675141893858
[2023-01-31T05:11:50.271+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:50 INFO Utils: Fetching spark://81d5fcd0285b:38201/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-2e902125-9b42-4774-ad20-edb52d9066a4/userFiles-88b57cc0-930e-4d2f-824b-4d63fbe50569/fetchFileTemp14765273635530257972.tmp
[2023-01-31T05:11:52.118+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:52 INFO Executor: Adding file:/tmp/spark-2e902125-9b42-4774-ad20-edb52d9066a4/userFiles-88b57cc0-930e-4d2f-824b-4d63fbe50569/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T05:11:52.200+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:52 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46237.
[2023-01-31T05:11:52.201+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:52 INFO NettyBlockTransferService: Server created on 81d5fcd0285b:46237
[2023-01-31T05:11:52.215+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:52 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T05:11:52.335+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:52 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 46237, None)
[2023-01-31T05:11:52.372+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:52 INFO BlockManagerMasterEndpoint: Registering block manager 81d5fcd0285b:46237 with 434.4 MiB RAM, BlockManagerId(driver, 81d5fcd0285b, 46237, None)
[2023-01-31T05:11:52.426+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:52 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 46237, None)
[2023-01-31T05:11:52.431+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:52 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 81d5fcd0285b, 46237, None)
[2023-01-31T05:12:00.202+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:00 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T05:12:00.241+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:00 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T05:12:24.020+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:24 INFO InMemoryFileIndex: It took 559 ms to list leaf files for 1 paths.
[2023-01-31T05:12:26.412+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:26 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T05:12:27.369+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:27 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T05:12:27.383+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:27 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 81d5fcd0285b:46237 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T05:12:27.406+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:27 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T05:12:32.359+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:32 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T05:12:33.577+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:33 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T05:12:33.976+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:33 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T05:12:34.245+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:34 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T05:12:34.245+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:34 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T05:12:34.260+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:34 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:12:34.270+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:34 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:12:34.310+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:34 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T05:12:34.906+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:34 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T05:12:34.954+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:34 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T05:12:34.959+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:34 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 81d5fcd0285b:46237 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T05:12:34.975+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:34 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:12:35.150+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:12:35.168+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:35 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T05:12:35.797+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:35 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T05:12:35.875+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:35 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T05:12:37.353+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:37 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010400__202101010500.json:0+9364
[2023-01-31T05:12:40.548+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:40 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3437 bytes result sent to driver
[2023-01-31T05:12:40.679+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:40 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 4987 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:12:40.701+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:40 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T05:12:40.750+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:40 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 6.271 s
[2023-01-31T05:12:40.796+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:40 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:12:40.801+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T05:12:40.821+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:40 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 6.843706 s
[2023-01-31T05:12:41.602+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:41 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 81d5fcd0285b:46237 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T05:12:41.688+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:41 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 81d5fcd0285b:46237 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T05:13:11.534+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:11 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:11.568+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:11 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:13:11.574+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:11 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:13.962+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:13 INFO CodeGenerator: Code generated in 1380.83111 ms
[2023-01-31T05:13:14.058+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T05:13:14.161+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T05:13:14.168+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 81d5fcd0285b:46237 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T05:13:14.181+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T05:13:14.377+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:14.906+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T05:13:14.907+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T05:13:14.908+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T05:13:14.908+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:14.908+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:14.912+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T05:13:14.966+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T05:13:14.997+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T05:13:15.006+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 81d5fcd0285b:46237 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T05:13:15.010+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:15.019+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:15.027+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T05:13:15.053+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:15.061+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T05:13:15.618+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T05:13:16.806+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:16 INFO CodeGenerator: Code generated in 1024.306847 ms
[2023-01-31T05:13:17.774+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1671 bytes result sent to driver
[2023-01-31T05:13:17.798+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2758 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:17.803+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 2.868 s
[2023-01-31T05:13:17.805+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:17.817+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T05:13:17.818+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T05:13:17.822+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 2.913919 s
[2023-01-31T05:13:19.142+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:19.142+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:13:19.143+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:19.324+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO CodeGenerator: Code generated in 119.405437 ms
[2023-01-31T05:13:19.351+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T05:13:19.519+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T05:13:19.536+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 81d5fcd0285b:46237 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:13:19.543+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T05:13:19.570+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:19.684+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T05:13:19.688+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T05:13:19.691+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T05:13:19.692+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:19.693+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:19.715+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T05:13:19.849+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T05:13:19.888+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T05:13:19.897+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 81d5fcd0285b:46237 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T05:13:19.901+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:19.916+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:19.917+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T05:13:19.928+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:19.929+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T05:13:20.049+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T05:13:20.466+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1585 bytes result sent to driver
[2023-01-31T05:13:20.476+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 551 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:20.477+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T05:13:20.483+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 0.728 s
[2023-01-31T05:13:20.483+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:20.484+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T05:13:20.484+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 0.796933 s
[2023-01-31T05:13:20.904+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:20.905+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T05:13:20.905+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:21.375+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO CodeGenerator: Code generated in 282.597068 ms
[2023-01-31T05:13:21.410+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T05:13:21.509+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T05:13:21.512+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 81d5fcd0285b:46237 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:13:21.516+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T05:13:21.530+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:21.670+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T05:13:21.674+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T05:13:21.674+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T05:13:21.676+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:21.677+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:21.687+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T05:13:21.707+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.6 MiB)
[2023-01-31T05:13:21.741+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.6 MiB)
[2023-01-31T05:13:21.748+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 81d5fcd0285b:46237 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T05:13:21.753+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:21.754+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:21.754+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T05:13:21.763+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:21.763+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T05:13:21.854+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T05:13:22.246+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1755 bytes result sent to driver
[2023-01-31T05:13:22.266+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 503 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:22.270+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 0.582 s
[2023-01-31T05:13:22.271+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:22.272+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T05:13:22.274+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T05:13:22.280+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 0.605478 s
[2023-01-31T05:13:22.624+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:22.625+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:13:22.633+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:23.057+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO CodeGenerator: Code generated in 301.081464 ms
[2023-01-31T05:13:23.121+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.4 MiB)
[2023-01-31T05:13:23.472+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T05:13:23.478+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 81d5fcd0285b:46237 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:13:23.489+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T05:13:23.518+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:23.662+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T05:13:23.667+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T05:13:23.668+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T05:13:23.668+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:23.669+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:23.670+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T05:13:23.709+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-31T05:13:23.745+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-01-31T05:13:23.756+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 81d5fcd0285b:46237 (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T05:13:23.762+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:23.928+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:23.985+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T05:13:23.986+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:23.986+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T05:13:24.195+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T05:13:24.585+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1586 bytes result sent to driver
[2023-01-31T05:13:24.588+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 652 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:24.589+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T05:13:24.597+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 0.913 s
[2023-01-31T05:13:24.597+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:24.598+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T05:13:24.598+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 0.927383 s
[2023-01-31T05:13:26.096+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 81d5fcd0285b:46237 in memory (size: 7.7 KiB, free: 434.2 MiB)
[2023-01-31T05:13:26.163+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 81d5fcd0285b:46237 in memory (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T05:13:26.284+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 81d5fcd0285b:46237 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T05:13:26.441+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 81d5fcd0285b:46237 in memory (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T05:13:27.028+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:27 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:27.031+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:27 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T05:13:27.032+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:27 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:28.668+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO CodeGenerator: Code generated in 843.954319 ms
[2023-01-31T05:13:28.769+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.3 MiB)
[2023-01-31T05:13:28.974+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T05:13:28.981+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 81d5fcd0285b:46237 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:13:28.993+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T05:13:29.025+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:29.451+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T05:13:29.456+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) with 1 output partitions
[2023-01-31T05:13:29.457+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201)
[2023-01-31T05:13:29.457+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:29.457+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:29.464+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201), which has no missing parents
[2023-01-31T05:13:29.558+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.2 MiB)
[2023-01-31T05:13:29.562+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.2 MiB)
[2023-01-31T05:13:29.567+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 81d5fcd0285b:46237 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T05:13:29.569+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:29.570+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:29.571+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T05:13:29.574+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:29.590+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T05:13:29.690+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T05:13:30.348+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2537 bytes result sent to driver
[2023-01-31T05:13:30.374+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 801 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:30.375+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T05:13:30.379+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) finished in 0.914 s
[2023-01-31T05:13:30.379+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:30.380+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T05:13:30.380+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201, took 0.922521 s
[2023-01-31T05:13:31.018+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:31 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:31.041+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:31 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T05:13:31.073+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:31 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:33.663+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO CodeGenerator: Code generated in 1877.445932 ms
[2023-01-31T05:13:33.736+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 433.0 MiB)
[2023-01-31T05:13:33.807+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.0 MiB)
[2023-01-31T05:13:33.812+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 81d5fcd0285b:46237 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:13:33.817+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T05:13:33.825+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:33.895+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T05:13:33.899+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T05:13:33.899+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T05:13:33.900+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:33.900+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:33.910+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T05:13:33.940+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 432.9 MiB)
[2023-01-31T05:13:33.973+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.9 MiB)
[2023-01-31T05:13:33.980+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 81d5fcd0285b:46237 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T05:13:33.993+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:33 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:34.016+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:34.048+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:34 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T05:13:34.053+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:34 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:34.054+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:34 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T05:13:34.324+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:34 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T05:13:34.592+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:34 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1827 bytes result sent to driver
[2023-01-31T05:13:34.596+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:34 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 544 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:34.600+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:34 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 0.689 s
[2023-01-31T05:13:34.603+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:34 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T05:13:34.603+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:34 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:34.603+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T05:13:34.609+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:34 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 0.710455 s
[2023-01-31T05:13:35.398+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:35 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:35.421+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:35 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T05:13:35.422+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:35 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:36.482+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO CodeGenerator: Code generated in 670.81282 ms
[2023-01-31T05:13:36.532+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.7 MiB)
[2023-01-31T05:13:36.716+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.7 MiB)
[2023-01-31T05:13:36.753+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 81d5fcd0285b:46237 (size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T05:13:36.755+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T05:13:36.773+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:36 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:37.045+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:37 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T05:13:37.055+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:37 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T05:13:37.056+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:37 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T05:13:37.056+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:37 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:37.057+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:37 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:37.070+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:37 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T05:13:37.131+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:37 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.7 MiB)
[2023-01-31T05:13:37.231+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:37 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.6 MiB)
[2023-01-31T05:13:37.260+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:37 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 81d5fcd0285b:46237 (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T05:13:37.282+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:37 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:37.314+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:37.315+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:37 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T05:13:37.319+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:37 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:37.332+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:37 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T05:13:37.426+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:37 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T05:13:37.725+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:37 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1729 bytes result sent to driver
[2023-01-31T05:13:37.736+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:37 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 416 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:37.739+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:37 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 0.668 s
[2023-01-31T05:13:37.740+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:37 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:37.744+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:37 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T05:13:37.749+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T05:13:37.751+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:37 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 0.703634 s
[2023-01-31T05:13:40.594+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:40 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 81d5fcd0285b:46237 in memory (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T05:13:40.785+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:40 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 81d5fcd0285b:46237 in memory (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T05:13:40.876+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:40 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 81d5fcd0285b:46237 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:13:41.022+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:41 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 81d5fcd0285b:46237 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:13:41.093+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:41 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 81d5fcd0285b:46237 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:13:41.167+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:41 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 81d5fcd0285b:46237 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:13:41.287+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:41 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 81d5fcd0285b:46237 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:13:41.373+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:41 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 81d5fcd0285b:46237 in memory (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T05:13:41.419+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:41 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 81d5fcd0285b:46237 in memory (size: 12.6 KiB, free: 434.4 MiB)
[2023-01-31T05:14:05.140+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:14:05.324+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:14:05.325+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:14:05.331+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:14:05.332+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:14:05.333+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:14:05.342+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:14:06.338+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-31T05:14:06.339+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO DAGScheduler: Got job 8 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T05:14:06.339+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO DAGScheduler: Final stage: ResultStage 8 (save at BigQueryWriteHelper.java:105)
[2023-01-31T05:14:06.339+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:14:06.340+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:14:06.341+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO DAGScheduler: Submitting ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T05:14:06.618+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 209.9 KiB, free 434.0 MiB)
[2023-01-31T05:14:06.637+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 74.3 KiB, free 433.9 MiB)
[2023-01-31T05:14:06.639+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 81d5fcd0285b:46237 (size: 74.3 KiB, free: 434.3 MiB)
[2023-01-31T05:14:06.645+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:14:06.654+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:14:06.654+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2023-01-31T05:14:06.668+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7484 bytes) taskResourceAssignments Map()
[2023-01-31T05:14:06.672+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2023-01-31T05:14:06.949+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:14:06.950+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:14:06.950+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:14:06.951+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:14:06.951+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:14:06.960+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:14:07.037+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:07 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T05:14:07.127+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:07 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T05:14:07.924+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:07 INFO ParquetOutputFormat: Parquet block size to 134217728
[2023-01-31T05:14:07.925+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:07 INFO ParquetOutputFormat: Validation is off
[2023-01-31T05:14:07.926+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:07 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-01-31T05:14:07.926+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:07 INFO ParquetOutputFormat: Parquet properties are:
[2023-01-31T05:14:07.926+0000] {spark_submit.py:495} INFO - Parquet page size to 1048576
[2023-01-31T05:14:07.927+0000] {spark_submit.py:495} INFO - Parquet dictionary page size to 1048576
[2023-01-31T05:14:07.927+0000] {spark_submit.py:495} INFO - Dictionary is true
[2023-01-31T05:14:07.928+0000] {spark_submit.py:495} INFO - Writer version is: PARQUET_1_0
[2023-01-31T05:14:07.928+0000] {spark_submit.py:495} INFO - Page size checking is: estimated
[2023-01-31T05:14:07.928+0000] {spark_submit.py:495} INFO - Min row count for page size check is: 100
[2023-01-31T05:14:07.944+0000] {spark_submit.py:495} INFO - Max row count for page size check is: 10000
[2023-01-31T05:14:07.945+0000] {spark_submit.py:495} INFO - Truncate length for column indexes is: 64
[2023-01-31T05:14:07.945+0000] {spark_submit.py:495} INFO - Truncate length for statistics min/max  is: 2147483647
[2023-01-31T05:14:07.946+0000] {spark_submit.py:495} INFO - Bloom filter enabled: false
[2023-01-31T05:14:07.946+0000] {spark_submit.py:495} INFO - Max Bloom filter size for a column is 1048576
[2023-01-31T05:14:07.946+0000] {spark_submit.py:495} INFO - Bloom filter expected number of distinct values are: null
[2023-01-31T05:14:07.947+0000] {spark_submit.py:495} INFO - Page row count limit to 20000
[2023-01-31T05:14:07.947+0000] {spark_submit.py:495} INFO - Writing page checksums is: on
[2023-01-31T05:14:08.323+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:08 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-01-31T05:14:08.324+0000] {spark_submit.py:495} INFO - {
[2023-01-31T05:14:08.324+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-31T05:14:08.325+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-31T05:14:08.325+0000] {spark_submit.py:495} INFO - "name" : "measured_at",
[2023-01-31T05:14:08.326+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T05:14:08.326+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T05:14:08.327+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:08.327+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:08.328+0000] {spark_submit.py:495} INFO - "name" : "created_at",
[2023-01-31T05:14:08.328+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T05:14:08.371+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T05:14:08.372+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:08.373+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:08.373+0000] {spark_submit.py:495} INFO - "name" : "Biomass - Actual Aggregated",
[2023-01-31T05:14:08.374+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:08.415+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:08.415+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:08.416+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:08.483+0000] {spark_submit.py:495} INFO - "name" : "Fossil Brown coal/Lignite - Actual Aggregated",
[2023-01-31T05:14:08.483+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:08.484+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:08.484+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:08.492+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:08.493+0000] {spark_submit.py:495} INFO - "name" : "Fossil Gas - Actual Aggregated",
[2023-01-31T05:14:08.553+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:08.554+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:08.554+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:08.554+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:08.555+0000] {spark_submit.py:495} INFO - "name" : "Fossil Hard coal - Actual Aggregated",
[2023-01-31T05:14:08.555+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:08.556+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:08.556+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:08.557+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:08.557+0000] {spark_submit.py:495} INFO - "name" : "Fossil Oil - Actual Aggregated",
[2023-01-31T05:14:08.557+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:08.558+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:08.558+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:08.559+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:08.560+0000] {spark_submit.py:495} INFO - "name" : "Geothermal - Actual Aggregated",
[2023-01-31T05:14:08.560+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:08.561+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:08.561+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:08.563+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:08.563+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Aggregated",
[2023-01-31T05:14:08.564+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:08.565+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:08.565+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:08.565+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:08.566+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Consumption",
[2023-01-31T05:14:08.567+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:08.567+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:08.567+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:08.568+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:08.568+0000] {spark_submit.py:495} INFO - "name" : "Hydro Run-of-river and poundage - Actual Aggregated",
[2023-01-31T05:14:08.569+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:08.569+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:08.570+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:08.570+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:08.570+0000] {spark_submit.py:495} INFO - "name" : "Hydro Water Reservoir - Actual Aggregated",
[2023-01-31T05:14:08.571+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:08.571+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:08.572+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:08.572+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:08.621+0000] {spark_submit.py:495} INFO - "name" : "Nuclear - Actual Aggregated",
[2023-01-31T05:14:08.622+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:08.622+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:08.623+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:08.623+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:08.623+0000] {spark_submit.py:495} INFO - "name" : "Other - Actual Aggregated",
[2023-01-31T05:14:08.624+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:08.624+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:08.676+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:08.676+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:08.677+0000] {spark_submit.py:495} INFO - "name" : "Other renewable - Actual Aggregated",
[2023-01-31T05:14:08.677+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:08.701+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:08.714+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:08.722+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:08.723+0000] {spark_submit.py:495} INFO - "name" : "Solar - Actual Aggregated",
[2023-01-31T05:14:08.723+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:08.723+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:08.724+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:08.724+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:08.740+0000] {spark_submit.py:495} INFO - "name" : "Waste - Actual Aggregated",
[2023-01-31T05:14:08.741+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:08.741+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:08.741+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:08.742+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:08.742+0000] {spark_submit.py:495} INFO - "name" : "Wind Offshore - Actual Aggregated",
[2023-01-31T05:14:08.743+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:08.743+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:08.743+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:08.749+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:08.750+0000] {spark_submit.py:495} INFO - "name" : "Wind Onshore - Actual Aggregated",
[2023-01-31T05:14:08.750+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:08.751+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:08.751+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:08.752+0000] {spark_submit.py:495} INFO - } ]
[2023-01-31T05:14:08.752+0000] {spark_submit.py:495} INFO - }
[2023-01-31T05:14:08.752+0000] {spark_submit.py:495} INFO - and corresponding Parquet message type:
[2023-01-31T05:14:08.753+0000] {spark_submit.py:495} INFO - message spark_schema {
[2023-01-31T05:14:08.753+0000] {spark_submit.py:495} INFO - required int96 measured_at;
[2023-01-31T05:14:08.753+0000] {spark_submit.py:495} INFO - required int96 created_at;
[2023-01-31T05:14:08.754+0000] {spark_submit.py:495} INFO - optional double Biomass - Actual Aggregated;
[2023-01-31T05:14:08.754+0000] {spark_submit.py:495} INFO - optional double Fossil Brown coal/Lignite - Actual Aggregated;
[2023-01-31T05:14:08.755+0000] {spark_submit.py:495} INFO - optional double Fossil Gas - Actual Aggregated;
[2023-01-31T05:14:08.755+0000] {spark_submit.py:495} INFO - optional double Fossil Hard coal - Actual Aggregated;
[2023-01-31T05:14:08.755+0000] {spark_submit.py:495} INFO - optional double Fossil Oil - Actual Aggregated;
[2023-01-31T05:14:08.756+0000] {spark_submit.py:495} INFO - optional double Geothermal - Actual Aggregated;
[2023-01-31T05:14:08.756+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Aggregated;
[2023-01-31T05:14:08.756+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Consumption;
[2023-01-31T05:14:08.757+0000] {spark_submit.py:495} INFO - optional double Hydro Run-of-river and poundage - Actual Aggregated;
[2023-01-31T05:14:08.757+0000] {spark_submit.py:495} INFO - optional double Hydro Water Reservoir - Actual Aggregated;
[2023-01-31T05:14:08.757+0000] {spark_submit.py:495} INFO - optional double Nuclear - Actual Aggregated;
[2023-01-31T05:14:08.758+0000] {spark_submit.py:495} INFO - optional double Other - Actual Aggregated;
[2023-01-31T05:14:08.758+0000] {spark_submit.py:495} INFO - optional double Other renewable - Actual Aggregated;
[2023-01-31T05:14:08.758+0000] {spark_submit.py:495} INFO - optional double Solar - Actual Aggregated;
[2023-01-31T05:14:08.759+0000] {spark_submit.py:495} INFO - optional double Waste - Actual Aggregated;
[2023-01-31T05:14:08.759+0000] {spark_submit.py:495} INFO - optional double Wind Offshore - Actual Aggregated;
[2023-01-31T05:14:08.760+0000] {spark_submit.py:495} INFO - optional double Wind Onshore - Actual Aggregated;
[2023-01-31T05:14:08.760+0000] {spark_submit.py:495} INFO - }
[2023-01-31T05:14:08.760+0000] {spark_submit.py:495} INFO - 
[2023-01-31T05:14:08.761+0000] {spark_submit.py:495} INFO - 
[2023-01-31T05:14:09.548+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:09 INFO CodecPool: Got brand-new compressor [.snappy]
[2023-01-31T05:14:15.651+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:15 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675141906271-780309dd-bd4d-465b-a624-a0be266360aa/_temporary/0/_temporary/' directory.
[2023-01-31T05:14:15.652+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:15 INFO FileOutputCommitter: Saved output of task 'attempt_202301310514058164888540227616751_0008_m_000000_8' to gs://entsoe_temp_1009/.spark-bigquery-local-1675141906271-780309dd-bd4d-465b-a624-a0be266360aa/_temporary/0/task_202301310514058164888540227616751_0008_m_000000
[2023-01-31T05:14:15.654+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:15 INFO SparkHadoopMapRedUtil: attempt_202301310514058164888540227616751_0008_m_000000_8: Committed. Elapsed time: 1997 ms.
[2023-01-31T05:14:15.692+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:15 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2588 bytes result sent to driver
[2023-01-31T05:14:15.696+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:15 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 9039 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:14:15.697+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:15 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2023-01-31T05:14:15.702+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:15 INFO DAGScheduler: ResultStage 8 (save at BigQueryWriteHelper.java:105) finished in 9.358 s
[2023-01-31T05:14:15.703+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:15 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:14:15.703+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2023-01-31T05:14:15.778+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:15 INFO DAGScheduler: Job 8 finished: save at BigQueryWriteHelper.java:105, took 9.438383 s
[2023-01-31T05:14:15.790+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:15 INFO FileFormatWriter: Start to commit write Job 878d402f-afe2-4b6f-875e-90bcca357dd4.
[2023-01-31T05:14:16.168+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:16 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 81d5fcd0285b:46237 in memory (size: 74.3 KiB, free: 434.4 MiB)
[2023-01-31T05:14:16.843+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:16 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675141906271-780309dd-bd4d-465b-a624-a0be266360aa/_temporary/0/task_202301310514058164888540227616751_0008_m_000000/' directory.
[2023-01-31T05:14:17.191+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:17 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675141906271-780309dd-bd4d-465b-a624-a0be266360aa/' directory.
[2023-01-31T05:14:18.111+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:18 INFO FileFormatWriter: Write Job 878d402f-afe2-4b6f-875e-90bcca357dd4 committed. Elapsed time: 2311 ms.
[2023-01-31T05:14:18.124+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:18 INFO FileFormatWriter: Finished processing stats for write job 878d402f-afe2-4b6f-875e-90bcca357dd4.
[2023-01-31T05:14:20.053+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:20 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=entsoe_analytics_1009, projectId=rafzul-analytics-1009, tableId=TEST_total_generation_staging}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=null, ignoreUnknownValue=null, sourceUris=[gs://entsoe_temp_1009/.spark-bigquery-local-1675141906271-780309dd-bd4d-465b-a624-a0be266360aa/part-00000-dbb4e9bb-6c30-4afb-8813-12364390565e-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=true, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=rafzul-analytics-1009, job=8df84e98-13a3-40b2-819b-f2f21f3159a4, location=US}
[2023-01-31T05:14:25.371+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:25 INFO BigQueryClient: Done loading to rafzul-analytics-1009.entsoe_analytics_1009.TEST_total_generation_staging. jobId: JobId{project=rafzul-analytics-1009, job=8df84e98-13a3-40b2-819b-f2f21f3159a4, location=US}
[2023-01-31T05:14:26.234+0000] {spark_submit.py:495} INFO - MANTEP!!!!
[2023-01-31T05:14:26.436+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:26 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-31T05:14:26.455+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:26 INFO SparkUI: Stopped Spark web UI at http://81d5fcd0285b:4041
[2023-01-31T05:14:26.478+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:26 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-31T05:14:26.501+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:26 INFO MemoryStore: MemoryStore cleared
[2023-01-31T05:14:26.502+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:26 INFO BlockManager: BlockManager stopped
[2023-01-31T05:14:26.507+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:26 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-31T05:14:26.513+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:26 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-31T05:14:26.523+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:26 INFO SparkContext: Successfully stopped SparkContext
[2023-01-31T05:14:26.523+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:26 INFO ShutdownHookManager: Shutdown hook called
[2023-01-31T05:14:26.524+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:26 INFO ShutdownHookManager: Deleting directory /tmp/spark-245c7a30-c989-4099-92ed-f25a4ca60d05
[2023-01-31T05:14:26.532+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:26 INFO ShutdownHookManager: Deleting directory /tmp/spark-2e902125-9b42-4774-ad20-edb52d9066a4
[2023-01-31T05:14:26.540+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:26 INFO ShutdownHookManager: Deleting directory /tmp/spark-2e902125-9b42-4774-ad20-edb52d9066a4/pyspark-de5d957f-f57c-45a5-a60d-87bacd78c3ba
[2023-01-31T05:14:26.727+0000] {taskinstance.py:1322} INFO - Marking task as SUCCESS. dag_id=entsoe-energydata-backfill, task_id=stage_total_generation, execution_date=20210101T040000, start_date=20230131T051100, end_date=20230131T051426
[2023-01-31T05:14:26.856+0000] {local_task_job.py:159} INFO - Task exited with return code 0
[2023-01-31T05:14:26.891+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T05:30:40.948+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T04:00:00+00:00 [queued]>
[2023-01-31T05:30:41.218+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T04:00:00+00:00 [queued]>
[2023-01-31T05:30:41.219+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T05:30:41.219+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T05:30:41.220+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T05:30:41.302+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 04:00:00+00:00
[2023-01-31T05:30:41.342+0000] {standard_task_runner.py:55} INFO - Started process 15738 to run task
[2023-01-31T05:30:41.378+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T04:00:00+00:00', '--job-id', '894', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpxd47jwp8']
[2023-01-31T05:30:41.392+0000] {standard_task_runner.py:83} INFO - Job 894: Subtask stage_total_generation
[2023-01-31T05:30:42.062+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T04:00:00+00:00 [running]> on host 81d5fcd0285b
[2023-01-31T05:30:42.986+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T04:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T04:00:00+00:00
[2023-01-31T05:30:43.138+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T05:30:43.144+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010400 202101010500 DE_TENNET
[2023-01-31T05:31:50.030+0000] {spark_submit.py:495} INFO - 23/01/31 05:31:49 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T05:31:51.857+0000] {spark_submit.py:495} INFO - 23/01/31 05:31:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T05:31:54.880+0000] {spark_submit.py:495} INFO - 23/01/31 05:31:54 INFO ResourceUtils: ==============================================================
[2023-01-31T05:31:54.909+0000] {spark_submit.py:495} INFO - 23/01/31 05:31:54 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T05:31:54.917+0000] {spark_submit.py:495} INFO - 23/01/31 05:31:54 INFO ResourceUtils: ==============================================================
[2023-01-31T05:31:54.930+0000] {spark_submit.py:495} INFO - 23/01/31 05:31:54 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T05:31:55.504+0000] {spark_submit.py:495} INFO - 23/01/31 05:31:55 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T05:31:55.575+0000] {spark_submit.py:495} INFO - 23/01/31 05:31:55 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T05:31:55.597+0000] {spark_submit.py:495} INFO - 23/01/31 05:31:55 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T05:31:56.300+0000] {spark_submit.py:495} INFO - 23/01/31 05:31:56 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T05:31:56.312+0000] {spark_submit.py:495} INFO - 23/01/31 05:31:56 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T05:31:56.319+0000] {spark_submit.py:495} INFO - 23/01/31 05:31:56 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T05:31:56.323+0000] {spark_submit.py:495} INFO - 23/01/31 05:31:56 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T05:31:56.331+0000] {spark_submit.py:495} INFO - 23/01/31 05:31:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T05:32:04.883+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:04 INFO Utils: Successfully started service 'sparkDriver' on port 36347.
[2023-01-31T05:32:05.659+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:05 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T05:32:05.931+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:05 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T05:32:06.105+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:06 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T05:32:06.112+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:06 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T05:32:06.212+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:06 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T05:32:07.216+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:07 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-97863f17-b83d-42b9-9271-20502b2bc97f
[2023-01-31T05:32:07.761+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:07 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T05:32:09.182+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:09 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T05:32:12.411+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:12 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T05:32:12.582+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:12 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2023-01-31T05:32:14.306+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:14 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://81d5fcd0285b:36347/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675143109734
[2023-01-31T05:32:14.313+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:14 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://81d5fcd0285b:36347/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675143109734
[2023-01-31T05:32:15.516+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:15 INFO Executor: Starting executor ID driver on host 81d5fcd0285b
[2023-01-31T05:32:15.666+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:15 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T05:32:15.926+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:15 INFO Executor: Fetching spark://81d5fcd0285b:36347/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675143109734
[2023-01-31T05:32:18.686+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:18 INFO TransportClientFactory: Successfully created connection to 81d5fcd0285b/172.19.0.9:36347 after 1771 ms (0 ms spent in bootstraps)
[2023-01-31T05:32:18.939+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:18 INFO Utils: Fetching spark://81d5fcd0285b:36347/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-d03a91e0-49a1-4863-8c51-e4f8995ecc1e/userFiles-9bb70489-048a-4449-a38d-96b18ba28103/fetchFileTemp5590130344755327601.tmp
[2023-01-31T05:32:25.619+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:25 INFO Executor: Adding file:/tmp/spark-d03a91e0-49a1-4863-8c51-e4f8995ecc1e/userFiles-9bb70489-048a-4449-a38d-96b18ba28103/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T05:32:25.630+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:25 INFO Executor: Fetching spark://81d5fcd0285b:36347/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675143109734
[2023-01-31T05:32:25.654+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:25 INFO Utils: Fetching spark://81d5fcd0285b:36347/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-d03a91e0-49a1-4863-8c51-e4f8995ecc1e/userFiles-9bb70489-048a-4449-a38d-96b18ba28103/fetchFileTemp3238725152600480153.tmp
[2023-01-31T05:32:26.833+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:26 INFO Executor: Told to re-register on heartbeat
[2023-01-31T05:32:26.857+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:26 INFO BlockManager: BlockManager null re-registering with master
[2023-01-31T05:32:26.895+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:26 INFO BlockManagerMaster: Registering BlockManager null
[2023-01-31T05:32:27.185+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:27 ERROR Inbox: Ignoring error
[2023-01-31T05:32:27.186+0000] {spark_submit.py:495} INFO - java.lang.NullPointerException
[2023-01-31T05:32:27.187+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)
[2023-01-31T05:32:27.187+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)
[2023-01-31T05:32:27.188+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-01-31T05:32:27.188+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-01-31T05:32:27.208+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-01-31T05:32:27.213+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-01-31T05:32:27.214+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-01-31T05:32:27.214+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-01-31T05:32:27.215+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-01-31T05:32:27.215+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-31T05:32:27.216+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-31T05:32:27.262+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-31T05:32:27.298+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:27 WARN Executor: Issue communicating with driver in heartbeater
[2023-01-31T05:32:27.299+0000] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-01-31T05:32:27.300+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-01-31T05:32:27.301+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-01-31T05:32:27.301+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2023-01-31T05:32:27.302+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2023-01-31T05:32:27.302+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:79)
[2023-01-31T05:32:27.303+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:643)
[2023-01-31T05:32:27.303+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2023-01-31T05:32:27.304+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2023-01-31T05:32:27.304+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
[2023-01-31T05:32:27.318+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2023-01-31T05:32:27.318+0000] {spark_submit.py:495} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2023-01-31T05:32:27.319+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-01-31T05:32:27.320+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2023-01-31T05:32:27.321+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2023-01-31T05:32:27.321+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-31T05:32:27.322+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-31T05:32:27.323+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-31T05:32:27.324+0000] {spark_submit.py:495} INFO - Caused by: java.lang.NullPointerException
[2023-01-31T05:32:27.325+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)
[2023-01-31T05:32:27.325+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)
[2023-01-31T05:32:27.326+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-01-31T05:32:27.327+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-01-31T05:32:27.327+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-01-31T05:32:27.328+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-01-31T05:32:27.329+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-01-31T05:32:27.329+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-01-31T05:32:27.330+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-01-31T05:32:27.330+0000] {spark_submit.py:495} INFO - ... 3 more
[2023-01-31T05:32:30.901+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:30 INFO Executor: Adding file:/tmp/spark-d03a91e0-49a1-4863-8c51-e4f8995ecc1e/userFiles-9bb70489-048a-4449-a38d-96b18ba28103/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T05:32:31.107+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:31 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44651.
[2023-01-31T05:32:31.125+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:31 INFO NettyBlockTransferService: Server created on 81d5fcd0285b:44651
[2023-01-31T05:32:31.219+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:31 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T05:32:31.366+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:31 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 44651, None)
[2023-01-31T05:32:31.384+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:31 INFO BlockManagerMasterEndpoint: Registering block manager 81d5fcd0285b:44651 with 434.4 MiB RAM, BlockManagerId(driver, 81d5fcd0285b, 44651, None)
[2023-01-31T05:32:31.433+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:31 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 44651, None)
[2023-01-31T05:32:31.449+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:31 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 81d5fcd0285b, 44651, None)
[2023-01-31T05:32:36.098+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:36 INFO Executor: Told to re-register on heartbeat
[2023-01-31T05:32:36.100+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:36 INFO BlockManager: BlockManager BlockManagerId(driver, 81d5fcd0285b, 44651, None) re-registering with master
[2023-01-31T05:32:36.101+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:36 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 44651, None)
[2023-01-31T05:32:36.209+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:36 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 44651, None)
[2023-01-31T05:32:36.213+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:36 INFO BlockManager: Reporting 0 blocks to the master.
[2023-01-31T05:32:54.734+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:54 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T05:32:54.928+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:54 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T05:33:58.628+0000] {spark_submit.py:495} INFO - 23/01/31 05:33:58 INFO InMemoryFileIndex: It took 1682 ms to list leaf files for 1 paths.
[2023-01-31T05:34:05.177+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:05 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T05:34:09.088+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:09 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T05:34:09.157+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:09 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 81d5fcd0285b:44651 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T05:34:09.338+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:09 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T05:34:25.884+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:25 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T05:34:26.211+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:26 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T05:34:26.829+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:26 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T05:34:27.401+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:27 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T05:34:27.406+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:27 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T05:34:27.409+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:27 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:34:27.428+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:27 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:34:27.500+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:27 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T05:34:29.989+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:29 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T05:34:30.364+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:30 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T05:34:30.408+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:30 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 81d5fcd0285b:44651 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T05:34:30.426+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:30 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:34:31.550+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:34:31.620+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:31 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T05:34:35.980+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:35 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T05:34:36.817+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:36 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T05:34:42.343+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:42 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010400__202101010500.json:0+9364
[2023-01-31T05:34:50.812+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:50 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3480 bytes result sent to driver
[2023-01-31T05:34:51.279+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:51 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 17145 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:34:51.349+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:51 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T05:34:51.471+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:51 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 23.569 s
[2023-01-31T05:34:51.715+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:51 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:34:51.754+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T05:34:51.859+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:51 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 25.023190 s
[2023-01-31T05:35:33.761+0000] {spark_submit.py:495} INFO - 23/01/31 05:35:33 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 81d5fcd0285b:44651 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T05:35:33.899+0000] {spark_submit.py:495} INFO - 23/01/31 05:35:33 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 81d5fcd0285b:44651 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T05:36:04.777+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:04 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:36:04.786+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:04 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:36:04.861+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:04 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:36:17.409+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:17 INFO CodeGenerator: Code generated in 4685.41229 ms
[2023-01-31T05:36:17.622+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:17 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T05:36:18.240+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:18 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T05:36:18.250+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:18 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 81d5fcd0285b:44651 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T05:36:18.261+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:18 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T05:36:18.729+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:18 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:36:20.112+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:20 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T05:36:20.119+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:20 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T05:36:20.120+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:20 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T05:36:20.164+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:20 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:36:20.166+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:20 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:36:20.205+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:20 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T05:36:20.615+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:20 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T05:36:20.718+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:20 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T05:36:20.732+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:20 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 81d5fcd0285b:44651 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T05:36:20.750+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:20 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:36:20.777+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:36:20.786+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:20 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T05:36:21.250+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:21 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:36:21.418+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:21 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T05:36:23.350+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:23 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T05:36:28.325+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:28 INFO CodeGenerator: Code generated in 2968.571043 ms
[2023-01-31T05:36:31.326+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:31 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1671 bytes result sent to driver
[2023-01-31T05:36:31.396+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:31 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 10560 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:36:31.401+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:31 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T05:36:31.450+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:31 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 10.909 s
[2023-01-31T05:36:31.462+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:31 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:36:31.488+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T05:36:31.507+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:31 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 11.395389 s
[2023-01-31T05:36:33.562+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:33 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:36:33.563+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:33 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:36:33.573+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:33 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:36:34.318+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:34 INFO CodeGenerator: Code generated in 370.649378 ms
[2023-01-31T05:36:34.489+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:34 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T05:36:34.933+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:34 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T05:36:34.933+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:34 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 81d5fcd0285b:44651 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:36:34.934+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:34 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T05:36:35.002+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:34 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:36:35.296+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:35 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T05:36:35.302+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:35 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T05:36:35.310+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:35 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T05:36:35.331+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:35 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:36:35.338+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:35 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:36:35.358+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:35 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T05:36:35.541+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:35 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T05:36:35.632+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:35 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T05:36:35.669+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:35 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 81d5fcd0285b:44651 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T05:36:35.682+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:35 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:36:35.745+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:36:35.748+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:35 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T05:36:35.845+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:35 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:36:35.857+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:35 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T05:36:36.643+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:36 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T05:36:37.987+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:37 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1585 bytes result sent to driver
[2023-01-31T05:36:38.117+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:38 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 2258 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:36:38.125+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:38 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T05:36:38.149+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:38 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 2.715 s
[2023-01-31T05:36:38.150+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:38 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:36:38.150+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T05:36:38.151+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:38 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 2.851157 s
[2023-01-31T05:36:40.354+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:40 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 81d5fcd0285b:44651 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T05:36:40.803+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:40 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 81d5fcd0285b:44651 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T05:36:44.956+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:44 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:36:45.077+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:45 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T05:36:45.103+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:45 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:36:47.814+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:47 INFO CodeGenerator: Code generated in 782.315865 ms
[2023-01-31T05:36:47.836+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:47 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T05:36:47.894+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:47 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T05:36:47.899+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:47 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 81d5fcd0285b:44651 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:36:47.906+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:47 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T05:36:47.928+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:47 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:36:48.027+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:48 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T05:36:48.056+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:48 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T05:36:48.098+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:48 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T05:36:48.099+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:48 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:36:48.209+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:48 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:36:48.245+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:48 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T05:36:48.397+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:48 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.7 MiB)
[2023-01-31T05:36:48.589+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:48 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.7 MiB)
[2023-01-31T05:36:48.697+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:48 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 81d5fcd0285b:44651 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T05:36:48.719+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:48 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:36:48.762+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:36:48.774+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:48 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T05:36:48.804+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:48 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:36:48.854+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:48 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T05:36:49.240+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:49 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T05:36:49.712+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:49 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-01-31T05:36:49.758+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:49 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 959 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:36:49.764+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:49 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 1.508 s
[2023-01-31T05:36:49.768+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:49 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T05:36:49.775+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:49 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:36:49.778+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T05:36:49.786+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:49 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 1.758112 s
[2023-01-31T05:36:50.144+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:50 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:36:50.146+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:50 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:36:50.150+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:50 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:36:50.371+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:50 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 81d5fcd0285b:44651 in memory (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T05:36:50.646+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:50 INFO CodeGenerator: Code generated in 210.488631 ms
[2023-01-31T05:36:50.667+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:50 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.5 MiB)
[2023-01-31T05:36:50.731+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:50 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.5 MiB)
[2023-01-31T05:36:50.736+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:50 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 81d5fcd0285b:44651 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:36:50.741+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:50 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T05:36:50.752+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:50 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:36:50.877+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:50 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T05:36:50.880+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:50 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T05:36:50.884+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:50 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T05:36:50.887+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:50 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:36:50.895+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:50 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:36:50.909+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:50 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T05:36:51.007+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:51 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.5 MiB)
[2023-01-31T05:36:51.075+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:51 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.5 MiB)
[2023-01-31T05:36:51.104+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:51 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 81d5fcd0285b:44651 (size: 7.6 KiB, free: 434.3 MiB)
[2023-01-31T05:36:51.117+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:51 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:36:51.129+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:36:51.132+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:51 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T05:36:51.168+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:51 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:36:51.185+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:51 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T05:36:51.415+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:51 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T05:36:52.348+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:52 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1585 bytes result sent to driver
[2023-01-31T05:36:52.429+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:52 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 1280 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:36:52.438+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:52 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 1.505 s
[2023-01-31T05:36:52.474+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:52 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:36:52.479+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:52 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T05:36:52.495+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T05:36:52.517+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:52 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 1.637907 s
[2023-01-31T05:36:58.850+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:58 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:36:58.889+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:58 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T05:36:58.893+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:58 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:37:01.246+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:01 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 81d5fcd0285b:44651 in memory (size: 7.6 KiB, free: 434.3 MiB)
[2023-01-31T05:37:06.296+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:06 INFO CodeGenerator: Code generated in 5106.214807 ms
[2023-01-31T05:37:06.903+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:06 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.3 MiB)
[2023-01-31T05:37:07.479+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:07 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T05:37:07.486+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:07 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 81d5fcd0285b:44651 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:37:07.515+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:07 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T05:37:07.622+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:07 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:37:08.598+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:08 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T05:37:08.609+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:08 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) with 1 output partitions
[2023-01-31T05:37:08.611+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:08 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201)
[2023-01-31T05:37:08.619+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:08 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:37:08.630+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:08 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:37:08.729+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:08 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201), which has no missing parents
[2023-01-31T05:37:08.950+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:08 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.2 MiB)
[2023-01-31T05:37:09.026+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:09 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.2 MiB)
[2023-01-31T05:37:09.032+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:09 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 81d5fcd0285b:44651 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T05:37:09.065+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:09 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:37:09.131+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:37:09.146+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:09 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T05:37:09.189+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:09 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:37:09.197+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:09 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T05:37:09.905+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:09 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T05:37:11.410+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2537 bytes result sent to driver
[2023-01-31T05:37:11.419+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 2231 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:37:11.422+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T05:37:11.424+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) finished in 2.654 s
[2023-01-31T05:37:11.464+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:37:11.465+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T05:37:11.473+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201, took 2.868808 s
[2023-01-31T05:37:13.556+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:13 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:37:13.573+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:13 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T05:37:13.595+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:13 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:37:15.321+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:15 INFO CodeGenerator: Code generated in 1327.710178 ms
[2023-01-31T05:37:15.462+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:15 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 433.0 MiB)
[2023-01-31T05:37:16.264+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:16 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 81d5fcd0285b:44651 in memory (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T05:37:16.269+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:16 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.0 MiB)
[2023-01-31T05:37:16.273+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:16 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 81d5fcd0285b:44651 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:37:16.277+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:16 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T05:37:16.284+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:16 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:37:16.682+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:16 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T05:37:16.683+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:16 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T05:37:16.683+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:16 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T05:37:16.684+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:16 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:37:16.721+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:16 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:37:16.757+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:16 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T05:37:17.003+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:17 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 433.0 MiB)
[2023-01-31T05:37:17.085+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:17 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 433.0 MiB)
[2023-01-31T05:37:17.100+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:17 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 81d5fcd0285b:44651 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T05:37:17.116+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:17 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:37:17.119+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:37:17.128+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:17 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T05:37:17.129+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:17 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:37:17.155+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:17 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T05:37:17.524+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:17 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T05:37:19.391+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:19 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-01-31T05:37:19.402+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:19 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 2276 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:37:19.410+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:19 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 2.637 s
[2023-01-31T05:37:19.430+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:19 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T05:37:19.437+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:19 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:37:19.438+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T05:37:19.441+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:19 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 2.800971 s
[2023-01-31T05:37:20.863+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:20 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:37:20.867+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:20 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T05:37:20.871+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:20 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:37:25.528+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:25 INFO CodeGenerator: Code generated in 3532.51344 ms
[2023-01-31T05:37:25.565+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:25 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.8 MiB)
[2023-01-31T05:37:26.926+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:26 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.7 MiB)
[2023-01-31T05:37:26.931+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:26 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 81d5fcd0285b:44651 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:37:26.955+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:26 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T05:37:26.985+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:26 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:37:27.201+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:27 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T05:37:27.212+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:27 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T05:37:27.213+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:27 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T05:37:27.214+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:27 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:37:27.214+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:27 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:37:27.281+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:27 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T05:37:27.352+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:27 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.7 MiB)
[2023-01-31T05:37:27.501+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:27 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.7 MiB)
[2023-01-31T05:37:27.511+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:27 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 81d5fcd0285b:44651 (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T05:37:27.524+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:27 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:37:27.529+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:37:27.531+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:27 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T05:37:27.560+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:27 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:37:27.609+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:27 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T05:37:27.815+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:27 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T05:37:28.875+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:28 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1729 bytes result sent to driver
[2023-01-31T05:37:28.884+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:28 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 1329 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:37:28.889+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:28 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 1.609 s
[2023-01-31T05:37:28.890+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:28 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:37:28.902+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:28 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T05:37:28.919+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T05:37:28.942+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:28 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 1.738495 s
[2023-01-31T05:37:29.127+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 81d5fcd0285b:44651 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T05:37:29.204+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 81d5fcd0285b:44651 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T05:37:42.171+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:42 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 81d5fcd0285b:44651 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:37:42.359+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:42 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 81d5fcd0285b:44651 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:37:42.437+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:42 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 81d5fcd0285b:44651 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:37:42.602+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:42 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 81d5fcd0285b:44651 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:38:38.613+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:38 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:38:38.853+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:38:38.856+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:38:38.879+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:38 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:38:38.885+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:38:38.886+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:38:38.925+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:38:42.841+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:42 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-31T05:38:42.844+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:42 INFO DAGScheduler: Got job 8 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T05:38:42.845+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:42 INFO DAGScheduler: Final stage: ResultStage 8 (save at BigQueryWriteHelper.java:105)
[2023-01-31T05:38:42.845+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:42 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:38:42.854+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:42 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:38:42.858+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:42 INFO DAGScheduler: Submitting ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T05:38:43.733+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:43 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 209.9 KiB, free 433.5 MiB)
[2023-01-31T05:38:43.830+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:43 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 74.3 KiB, free 433.4 MiB)
[2023-01-31T05:38:43.833+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:43 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 81d5fcd0285b:44651 (size: 74.3 KiB, free: 434.2 MiB)
[2023-01-31T05:38:43.847+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:43 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:38:43.850+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:38:43.851+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:43 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2023-01-31T05:38:43.874+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:43 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7484 bytes) taskResourceAssignments Map()
[2023-01-31T05:38:43.902+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:43 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2023-01-31T05:38:45.236+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:38:45.237+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:38:45.250+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:45 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:38:45.250+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:38:45.253+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:38:45.269+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:38:45.371+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:45 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T05:38:45.435+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:45 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T05:38:46.169+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:46 INFO ParquetOutputFormat: Parquet block size to 134217728
[2023-01-31T05:38:46.172+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:46 INFO ParquetOutputFormat: Validation is off
[2023-01-31T05:38:46.173+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:46 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-01-31T05:38:46.176+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:46 INFO ParquetOutputFormat: Parquet properties are:
[2023-01-31T05:38:46.179+0000] {spark_submit.py:495} INFO - Parquet page size to 1048576
[2023-01-31T05:38:46.192+0000] {spark_submit.py:495} INFO - Parquet dictionary page size to 1048576
[2023-01-31T05:38:46.193+0000] {spark_submit.py:495} INFO - Dictionary is true
[2023-01-31T05:38:46.207+0000] {spark_submit.py:495} INFO - Writer version is: PARQUET_1_0
[2023-01-31T05:38:46.208+0000] {spark_submit.py:495} INFO - Page size checking is: estimated
[2023-01-31T05:38:46.208+0000] {spark_submit.py:495} INFO - Min row count for page size check is: 100
[2023-01-31T05:38:46.209+0000] {spark_submit.py:495} INFO - Max row count for page size check is: 10000
[2023-01-31T05:38:46.209+0000] {spark_submit.py:495} INFO - Truncate length for column indexes is: 64
[2023-01-31T05:38:46.221+0000] {spark_submit.py:495} INFO - Truncate length for statistics min/max  is: 2147483647
[2023-01-31T05:38:46.227+0000] {spark_submit.py:495} INFO - Bloom filter enabled: false
[2023-01-31T05:38:46.232+0000] {spark_submit.py:495} INFO - Max Bloom filter size for a column is 1048576
[2023-01-31T05:38:46.233+0000] {spark_submit.py:495} INFO - Bloom filter expected number of distinct values are: null
[2023-01-31T05:38:46.238+0000] {spark_submit.py:495} INFO - Page row count limit to 20000
[2023-01-31T05:38:46.244+0000] {spark_submit.py:495} INFO - Writing page checksums is: on
[2023-01-31T05:38:47.732+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:47 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-01-31T05:38:47.733+0000] {spark_submit.py:495} INFO - {
[2023-01-31T05:38:47.734+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-31T05:38:47.734+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-31T05:38:47.735+0000] {spark_submit.py:495} INFO - "name" : "measured_at",
[2023-01-31T05:38:47.750+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T05:38:47.751+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T05:38:47.752+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:47.753+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:47.753+0000] {spark_submit.py:495} INFO - "name" : "created_at",
[2023-01-31T05:38:47.772+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T05:38:47.772+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T05:38:47.773+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:47.784+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:47.791+0000] {spark_submit.py:495} INFO - "name" : "Biomass - Actual Aggregated",
[2023-01-31T05:38:47.812+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:47.813+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:47.837+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:47.838+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:47.839+0000] {spark_submit.py:495} INFO - "name" : "Fossil Brown coal/Lignite - Actual Aggregated",
[2023-01-31T05:38:47.841+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:47.842+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:47.843+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:47.844+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:47.871+0000] {spark_submit.py:495} INFO - "name" : "Fossil Gas - Actual Aggregated",
[2023-01-31T05:38:47.874+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:47.874+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:47.875+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:47.876+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:47.877+0000] {spark_submit.py:495} INFO - "name" : "Fossil Hard coal - Actual Aggregated",
[2023-01-31T05:38:47.878+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:47.879+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:47.880+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:47.880+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:47.890+0000] {spark_submit.py:495} INFO - "name" : "Fossil Oil - Actual Aggregated",
[2023-01-31T05:38:47.895+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:47.896+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:47.898+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:47.899+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:47.900+0000] {spark_submit.py:495} INFO - "name" : "Geothermal - Actual Aggregated",
[2023-01-31T05:38:47.901+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:47.903+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:47.915+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:47.917+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:47.918+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Aggregated",
[2023-01-31T05:38:47.920+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:47.920+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:47.921+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:47.922+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:47.933+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Consumption",
[2023-01-31T05:38:47.934+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:47.935+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:47.937+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:47.937+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:47.939+0000] {spark_submit.py:495} INFO - "name" : "Hydro Run-of-river and poundage - Actual Aggregated",
[2023-01-31T05:38:47.942+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:47.953+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:47.954+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:47.956+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:47.957+0000] {spark_submit.py:495} INFO - "name" : "Hydro Water Reservoir - Actual Aggregated",
[2023-01-31T05:38:47.958+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:47.958+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:47.959+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:47.960+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:47.961+0000] {spark_submit.py:495} INFO - "name" : "Nuclear - Actual Aggregated",
[2023-01-31T05:38:47.961+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:47.962+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:47.962+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:47.963+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:47.964+0000] {spark_submit.py:495} INFO - "name" : "Other - Actual Aggregated",
[2023-01-31T05:38:47.964+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:47.977+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:47.981+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:47.982+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:47.982+0000] {spark_submit.py:495} INFO - "name" : "Other renewable - Actual Aggregated",
[2023-01-31T05:38:47.983+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:47.983+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:47.984+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:47.985+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:47.993+0000] {spark_submit.py:495} INFO - "name" : "Solar - Actual Aggregated",
[2023-01-31T05:38:48.002+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:48.003+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:48.004+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:48.005+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:48.005+0000] {spark_submit.py:495} INFO - "name" : "Waste - Actual Aggregated",
[2023-01-31T05:38:48.033+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:48.063+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:48.107+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:48.108+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:48.128+0000] {spark_submit.py:495} INFO - "name" : "Wind Offshore - Actual Aggregated",
[2023-01-31T05:38:48.130+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:48.130+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:48.131+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:48.151+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:48.169+0000] {spark_submit.py:495} INFO - "name" : "Wind Onshore - Actual Aggregated",
[2023-01-31T05:38:48.176+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:48.181+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:48.196+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:48.212+0000] {spark_submit.py:495} INFO - } ]
[2023-01-31T05:38:48.220+0000] {spark_submit.py:495} INFO - }
[2023-01-31T05:38:48.228+0000] {spark_submit.py:495} INFO - and corresponding Parquet message type:
[2023-01-31T05:38:48.234+0000] {spark_submit.py:495} INFO - message spark_schema {
[2023-01-31T05:38:48.243+0000] {spark_submit.py:495} INFO - required int96 measured_at;
[2023-01-31T05:38:48.244+0000] {spark_submit.py:495} INFO - required int96 created_at;
[2023-01-31T05:38:48.251+0000] {spark_submit.py:495} INFO - optional double Biomass - Actual Aggregated;
[2023-01-31T05:38:48.257+0000] {spark_submit.py:495} INFO - optional double Fossil Brown coal/Lignite - Actual Aggregated;
[2023-01-31T05:38:48.265+0000] {spark_submit.py:495} INFO - optional double Fossil Gas - Actual Aggregated;
[2023-01-31T05:38:48.270+0000] {spark_submit.py:495} INFO - optional double Fossil Hard coal - Actual Aggregated;
[2023-01-31T05:38:48.276+0000] {spark_submit.py:495} INFO - optional double Fossil Oil - Actual Aggregated;
[2023-01-31T05:38:48.279+0000] {spark_submit.py:495} INFO - optional double Geothermal - Actual Aggregated;
[2023-01-31T05:38:48.289+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Aggregated;
[2023-01-31T05:38:48.293+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Consumption;
[2023-01-31T05:38:48.301+0000] {spark_submit.py:495} INFO - optional double Hydro Run-of-river and poundage - Actual Aggregated;
[2023-01-31T05:38:48.307+0000] {spark_submit.py:495} INFO - optional double Hydro Water Reservoir - Actual Aggregated;
[2023-01-31T05:38:48.308+0000] {spark_submit.py:495} INFO - optional double Nuclear - Actual Aggregated;
[2023-01-31T05:38:48.314+0000] {spark_submit.py:495} INFO - optional double Other - Actual Aggregated;
[2023-01-31T05:38:48.324+0000] {spark_submit.py:495} INFO - optional double Other renewable - Actual Aggregated;
[2023-01-31T05:38:48.335+0000] {spark_submit.py:495} INFO - optional double Solar - Actual Aggregated;
[2023-01-31T05:38:48.339+0000] {spark_submit.py:495} INFO - optional double Waste - Actual Aggregated;
[2023-01-31T05:38:48.340+0000] {spark_submit.py:495} INFO - optional double Wind Offshore - Actual Aggregated;
[2023-01-31T05:38:48.350+0000] {spark_submit.py:495} INFO - optional double Wind Onshore - Actual Aggregated;
[2023-01-31T05:38:48.405+0000] {spark_submit.py:495} INFO - }
[2023-01-31T05:38:48.406+0000] {spark_submit.py:495} INFO - 
[2023-01-31T05:38:48.406+0000] {spark_submit.py:495} INFO - 
[2023-01-31T05:38:52.105+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:52 INFO CodecPool: Got brand-new compressor [.snappy]
[2023-01-31T05:38:52.770+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:52 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 81d5fcd0285b:44651 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:38:52.885+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:52 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 81d5fcd0285b:44651 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:39:08.816+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:08 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675143134907-ab42412b-af6b-46e6-bcc8-ab63bb58fe4d/_temporary/0/_temporary/' directory.
[2023-01-31T05:39:08.818+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:08 INFO FileOutputCommitter: Saved output of task 'attempt_202301310538407918692306928747360_0008_m_000000_8' to gs://entsoe_temp_1009/.spark-bigquery-local-1675143134907-ab42412b-af6b-46e6-bcc8-ab63bb58fe4d/_temporary/0/task_202301310538407918692306928747360_0008_m_000000
[2023-01-31T05:39:08.824+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:08 INFO SparkHadoopMapRedUtil: attempt_202301310538407918692306928747360_0008_m_000000_8: Committed. Elapsed time: 3347 ms.
[2023-01-31T05:39:08.874+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:08 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2588 bytes result sent to driver
[2023-01-31T05:39:08.886+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:08 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 25023 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:39:08.891+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:08 INFO DAGScheduler: ResultStage 8 (save at BigQueryWriteHelper.java:105) finished in 26.020 s
[2023-01-31T05:39:08.895+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:08 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:39:08.898+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:08 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2023-01-31T05:39:08.902+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2023-01-31T05:39:08.911+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:08 INFO DAGScheduler: Job 8 finished: save at BigQueryWriteHelper.java:105, took 26.068069 s
[2023-01-31T05:39:08.915+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:08 INFO FileFormatWriter: Start to commit write Job 14a686c2-1d12-4934-93bd-86f869e5b4e9.
[2023-01-31T05:39:10.412+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:10 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675143134907-ab42412b-af6b-46e6-bcc8-ab63bb58fe4d/_temporary/0/task_202301310538407918692306928747360_0008_m_000000/' directory.
[2023-01-31T05:39:11.706+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:11 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675143134907-ab42412b-af6b-46e6-bcc8-ab63bb58fe4d/' directory.
[2023-01-31T05:39:13.296+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:13 INFO FileFormatWriter: Write Job 14a686c2-1d12-4934-93bd-86f869e5b4e9 committed. Elapsed time: 4374 ms.
[2023-01-31T05:39:13.348+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:13 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 81d5fcd0285b:44651 in memory (size: 74.3 KiB, free: 434.4 MiB)
[2023-01-31T05:39:13.425+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:13 INFO FileFormatWriter: Finished processing stats for write job 14a686c2-1d12-4934-93bd-86f869e5b4e9.
[2023-01-31T05:39:19.003+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:18 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=entsoe_analytics_1009, projectId=rafzul-analytics-1009, tableId=TEST_total_generation_staging}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=null, ignoreUnknownValue=null, sourceUris=[gs://entsoe_temp_1009/.spark-bigquery-local-1675143134907-ab42412b-af6b-46e6-bcc8-ab63bb58fe4d/part-00000-8fd68528-b321-40fd-927b-a7ce5b4fc672-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=true, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=rafzul-analytics-1009, job=ccafaaad-9418-4e9b-af10-edc735f5629a, location=US}
[2023-01-31T05:39:21.861+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:21 INFO BigQueryClient: Done loading to rafzul-analytics-1009.entsoe_analytics_1009.TEST_total_generation_staging. jobId: JobId{project=rafzul-analytics-1009, job=ccafaaad-9418-4e9b-af10-edc735f5629a, location=US}
[2023-01-31T05:39:24.697+0000] {spark_submit.py:495} INFO - MANTEP!!!!
[2023-01-31T05:39:26.113+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:26 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-31T05:39:26.418+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:26 INFO SparkUI: Stopped Spark web UI at http://81d5fcd0285b:4041
[2023-01-31T05:39:26.590+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:26 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-31T05:39:26.742+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:26 INFO MemoryStore: MemoryStore cleared
[2023-01-31T05:39:26.753+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:26 INFO BlockManager: BlockManager stopped
[2023-01-31T05:39:26.799+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:26 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-31T05:39:26.856+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:26 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-31T05:39:26.961+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:26 INFO SparkContext: Successfully stopped SparkContext
[2023-01-31T05:39:26.964+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:26 INFO ShutdownHookManager: Shutdown hook called
[2023-01-31T05:39:26.968+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:26 INFO ShutdownHookManager: Deleting directory /tmp/spark-d03a91e0-49a1-4863-8c51-e4f8995ecc1e
[2023-01-31T05:39:27.002+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:27 INFO ShutdownHookManager: Deleting directory /tmp/spark-93342d3b-a043-45de-ae7e-2ee9a8c10852
[2023-01-31T05:39:27.119+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:27 INFO ShutdownHookManager: Deleting directory /tmp/spark-d03a91e0-49a1-4863-8c51-e4f8995ecc1e/pyspark-c23989f0-832b-469d-a622-e503a43c3b69
[2023-01-31T05:39:28.897+0000] {taskinstance.py:1322} INFO - Marking task as SUCCESS. dag_id=entsoe-energydata-backfill, task_id=stage_total_generation, execution_date=20210101T040000, start_date=20230131T053041, end_date=20230131T053928
[2023-01-31T05:39:29.233+0000] {local_task_job.py:159} INFO - Task exited with return code 0
[2023-01-31T05:39:29.571+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T07:38:49.871+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T04:00:00+00:00 [queued]>
[2023-01-31T07:38:50.122+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T04:00:00+00:00 [queued]>
[2023-01-31T07:38:50.123+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T07:38:50.124+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T07:38:50.153+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T07:38:50.535+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 04:00:00+00:00
[2023-01-31T07:38:50.732+0000] {standard_task_runner.py:55} INFO - Started process 25754 to run task
[2023-01-31T07:38:50.781+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T04:00:00+00:00', '--job-id', '907', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpfi6_7th9']
[2023-01-31T07:38:50.851+0000] {standard_task_runner.py:83} INFO - Job 907: Subtask stage_total_generation
[2023-01-31T07:38:52.492+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T04:00:00+00:00 [running]> on host 81d5fcd0285b
[2023-01-31T07:38:53.036+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T04:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T04:00:00+00:00
[2023-01-31T07:38:53.097+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T07:38:53.104+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010400 202101010500 DE_TENNET
[2023-01-31T07:40:14.036+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:14 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T07:40:14.854+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T07:40:17.236+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:17 INFO ResourceUtils: ==============================================================
[2023-01-31T07:40:17.241+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:17 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T07:40:17.275+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:17 INFO ResourceUtils: ==============================================================
[2023-01-31T07:40:17.285+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:17 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T07:40:18.449+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:18 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T07:40:18.477+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:18 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T07:40:18.513+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:18 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T07:40:20.362+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:20 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T07:40:20.385+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:20 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T07:40:20.392+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:20 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T07:40:20.407+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:20 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T07:40:20.419+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:20 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T07:40:28.826+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:28 INFO Utils: Successfully started service 'sparkDriver' on port 43307.
[2023-01-31T07:40:30.387+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:30 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T07:40:31.200+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:31 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T07:40:31.551+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:31 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T07:40:31.562+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:31 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T07:40:31.597+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:31 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T07:40:32.434+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:32 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-8daef111-83cb-43d8-9f74-a19645eb8e8e
[2023-01-31T07:40:33.544+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:33 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T07:40:34.396+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:34 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T07:40:44.198+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:44 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T07:40:44.381+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:44 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2023-01-31T07:40:44.973+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:44 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://81d5fcd0285b:43307/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675150813979
[2023-01-31T07:40:44.981+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:44 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://81d5fcd0285b:43307/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675150813979
[2023-01-31T07:40:48.105+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:48 INFO Executor: Starting executor ID driver on host 81d5fcd0285b
[2023-01-31T07:40:48.557+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:48 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T07:40:49.281+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:49 INFO Executor: Fetching spark://81d5fcd0285b:43307/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675150813979
[2023-01-31T07:40:52.315+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:52 INFO TransportClientFactory: Successfully created connection to 81d5fcd0285b/172.19.0.9:43307 after 2188 ms (0 ms spent in bootstraps)
[2023-01-31T07:40:52.643+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:52 INFO Utils: Fetching spark://81d5fcd0285b:43307/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-49160a72-d461-486e-9677-ca9e4c642c5e/userFiles-ccb505f7-103c-4ab8-a9b6-05c6a3cc08cd/fetchFileTemp2101348999276565948.tmp
[2023-01-31T07:41:02.057+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:02 INFO Executor: Adding file:/tmp/spark-49160a72-d461-486e-9677-ca9e4c642c5e/userFiles-ccb505f7-103c-4ab8-a9b6-05c6a3cc08cd/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T07:41:02.058+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:02 INFO Executor: Fetching spark://81d5fcd0285b:43307/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675150813979
[2023-01-31T07:41:02.121+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:02 INFO Utils: Fetching spark://81d5fcd0285b:43307/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-49160a72-d461-486e-9677-ca9e4c642c5e/userFiles-ccb505f7-103c-4ab8-a9b6-05c6a3cc08cd/fetchFileTemp12994635822341539556.tmp
[2023-01-31T07:41:05.151+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:05 INFO Executor: Told to re-register on heartbeat
[2023-01-31T07:41:05.160+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:05 INFO BlockManager: BlockManager null re-registering with master
[2023-01-31T07:41:05.184+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:05 INFO BlockManagerMaster: Registering BlockManager null
[2023-01-31T07:41:05.253+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:05 ERROR Inbox: Ignoring error
[2023-01-31T07:41:05.254+0000] {spark_submit.py:495} INFO - java.lang.NullPointerException
[2023-01-31T07:41:05.254+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)
[2023-01-31T07:41:05.256+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)
[2023-01-31T07:41:05.256+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-01-31T07:41:05.257+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-01-31T07:41:05.258+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-01-31T07:41:05.258+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-01-31T07:41:05.264+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-01-31T07:41:05.264+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-01-31T07:41:05.265+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-01-31T07:41:05.265+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-31T07:41:05.266+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-31T07:41:05.266+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-31T07:41:05.413+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:05 INFO Executor: Adding file:/tmp/spark-49160a72-d461-486e-9677-ca9e4c642c5e/userFiles-ccb505f7-103c-4ab8-a9b6-05c6a3cc08cd/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T07:41:05.448+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:05 WARN Executor: Issue communicating with driver in heartbeater
[2023-01-31T07:41:05.449+0000] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-01-31T07:41:05.450+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-01-31T07:41:05.451+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-01-31T07:41:05.451+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2023-01-31T07:41:05.452+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2023-01-31T07:41:05.452+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:79)
[2023-01-31T07:41:05.461+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:643)
[2023-01-31T07:41:05.461+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2023-01-31T07:41:05.462+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2023-01-31T07:41:05.462+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
[2023-01-31T07:41:05.463+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2023-01-31T07:41:05.464+0000] {spark_submit.py:495} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2023-01-31T07:41:05.464+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-01-31T07:41:05.483+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2023-01-31T07:41:05.484+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2023-01-31T07:41:05.605+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-31T07:41:05.605+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-31T07:41:05.606+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-31T07:41:05.606+0000] {spark_submit.py:495} INFO - Caused by: java.lang.NullPointerException
[2023-01-31T07:41:05.607+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)
[2023-01-31T07:41:05.608+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)
[2023-01-31T07:41:05.608+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-01-31T07:41:05.749+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-01-31T07:41:05.750+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-01-31T07:41:05.750+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-01-31T07:41:05.751+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-01-31T07:41:05.751+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-01-31T07:41:05.752+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-01-31T07:41:05.764+0000] {spark_submit.py:495} INFO - ... 3 more
[2023-01-31T07:41:05.765+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:05 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39973.
[2023-01-31T07:41:05.766+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:05 INFO NettyBlockTransferService: Server created on 81d5fcd0285b:39973
[2023-01-31T07:41:05.767+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:05 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T07:41:06.163+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:06 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 39973, None)
[2023-01-31T07:41:06.202+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:06 INFO BlockManagerMasterEndpoint: Registering block manager 81d5fcd0285b:39973 with 434.4 MiB RAM, BlockManagerId(driver, 81d5fcd0285b, 39973, None)
[2023-01-31T07:41:06.221+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:06 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 39973, None)
[2023-01-31T07:41:06.251+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:06 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 81d5fcd0285b, 39973, None)
[2023-01-31T07:41:10.566+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:10 INFO AsyncEventQueue: Process of event SparkListenerResourceProfileAdded(Profile: id = 0, executor resources: cores -> name: cores, amount: 1, script: , vendor: ,memory -> name: memory, amount: 1024, script: , vendor: ,offHeap -> name: offHeap, amount: 0, script: , vendor: , task resources: cpus -> name: cpus, amount: 1.0) by listener AppStatusListener took 1.013889961s.
[2023-01-31T07:41:32.155+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:32 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T07:41:32.442+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:32 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T07:42:20.853+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:20 INFO InMemoryFileIndex: It took 2799 ms to list leaf files for 1 paths.
[2023-01-31T07:42:28.277+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:28 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T07:42:30.435+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:30 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T07:42:30.665+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:30 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 81d5fcd0285b:39973 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T07:42:30.768+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:30 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T07:42:44.600+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:44 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T07:42:45.426+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:45 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T07:42:45.964+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:45 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T07:42:46.567+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:46 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T07:42:46.626+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:46 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T07:42:46.670+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:46 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:42:46.806+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:46 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:42:47.297+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:47 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T07:42:48.994+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:48 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T07:42:49.220+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:49 INFO AsyncEventQueue: Process of event SparkListenerJobStart(0,1675150966825,ArraySeq(org.apache.spark.scheduler.StageInfo@3c59fc4e),{spark.master=local, spark.driver.port=43307, spark.submit.pyFiles=, spark.app.startTime=1675150813979, spark.rdd.compress=True, spark.executor.id=driver, spark.app.name=gcp_playground, spark.submit.deployMode=client, spark.hadoop.google.cloud.auth.service.account.json.keyfile=/.google/credentials/google_credentials.json, spark.driver.host=81d5fcd0285b, spark.app.id=local-1675150846386, spark.app.submitTime=1675150800738, spark.hadoop.fs.AbstractFileSystem.gs.impl=google.cloud.hadoop.fs.gcs.GoogleHadoopFS, spark.sql.session.timeZone=UTC, spark.hadoop.google.cloud.auth.service.account.enable=true, spark.app.initial.jar.urls=spark://81d5fcd0285b:43307/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar,spark://81d5fcd0285b:43307/jars/gcs-connector-hadoop3-latest.jar, spark.executor.extraJavaOptions=-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED, spark.jars=/opt/spark/jars/gcs-connector-hadoop3-latest.jar, /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar, spark.sql.warehouse.dir=file:/opt/***/spark-warehouse, spark.serializer.objectStreamReset=100, spark.driver.extraJavaOptions=-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED}) by listener AppStatusListener took 2.286218913s.
[2023-01-31T07:42:49.227+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:49 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T07:42:49.233+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:49 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 81d5fcd0285b:39973 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T07:42:49.289+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:49 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:42:50.112+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:42:50.185+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:50 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T07:42:50.693+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:50 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T07:42:50.868+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:50 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T07:42:51.596+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:51 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010400__202101010500.json:0+9364
[2023-01-31T07:42:57.734+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:57 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3480 bytes result sent to driver
[2023-01-31T07:42:57.875+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:57 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 7284 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:42:57.921+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:57 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T07:42:57.996+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:57 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 9.630 s
[2023-01-31T07:42:58.078+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:58 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:42:58.080+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T07:42:58.140+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:58 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 12.173289 s
[2023-01-31T07:43:11.004+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:11 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 81d5fcd0285b:39973 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T07:43:11.100+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:11 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 81d5fcd0285b:39973 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T07:43:24.632+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:24 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:43:24.634+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:24 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T07:43:24.645+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:24 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:43:26.363+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:26 INFO CodeGenerator: Code generated in 1210.990357 ms
[2023-01-31T07:43:26.390+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:26 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T07:43:26.428+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:26 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T07:43:26.441+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:26 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 81d5fcd0285b:39973 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T07:43:26.444+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:26 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T07:43:26.530+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:26 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:43:26.947+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:26 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T07:43:26.953+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:26 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T07:43:26.954+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:26 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T07:43:26.955+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:26 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:43:26.955+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:26 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:43:26.986+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:26 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T07:43:27.056+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T07:43:27.092+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T07:43:27.103+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 81d5fcd0285b:39973 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T07:43:27.112+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:43:27.128+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:43:27.133+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T07:43:27.179+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:43:27.193+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T07:43:27.798+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T07:43:28.408+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:28 INFO CodeGenerator: Code generated in 390.559398 ms
[2023-01-31T07:43:29.159+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1628 bytes result sent to driver
[2023-01-31T07:43:29.176+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2033 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:43:29.178+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T07:43:29.186+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 2.187 s
[2023-01-31T07:43:29.188+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:43:29.189+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T07:43:29.194+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 2.242994 s
[2023-01-31T07:43:29.577+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:43:29.585+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T07:43:29.591+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:43:29.777+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO CodeGenerator: Code generated in 123.978575 ms
[2023-01-31T07:43:29.798+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T07:43:29.832+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T07:43:29.835+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 81d5fcd0285b:39973 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:43:29.839+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T07:43:29.841+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:43:29.891+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T07:43:29.894+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T07:43:29.895+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T07:43:29.895+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:43:29.895+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:43:29.903+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T07:43:29.925+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T07:43:29.947+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T07:43:29.948+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 81d5fcd0285b:39973 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T07:43:29.958+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:43:29.960+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:43:29.960+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T07:43:29.971+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:43:29.978+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T07:43:30.031+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T07:43:30.270+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1585 bytes result sent to driver
[2023-01-31T07:43:30.316+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 349 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:43:30.325+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T07:43:30.326+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 0.413 s
[2023-01-31T07:43:30.326+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:43:30.326+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T07:43:30.326+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 0.429814 s
[2023-01-31T07:43:32.925+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:43:32.927+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T07:43:32.938+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:43:33.771+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO CodeGenerator: Code generated in 578.654911 ms
[2023-01-31T07:43:33.884+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T07:43:34.075+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T07:43:34.084+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 81d5fcd0285b:39973 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:43:34.147+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T07:43:34.167+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:43:34.321+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T07:43:34.323+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T07:43:34.324+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T07:43:34.324+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:43:34.324+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:43:34.339+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T07:43:34.348+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.6 MiB)
[2023-01-31T07:43:34.414+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.6 MiB)
[2023-01-31T07:43:34.425+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 81d5fcd0285b:39973 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T07:43:34.436+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:43:34.447+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:43:34.453+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T07:43:34.466+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:43:34.474+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T07:43:34.511+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T07:43:34.864+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1755 bytes result sent to driver
[2023-01-31T07:43:34.911+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 447 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:43:34.919+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 0.583 s
[2023-01-31T07:43:34.919+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:43:34.921+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T07:43:34.924+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T07:43:34.925+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 0.603779 s
[2023-01-31T07:43:35.279+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:35 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:43:35.280+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:35 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T07:43:35.280+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:35 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:43:35.652+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:35 INFO CodeGenerator: Code generated in 298.143739 ms
[2023-01-31T07:43:35.840+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:35 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.4 MiB)
[2023-01-31T07:43:36.192+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:36 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T07:43:36.240+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:36 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 81d5fcd0285b:39973 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T07:43:36.305+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:36 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T07:43:36.334+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:36 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:43:36.919+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:36 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 81d5fcd0285b:39973 in memory (size: 7.7 KiB, free: 434.2 MiB)
[2023-01-31T07:43:37.070+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:37 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T07:43:37.091+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:37 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T07:43:37.092+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:37 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T07:43:37.092+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:37 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:43:37.092+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:37 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:43:37.101+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:37 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T07:43:37.132+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:37 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 81d5fcd0285b:39973 in memory (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T07:43:37.228+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:37 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-31T07:43:37.308+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:37 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 81d5fcd0285b:39973 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T07:43:37.322+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:37 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.5 MiB)
[2023-01-31T07:43:37.329+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:37 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 81d5fcd0285b:39973 (size: 7.6 KiB, free: 434.3 MiB)
[2023-01-31T07:43:37.385+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:37 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:43:37.389+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:43:37.391+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:37 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T07:43:37.471+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:37 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:43:37.477+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:37 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T07:43:37.819+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:37 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T07:43:38.624+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:38 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1586 bytes result sent to driver
[2023-01-31T07:43:38.637+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:38 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 1166 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:43:38.647+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:38 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T07:43:38.657+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:38 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 1.519 s
[2023-01-31T07:43:38.659+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:38 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:43:38.660+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T07:43:38.663+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:38 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 1.562922 s
[2023-01-31T07:43:41.861+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:41 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:43:41.874+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:41 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T07:43:41.882+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:41 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:43:45.825+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO CodeGenerator: Code generated in 2982.823223 ms
[2023-01-31T07:43:46.242+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:46 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.3 MiB)
[2023-01-31T07:43:46.475+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:46 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T07:43:46.477+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:46 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 81d5fcd0285b:39973 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T07:43:46.478+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:46 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T07:43:46.485+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:46 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:43:46.776+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:46 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T07:43:46.812+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:46 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) with 1 output partitions
[2023-01-31T07:43:46.813+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:46 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201)
[2023-01-31T07:43:46.840+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:46 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:43:46.868+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:46 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:43:46.904+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:46 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201), which has no missing parents
[2023-01-31T07:43:47.171+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:47 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.2 MiB)
[2023-01-31T07:43:47.338+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:47 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.2 MiB)
[2023-01-31T07:43:47.358+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:47 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 81d5fcd0285b:39973 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T07:43:47.409+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:47 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:43:47.418+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:43:47.420+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:47 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T07:43:47.420+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:47 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:43:47.437+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:47 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T07:43:47.636+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:47 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T07:43:49.103+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:49 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2580 bytes result sent to driver
[2023-01-31T07:43:49.214+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:49 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 1795 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:43:49.215+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:49 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T07:43:49.222+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:49 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) finished in 2.262 s
[2023-01-31T07:43:49.223+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:49 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:43:49.265+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T07:43:49.274+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:49 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201, took 2.470055 s
[2023-01-31T07:43:51.581+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:51 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:43:51.645+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:51 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T07:43:51.646+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:51 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:43:55.673+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:55 INFO CodeGenerator: Code generated in 3244.65601 ms
[2023-01-31T07:43:56.095+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:56 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 433.0 MiB)
[2023-01-31T07:43:56.969+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:56 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.9 MiB)
[2023-01-31T07:43:56.970+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:56 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 81d5fcd0285b:39973 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T07:43:56.982+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:56 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T07:43:57.081+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:57 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:43:57.274+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:57 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T07:43:57.280+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:57 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T07:43:57.281+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:57 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T07:43:57.281+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:57 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:43:57.282+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:57 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:43:57.314+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:57 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T07:43:57.512+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:57 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 432.9 MiB)
[2023-01-31T07:43:57.699+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:57 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.9 MiB)
[2023-01-31T07:43:57.739+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:57 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 81d5fcd0285b:39973 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T07:43:57.740+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:57 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:43:57.744+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:43:57.768+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:57 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T07:43:57.797+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:57 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:43:57.800+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:57 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T07:43:58.165+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:58 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T07:43:58.957+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:58 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-01-31T07:43:58.985+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:58 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 1185 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:43:58.986+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:58 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 1.636 s
[2023-01-31T07:43:58.987+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:58 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:43:59.014+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:59 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T07:43:59.037+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T07:43:59.038+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:59 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 1.750484 s
[2023-01-31T07:44:00.509+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:00 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:44:00.509+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:00 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T07:44:00.510+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:00 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:44:04.117+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO CodeGenerator: Code generated in 867.383825 ms
[2023-01-31T07:44:04.133+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.7 MiB)
[2023-01-31T07:44:04.190+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.7 MiB)
[2023-01-31T07:44:04.200+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 81d5fcd0285b:39973 (size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T07:44:04.204+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T07:44:04.220+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:44:04.309+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T07:44:04.316+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T07:44:04.320+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T07:44:04.321+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:44:04.335+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:44:04.358+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T07:44:04.394+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.6 MiB)
[2023-01-31T07:44:04.422+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.6 MiB)
[2023-01-31T07:44:04.494+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 81d5fcd0285b:39973 (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T07:44:04.501+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:44:04.517+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:44:04.518+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T07:44:04.519+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:44:04.519+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T07:44:04.785+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T07:44:05.337+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:05 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1729 bytes result sent to driver
[2023-01-31T07:44:05.377+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:05 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 857 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:44:05.397+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:05 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 1.023 s
[2023-01-31T07:44:05.398+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:05 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:44:05.400+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:05 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T07:44:05.408+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T07:44:05.435+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:05 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 1.110802 s
[2023-01-31T07:44:07.572+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:07 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 81d5fcd0285b:39973 in memory (size: 13.4 KiB, free: 434.1 MiB)
[2023-01-31T07:44:07.704+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:07 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 81d5fcd0285b:39973 in memory (size: 7.6 KiB, free: 434.1 MiB)
[2023-01-31T07:44:08.049+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:08 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 81d5fcd0285b:39973 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T07:44:08.264+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:08 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 81d5fcd0285b:39973 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T07:44:38.366+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:38 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 81d5fcd0285b:39973 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T07:44:38.467+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:38 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 81d5fcd0285b:39973 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T07:44:38.647+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:38 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 81d5fcd0285b:39973 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:44:38.821+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:38 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 81d5fcd0285b:39973 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:44:38.968+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:38 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 81d5fcd0285b:39973 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:44:39.240+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:39 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 81d5fcd0285b:39973 in memory (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T07:45:09.524+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:09 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T07:45:09.820+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T07:45:09.821+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T07:45:09.835+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:09 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T07:45:09.843+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T07:45:09.846+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T07:45:09.869+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T07:45:11.450+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-31T07:45:11.462+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO DAGScheduler: Got job 8 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T07:45:11.463+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO DAGScheduler: Final stage: ResultStage 8 (save at BigQueryWriteHelper.java:105)
[2023-01-31T07:45:11.463+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:45:11.463+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:45:11.475+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO DAGScheduler: Submitting ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T07:45:11.549+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 209.9 KiB, free 434.0 MiB)
[2023-01-31T07:45:11.558+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 74.3 KiB, free 433.9 MiB)
[2023-01-31T07:45:11.558+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 81d5fcd0285b:39973 (size: 74.3 KiB, free: 434.3 MiB)
[2023-01-31T07:45:11.561+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:45:11.562+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:45:11.564+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2023-01-31T07:45:11.573+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7484 bytes) taskResourceAssignments Map()
[2023-01-31T07:45:11.578+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2023-01-31T07:45:11.736+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T07:45:11.737+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T07:45:11.739+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T07:45:11.739+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T07:45:11.740+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T07:45:11.750+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T07:45:11.789+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T07:45:11.804+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T07:45:11.930+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO ParquetOutputFormat: Parquet block size to 134217728
[2023-01-31T07:45:11.930+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO ParquetOutputFormat: Validation is off
[2023-01-31T07:45:11.930+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-01-31T07:45:11.932+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:11 INFO ParquetOutputFormat: Parquet properties are:
[2023-01-31T07:45:11.932+0000] {spark_submit.py:495} INFO - Parquet page size to 1048576
[2023-01-31T07:45:11.932+0000] {spark_submit.py:495} INFO - Parquet dictionary page size to 1048576
[2023-01-31T07:45:11.932+0000] {spark_submit.py:495} INFO - Dictionary is true
[2023-01-31T07:45:11.932+0000] {spark_submit.py:495} INFO - Writer version is: PARQUET_1_0
[2023-01-31T07:45:11.933+0000] {spark_submit.py:495} INFO - Page size checking is: estimated
[2023-01-31T07:45:11.933+0000] {spark_submit.py:495} INFO - Min row count for page size check is: 100
[2023-01-31T07:45:11.933+0000] {spark_submit.py:495} INFO - Max row count for page size check is: 10000
[2023-01-31T07:45:11.933+0000] {spark_submit.py:495} INFO - Truncate length for column indexes is: 64
[2023-01-31T07:45:11.933+0000] {spark_submit.py:495} INFO - Truncate length for statistics min/max  is: 2147483647
[2023-01-31T07:45:11.933+0000] {spark_submit.py:495} INFO - Bloom filter enabled: false
[2023-01-31T07:45:11.934+0000] {spark_submit.py:495} INFO - Max Bloom filter size for a column is 1048576
[2023-01-31T07:45:11.934+0000] {spark_submit.py:495} INFO - Bloom filter expected number of distinct values are: null
[2023-01-31T07:45:11.934+0000] {spark_submit.py:495} INFO - Page row count limit to 20000
[2023-01-31T07:45:11.934+0000] {spark_submit.py:495} INFO - Writing page checksums is: on
[2023-01-31T07:45:12.068+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:12 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-01-31T07:45:12.068+0000] {spark_submit.py:495} INFO - {
[2023-01-31T07:45:12.069+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-31T07:45:12.069+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-31T07:45:12.069+0000] {spark_submit.py:495} INFO - "name" : "measured_at",
[2023-01-31T07:45:12.069+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T07:45:12.070+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T07:45:12.070+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.070+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.070+0000] {spark_submit.py:495} INFO - "name" : "created_at",
[2023-01-31T07:45:12.070+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T07:45:12.070+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T07:45:12.071+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.071+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.071+0000] {spark_submit.py:495} INFO - "name" : "Biomass - Actual Aggregated",
[2023-01-31T07:45:12.071+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.071+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.071+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.071+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.072+0000] {spark_submit.py:495} INFO - "name" : "Fossil Brown coal/Lignite - Actual Aggregated",
[2023-01-31T07:45:12.072+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.072+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.072+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.072+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.074+0000] {spark_submit.py:495} INFO - "name" : "Fossil Gas - Actual Aggregated",
[2023-01-31T07:45:12.074+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.074+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.074+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.074+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.075+0000] {spark_submit.py:495} INFO - "name" : "Fossil Hard coal - Actual Aggregated",
[2023-01-31T07:45:12.075+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.077+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.077+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.077+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.078+0000] {spark_submit.py:495} INFO - "name" : "Fossil Oil - Actual Aggregated",
[2023-01-31T07:45:12.078+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.078+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.079+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.079+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.080+0000] {spark_submit.py:495} INFO - "name" : "Geothermal - Actual Aggregated",
[2023-01-31T07:45:12.080+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.080+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.080+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.083+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.083+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Aggregated",
[2023-01-31T07:45:12.084+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.084+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.085+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.085+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.086+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Consumption",
[2023-01-31T07:45:12.086+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.087+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.087+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.090+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.090+0000] {spark_submit.py:495} INFO - "name" : "Hydro Run-of-river and poundage - Actual Aggregated",
[2023-01-31T07:45:12.092+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.092+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.093+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.093+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.094+0000] {spark_submit.py:495} INFO - "name" : "Hydro Water Reservoir - Actual Aggregated",
[2023-01-31T07:45:12.095+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.095+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.095+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.097+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.097+0000] {spark_submit.py:495} INFO - "name" : "Nuclear - Actual Aggregated",
[2023-01-31T07:45:12.097+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.101+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.101+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.101+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.101+0000] {spark_submit.py:495} INFO - "name" : "Other - Actual Aggregated",
[2023-01-31T07:45:12.102+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.102+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.102+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.104+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.104+0000] {spark_submit.py:495} INFO - "name" : "Other renewable - Actual Aggregated",
[2023-01-31T07:45:12.104+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.104+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.107+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.107+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.108+0000] {spark_submit.py:495} INFO - "name" : "Solar - Actual Aggregated",
[2023-01-31T07:45:12.111+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.111+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.112+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.112+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.112+0000] {spark_submit.py:495} INFO - "name" : "Waste - Actual Aggregated",
[2023-01-31T07:45:12.112+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.112+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.117+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.117+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.117+0000] {spark_submit.py:495} INFO - "name" : "Wind Offshore - Actual Aggregated",
[2023-01-31T07:45:12.118+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.118+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.118+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.118+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:12.118+0000] {spark_submit.py:495} INFO - "name" : "Wind Onshore - Actual Aggregated",
[2023-01-31T07:45:12.118+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:12.119+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:12.119+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:12.119+0000] {spark_submit.py:495} INFO - } ]
[2023-01-31T07:45:12.119+0000] {spark_submit.py:495} INFO - }
[2023-01-31T07:45:12.119+0000] {spark_submit.py:495} INFO - and corresponding Parquet message type:
[2023-01-31T07:45:12.119+0000] {spark_submit.py:495} INFO - message spark_schema {
[2023-01-31T07:45:12.119+0000] {spark_submit.py:495} INFO - required int96 measured_at;
[2023-01-31T07:45:12.120+0000] {spark_submit.py:495} INFO - required int96 created_at;
[2023-01-31T07:45:12.120+0000] {spark_submit.py:495} INFO - optional double Biomass - Actual Aggregated;
[2023-01-31T07:45:12.120+0000] {spark_submit.py:495} INFO - optional double Fossil Brown coal/Lignite - Actual Aggregated;
[2023-01-31T07:45:12.120+0000] {spark_submit.py:495} INFO - optional double Fossil Gas - Actual Aggregated;
[2023-01-31T07:45:12.120+0000] {spark_submit.py:495} INFO - optional double Fossil Hard coal - Actual Aggregated;
[2023-01-31T07:45:12.126+0000] {spark_submit.py:495} INFO - optional double Fossil Oil - Actual Aggregated;
[2023-01-31T07:45:12.127+0000] {spark_submit.py:495} INFO - optional double Geothermal - Actual Aggregated;
[2023-01-31T07:45:12.127+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Aggregated;
[2023-01-31T07:45:12.127+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Consumption;
[2023-01-31T07:45:12.129+0000] {spark_submit.py:495} INFO - optional double Hydro Run-of-river and poundage - Actual Aggregated;
[2023-01-31T07:45:12.129+0000] {spark_submit.py:495} INFO - optional double Hydro Water Reservoir - Actual Aggregated;
[2023-01-31T07:45:12.129+0000] {spark_submit.py:495} INFO - optional double Nuclear - Actual Aggregated;
[2023-01-31T07:45:12.129+0000] {spark_submit.py:495} INFO - optional double Other - Actual Aggregated;
[2023-01-31T07:45:12.131+0000] {spark_submit.py:495} INFO - optional double Other renewable - Actual Aggregated;
[2023-01-31T07:45:12.131+0000] {spark_submit.py:495} INFO - optional double Solar - Actual Aggregated;
[2023-01-31T07:45:12.131+0000] {spark_submit.py:495} INFO - optional double Waste - Actual Aggregated;
[2023-01-31T07:45:12.131+0000] {spark_submit.py:495} INFO - optional double Wind Offshore - Actual Aggregated;
[2023-01-31T07:45:12.133+0000] {spark_submit.py:495} INFO - optional double Wind Onshore - Actual Aggregated;
[2023-01-31T07:45:12.133+0000] {spark_submit.py:495} INFO - }
[2023-01-31T07:45:12.133+0000] {spark_submit.py:495} INFO - 
[2023-01-31T07:45:12.134+0000] {spark_submit.py:495} INFO - 
[2023-01-31T07:45:12.813+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:12 INFO CodecPool: Got brand-new compressor [.snappy]
[2023-01-31T07:45:19.380+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:19 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675150846386-5377ac7c-030c-48c1-9072-d4b410b36df9/_temporary/0/_temporary/' directory.
[2023-01-31T07:45:19.384+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:19 INFO FileOutputCommitter: Saved output of task 'attempt_202301310745106228840720220338155_0008_m_000000_8' to gs://entsoe_temp_1009/.spark-bigquery-local-1675150846386-5377ac7c-030c-48c1-9072-d4b410b36df9/_temporary/0/task_202301310745106228840720220338155_0008_m_000000
[2023-01-31T07:45:19.410+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:19 INFO SparkHadoopMapRedUtil: attempt_202301310745106228840720220338155_0008_m_000000_8: Committed. Elapsed time: 1424 ms.
[2023-01-31T07:45:19.476+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:19 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2588 bytes result sent to driver
[2023-01-31T07:45:19.482+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:19 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 7913 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:45:19.483+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:19 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2023-01-31T07:45:19.492+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:19 INFO DAGScheduler: ResultStage 8 (save at BigQueryWriteHelper.java:105) finished in 8.010 s
[2023-01-31T07:45:19.493+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:19 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:45:19.493+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2023-01-31T07:45:19.494+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:19 INFO DAGScheduler: Job 8 finished: save at BigQueryWriteHelper.java:105, took 8.040117 s
[2023-01-31T07:45:19.496+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:19 INFO FileFormatWriter: Start to commit write Job c99abbf7-4bc0-4144-93ab-c9172ef28c48.
[2023-01-31T07:45:20.632+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:20 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675150846386-5377ac7c-030c-48c1-9072-d4b410b36df9/_temporary/0/task_202301310745106228840720220338155_0008_m_000000/' directory.
[2023-01-31T07:45:21.374+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:21 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675150846386-5377ac7c-030c-48c1-9072-d4b410b36df9/' directory.
[2023-01-31T07:45:22.637+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:22 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 81d5fcd0285b:39973 in memory (size: 74.3 KiB, free: 434.4 MiB)
[2023-01-31T07:45:22.900+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:22 INFO FileFormatWriter: Write Job c99abbf7-4bc0-4144-93ab-c9172ef28c48 committed. Elapsed time: 3396 ms.
[2023-01-31T07:45:22.956+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:22 INFO FileFormatWriter: Finished processing stats for write job c99abbf7-4bc0-4144-93ab-c9172ef28c48.
[2023-01-31T07:45:27.052+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:27 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=entsoe_analytics_1009, projectId=rafzul-analytics-1009, tableId=TEST_total_generation_staging}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=null, ignoreUnknownValue=null, sourceUris=[gs://entsoe_temp_1009/.spark-bigquery-local-1675150846386-5377ac7c-030c-48c1-9072-d4b410b36df9/part-00000-ffaf89c2-a50a-4151-b023-fa4748283c14-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=true, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=rafzul-analytics-1009, job=037d4c4b-c720-4594-8603-4e771ae03cd8, location=US}
[2023-01-31T07:45:30.006+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:30 INFO BigQueryClient: Done loading to rafzul-analytics-1009.entsoe_analytics_1009.TEST_total_generation_staging. jobId: JobId{project=rafzul-analytics-1009, job=037d4c4b-c720-4594-8603-4e771ae03cd8, location=US}
[2023-01-31T07:45:31.643+0000] {spark_submit.py:495} INFO - MANTEP!!!!
[2023-01-31T07:45:32.985+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:32 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-31T07:45:33.258+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:33 INFO SparkUI: Stopped Spark web UI at http://81d5fcd0285b:4041
[2023-01-31T07:45:33.914+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:33 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-31T07:45:34.253+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:34 INFO MemoryStore: MemoryStore cleared
[2023-01-31T07:45:34.253+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:34 INFO BlockManager: BlockManager stopped
[2023-01-31T07:45:34.295+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:34 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-31T07:45:34.295+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:34 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-31T07:45:34.379+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:34 INFO SparkContext: Successfully stopped SparkContext
[2023-01-31T07:45:34.382+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:34 INFO ShutdownHookManager: Shutdown hook called
[2023-01-31T07:45:34.388+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-49160a72-d461-486e-9677-ca9e4c642c5e/pyspark-709a1f0f-3b9a-46f4-9c15-6fe7ef4bac25
[2023-01-31T07:45:34.640+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-49160a72-d461-486e-9677-ca9e4c642c5e
[2023-01-31T07:45:34.723+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-c121c4c1-7526-4ff3-8c5d-fb884e0f1976
[2023-01-31T07:45:35.417+0000] {taskinstance.py:1322} INFO - Marking task as SUCCESS. dag_id=entsoe-energydata-backfill, task_id=stage_total_generation, execution_date=20210101T040000, start_date=20230131T073849, end_date=20230131T074535
[2023-01-31T07:45:35.530+0000] {local_task_job.py:159} INFO - Task exited with return code 0
[2023-01-31T07:45:35.654+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T08:21:35.277+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T04:00:00+00:00 [queued]>
[2023-01-31T08:21:35.306+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T04:00:00+00:00 [queued]>
[2023-01-31T08:21:35.306+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T08:21:35.306+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T08:21:35.310+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T08:21:35.349+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 04:00:00+00:00
[2023-01-31T08:21:35.375+0000] {standard_task_runner.py:55} INFO - Started process 2075 to run task
[2023-01-31T08:21:35.397+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T04:00:00+00:00', '--job-id', '924', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpzuj92nvc']
[2023-01-31T08:21:35.405+0000] {standard_task_runner.py:83} INFO - Job 924: Subtask stage_total_generation
[2023-01-31T08:21:35.676+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T04:00:00+00:00 [running]> on host 81d5fcd0285b
[2023-01-31T08:21:35.928+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T04:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T04:00:00+00:00
[2023-01-31T08:21:35.968+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T08:21:35.983+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010400 202101010500 DE_TENNET
[2023-01-31T08:21:56.566+0000] {spark_submit.py:495} INFO - {{ start_date }}
[2023-01-31T08:21:56.567+0000] {spark_submit.py:495} INFO - mulai periode: {{ start_date }}
[2023-01-31T08:21:56.567+0000] {spark_submit.py:495} INFO - selesai periode: {{ end_date }}
[2023-01-31T08:21:56.804+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:56 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T08:21:57.054+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T08:21:57.578+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:57 INFO ResourceUtils: ==============================================================
[2023-01-31T08:21:57.579+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:57 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T08:21:57.579+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:57 INFO ResourceUtils: ==============================================================
[2023-01-31T08:21:57.579+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:57 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T08:21:57.733+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:57 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T08:21:57.765+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:57 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T08:21:57.770+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:57 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T08:21:58.101+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:58 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T08:21:58.102+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:58 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T08:21:58.105+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:58 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T08:21:58.113+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:58 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T08:21:58.132+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T08:22:00.333+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:00 INFO Utils: Successfully started service 'sparkDriver' on port 40775.
[2023-01-31T08:22:00.998+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:00 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T08:22:01.229+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:01 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T08:22:01.572+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:01 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T08:22:01.574+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:01 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T08:22:01.641+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:01 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T08:22:01.974+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:01 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-449e39f4-8afb-4fa4-b81e-6e9ca87ad74c
[2023-01-31T08:22:02.176+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:02 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T08:22:02.398+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:02 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T08:22:04.238+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:04 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-01-31T08:22:04.849+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:04 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://81d5fcd0285b:40775/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675153316776
[2023-01-31T08:22:04.857+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:04 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://81d5fcd0285b:40775/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675153316776
[2023-01-31T08:22:06.217+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:06 INFO Executor: Starting executor ID driver on host 81d5fcd0285b
[2023-01-31T08:22:06.520+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:06 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T08:22:06.792+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:06 INFO Executor: Fetching spark://81d5fcd0285b:40775/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675153316776
[2023-01-31T08:22:07.443+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:07 INFO TransportClientFactory: Successfully created connection to 81d5fcd0285b/172.19.0.5:40775 after 565 ms (0 ms spent in bootstraps)
[2023-01-31T08:22:07.528+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:07 INFO Utils: Fetching spark://81d5fcd0285b:40775/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-15782b57-5435-4e2b-a0ce-07130928cb23/userFiles-53b51ab1-6962-4884-b1d9-ff94bb0c4e13/fetchFileTemp5873598423532977477.tmp
[2023-01-31T08:22:09.190+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:09 INFO Executor: Adding file:/tmp/spark-15782b57-5435-4e2b-a0ce-07130928cb23/userFiles-53b51ab1-6962-4884-b1d9-ff94bb0c4e13/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T08:22:09.191+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:09 INFO Executor: Fetching spark://81d5fcd0285b:40775/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675153316776
[2023-01-31T08:22:09.191+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:09 INFO Utils: Fetching spark://81d5fcd0285b:40775/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-15782b57-5435-4e2b-a0ce-07130928cb23/userFiles-53b51ab1-6962-4884-b1d9-ff94bb0c4e13/fetchFileTemp15064632572910646398.tmp
[2023-01-31T08:22:09.980+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:09 INFO Executor: Adding file:/tmp/spark-15782b57-5435-4e2b-a0ce-07130928cb23/userFiles-53b51ab1-6962-4884-b1d9-ff94bb0c4e13/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T08:22:10.067+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:10 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43721.
[2023-01-31T08:22:10.068+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:10 INFO NettyBlockTransferService: Server created on 81d5fcd0285b:43721
[2023-01-31T08:22:10.073+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:10 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T08:22:10.124+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:10 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 43721, None)
[2023-01-31T08:22:10.150+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:10 INFO BlockManagerMasterEndpoint: Registering block manager 81d5fcd0285b:43721 with 434.4 MiB RAM, BlockManagerId(driver, 81d5fcd0285b, 43721, None)
[2023-01-31T08:22:10.182+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:10 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 43721, None)
[2023-01-31T08:22:10.186+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:10 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 81d5fcd0285b, 43721, None)
[2023-01-31T08:22:16.513+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:16 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T08:22:16.555+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:16 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T08:22:30.083+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:30 INFO InMemoryFileIndex: It took 400 ms to list leaf files for 1 paths.
[2023-01-31T08:22:31.559+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:31 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T08:22:31.900+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:31 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T08:22:31.910+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:31 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 81d5fcd0285b:43721 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T08:22:31.928+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:31 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T08:22:34.719+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:34 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T08:22:34.872+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:34 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T08:22:35.164+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:35 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T08:22:35.299+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:35 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T08:22:35.305+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:35 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T08:22:35.307+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:35 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:22:35.322+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:35 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:22:35.376+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:35 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T08:22:35.941+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:35 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T08:22:35.975+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:35 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T08:22:35.979+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:35 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 81d5fcd0285b:43721 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T08:22:35.982+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:35 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:22:36.061+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:22:36.072+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:36 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T08:22:36.376+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:36 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T08:22:36.460+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:36 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T08:22:36.822+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:36 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010400__202101010500.json:0+9364
[2023-01-31T08:22:37.720+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:37 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3437 bytes result sent to driver
[2023-01-31T08:22:37.811+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:37 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1473 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:22:37.834+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:37 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T08:22:37.869+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:37 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 2.233 s
[2023-01-31T08:22:37.896+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:37 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:22:37.900+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T08:22:37.907+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:37 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 2.741552 s
[2023-01-31T08:22:42.294+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:42 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 81d5fcd0285b:43721 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T08:22:42.414+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:42 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 81d5fcd0285b:43721 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T08:23:00.338+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:00 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:00.344+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:00 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T08:23:00.354+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:00 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:03.861+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:03 INFO CodeGenerator: Code generated in 2098.07634 ms
[2023-01-31T08:23:03.890+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:03 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T08:23:03.981+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:03 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T08:23:03.993+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:03 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 81d5fcd0285b:43721 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T08:23:04.003+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:04 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T08:23:04.071+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:04 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:04.316+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:04 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T08:23:04.327+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:04 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T08:23:04.328+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:04 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T08:23:04.328+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:04 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:04.329+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:04 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:04.334+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:04 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T08:23:04.399+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:04 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T08:23:04.462+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:04 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T08:23:04.463+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:04 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 81d5fcd0285b:43721 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T08:23:04.463+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:04 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:04.485+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:04.492+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:04 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T08:23:04.516+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:04 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:04.527+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:04 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T08:23:05.978+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:05 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T08:23:06.618+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:06 INFO CodeGenerator: Code generated in 377.099821 ms
[2023-01-31T08:23:07.843+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:07 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1628 bytes result sent to driver
[2023-01-31T08:23:07.876+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:07 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 3377 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:07.877+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:07 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T08:23:07.881+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:07 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 3.503 s
[2023-01-31T08:23:07.882+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:07 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:07.882+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T08:23:07.894+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:07 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 3.562839 s
[2023-01-31T08:23:08.897+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:08 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:08.903+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:08 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T08:23:08.905+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:08 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:09.435+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:09 INFO CodeGenerator: Code generated in 274.555397 ms
[2023-01-31T08:23:09.563+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:09 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T08:23:09.733+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:09 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T08:23:09.811+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:09 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 81d5fcd0285b:43721 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T08:23:09.813+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:09 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T08:23:09.882+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:10.892+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:10 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T08:23:10.897+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:10 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T08:23:10.897+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:10 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T08:23:10.898+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:10 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:10.898+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:10 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:10.905+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:10 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T08:23:10.915+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:10 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T08:23:10.935+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:10 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T08:23:10.941+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:10 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 81d5fcd0285b:43721 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T08:23:10.945+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:10 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:10.962+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:10.962+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:10 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T08:23:10.971+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:10 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:10.972+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:10 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T08:23:11.177+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:11 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T08:23:11.709+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:11 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1585 bytes result sent to driver
[2023-01-31T08:23:11.765+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:11 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 790 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:11.766+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:11 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T08:23:11.769+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:11 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 0.861 s
[2023-01-31T08:23:11.769+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:11 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:11.770+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T08:23:11.771+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:11 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 0.873491 s
[2023-01-31T08:23:13.763+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:13 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:13.770+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:13 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T08:23:13.781+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:13 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:14.338+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:14 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 81d5fcd0285b:43721 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T08:23:14.486+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:14 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 81d5fcd0285b:43721 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T08:23:16.270+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:16 INFO CodeGenerator: Code generated in 1234.970546 ms
[2023-01-31T08:23:16.287+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:16 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T08:23:16.358+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:16 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T08:23:16.368+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:16 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 81d5fcd0285b:43721 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T08:23:16.371+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:16 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T08:23:16.391+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:16 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:16.522+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:16 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T08:23:16.523+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:16 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T08:23:16.524+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:16 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T08:23:16.524+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:16 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:16.524+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:16 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:16.538+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:16 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T08:23:16.653+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:16 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.7 MiB)
[2023-01-31T08:23:16.736+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:16 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.7 MiB)
[2023-01-31T08:23:16.737+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:16 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 81d5fcd0285b:43721 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T08:23:16.740+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:16 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:16.903+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:16.916+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:16 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T08:23:16.926+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:16 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:16.935+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:16 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T08:23:17.299+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:17 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T08:23:18.115+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:18 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-01-31T08:23:18.164+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:18 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 1242 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:18.165+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:18 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T08:23:18.185+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:18 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 1.638 s
[2023-01-31T08:23:18.187+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:18 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:18.187+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T08:23:18.194+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:18 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 1.681987 s
[2023-01-31T08:23:19.453+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:19 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:19.461+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:19 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T08:23:19.465+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:19 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:19.818+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:19 INFO CodeGenerator: Code generated in 180.320789 ms
[2023-01-31T08:23:20.173+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.5 MiB)
[2023-01-31T08:23:20.558+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T08:23:20.559+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 81d5fcd0285b:43721 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T08:23:20.564+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T08:23:20.569+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:21.252+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T08:23:21.253+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T08:23:21.253+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T08:23:21.253+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:21.254+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:21.269+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T08:23:21.327+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-31T08:23:21.364+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-01-31T08:23:21.367+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 81d5fcd0285b:43721 (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T08:23:21.382+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:21.400+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:21.415+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T08:23:21.427+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:21.427+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T08:23:21.545+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T08:23:22.207+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:22 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1586 bytes result sent to driver
[2023-01-31T08:23:22.218+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:22 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 793 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:22.218+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:22 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T08:23:22.219+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:22 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 0.954 s
[2023-01-31T08:23:22.219+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:22 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:22.219+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T08:23:22.220+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:22 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 0.979546 s
[2023-01-31T08:23:25.550+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:25.554+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T08:23:25.554+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:26.338+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO CodeGenerator: Code generated in 414.473948 ms
[2023-01-31T08:23:26.370+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.2 MiB)
[2023-01-31T08:23:26.466+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T08:23:26.472+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 81d5fcd0285b:43721 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T08:23:26.473+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T08:23:26.482+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:26.573+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T08:23:26.575+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) with 1 output partitions
[2023-01-31T08:23:26.576+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201)
[2023-01-31T08:23:26.576+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:26.576+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:26.581+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201), which has no missing parents
[2023-01-31T08:23:26.600+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.1 MiB)
[2023-01-31T08:23:26.631+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.1 MiB)
[2023-01-31T08:23:26.634+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 81d5fcd0285b:43721 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T08:23:26.638+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:26.653+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:26.653+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T08:23:26.663+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:26.663+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T08:23:26.739+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T08:23:27.088+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2537 bytes result sent to driver
[2023-01-31T08:23:27.089+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 427 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:27.089+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T08:23:27.091+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) finished in 0.507 s
[2023-01-31T08:23:27.092+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:27.094+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T08:23:27.095+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201, took 0.521255 s
[2023-01-31T08:23:27.499+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:27.502+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T08:23:27.510+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:28.822+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO CodeGenerator: Code generated in 1140.814704 ms
[2023-01-31T08:23:28.871+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 432.9 MiB)
[2023-01-31T08:23:29.066+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.9 MiB)
[2023-01-31T08:23:29.118+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 81d5fcd0285b:43721 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T08:23:29.138+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T08:23:29.189+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:29.412+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T08:23:29.414+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T08:23:29.416+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T08:23:29.416+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:29.417+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:29.419+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T08:23:29.455+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 432.9 MiB)
[2023-01-31T08:23:29.498+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.8 MiB)
[2023-01-31T08:23:29.503+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 81d5fcd0285b:43721 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T08:23:29.506+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:29.516+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:29.517+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T08:23:29.553+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:29.558+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T08:23:29.628+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T08:23:29.882+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-01-31T08:23:29.886+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 332 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:29.886+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T08:23:29.890+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 0.465 s
[2023-01-31T08:23:29.891+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:29.891+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T08:23:29.891+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 0.476275 s
[2023-01-31T08:23:30.634+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:30 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:30.635+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:30 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T08:23:30.635+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:30 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:31.324+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 81d5fcd0285b:43721 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T08:23:31.519+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 81d5fcd0285b:43721 in memory (size: 12.7 KiB, free: 434.2 MiB)
[2023-01-31T08:23:31.752+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 81d5fcd0285b:43721 in memory (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T08:23:31.841+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 81d5fcd0285b:43721 in memory (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T08:23:31.858+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO CodeGenerator: Code generated in 892.156829 ms
[2023-01-31T08:23:31.918+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.8 MiB)
[2023-01-31T08:23:32.022+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.8 MiB)
[2023-01-31T08:23:32.023+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 81d5fcd0285b:43721 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T08:23:32.034+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T08:23:32.056+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203668 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:32.102+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T08:23:32.103+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T08:23:32.103+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T08:23:32.103+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:32.105+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:32.114+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T08:23:32.153+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.8 MiB)
[2023-01-31T08:23:32.196+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.7 MiB)
[2023-01-31T08:23:32.212+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 81d5fcd0285b:43721 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T08:23:32.223+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:32.232+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:32.238+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T08:23:32.250+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:32.256+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T08:23:32.294+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010400__202101010500.json, range: 0-9364, partition values: [empty row]
[2023-01-31T08:23:32.549+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1729 bytes result sent to driver
[2023-01-31T08:23:32.555+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 306 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:32.560+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T08:23:32.561+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 0.448 s
[2023-01-31T08:23:32.562+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:32.562+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T08:23:32.563+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 0.458607 s
[2023-01-31T08:23:46.940+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:46 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 81d5fcd0285b:43721 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T08:23:47.054+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:47 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 81d5fcd0285b:43721 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T08:23:47.205+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:47 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 81d5fcd0285b:43721 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T08:23:47.290+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:47 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 81d5fcd0285b:43721 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T08:23:47.396+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:47 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 81d5fcd0285b:43721 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T08:23:47.507+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:47 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 81d5fcd0285b:43721 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T08:23:47.749+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:47 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 81d5fcd0285b:43721 in memory (size: 34.5 KiB, free: 434.4 MiB)
