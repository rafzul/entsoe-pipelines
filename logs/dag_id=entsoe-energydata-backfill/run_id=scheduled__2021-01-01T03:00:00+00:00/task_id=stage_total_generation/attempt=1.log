[2023-01-31T05:10:59.476+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T03:00:00+00:00 [queued]>
[2023-01-31T05:10:59.520+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T03:00:00+00:00 [queued]>
[2023-01-31T05:10:59.521+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T05:10:59.522+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T05:10:59.523+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T05:10:59.581+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 03:00:00+00:00
[2023-01-31T05:10:59.613+0000] {standard_task_runner.py:55} INFO - Started process 12256 to run task
[2023-01-31T05:10:59.634+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T03:00:00+00:00', '--job-id', '879', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpvlh0d7w5']
[2023-01-31T05:10:59.643+0000] {standard_task_runner.py:83} INFO - Job 879: Subtask stage_total_generation
[2023-01-31T05:11:00.024+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T03:00:00+00:00 [running]> on host 81d5fcd0285b
[2023-01-31T05:11:00.425+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T03:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T03:00:00+00:00
[2023-01-31T05:11:00.469+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T05:11:00.474+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010300 202101010400 DE_TENNET
[2023-01-31T05:11:34.077+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:34 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T05:11:34.626+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T05:11:35.440+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:35 INFO ResourceUtils: ==============================================================
[2023-01-31T05:11:35.457+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:35 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T05:11:35.459+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:35 INFO ResourceUtils: ==============================================================
[2023-01-31T05:11:35.462+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:35 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T05:11:35.683+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:35 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T05:11:35.740+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:35 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T05:11:35.756+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:35 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T05:11:36.636+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:36 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T05:11:36.642+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:36 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T05:11:36.653+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:36 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T05:11:36.656+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:36 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T05:11:36.673+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:36 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T05:11:40.149+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:40 INFO Utils: Successfully started service 'sparkDriver' on port 45699.
[2023-01-31T05:11:40.545+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:40 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T05:11:40.852+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:40 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T05:11:41.182+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:41 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T05:11:41.187+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:41 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T05:11:41.269+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:41 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T05:11:41.544+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:41 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-bbb6156a-0199-4d8e-9117-a5793bb3cfc8
[2023-01-31T05:11:41.666+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:41 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T05:11:41.831+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:41 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T05:11:44.924+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:44 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-01-31T05:11:45.340+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:45 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://81d5fcd0285b:45699/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675141894040
[2023-01-31T05:11:45.350+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:45 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://81d5fcd0285b:45699/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675141894040
[2023-01-31T05:11:46.211+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:46 INFO Executor: Starting executor ID driver on host 81d5fcd0285b
[2023-01-31T05:11:46.354+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:46 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T05:11:46.541+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:46 INFO Executor: Fetching spark://81d5fcd0285b:45699/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675141894040
[2023-01-31T05:11:47.098+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:47 INFO TransportClientFactory: Successfully created connection to 81d5fcd0285b/172.19.0.9:45699 after 399 ms (0 ms spent in bootstraps)
[2023-01-31T05:11:47.255+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:47 INFO Utils: Fetching spark://81d5fcd0285b:45699/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-3c325c18-8fa8-427a-8915-620ffcd66178/userFiles-257acf5f-6f43-43d6-84df-e95eda652fbd/fetchFileTemp8351834586248949684.tmp
[2023-01-31T05:11:49.168+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:49 INFO Executor: Adding file:/tmp/spark-3c325c18-8fa8-427a-8915-620ffcd66178/userFiles-257acf5f-6f43-43d6-84df-e95eda652fbd/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T05:11:49.174+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:49 INFO Executor: Fetching spark://81d5fcd0285b:45699/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675141894040
[2023-01-31T05:11:49.196+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:49 INFO Utils: Fetching spark://81d5fcd0285b:45699/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-3c325c18-8fa8-427a-8915-620ffcd66178/userFiles-257acf5f-6f43-43d6-84df-e95eda652fbd/fetchFileTemp12406206737731780401.tmp
[2023-01-31T05:11:50.267+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:50 INFO Executor: Adding file:/tmp/spark-3c325c18-8fa8-427a-8915-620ffcd66178/userFiles-257acf5f-6f43-43d6-84df-e95eda652fbd/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T05:11:50.426+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:50 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37711.
[2023-01-31T05:11:50.427+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:50 INFO NettyBlockTransferService: Server created on 81d5fcd0285b:37711
[2023-01-31T05:11:50.449+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:50 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T05:11:50.597+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:50 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 37711, None)
[2023-01-31T05:11:50.640+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:50 INFO BlockManagerMasterEndpoint: Registering block manager 81d5fcd0285b:37711 with 434.4 MiB RAM, BlockManagerId(driver, 81d5fcd0285b, 37711, None)
[2023-01-31T05:11:50.662+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:50 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 37711, None)
[2023-01-31T05:11:50.678+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:50 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 81d5fcd0285b, 37711, None)
[2023-01-31T05:11:59.045+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:59 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T05:11:59.105+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:59 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T05:12:20.852+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:20 INFO InMemoryFileIndex: It took 1692 ms to list leaf files for 1 paths.
[2023-01-31T05:12:22.329+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:22 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T05:12:22.996+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:22 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T05:12:23.005+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:23 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 81d5fcd0285b:37711 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T05:12:23.023+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:23 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T05:12:28.153+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:28 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T05:12:28.303+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:28 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T05:12:28.589+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:28 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T05:12:28.752+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:28 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T05:12:28.759+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:28 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T05:12:28.766+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:28 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:12:28.788+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:28 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:12:28.854+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:28 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T05:12:30.044+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:30 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T05:12:30.133+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:30 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T05:12:30.134+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:30 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 81d5fcd0285b:37711 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T05:12:30.142+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:30 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:12:30.289+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:12:30.302+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:30 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T05:12:31.261+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:31 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T05:12:31.526+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:31 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T05:12:34.532+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:34 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010300__202101010400.json:0+9366
[2023-01-31T05:12:36.899+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:36 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3437 bytes result sent to driver
[2023-01-31T05:12:37.190+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:37 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 6253 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:12:37.218+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:37 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T05:12:37.423+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:37 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 8.075 s
[2023-01-31T05:12:37.552+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:37 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:12:37.560+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T05:12:37.598+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:37 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 9.006841 s
[2023-01-31T05:12:43.554+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:43 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 81d5fcd0285b:37711 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T05:12:43.646+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:43 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 81d5fcd0285b:37711 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T05:13:09.205+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:09 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:09.213+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:09 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:13:09.233+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:09 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:11.770+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:11 INFO CodeGenerator: Code generated in 833.585902 ms
[2023-01-31T05:13:11.805+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:11 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T05:13:11.990+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:11 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T05:13:11.992+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:11 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 81d5fcd0285b:37711 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T05:13:11.995+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:11 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T05:13:12.025+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203670 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:12.260+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:12 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T05:13:12.262+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:12 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T05:13:12.262+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:12 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T05:13:12.262+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:12 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:12.263+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:12 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:12.270+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:12 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T05:13:12.332+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:12 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T05:13:12.357+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:12 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T05:13:12.359+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:12 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 81d5fcd0285b:37711 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T05:13:12.365+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:12 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:12.367+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:12.369+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:12 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T05:13:12.383+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:12 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:12.392+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:12 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T05:13:13.163+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:13 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010300__202101010400.json, range: 0-9366, partition values: [empty row]
[2023-01-31T05:13:13.596+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:13 INFO CodeGenerator: Code generated in 311.396577 ms
[2023-01-31T05:13:14.523+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1628 bytes result sent to driver
[2023-01-31T05:13:14.543+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2172 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:14.549+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 2.258 s
[2023-01-31T05:13:14.549+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:14.555+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T05:13:14.560+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T05:13:14.560+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 2.297403 s
[2023-01-31T05:13:15.062+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:15.063+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:13:15.068+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:15.327+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO CodeGenerator: Code generated in 171.437215 ms
[2023-01-31T05:13:15.365+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T05:13:15.517+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T05:13:15.518+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 81d5fcd0285b:37711 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:13:15.526+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T05:13:15.539+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203670 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:15.655+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T05:13:15.658+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T05:13:15.659+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T05:13:15.659+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:15.659+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:15.663+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T05:13:15.694+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T05:13:15.722+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T05:13:15.733+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 81d5fcd0285b:37711 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T05:13:15.735+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:15.740+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:15.741+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T05:13:15.746+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:15.752+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T05:13:15.827+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010300__202101010400.json, range: 0-9366, partition values: [empty row]
[2023-01-31T05:13:16.088+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:16 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1628 bytes result sent to driver
[2023-01-31T05:13:16.122+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:16 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 365 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:16.130+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:16 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 0.461 s
[2023-01-31T05:13:16.131+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:16 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:16.139+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:16 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T05:13:16.140+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T05:13:16.140+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:16 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 0.486425 s
[2023-01-31T05:13:17.455+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:17.470+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T05:13:17.477+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:19.839+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO CodeGenerator: Code generated in 1632.129317 ms
[2023-01-31T05:13:19.935+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T05:13:20.084+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T05:13:20.093+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 81d5fcd0285b:37711 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:13:20.103+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T05:13:20.126+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203670 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:20.266+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T05:13:20.277+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T05:13:20.277+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T05:13:20.278+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:20.278+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:20.286+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T05:13:20.312+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.6 MiB)
[2023-01-31T05:13:20.347+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.6 MiB)
[2023-01-31T05:13:20.369+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 81d5fcd0285b:37711 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T05:13:20.372+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:20.382+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:20.384+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T05:13:20.392+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:20.392+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T05:13:20.474+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010300__202101010400.json, range: 0-9366, partition values: [empty row]
[2023-01-31T05:13:20.782+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1755 bytes result sent to driver
[2023-01-31T05:13:20.808+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 418 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:20.816+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T05:13:20.824+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 0.527 s
[2023-01-31T05:13:20.824+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:20.825+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T05:13:20.825+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 0.545667 s
[2023-01-31T05:13:20.922+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:20.922+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:13:20.923+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:21.001+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO CodeGenerator: Code generated in 61.798069 ms
[2023-01-31T05:13:21.019+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.4 MiB)
[2023-01-31T05:13:21.081+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T05:13:21.087+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 81d5fcd0285b:37711 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:13:21.090+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T05:13:21.097+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203670 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:21.243+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 81d5fcd0285b:37711 in memory (size: 7.7 KiB, free: 434.2 MiB)
[2023-01-31T05:13:21.302+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T05:13:21.303+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T05:13:21.304+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T05:13:21.304+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:21.305+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:21.329+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T05:13:21.342+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 81d5fcd0285b:37711 in memory (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T05:13:21.353+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-31T05:13:21.371+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-01-31T05:13:21.375+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 81d5fcd0285b:37711 (size: 7.6 KiB, free: 434.3 MiB)
[2023-01-31T05:13:21.378+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:21.388+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 81d5fcd0285b:37711 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T05:13:21.434+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:21.435+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T05:13:21.435+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:21.436+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T05:13:21.480+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010300__202101010400.json, range: 0-9366, partition values: [empty row]
[2023-01-31T05:13:21.728+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1586 bytes result sent to driver
[2023-01-31T05:13:21.732+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 299 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:21.733+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T05:13:21.735+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 0.410 s
[2023-01-31T05:13:21.736+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:21.737+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T05:13:21.738+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 0.433482 s
[2023-01-31T05:13:23.461+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:23.466+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T05:13:23.469+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:24.686+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO CodeGenerator: Code generated in 1108.204176 ms
[2023-01-31T05:13:24.797+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:24 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.3 MiB)
[2023-01-31T05:13:25.283+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T05:13:25.286+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 81d5fcd0285b:37711 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:13:25.327+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T05:13:25.355+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203670 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:25.591+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T05:13:25.592+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) with 1 output partitions
[2023-01-31T05:13:25.593+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201)
[2023-01-31T05:13:25.593+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:25.593+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:25.596+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201), which has no missing parents
[2023-01-31T05:13:25.620+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.2 MiB)
[2023-01-31T05:13:25.623+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.2 MiB)
[2023-01-31T05:13:25.625+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 81d5fcd0285b:37711 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T05:13:25.629+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:25.631+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:25.632+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T05:13:25.639+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:25.639+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T05:13:25.718+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010300__202101010400.json, range: 0-9366, partition values: [empty row]
[2023-01-31T05:13:26.018+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2515 bytes result sent to driver
[2023-01-31T05:13:26.029+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 391 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:26.030+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T05:13:26.035+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) finished in 0.431 s
[2023-01-31T05:13:26.035+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:26.036+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T05:13:26.036+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201, took 0.439914 s
[2023-01-31T05:13:26.466+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:26.467+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T05:13:26.467+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:27.712+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:27 INFO CodeGenerator: Code generated in 1062.776783 ms
[2023-01-31T05:13:27.783+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:27 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 433.0 MiB)
[2023-01-31T05:13:27.974+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:27 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.9 MiB)
[2023-01-31T05:13:27.980+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:27 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 81d5fcd0285b:37711 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:13:27.990+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:27 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T05:13:28.007+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203670 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:28.346+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T05:13:28.351+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T05:13:28.351+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T05:13:28.352+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:28.354+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:28.361+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T05:13:28.405+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 432.9 MiB)
[2023-01-31T05:13:28.432+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.9 MiB)
[2023-01-31T05:13:28.438+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 81d5fcd0285b:37711 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T05:13:28.442+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:28.449+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:28.456+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T05:13:28.462+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:28.463+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T05:13:28.568+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010300__202101010400.json, range: 0-9366, partition values: [empty row]
[2023-01-31T05:13:28.976+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-01-31T05:13:28.981+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 520 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:28.983+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T05:13:28.985+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 0.611 s
[2023-01-31T05:13:28.986+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:28.986+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T05:13:28.988+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 0.641324 s
[2023-01-31T05:13:29.425+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:29.426+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T05:13:29.426+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:30.037+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO CodeGenerator: Code generated in 381.035238 ms
[2023-01-31T05:13:30.037+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.7 MiB)
[2023-01-31T05:13:30.072+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.7 MiB)
[2023-01-31T05:13:30.073+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 81d5fcd0285b:37711 (size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T05:13:30.073+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T05:13:30.073+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203670 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:30.239+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T05:13:30.243+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T05:13:30.244+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T05:13:30.244+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:30.245+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:30.249+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 81d5fcd0285b:37711 in memory (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T05:13:30.249+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T05:13:30.262+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.7 MiB)
[2023-01-31T05:13:30.268+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.7 MiB)
[2023-01-31T05:13:30.270+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 81d5fcd0285b:37711 (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T05:13:30.274+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:30.274+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:30.279+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T05:13:30.287+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:30.290+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T05:13:30.324+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010300__202101010400.json, range: 0-9366, partition values: [empty row]
[2023-01-31T05:13:30.325+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 81d5fcd0285b:37711 in memory (size: 7.6 KiB, free: 434.1 MiB)
[2023-01-31T05:13:30.418+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 81d5fcd0285b:37711 in memory (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T05:13:30.517+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1729 bytes result sent to driver
[2023-01-31T05:13:30.521+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 238 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:30.521+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T05:13:30.523+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 0.274 s
[2023-01-31T05:13:30.523+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:30.523+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T05:13:30.525+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 0.284520 s
[2023-01-31T05:13:40.612+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:40 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 81d5fcd0285b:37711 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:13:40.709+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:40 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 81d5fcd0285b:37711 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:13:40.894+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:40 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 81d5fcd0285b:37711 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:13:40.985+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:40 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 81d5fcd0285b:37711 in memory (size: 12.6 KiB, free: 434.3 MiB)
[2023-01-31T05:13:41.157+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:41 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 81d5fcd0285b:37711 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:13:41.213+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:41 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 81d5fcd0285b:37711 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:13:41.337+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:41 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 81d5fcd0285b:37711 in memory (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T05:13:57.345+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:57 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:13:57.691+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:13:57.693+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:13:57.720+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:57 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:13:57.721+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:13:57.722+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:13:57.770+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:13:58.874+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:58 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-31T05:13:58.878+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:58 INFO DAGScheduler: Got job 8 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T05:13:58.879+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:58 INFO DAGScheduler: Final stage: ResultStage 8 (save at BigQueryWriteHelper.java:105)
[2023-01-31T05:13:58.879+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:58 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:58.881+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:58 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:58.888+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:58 INFO DAGScheduler: Submitting ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T05:13:59.098+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:59 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 209.9 KiB, free 434.0 MiB)
[2023-01-31T05:13:59.198+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:59 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 74.3 KiB, free 433.9 MiB)
[2023-01-31T05:13:59.203+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:59 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 81d5fcd0285b:37711 (size: 74.3 KiB, free: 434.3 MiB)
[2023-01-31T05:13:59.204+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:59 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:59.209+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:59.210+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:59 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2023-01-31T05:13:59.232+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:59 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7484 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:59.241+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:59 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2023-01-31T05:13:59.810+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:13:59.810+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:13:59.815+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:59 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:13:59.816+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:13:59.816+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:13:59.824+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:13:59.850+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:59 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T05:13:59.870+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:59 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T05:14:00.036+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:00 INFO ParquetOutputFormat: Parquet block size to 134217728
[2023-01-31T05:14:00.036+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:00 INFO ParquetOutputFormat: Validation is off
[2023-01-31T05:14:00.037+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:00 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-01-31T05:14:00.053+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:00 INFO ParquetOutputFormat: Parquet properties are:
[2023-01-31T05:14:00.053+0000] {spark_submit.py:495} INFO - Parquet page size to 1048576
[2023-01-31T05:14:00.053+0000] {spark_submit.py:495} INFO - Parquet dictionary page size to 1048576
[2023-01-31T05:14:00.054+0000] {spark_submit.py:495} INFO - Dictionary is true
[2023-01-31T05:14:00.054+0000] {spark_submit.py:495} INFO - Writer version is: PARQUET_1_0
[2023-01-31T05:14:00.054+0000] {spark_submit.py:495} INFO - Page size checking is: estimated
[2023-01-31T05:14:00.055+0000] {spark_submit.py:495} INFO - Min row count for page size check is: 100
[2023-01-31T05:14:00.055+0000] {spark_submit.py:495} INFO - Max row count for page size check is: 10000
[2023-01-31T05:14:00.055+0000] {spark_submit.py:495} INFO - Truncate length for column indexes is: 64
[2023-01-31T05:14:00.056+0000] {spark_submit.py:495} INFO - Truncate length for statistics min/max  is: 2147483647
[2023-01-31T05:14:00.056+0000] {spark_submit.py:495} INFO - Bloom filter enabled: false
[2023-01-31T05:14:00.056+0000] {spark_submit.py:495} INFO - Max Bloom filter size for a column is 1048576
[2023-01-31T05:14:00.057+0000] {spark_submit.py:495} INFO - Bloom filter expected number of distinct values are: null
[2023-01-31T05:14:00.057+0000] {spark_submit.py:495} INFO - Page row count limit to 20000
[2023-01-31T05:14:00.057+0000] {spark_submit.py:495} INFO - Writing page checksums is: on
[2023-01-31T05:14:00.179+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:00 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-01-31T05:14:00.181+0000] {spark_submit.py:495} INFO - {
[2023-01-31T05:14:00.181+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-31T05:14:00.182+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-31T05:14:00.182+0000] {spark_submit.py:495} INFO - "name" : "measured_at",
[2023-01-31T05:14:00.183+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T05:14:00.183+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T05:14:00.184+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:00.184+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:00.184+0000] {spark_submit.py:495} INFO - "name" : "created_at",
[2023-01-31T05:14:00.186+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T05:14:00.187+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T05:14:00.187+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:00.188+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:00.188+0000] {spark_submit.py:495} INFO - "name" : "Biomass - Actual Aggregated",
[2023-01-31T05:14:00.189+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:00.191+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:00.191+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:00.192+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:00.192+0000] {spark_submit.py:495} INFO - "name" : "Fossil Brown coal/Lignite - Actual Aggregated",
[2023-01-31T05:14:00.193+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:00.193+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:00.194+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:00.194+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:00.195+0000] {spark_submit.py:495} INFO - "name" : "Fossil Gas - Actual Aggregated",
[2023-01-31T05:14:00.195+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:00.195+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:00.196+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:00.196+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:00.197+0000] {spark_submit.py:495} INFO - "name" : "Fossil Hard coal - Actual Aggregated",
[2023-01-31T05:14:00.197+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:00.198+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:00.198+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:00.199+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:00.199+0000] {spark_submit.py:495} INFO - "name" : "Fossil Oil - Actual Aggregated",
[2023-01-31T05:14:00.200+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:00.200+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:00.201+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:00.201+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:00.202+0000] {spark_submit.py:495} INFO - "name" : "Geothermal - Actual Aggregated",
[2023-01-31T05:14:00.202+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:00.203+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:00.203+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:00.205+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:00.211+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Aggregated",
[2023-01-31T05:14:00.212+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:00.213+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:00.213+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:00.213+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:00.214+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Consumption",
[2023-01-31T05:14:00.214+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:00.216+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:00.217+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:00.217+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:00.218+0000] {spark_submit.py:495} INFO - "name" : "Hydro Run-of-river and poundage - Actual Aggregated",
[2023-01-31T05:14:00.219+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:00.220+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:00.222+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:00.222+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:00.223+0000] {spark_submit.py:495} INFO - "name" : "Hydro Water Reservoir - Actual Aggregated",
[2023-01-31T05:14:00.228+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:00.228+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:00.229+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:00.229+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:00.230+0000] {spark_submit.py:495} INFO - "name" : "Nuclear - Actual Aggregated",
[2023-01-31T05:14:00.230+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:00.231+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:00.231+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:00.231+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:00.232+0000] {spark_submit.py:495} INFO - "name" : "Other - Actual Aggregated",
[2023-01-31T05:14:00.232+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:00.233+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:00.234+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:00.235+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:00.235+0000] {spark_submit.py:495} INFO - "name" : "Other renewable - Actual Aggregated",
[2023-01-31T05:14:00.236+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:00.236+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:00.236+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:00.237+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:00.237+0000] {spark_submit.py:495} INFO - "name" : "Solar - Actual Aggregated",
[2023-01-31T05:14:00.238+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:00.238+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:00.238+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:00.239+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:00.239+0000] {spark_submit.py:495} INFO - "name" : "Waste - Actual Aggregated",
[2023-01-31T05:14:00.240+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:00.240+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:00.240+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:00.241+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:00.241+0000] {spark_submit.py:495} INFO - "name" : "Wind Offshore - Actual Aggregated",
[2023-01-31T05:14:00.242+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:00.242+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:00.242+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:00.243+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:00.243+0000] {spark_submit.py:495} INFO - "name" : "Wind Onshore - Actual Aggregated",
[2023-01-31T05:14:00.244+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:00.244+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:00.244+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:00.245+0000] {spark_submit.py:495} INFO - } ]
[2023-01-31T05:14:00.245+0000] {spark_submit.py:495} INFO - }
[2023-01-31T05:14:00.246+0000] {spark_submit.py:495} INFO - and corresponding Parquet message type:
[2023-01-31T05:14:00.246+0000] {spark_submit.py:495} INFO - message spark_schema {
[2023-01-31T05:14:00.253+0000] {spark_submit.py:495} INFO - required int96 measured_at;
[2023-01-31T05:14:00.256+0000] {spark_submit.py:495} INFO - required int96 created_at;
[2023-01-31T05:14:00.257+0000] {spark_submit.py:495} INFO - optional double Biomass - Actual Aggregated;
[2023-01-31T05:14:00.257+0000] {spark_submit.py:495} INFO - optional double Fossil Brown coal/Lignite - Actual Aggregated;
[2023-01-31T05:14:00.258+0000] {spark_submit.py:495} INFO - optional double Fossil Gas - Actual Aggregated;
[2023-01-31T05:14:00.259+0000] {spark_submit.py:495} INFO - optional double Fossil Hard coal - Actual Aggregated;
[2023-01-31T05:14:00.260+0000] {spark_submit.py:495} INFO - optional double Fossil Oil - Actual Aggregated;
[2023-01-31T05:14:00.260+0000] {spark_submit.py:495} INFO - optional double Geothermal - Actual Aggregated;
[2023-01-31T05:14:00.261+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Aggregated;
[2023-01-31T05:14:00.261+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Consumption;
[2023-01-31T05:14:00.261+0000] {spark_submit.py:495} INFO - optional double Hydro Run-of-river and poundage - Actual Aggregated;
[2023-01-31T05:14:00.262+0000] {spark_submit.py:495} INFO - optional double Hydro Water Reservoir - Actual Aggregated;
[2023-01-31T05:14:00.263+0000] {spark_submit.py:495} INFO - optional double Nuclear - Actual Aggregated;
[2023-01-31T05:14:00.263+0000] {spark_submit.py:495} INFO - optional double Other - Actual Aggregated;
[2023-01-31T05:14:00.264+0000] {spark_submit.py:495} INFO - optional double Other renewable - Actual Aggregated;
[2023-01-31T05:14:00.264+0000] {spark_submit.py:495} INFO - optional double Solar - Actual Aggregated;
[2023-01-31T05:14:00.265+0000] {spark_submit.py:495} INFO - optional double Waste - Actual Aggregated;
[2023-01-31T05:14:00.269+0000] {spark_submit.py:495} INFO - optional double Wind Offshore - Actual Aggregated;
[2023-01-31T05:14:00.269+0000] {spark_submit.py:495} INFO - optional double Wind Onshore - Actual Aggregated;
[2023-01-31T05:14:00.270+0000] {spark_submit.py:495} INFO - }
[2023-01-31T05:14:00.271+0000] {spark_submit.py:495} INFO - 
[2023-01-31T05:14:00.271+0000] {spark_submit.py:495} INFO - 
[2023-01-31T05:14:01.317+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO CodecPool: Got brand-new compressor [.snappy]
[2023-01-31T05:14:06.672+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675141905687-bfc89ec6-a6ad-4f50-8687-f2c6e286fa85/_temporary/0/_temporary/' directory.
[2023-01-31T05:14:06.673+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO FileOutputCommitter: Saved output of task 'attempt_202301310513584443386230288361947_0008_m_000000_8' to gs://entsoe_temp_1009/.spark-bigquery-local-1675141905687-bfc89ec6-a6ad-4f50-8687-f2c6e286fa85/_temporary/0/task_202301310513584443386230288361947_0008_m_000000
[2023-01-31T05:14:06.676+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO SparkHadoopMapRedUtil: attempt_202301310513584443386230288361947_0008_m_000000_8: Committed. Elapsed time: 1408 ms.
[2023-01-31T05:14:06.706+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2588 bytes result sent to driver
[2023-01-31T05:14:06.714+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 7499 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:14:06.715+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2023-01-31T05:14:06.733+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO DAGScheduler: ResultStage 8 (save at BigQueryWriteHelper.java:105) finished in 7.818 s
[2023-01-31T05:14:06.733+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:14:06.734+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2023-01-31T05:14:06.736+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO DAGScheduler: Job 8 finished: save at BigQueryWriteHelper.java:105, took 7.860547 s
[2023-01-31T05:14:06.740+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:06 INFO FileFormatWriter: Start to commit write Job bc0165b8-4ece-47d5-9cb2-b2df9edd5622.
[2023-01-31T05:14:07.925+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:07 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675141905687-bfc89ec6-a6ad-4f50-8687-f2c6e286fa85/_temporary/0/task_202301310513584443386230288361947_0008_m_000000/' directory.
[2023-01-31T05:14:08.758+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:08 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675141905687-bfc89ec6-a6ad-4f50-8687-f2c6e286fa85/' directory.
[2023-01-31T05:14:09.521+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:09 INFO FileFormatWriter: Write Job bc0165b8-4ece-47d5-9cb2-b2df9edd5622 committed. Elapsed time: 2771 ms.
[2023-01-31T05:14:09.577+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:09 INFO FileFormatWriter: Finished processing stats for write job bc0165b8-4ece-47d5-9cb2-b2df9edd5622.
[2023-01-31T05:14:09.959+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:09 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 81d5fcd0285b:37711 in memory (size: 74.3 KiB, free: 434.4 MiB)
[2023-01-31T05:14:11.770+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:11 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=entsoe_analytics_1009, projectId=rafzul-analytics-1009, tableId=TEST_total_generation_staging}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=null, ignoreUnknownValue=null, sourceUris=[gs://entsoe_temp_1009/.spark-bigquery-local-1675141905687-bfc89ec6-a6ad-4f50-8687-f2c6e286fa85/part-00000-ad5f68f0-64e8-43b1-a373-073ca2cf4f2b-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=true, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=rafzul-analytics-1009, job=5b09f2e6-b7c4-4bc5-b36c-96e45d749710, location=US}
[2023-01-31T05:14:17.292+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:17 INFO BigQueryClient: Done loading to rafzul-analytics-1009.entsoe_analytics_1009.TEST_total_generation_staging. jobId: JobId{project=rafzul-analytics-1009, job=5b09f2e6-b7c4-4bc5-b36c-96e45d749710, location=US}
[2023-01-31T05:14:17.839+0000] {spark_submit.py:495} INFO - MANTEP!!!!
[2023-01-31T05:14:18.081+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:18 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-31T05:14:18.105+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:18 INFO SparkUI: Stopped Spark web UI at http://81d5fcd0285b:4040
[2023-01-31T05:14:18.138+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:18 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-31T05:14:18.165+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:18 INFO MemoryStore: MemoryStore cleared
[2023-01-31T05:14:18.166+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:18 INFO BlockManager: BlockManager stopped
[2023-01-31T05:14:18.173+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:18 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-31T05:14:18.179+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:18 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-31T05:14:18.190+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:18 INFO SparkContext: Successfully stopped SparkContext
[2023-01-31T05:14:18.191+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:18 INFO ShutdownHookManager: Shutdown hook called
[2023-01-31T05:14:18.192+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-3c325c18-8fa8-427a-8915-620ffcd66178/pyspark-4389868b-dd4d-4970-92e1-366f12c42785
[2023-01-31T05:14:18.205+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-7094af15-9413-440e-9d94-9a33cd1c61ca
[2023-01-31T05:14:18.215+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-3c325c18-8fa8-427a-8915-620ffcd66178
[2023-01-31T05:14:18.425+0000] {taskinstance.py:1322} INFO - Marking task as SUCCESS. dag_id=entsoe-energydata-backfill, task_id=stage_total_generation, execution_date=20210101T030000, start_date=20230131T051059, end_date=20230131T051418
[2023-01-31T05:14:18.474+0000] {local_task_job.py:159} INFO - Task exited with return code 0
[2023-01-31T05:14:18.505+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T05:30:41.659+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T03:00:00+00:00 [queued]>
[2023-01-31T05:30:41.784+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T03:00:00+00:00 [queued]>
[2023-01-31T05:30:41.796+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T05:30:41.797+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T05:30:41.798+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T05:30:41.962+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 03:00:00+00:00
[2023-01-31T05:30:42.062+0000] {standard_task_runner.py:55} INFO - Started process 15739 to run task
[2023-01-31T05:30:42.165+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T03:00:00+00:00', '--job-id', '895', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpgt8qu0q4']
[2023-01-31T05:30:42.224+0000] {standard_task_runner.py:83} INFO - Job 895: Subtask stage_total_generation
[2023-01-31T05:30:43.139+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T03:00:00+00:00 [running]> on host 81d5fcd0285b
[2023-01-31T05:30:44.040+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T03:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T03:00:00+00:00
[2023-01-31T05:30:44.098+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T05:30:44.125+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010300 202101010400 DE_TENNET
[2023-01-31T05:31:48.531+0000] {spark_submit.py:495} INFO - 23/01/31 05:31:48 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T05:31:49.641+0000] {spark_submit.py:495} INFO - 23/01/31 05:31:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T05:31:52.297+0000] {spark_submit.py:495} INFO - 23/01/31 05:31:52 INFO ResourceUtils: ==============================================================
[2023-01-31T05:31:52.341+0000] {spark_submit.py:495} INFO - 23/01/31 05:31:52 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T05:31:52.349+0000] {spark_submit.py:495} INFO - 23/01/31 05:31:52 INFO ResourceUtils: ==============================================================
[2023-01-31T05:31:52.369+0000] {spark_submit.py:495} INFO - 23/01/31 05:31:52 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T05:31:52.818+0000] {spark_submit.py:495} INFO - 23/01/31 05:31:52 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T05:31:52.965+0000] {spark_submit.py:495} INFO - 23/01/31 05:31:52 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T05:31:52.983+0000] {spark_submit.py:495} INFO - 23/01/31 05:31:52 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T05:31:54.447+0000] {spark_submit.py:495} INFO - 23/01/31 05:31:54 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T05:31:54.465+0000] {spark_submit.py:495} INFO - 23/01/31 05:31:54 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T05:31:54.470+0000] {spark_submit.py:495} INFO - 23/01/31 05:31:54 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T05:31:54.484+0000] {spark_submit.py:495} INFO - 23/01/31 05:31:54 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T05:31:54.489+0000] {spark_submit.py:495} INFO - 23/01/31 05:31:54 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T05:32:00.180+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:00 INFO Utils: Successfully started service 'sparkDriver' on port 46649.
[2023-01-31T05:32:00.694+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:00 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T05:32:00.969+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:00 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T05:32:01.286+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:01 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T05:32:01.294+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:01 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T05:32:01.327+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:01 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T05:32:01.800+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:01 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-a4638217-b5ef-46ff-8619-a481034d6fcb
[2023-01-31T05:32:03.139+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:03 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T05:32:03.817+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:03 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T05:32:09.686+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:09 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-01-31T05:32:10.981+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:10 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://81d5fcd0285b:46649/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675143108361
[2023-01-31T05:32:11.023+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:11 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://81d5fcd0285b:46649/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675143108361
[2023-01-31T05:32:12.595+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:12 INFO Executor: Starting executor ID driver on host 81d5fcd0285b
[2023-01-31T05:32:13.263+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:13 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T05:32:14.397+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:14 INFO Executor: Fetching spark://81d5fcd0285b:46649/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675143108361
[2023-01-31T05:32:17.019+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:17 INFO TransportClientFactory: Successfully created connection to 81d5fcd0285b/172.19.0.9:46649 after 1573 ms (0 ms spent in bootstraps)
[2023-01-31T05:32:17.162+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:17 INFO Utils: Fetching spark://81d5fcd0285b:46649/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-6d4d3071-39b8-421c-9701-0af8084ae99e/userFiles-0083e526-6dbb-4e57-a22e-80bf5b032fd1/fetchFileTemp2494790643849862630.tmp
[2023-01-31T05:32:23.423+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:23 INFO Executor: Adding file:/tmp/spark-6d4d3071-39b8-421c-9701-0af8084ae99e/userFiles-0083e526-6dbb-4e57-a22e-80bf5b032fd1/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T05:32:23.445+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:23 INFO Executor: Fetching spark://81d5fcd0285b:46649/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675143108361
[2023-01-31T05:32:23.471+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:23 INFO Utils: Fetching spark://81d5fcd0285b:46649/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-6d4d3071-39b8-421c-9701-0af8084ae99e/userFiles-0083e526-6dbb-4e57-a22e-80bf5b032fd1/fetchFileTemp1116146976940157887.tmp
[2023-01-31T05:32:30.041+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:30 INFO Executor: Adding file:/tmp/spark-6d4d3071-39b8-421c-9701-0af8084ae99e/userFiles-0083e526-6dbb-4e57-a22e-80bf5b032fd1/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T05:32:30.458+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:30 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34161.
[2023-01-31T05:32:30.460+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:30 INFO NettyBlockTransferService: Server created on 81d5fcd0285b:34161
[2023-01-31T05:32:30.545+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:30 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T05:32:31.021+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:31 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 34161, None)
[2023-01-31T05:32:31.101+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:31 INFO BlockManagerMasterEndpoint: Registering block manager 81d5fcd0285b:34161 with 434.4 MiB RAM, BlockManagerId(driver, 81d5fcd0285b, 34161, None)
[2023-01-31T05:32:31.309+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:31 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 34161, None)
[2023-01-31T05:32:31.377+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:31 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 81d5fcd0285b, 34161, None)
[2023-01-31T05:32:32.048+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:32 INFO Executor: Told to re-register on heartbeat
[2023-01-31T05:32:32.055+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:32 INFO BlockManager: BlockManager BlockManagerId(driver, 81d5fcd0285b, 34161, None) re-registering with master
[2023-01-31T05:32:32.069+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:32 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 34161, None)
[2023-01-31T05:32:32.137+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:32 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 34161, None)
[2023-01-31T05:32:32.145+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:32 INFO BlockManager: Reporting 0 blocks to the master.
[2023-01-31T05:32:51.370+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:51 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T05:32:51.752+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:51 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T05:33:53.856+0000] {spark_submit.py:495} INFO - 23/01/31 05:33:53 INFO InMemoryFileIndex: It took 2566 ms to list leaf files for 1 paths.
[2023-01-31T05:34:01.041+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:01 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T05:34:05.714+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:05 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T05:34:05.795+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:05 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 81d5fcd0285b:34161 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T05:34:05.955+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:05 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T05:34:21.058+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:21 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T05:34:21.548+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:21 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T05:34:21.928+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:21 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T05:34:23.135+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:23 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T05:34:23.139+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:23 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T05:34:23.186+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:23 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:34:23.209+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:23 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:34:23.375+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:23 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T05:34:25.421+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:25 INFO AsyncEventQueue: Process of event SparkListenerJobStart(0,1675143263213,ArraySeq(org.apache.spark.scheduler.StageInfo@77ec39ad),{spark.master=local, spark.driver.port=46649, spark.submit.pyFiles=, spark.app.startTime=1675143108361, spark.rdd.compress=True, spark.executor.id=driver, spark.app.name=gcp_playground, spark.submit.deployMode=client, spark.hadoop.google.cloud.auth.service.account.json.keyfile=/.google/credentials/google_credentials.json, spark.driver.host=81d5fcd0285b, spark.app.id=local-1675143131524, spark.app.submitTime=1675143088317, spark.hadoop.fs.AbstractFileSystem.gs.impl=google.cloud.hadoop.fs.gcs.GoogleHadoopFS, spark.sql.session.timeZone=UTC, spark.hadoop.google.cloud.auth.service.account.enable=true, spark.app.initial.jar.urls=spark://81d5fcd0285b:46649/jars/gcs-connector-hadoop3-latest.jar,spark://81d5fcd0285b:46649/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar, spark.executor.extraJavaOptions=-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED, spark.jars=/opt/spark/jars/gcs-connector-hadoop3-latest.jar, /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar, spark.sql.warehouse.dir=file:/opt/***/spark-warehouse, spark.serializer.objectStreamReset=100, spark.driver.extraJavaOptions=-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED}) by listener AppStatusListener took 2.130519154s.
[2023-01-31T05:34:26.528+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:26 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T05:34:26.587+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:26 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T05:34:26.594+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:26 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 81d5fcd0285b:34161 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T05:34:26.610+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:26 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:34:27.073+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:34:27.079+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:27 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T05:34:28.924+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:28 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T05:34:29.459+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:29 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T05:34:33.173+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:33 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010300__202101010400.json:0+9366
[2023-01-31T05:34:42.246+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:42 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3437 bytes result sent to driver
[2023-01-31T05:34:42.646+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:42 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 14770 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:34:42.685+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:42 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T05:34:42.789+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:42 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 18.113 s
[2023-01-31T05:34:42.828+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:42 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:34:42.831+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T05:34:42.961+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:42 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 21.029551 s
[2023-01-31T05:36:02.789+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:02 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 81d5fcd0285b:34161 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T05:36:02.967+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:02 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 81d5fcd0285b:34161 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T05:36:03.821+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:03 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:36:03.858+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:03 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:36:03.978+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:03 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:36:19.585+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:19 INFO CodeGenerator: Code generated in 8962.595832 ms
[2023-01-31T05:36:19.956+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:19 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T05:36:20.228+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:20 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T05:36:20.273+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:20 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 81d5fcd0285b:34161 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T05:36:20.281+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:20 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T05:36:20.525+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:20 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203670 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:36:22.020+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:22 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T05:36:22.032+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:22 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T05:36:22.038+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:22 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T05:36:22.039+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:22 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:36:22.058+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:22 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:36:22.081+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:22 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T05:36:22.322+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:22 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T05:36:22.487+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:22 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T05:36:22.500+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:22 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 81d5fcd0285b:34161 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T05:36:22.522+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:22 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:36:22.571+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:36:22.574+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:22 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T05:36:23.027+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:23 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:36:23.080+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:23 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T05:36:27.092+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:27 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010300__202101010400.json, range: 0-9366, partition values: [empty row]
[2023-01-31T05:36:31.654+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:31 INFO CodeGenerator: Code generated in 4078.516941 ms
[2023-01-31T05:36:33.470+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:33 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1671 bytes result sent to driver
[2023-01-31T05:36:33.517+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:33 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 10829 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:36:33.519+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:33 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T05:36:33.539+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:33 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 11.281 s
[2023-01-31T05:36:33.543+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:33 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:36:33.548+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T05:36:33.550+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:33 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 11.528769 s
[2023-01-31T05:36:35.407+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:35 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:36:35.411+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:35 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:36:35.425+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:35 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:36:36.903+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:36 INFO CodeGenerator: Code generated in 1146.955918 ms
[2023-01-31T05:36:37.211+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:37 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T05:36:37.799+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:37 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T05:36:37.803+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:37 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 81d5fcd0285b:34161 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:36:37.829+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:37 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T05:36:37.890+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:37 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203670 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:36:38.665+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:38 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T05:36:38.671+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:38 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T05:36:38.673+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:38 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T05:36:38.674+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:38 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:36:38.694+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:38 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:36:38.734+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:38 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T05:36:38.948+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:38 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T05:36:38.974+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:38 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 81d5fcd0285b:34161 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T05:36:38.995+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:38 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T05:36:39.011+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:39 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 81d5fcd0285b:34161 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T05:36:39.020+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:39 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:36:39.039+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:36:39.044+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:39 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T05:36:39.066+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:39 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:36:39.076+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:39 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T05:36:39.238+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:39 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010300__202101010400.json, range: 0-9366, partition values: [empty row]
[2023-01-31T05:36:40.562+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:40 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1585 bytes result sent to driver
[2023-01-31T05:36:40.717+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:40 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1655 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:36:40.726+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:40 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T05:36:40.734+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:40 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 1.961 s
[2023-01-31T05:36:40.754+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:40 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:36:40.757+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T05:36:40.763+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:40 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 2.097890 s
[2023-01-31T05:36:46.125+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:46 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:36:46.349+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:46 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T05:36:46.386+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:46 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:36:52.729+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:52 INFO CodeGenerator: Code generated in 3784.810033 ms
[2023-01-31T05:36:53.133+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:53 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T05:36:54.109+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:54 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T05:36:54.114+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:54 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 81d5fcd0285b:34161 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:36:54.125+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:54 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T05:36:54.145+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:54 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203670 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:36:54.413+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:54 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T05:36:54.414+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:54 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T05:36:54.415+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:54 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T05:36:54.415+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:54 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:36:54.489+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:54 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:36:54.637+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:54 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T05:36:54.906+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:54 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.7 MiB)
[2023-01-31T05:36:55.210+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:55 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.6 MiB)
[2023-01-31T05:36:55.547+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:55 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 81d5fcd0285b:34161 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T05:36:55.553+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:55 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:36:55.581+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:36:55.588+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:55 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T05:36:55.608+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:55 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:36:55.627+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:55 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 81d5fcd0285b:34161 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T05:36:55.635+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:55 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T05:36:56.069+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:56 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010300__202101010400.json, range: 0-9366, partition values: [empty row]
[2023-01-31T05:36:56.970+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:56 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-01-31T05:36:57.021+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:57 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 1401 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:36:57.033+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:57 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 2.357 s
[2023-01-31T05:36:57.035+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:57 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:36:57.045+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:57 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T05:36:57.061+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T05:36:57.062+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:57 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 2.655966 s
[2023-01-31T05:36:58.168+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:58 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:36:58.170+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:58 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:36:58.173+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:58 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:36:59.459+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:59 INFO CodeGenerator: Code generated in 937.362534 ms
[2023-01-31T05:36:59.570+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:59 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.5 MiB)
[2023-01-31T05:37:00.043+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:00 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T05:37:00.105+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:00 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 81d5fcd0285b:34161 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:37:00.159+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:00 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T05:37:00.389+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:00 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203670 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:37:01.676+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:01 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T05:37:01.686+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:01 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T05:37:01.689+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:01 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T05:37:01.691+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:01 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:37:01.697+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:01 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:37:01.750+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:01 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T05:37:01.934+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:01 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-31T05:37:02.045+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:02 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-01-31T05:37:02.125+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:02 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 81d5fcd0285b:34161 (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T05:37:02.169+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:02 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:37:02.230+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:37:02.265+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:02 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T05:37:02.297+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:02 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:37:02.298+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:02 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T05:37:03.050+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:03 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010300__202101010400.json, range: 0-9366, partition values: [empty row]
[2023-01-31T05:37:04.835+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:04 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1586 bytes result sent to driver
[2023-01-31T05:37:04.840+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:04 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 2585 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:37:04.852+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:04 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 3.067 s
[2023-01-31T05:37:04.869+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:04 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T05:37:04.870+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:04 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:37:04.870+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T05:37:04.871+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:04 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 3.182358 s
[2023-01-31T05:37:12.657+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:12 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 81d5fcd0285b:34161 in memory (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T05:37:12.822+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:12 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 81d5fcd0285b:34161 in memory (size: 7.6 KiB, free: 434.3 MiB)
[2023-01-31T05:37:13.819+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:13 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:37:13.941+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:13 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T05:37:13.994+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:13 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:37:18.579+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:18 INFO CodeGenerator: Code generated in 2878.55775 ms
[2023-01-31T05:37:18.596+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:18 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.3 MiB)
[2023-01-31T05:37:18.654+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:18 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T05:37:18.689+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:18 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 81d5fcd0285b:34161 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:37:18.722+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:18 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T05:37:18.781+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:18 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203670 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:37:19.617+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:19 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T05:37:19.627+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:19 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) with 1 output partitions
[2023-01-31T05:37:19.628+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:19 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201)
[2023-01-31T05:37:19.655+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:19 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:37:19.656+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:19 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:37:19.751+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:19 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201), which has no missing parents
[2023-01-31T05:37:20.120+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:20 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.2 MiB)
[2023-01-31T05:37:20.386+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:20 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.2 MiB)
[2023-01-31T05:37:20.406+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:20 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 81d5fcd0285b:34161 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T05:37:20.452+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:20 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:37:20.596+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:37:20.599+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:20 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T05:37:20.635+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:20 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:37:20.645+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:20 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T05:37:21.209+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:21 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010300__202101010400.json, range: 0-9366, partition values: [empty row]
[2023-01-31T05:37:23.505+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:23 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2515 bytes result sent to driver
[2023-01-31T05:37:23.544+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:23 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 2911 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:37:23.593+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:23 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) finished in 3.731 s
[2023-01-31T05:37:23.596+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:23 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:37:23.645+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:23 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T05:37:23.701+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T05:37:23.717+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:23 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201, took 4.093767 s
[2023-01-31T05:37:25.158+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:25 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:37:25.163+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:25 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T05:37:25.166+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:25 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:37:27.622+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:27 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 81d5fcd0285b:34161 in memory (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T05:37:29.155+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO CodeGenerator: Code generated in 2011.567421 ms
[2023-01-31T05:37:29.190+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 433.1 MiB)
[2023-01-31T05:37:29.265+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.0 MiB)
[2023-01-31T05:37:29.270+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 81d5fcd0285b:34161 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:37:29.274+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T05:37:29.290+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203670 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:37:29.437+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T05:37:29.443+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T05:37:29.444+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T05:37:29.444+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:37:29.446+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:37:29.452+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T05:37:29.515+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 433.0 MiB)
[2023-01-31T05:37:29.538+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 433.0 MiB)
[2023-01-31T05:37:29.544+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 81d5fcd0285b:34161 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T05:37:29.554+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:37:29.561+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:37:29.567+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T05:37:29.574+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:37:29.574+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T05:37:29.695+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010300__202101010400.json, range: 0-9366, partition values: [empty row]
[2023-01-31T05:37:30.361+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:30 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-01-31T05:37:30.375+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:30 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 795 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:37:30.377+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:30 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 0.912 s
[2023-01-31T05:37:30.378+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:30 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:37:30.379+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:30 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T05:37:30.382+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T05:37:30.389+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:30 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 0.945976 s
[2023-01-31T05:37:30.929+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:30 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:37:30.931+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:30 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T05:37:30.933+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:30 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:37:32.894+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:32 INFO CodeGenerator: Code generated in 1564.753967 ms
[2023-01-31T05:37:33.041+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:33 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.8 MiB)
[2023-01-31T05:37:33.953+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:33 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.7 MiB)
[2023-01-31T05:37:33.957+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:33 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 81d5fcd0285b:34161 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:37:33.985+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:33 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T05:37:34.088+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:34 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203670 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:37:34.558+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:34 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 81d5fcd0285b:34161 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T05:37:34.817+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:34 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T05:37:34.846+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:34 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T05:37:34.849+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:34 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T05:37:34.853+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:34 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:37:34.857+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:34 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:37:34.866+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:34 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T05:37:34.924+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:34 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.8 MiB)
[2023-01-31T05:37:34.962+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:34 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.7 MiB)
[2023-01-31T05:37:34.975+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:34 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 81d5fcd0285b:34161 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T05:37:34.982+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:34 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:37:34.999+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:37:35.004+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:35 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T05:37:35.025+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:35 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:37:35.037+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:35 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T05:37:35.211+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:35 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010300__202101010400.json, range: 0-9366, partition values: [empty row]
[2023-01-31T05:37:35.854+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:35 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1729 bytes result sent to driver
[2023-01-31T05:37:35.860+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:35 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 839 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:37:35.861+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:35 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T05:37:35.972+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:35 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 0.981 s
[2023-01-31T05:37:35.973+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:35 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:37:35.974+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T05:37:35.974+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:35 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 1.126459 s
[2023-01-31T05:37:40.439+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:40 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 81d5fcd0285b:34161 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:37:40.516+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:40 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 81d5fcd0285b:34161 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T05:37:40.572+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:40 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 81d5fcd0285b:34161 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:37:40.749+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:40 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 81d5fcd0285b:34161 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:37:40.959+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:40 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 81d5fcd0285b:34161 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:38:36.448+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:36 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:38:36.551+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:38:36.552+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:38:36.561+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:36 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:38:36.563+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:38:36.563+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:38:36.582+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:38:37.789+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:37 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-31T05:38:37.794+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:37 INFO DAGScheduler: Got job 8 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T05:38:37.795+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:37 INFO DAGScheduler: Final stage: ResultStage 8 (save at BigQueryWriteHelper.java:105)
[2023-01-31T05:38:37.796+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:37 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:38:37.797+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:37 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:38:37.807+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:37 INFO DAGScheduler: Submitting ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T05:38:38.145+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:38 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 209.9 KiB, free 433.5 MiB)
[2023-01-31T05:38:38.154+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:38 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 74.2 KiB, free 433.4 MiB)
[2023-01-31T05:38:38.174+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:38 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 81d5fcd0285b:34161 (size: 74.2 KiB, free: 434.2 MiB)
[2023-01-31T05:38:38.177+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:38 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:38:38.179+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:38:38.199+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:38 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2023-01-31T05:38:38.224+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:38 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7484 bytes) taskResourceAssignments Map()
[2023-01-31T05:38:38.329+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:38 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2023-01-31T05:38:39.481+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:38:39.482+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:38:39.493+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:39 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:38:39.494+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:38:39.506+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:38:39.577+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:38:39.629+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:39 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T05:38:39.678+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:39 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T05:38:40.138+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:40 INFO ParquetOutputFormat: Parquet block size to 134217728
[2023-01-31T05:38:40.140+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:40 INFO ParquetOutputFormat: Validation is off
[2023-01-31T05:38:40.144+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:40 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-01-31T05:38:40.148+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:40 INFO ParquetOutputFormat: Parquet properties are:
[2023-01-31T05:38:40.149+0000] {spark_submit.py:495} INFO - Parquet page size to 1048576
[2023-01-31T05:38:40.149+0000] {spark_submit.py:495} INFO - Parquet dictionary page size to 1048576
[2023-01-31T05:38:40.150+0000] {spark_submit.py:495} INFO - Dictionary is true
[2023-01-31T05:38:40.152+0000] {spark_submit.py:495} INFO - Writer version is: PARQUET_1_0
[2023-01-31T05:38:40.160+0000] {spark_submit.py:495} INFO - Page size checking is: estimated
[2023-01-31T05:38:40.165+0000] {spark_submit.py:495} INFO - Min row count for page size check is: 100
[2023-01-31T05:38:40.171+0000] {spark_submit.py:495} INFO - Max row count for page size check is: 10000
[2023-01-31T05:38:40.173+0000] {spark_submit.py:495} INFO - Truncate length for column indexes is: 64
[2023-01-31T05:38:40.176+0000] {spark_submit.py:495} INFO - Truncate length for statistics min/max  is: 2147483647
[2023-01-31T05:38:40.180+0000] {spark_submit.py:495} INFO - Bloom filter enabled: false
[2023-01-31T05:38:40.182+0000] {spark_submit.py:495} INFO - Max Bloom filter size for a column is 1048576
[2023-01-31T05:38:40.184+0000] {spark_submit.py:495} INFO - Bloom filter expected number of distinct values are: null
[2023-01-31T05:38:40.187+0000] {spark_submit.py:495} INFO - Page row count limit to 20000
[2023-01-31T05:38:40.192+0000] {spark_submit.py:495} INFO - Writing page checksums is: on
[2023-01-31T05:38:40.339+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:40 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-01-31T05:38:40.340+0000] {spark_submit.py:495} INFO - {
[2023-01-31T05:38:40.340+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-31T05:38:40.342+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-31T05:38:40.343+0000] {spark_submit.py:495} INFO - "name" : "measured_at",
[2023-01-31T05:38:40.351+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T05:38:40.356+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T05:38:40.358+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:40.369+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:40.369+0000] {spark_submit.py:495} INFO - "name" : "created_at",
[2023-01-31T05:38:40.370+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T05:38:40.371+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T05:38:40.372+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:40.372+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:40.405+0000] {spark_submit.py:495} INFO - "name" : "Biomass - Actual Aggregated",
[2023-01-31T05:38:40.405+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:40.406+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:40.407+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:40.407+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:40.444+0000] {spark_submit.py:495} INFO - "name" : "Fossil Brown coal/Lignite - Actual Aggregated",
[2023-01-31T05:38:40.445+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:40.445+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:40.446+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:40.447+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:40.447+0000] {spark_submit.py:495} INFO - "name" : "Fossil Gas - Actual Aggregated",
[2023-01-31T05:38:40.448+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:40.448+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:40.517+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:40.553+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:40.570+0000] {spark_submit.py:495} INFO - "name" : "Fossil Hard coal - Actual Aggregated",
[2023-01-31T05:38:40.590+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:40.590+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:40.601+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:40.640+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:40.643+0000] {spark_submit.py:495} INFO - "name" : "Fossil Oil - Actual Aggregated",
[2023-01-31T05:38:40.665+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:40.683+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:40.688+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:40.692+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:40.697+0000] {spark_submit.py:495} INFO - "name" : "Geothermal - Actual Aggregated",
[2023-01-31T05:38:40.698+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:40.698+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:40.704+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:40.712+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:40.717+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Aggregated",
[2023-01-31T05:38:40.721+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:40.742+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:40.744+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:40.761+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:40.762+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Consumption",
[2023-01-31T05:38:40.762+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:40.763+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:40.763+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:40.764+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:40.764+0000] {spark_submit.py:495} INFO - "name" : "Hydro Run-of-river and poundage - Actual Aggregated",
[2023-01-31T05:38:40.821+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:40.821+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:40.822+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:40.823+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:40.824+0000] {spark_submit.py:495} INFO - "name" : "Hydro Water Reservoir - Actual Aggregated",
[2023-01-31T05:38:40.825+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:40.826+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:40.827+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:40.827+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:40.828+0000] {spark_submit.py:495} INFO - "name" : "Nuclear - Actual Aggregated",
[2023-01-31T05:38:40.829+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:40.831+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:40.836+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:40.838+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:40.839+0000] {spark_submit.py:495} INFO - "name" : "Other - Actual Aggregated",
[2023-01-31T05:38:40.840+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:40.841+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:40.842+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:40.843+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:40.844+0000] {spark_submit.py:495} INFO - "name" : "Other renewable - Actual Aggregated",
[2023-01-31T05:38:40.845+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:40.845+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:40.846+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:40.848+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:40.849+0000] {spark_submit.py:495} INFO - "name" : "Solar - Actual Aggregated",
[2023-01-31T05:38:40.850+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:40.851+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:40.852+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:40.853+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:40.854+0000] {spark_submit.py:495} INFO - "name" : "Waste - Actual Aggregated",
[2023-01-31T05:38:40.855+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:40.856+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:40.857+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:40.858+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:40.858+0000] {spark_submit.py:495} INFO - "name" : "Wind Offshore - Actual Aggregated",
[2023-01-31T05:38:40.859+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:40.860+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:40.860+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:40.878+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:38:40.898+0000] {spark_submit.py:495} INFO - "name" : "Wind Onshore - Actual Aggregated",
[2023-01-31T05:38:40.899+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:38:40.900+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:38:40.901+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:38:40.902+0000] {spark_submit.py:495} INFO - } ]
[2023-01-31T05:38:40.902+0000] {spark_submit.py:495} INFO - }
[2023-01-31T05:38:40.903+0000] {spark_submit.py:495} INFO - and corresponding Parquet message type:
[2023-01-31T05:38:40.904+0000] {spark_submit.py:495} INFO - message spark_schema {
[2023-01-31T05:38:40.904+0000] {spark_submit.py:495} INFO - required int96 measured_at;
[2023-01-31T05:38:40.905+0000] {spark_submit.py:495} INFO - required int96 created_at;
[2023-01-31T05:38:40.906+0000] {spark_submit.py:495} INFO - optional double Biomass - Actual Aggregated;
[2023-01-31T05:38:40.906+0000] {spark_submit.py:495} INFO - optional double Fossil Brown coal/Lignite - Actual Aggregated;
[2023-01-31T05:38:40.907+0000] {spark_submit.py:495} INFO - optional double Fossil Gas - Actual Aggregated;
[2023-01-31T05:38:40.907+0000] {spark_submit.py:495} INFO - optional double Fossil Hard coal - Actual Aggregated;
[2023-01-31T05:38:40.908+0000] {spark_submit.py:495} INFO - optional double Fossil Oil - Actual Aggregated;
[2023-01-31T05:38:40.908+0000] {spark_submit.py:495} INFO - optional double Geothermal - Actual Aggregated;
[2023-01-31T05:38:40.909+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Aggregated;
[2023-01-31T05:38:40.909+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Consumption;
[2023-01-31T05:38:40.910+0000] {spark_submit.py:495} INFO - optional double Hydro Run-of-river and poundage - Actual Aggregated;
[2023-01-31T05:38:40.910+0000] {spark_submit.py:495} INFO - optional double Hydro Water Reservoir - Actual Aggregated;
[2023-01-31T05:38:40.911+0000] {spark_submit.py:495} INFO - optional double Nuclear - Actual Aggregated;
[2023-01-31T05:38:40.911+0000] {spark_submit.py:495} INFO - optional double Other - Actual Aggregated;
[2023-01-31T05:38:40.912+0000] {spark_submit.py:495} INFO - optional double Other renewable - Actual Aggregated;
[2023-01-31T05:38:40.912+0000] {spark_submit.py:495} INFO - optional double Solar - Actual Aggregated;
[2023-01-31T05:38:40.913+0000] {spark_submit.py:495} INFO - optional double Waste - Actual Aggregated;
[2023-01-31T05:38:40.913+0000] {spark_submit.py:495} INFO - optional double Wind Offshore - Actual Aggregated;
[2023-01-31T05:38:40.914+0000] {spark_submit.py:495} INFO - optional double Wind Onshore - Actual Aggregated;
[2023-01-31T05:38:40.915+0000] {spark_submit.py:495} INFO - }
[2023-01-31T05:38:40.933+0000] {spark_submit.py:495} INFO - 
[2023-01-31T05:38:40.933+0000] {spark_submit.py:495} INFO - 
[2023-01-31T05:38:43.973+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:43 INFO CodecPool: Got brand-new compressor [.snappy]
[2023-01-31T05:38:44.577+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:44 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 81d5fcd0285b:34161 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:38:44.740+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:44 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 81d5fcd0285b:34161 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:39:03.674+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:03 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675143131524-64cb3936-1c7f-4df2-a278-8345d2c4c1c7/_temporary/0/_temporary/' directory.
[2023-01-31T05:39:03.678+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:03 INFO FileOutputCommitter: Saved output of task 'attempt_202301310538376408816582472563100_0008_m_000000_8' to gs://entsoe_temp_1009/.spark-bigquery-local-1675143131524-64cb3936-1c7f-4df2-a278-8345d2c4c1c7/_temporary/0/task_202301310538376408816582472563100_0008_m_000000
[2023-01-31T05:39:03.688+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:03 INFO SparkHadoopMapRedUtil: attempt_202301310538376408816582472563100_0008_m_000000_8: Committed. Elapsed time: 4609 ms.
[2023-01-31T05:39:03.797+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:03 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2588 bytes result sent to driver
[2023-01-31T05:39:03.817+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:03 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 25614 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:39:03.824+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:03 INFO DAGScheduler: ResultStage 8 (save at BigQueryWriteHelper.java:105) finished in 25.965 s
[2023-01-31T05:39:03.831+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:03 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2023-01-31T05:39:03.833+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:03 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:39:03.835+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2023-01-31T05:39:03.840+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:03 INFO DAGScheduler: Job 8 finished: save at BigQueryWriteHelper.java:105, took 26.045651 s
[2023-01-31T05:39:03.848+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:03 INFO FileFormatWriter: Start to commit write Job edd6388a-cdb4-452e-ba86-fd49230f9cb1.
[2023-01-31T05:39:05.365+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:05 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675143131524-64cb3936-1c7f-4df2-a278-8345d2c4c1c7/_temporary/0/task_202301310538376408816582472563100_0008_m_000000/' directory.
[2023-01-31T05:39:06.280+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:06 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675143131524-64cb3936-1c7f-4df2-a278-8345d2c4c1c7/' directory.
[2023-01-31T05:39:07.522+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:07 INFO FileFormatWriter: Write Job edd6388a-cdb4-452e-ba86-fd49230f9cb1 committed. Elapsed time: 3661 ms.
[2023-01-31T05:39:07.730+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:07 INFO FileFormatWriter: Finished processing stats for write job edd6388a-cdb4-452e-ba86-fd49230f9cb1.
[2023-01-31T05:39:08.294+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:08 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 81d5fcd0285b:34161 in memory (size: 74.2 KiB, free: 434.4 MiB)
[2023-01-31T05:39:11.933+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:11 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=entsoe_analytics_1009, projectId=rafzul-analytics-1009, tableId=TEST_total_generation_staging}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=null, ignoreUnknownValue=null, sourceUris=[gs://entsoe_temp_1009/.spark-bigquery-local-1675143131524-64cb3936-1c7f-4df2-a278-8345d2c4c1c7/part-00000-1108f01a-9e01-4263-9da3-faaafe96c090-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=true, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=rafzul-analytics-1009, job=8d73b353-2232-43d8-8107-20b2ca1ecb1c, location=US}
[2023-01-31T05:39:16.671+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:16 INFO BigQueryClient: Done loading to rafzul-analytics-1009.entsoe_analytics_1009.TEST_total_generation_staging. jobId: JobId{project=rafzul-analytics-1009, job=8d73b353-2232-43d8-8107-20b2ca1ecb1c, location=US}
[2023-01-31T05:39:19.781+0000] {spark_submit.py:495} INFO - MANTEP!!!!
[2023-01-31T05:39:21.413+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:21 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-31T05:39:21.860+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:21 INFO SparkUI: Stopped Spark web UI at http://81d5fcd0285b:4040
[2023-01-31T05:39:22.128+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:22 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-31T05:39:22.261+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:22 INFO MemoryStore: MemoryStore cleared
[2023-01-31T05:39:22.263+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:22 INFO BlockManager: BlockManager stopped
[2023-01-31T05:39:22.273+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:22 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-31T05:39:22.286+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:22 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-31T05:39:22.347+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:22 INFO SparkContext: Successfully stopped SparkContext
[2023-01-31T05:39:22.361+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:22 INFO ShutdownHookManager: Shutdown hook called
[2023-01-31T05:39:22.367+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-6d4d3071-39b8-421c-9701-0af8084ae99e
[2023-01-31T05:39:22.421+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-d579cd2a-5d9b-4d85-94e2-d20f53f26aed
[2023-01-31T05:39:22.465+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-6d4d3071-39b8-421c-9701-0af8084ae99e/pyspark-b5fa96fb-f115-4bd1-9974-d1716e24d1d8
[2023-01-31T05:39:24.539+0000] {taskinstance.py:1322} INFO - Marking task as SUCCESS. dag_id=entsoe-energydata-backfill, task_id=stage_total_generation, execution_date=20210101T030000, start_date=20230131T053041, end_date=20230131T053924
[2023-01-31T05:39:24.917+0000] {local_task_job.py:159} INFO - Task exited with return code 0
[2023-01-31T05:39:25.240+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T07:39:02.696+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T03:00:00+00:00 [queued]>
[2023-01-31T07:39:02.869+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T03:00:00+00:00 [queued]>
[2023-01-31T07:39:02.870+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T07:39:02.871+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T07:39:02.874+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T07:39:03.153+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 03:00:00+00:00
[2023-01-31T07:39:03.266+0000] {standard_task_runner.py:55} INFO - Started process 25807 to run task
[2023-01-31T07:39:03.357+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T03:00:00+00:00', '--job-id', '910', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpvoueov7i']
[2023-01-31T07:39:03.435+0000] {standard_task_runner.py:83} INFO - Job 910: Subtask stage_total_generation
[2023-01-31T07:39:04.209+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T03:00:00+00:00 [running]> on host 81d5fcd0285b
[2023-01-31T07:39:05.117+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T03:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T03:00:00+00:00
[2023-01-31T07:39:05.366+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T07:39:05.393+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010300 202101010400 DE_TENNET
[2023-01-31T07:40:15.437+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:15 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T07:40:17.537+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T07:40:19.019+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:19 INFO ResourceUtils: ==============================================================
[2023-01-31T07:40:19.029+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:19 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T07:40:19.034+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:19 INFO ResourceUtils: ==============================================================
[2023-01-31T07:40:19.060+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:19 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T07:40:19.399+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:19 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T07:40:19.478+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:19 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T07:40:19.498+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:19 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T07:40:20.337+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:20 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T07:40:20.343+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:20 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T07:40:20.349+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:20 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T07:40:20.356+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:20 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T07:40:20.367+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:20 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T07:40:28.906+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:28 INFO Utils: Successfully started service 'sparkDriver' on port 41451.
[2023-01-31T07:40:30.464+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:30 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T07:40:32.364+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:32 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T07:40:32.753+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:32 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T07:40:32.761+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:32 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T07:40:32.886+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:32 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T07:40:33.313+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:33 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-862f225d-5377-4e2f-a3f1-af85bc79d0b8
[2023-01-31T07:40:34.448+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:34 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T07:40:34.860+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:34 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T07:40:49.288+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:49 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T07:40:49.324+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:49 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2023-01-31T07:40:49.597+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:49 INFO Utils: Successfully started service 'SparkUI' on port 4042.
[2023-01-31T07:40:50.114+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:50 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://81d5fcd0285b:41451/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675150815228
[2023-01-31T07:40:50.130+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:50 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://81d5fcd0285b:41451/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675150815228
[2023-01-31T07:40:52.454+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:52 INFO Executor: Starting executor ID driver on host 81d5fcd0285b
[2023-01-31T07:40:52.549+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:52 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T07:40:52.736+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:52 INFO Executor: Fetching spark://81d5fcd0285b:41451/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675150815228
[2023-01-31T07:40:54.845+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:54 INFO TransportClientFactory: Successfully created connection to 81d5fcd0285b/172.19.0.9:41451 after 1576 ms (0 ms spent in bootstraps)
[2023-01-31T07:40:55.054+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:55 INFO Utils: Fetching spark://81d5fcd0285b:41451/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-9adc8952-75e2-41db-84df-f10c9059cc8c/userFiles-87df5c93-fad1-4bba-b7af-f42a3f3f0768/fetchFileTemp12810661813630043133.tmp
[2023-01-31T07:41:00.413+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:00 INFO Executor: Adding file:/tmp/spark-9adc8952-75e2-41db-84df-f10c9059cc8c/userFiles-87df5c93-fad1-4bba-b7af-f42a3f3f0768/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T07:41:00.445+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:00 INFO Executor: Fetching spark://81d5fcd0285b:41451/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675150815228
[2023-01-31T07:41:00.451+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:00 INFO Utils: Fetching spark://81d5fcd0285b:41451/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-9adc8952-75e2-41db-84df-f10c9059cc8c/userFiles-87df5c93-fad1-4bba-b7af-f42a3f3f0768/fetchFileTemp4301576016061600043.tmp
[2023-01-31T07:41:05.176+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:05 INFO Executor: Adding file:/tmp/spark-9adc8952-75e2-41db-84df-f10c9059cc8c/userFiles-87df5c93-fad1-4bba-b7af-f42a3f3f0768/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T07:41:05.937+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:05 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37465.
[2023-01-31T07:41:05.940+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:05 INFO NettyBlockTransferService: Server created on 81d5fcd0285b:37465
[2023-01-31T07:41:05.949+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:05 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T07:41:06.009+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:06 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 37465, None)
[2023-01-31T07:41:06.181+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:06 INFO BlockManagerMasterEndpoint: Registering block manager 81d5fcd0285b:37465 with 434.4 MiB RAM, BlockManagerId(driver, 81d5fcd0285b, 37465, None)
[2023-01-31T07:41:06.642+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:06 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 37465, None)
[2023-01-31T07:41:06.969+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:06 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 81d5fcd0285b, 37465, None)
[2023-01-31T07:41:11.544+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:11 INFO Executor: Told to re-register on heartbeat
[2023-01-31T07:41:11.587+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:11 INFO BlockManager: BlockManager BlockManagerId(driver, 81d5fcd0285b, 37465, None) re-registering with master
[2023-01-31T07:41:11.589+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:11 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 37465, None)
[2023-01-31T07:41:11.717+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:11 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 37465, None)
[2023-01-31T07:41:11.717+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:11 INFO BlockManager: Reporting 0 blocks to the master.
[2023-01-31T07:41:32.037+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:32 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T07:41:32.181+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:32 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T07:42:18.965+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:18 INFO InMemoryFileIndex: It took 3538 ms to list leaf files for 1 paths.
[2023-01-31T07:42:23.717+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:23 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T07:42:25.615+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:25 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T07:42:25.657+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:25 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 81d5fcd0285b:37465 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T07:42:25.692+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:25 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T07:42:38.071+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:38 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T07:42:38.502+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:38 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T07:42:39.365+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:39 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T07:42:39.658+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:39 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T07:42:39.665+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:39 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T07:42:39.672+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:39 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:42:39.778+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:39 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:42:39.923+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:39 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T07:42:41.301+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:41 INFO AsyncEventQueue: Process of event SparkListenerJobStart(0,1675150959778,ArraySeq(org.apache.spark.scheduler.StageInfo@5b0ad0e4),{spark.master=local, spark.driver.port=41451, spark.submit.pyFiles=, spark.app.startTime=1675150815228, spark.rdd.compress=True, spark.executor.id=driver, spark.app.name=gcp_playground, spark.submit.deployMode=client, spark.hadoop.google.cloud.auth.service.account.json.keyfile=/.google/credentials/google_credentials.json, spark.driver.host=81d5fcd0285b, spark.app.id=local-1675150851009, spark.app.submitTime=1675150800940, spark.hadoop.fs.AbstractFileSystem.gs.impl=google.cloud.hadoop.fs.gcs.GoogleHadoopFS, spark.sql.session.timeZone=UTC, spark.hadoop.google.cloud.auth.service.account.enable=true, spark.app.initial.jar.urls=spark://81d5fcd0285b:41451/jars/gcs-connector-hadoop3-latest.jar,spark://81d5fcd0285b:41451/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar, spark.executor.extraJavaOptions=-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED, spark.jars=/opt/spark/jars/gcs-connector-hadoop3-latest.jar, /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar, spark.sql.warehouse.dir=file:/opt/***/spark-warehouse, spark.serializer.objectStreamReset=100, spark.driver.extraJavaOptions=-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED}) by listener AppStatusListener took 1.429140128s.
[2023-01-31T07:42:41.377+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:41 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T07:42:41.399+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:41 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T07:42:41.405+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:41 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 81d5fcd0285b:37465 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T07:42:41.414+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:41 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:42:41.524+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:42:41.531+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:41 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T07:42:43.778+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:43 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T07:42:44.088+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:44 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T07:42:50.562+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:50 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010300__202101010400.json:0+9366
[2023-01-31T07:42:56.642+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:56 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3480 bytes result sent to driver
[2023-01-31T07:42:56.711+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:56 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 13569 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:42:56.736+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:56 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T07:42:56.769+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:56 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 16.345 s
[2023-01-31T07:42:56.806+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:56 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:42:56.811+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T07:42:56.837+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:56 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 17.466075 s
[2023-01-31T07:43:11.979+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:11 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 81d5fcd0285b:37465 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T07:43:12.089+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:12 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 81d5fcd0285b:37465 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T07:43:24.487+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:24 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:43:24.493+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:24 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T07:43:24.504+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:24 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:43:26.901+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:26 INFO CodeGenerator: Code generated in 1163.675498 ms
[2023-01-31T07:43:26.946+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:26 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T07:43:27.089+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T07:43:27.091+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 81d5fcd0285b:37465 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T07:43:27.094+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T07:43:27.319+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203670 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:43:28.894+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:28 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T07:43:28.902+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:28 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T07:43:28.902+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:28 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T07:43:28.902+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:28 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:43:28.902+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:28 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:43:28.912+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:28 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T07:43:29.023+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T07:43:29.061+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T07:43:29.073+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 81d5fcd0285b:37465 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T07:43:29.083+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:43:29.091+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:43:29.094+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T07:43:29.130+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:43:29.143+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T07:43:29.997+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:29 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010300__202101010400.json, range: 0-9366, partition values: [empty row]
[2023-01-31T07:43:30.532+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:30 INFO CodeGenerator: Code generated in 407.339109 ms
[2023-01-31T07:43:31.608+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:31 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1628 bytes result sent to driver
[2023-01-31T07:43:31.708+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:31 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2595 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:43:31.714+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:31 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 2.732 s
[2023-01-31T07:43:31.714+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:31 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:43:31.748+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:31 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T07:43:31.757+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T07:43:31.757+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:31 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 2.852056 s
[2023-01-31T07:43:32.342+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:43:32.346+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T07:43:32.357+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:43:32.673+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO CodeGenerator: Code generated in 225.366703 ms
[2023-01-31T07:43:32.679+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 81d5fcd0285b:37465 in memory (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T07:43:32.742+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T07:43:32.927+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T07:43:32.942+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 81d5fcd0285b:37465 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:43:32.946+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T07:43:32.962+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203670 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:43:33.375+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T07:43:33.378+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T07:43:33.379+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T07:43:33.381+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:43:33.382+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:43:33.401+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T07:43:33.443+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T07:43:33.464+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T07:43:33.468+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 81d5fcd0285b:37465 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T07:43:33.488+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:43:33.501+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:43:33.502+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T07:43:33.520+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:43:33.529+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T07:43:33.885+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010300__202101010400.json, range: 0-9366, partition values: [empty row]
[2023-01-31T07:43:34.478+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1585 bytes result sent to driver
[2023-01-31T07:43:34.489+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 973 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:43:34.492+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T07:43:34.505+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 1.086 s
[2023-01-31T07:43:34.505+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:43:34.506+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T07:43:34.506+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 1.123967 s
[2023-01-31T07:43:35.199+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:35 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:43:35.207+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:35 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T07:43:35.209+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:35 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:43:37.245+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:37 INFO CodeGenerator: Code generated in 1280.594313 ms
[2023-01-31T07:43:37.445+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:37 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T07:43:37.753+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:37 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T07:43:37.754+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:37 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 81d5fcd0285b:37465 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:43:37.781+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:37 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T07:43:37.781+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:37 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203670 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:43:38.269+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:38 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T07:43:38.291+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:38 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T07:43:38.292+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:38 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T07:43:38.295+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:38 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:43:38.296+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:38 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:43:38.330+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:38 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T07:43:38.506+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:38 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.7 MiB)
[2023-01-31T07:43:38.668+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:38 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.6 MiB)
[2023-01-31T07:43:38.677+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:38 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 81d5fcd0285b:37465 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T07:43:38.687+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:38 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:43:38.712+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:43:38.719+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:38 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T07:43:38.793+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:38 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:43:38.821+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:38 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T07:43:39.329+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:39 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010300__202101010400.json, range: 0-9366, partition values: [empty row]
[2023-01-31T07:43:40.133+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-01-31T07:43:40.172+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 1406 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:43:40.181+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 1.837 s
[2023-01-31T07:43:40.182+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:43:40.196+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T07:43:40.213+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T07:43:40.215+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 1.941748 s
[2023-01-31T07:43:41.108+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:41 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:43:41.128+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:41 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T07:43:41.146+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:41 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:43:41.837+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:41 INFO CodeGenerator: Code generated in 212.469471 ms
[2023-01-31T07:43:41.921+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:41 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.4 MiB)
[2023-01-31T07:43:42.392+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:42 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T07:43:42.456+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:42 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 81d5fcd0285b:37465 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T07:43:42.491+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:42 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T07:43:42.668+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:42 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203670 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:43:44.394+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:44 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T07:43:44.413+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:44 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T07:43:44.414+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:44 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T07:43:44.415+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:44 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:43:44.472+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:44 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:43:44.524+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:44 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T07:43:44.746+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:44 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-31T07:43:45.031+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-01-31T07:43:45.040+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 81d5fcd0285b:37465 (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T07:43:45.089+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:43:45.160+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:43:45.177+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T07:43:45.253+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:43:45.254+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T07:43:45.419+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:45 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010300__202101010400.json, range: 0-9366, partition values: [empty row]
[2023-01-31T07:43:46.754+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:46 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1585 bytes result sent to driver
[2023-01-31T07:43:46.779+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:46 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 1579 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:43:46.780+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:46 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T07:43:46.781+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:46 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 2.243 s
[2023-01-31T07:43:46.782+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:46 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:43:46.850+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T07:43:46.882+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:46 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 2.393571 s
[2023-01-31T07:43:54.050+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:54 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:43:54.099+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:54 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T07:43:54.105+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:54 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:43:56.798+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:56 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 81d5fcd0285b:37465 in memory (size: 7.7 KiB, free: 434.2 MiB)
[2023-01-31T07:43:56.969+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:56 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 81d5fcd0285b:37465 in memory (size: 7.6 KiB, free: 434.3 MiB)
[2023-01-31T07:43:57.277+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:57 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 81d5fcd0285b:37465 in memory (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T07:43:59.422+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:59 INFO CodeGenerator: Code generated in 3718.434314 ms
[2023-01-31T07:43:59.502+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:59 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.3 MiB)
[2023-01-31T07:44:00.220+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:00 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T07:44:00.241+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:00 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 81d5fcd0285b:37465 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T07:44:00.271+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:00 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T07:44:00.468+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:00 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203670 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:44:02.105+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:02 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T07:44:02.133+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:02 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) with 1 output partitions
[2023-01-31T07:44:02.134+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:02 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201)
[2023-01-31T07:44:02.150+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:02 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:44:02.151+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:02 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:44:02.190+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:02 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201), which has no missing parents
[2023-01-31T07:44:02.544+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:02 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.2 MiB)
[2023-01-31T07:44:02.730+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:02 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.2 MiB)
[2023-01-31T07:44:02.797+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:02 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 81d5fcd0285b:37465 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T07:44:02.820+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:02 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:44:02.919+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:44:02.923+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:02 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T07:44:02.988+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:02 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:44:02.992+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:02 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T07:44:03.305+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:03 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010300__202101010400.json, range: 0-9366, partition values: [empty row]
[2023-01-31T07:44:04.605+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2558 bytes result sent to driver
[2023-01-31T07:44:04.632+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 1645 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:44:04.633+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T07:44:04.659+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) finished in 2.393 s
[2023-01-31T07:44:04.664+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:44:04.666+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T07:44:04.667+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:04 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201, took 2.524896 s
[2023-01-31T07:44:06.779+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:06 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:44:06.780+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:06 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T07:44:06.780+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:06 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:44:09.764+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:09 INFO CodeGenerator: Code generated in 2224.971306 ms
[2023-01-31T07:44:10.105+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:10 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 433.0 MiB)
[2023-01-31T07:44:10.703+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:10 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.0 MiB)
[2023-01-31T07:44:10.748+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:10 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 81d5fcd0285b:37465 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T07:44:10.800+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:10 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T07:44:10.895+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:10 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203670 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:44:12.455+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:12 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T07:44:12.463+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:12 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T07:44:12.464+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:12 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T07:44:12.464+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:12 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:44:12.467+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:12 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:44:12.492+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:12 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T07:44:12.559+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:12 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 432.9 MiB)
[2023-01-31T07:44:12.702+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:12 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.9 MiB)
[2023-01-31T07:44:12.714+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:12 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 81d5fcd0285b:37465 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T07:44:12.731+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:12 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:44:12.767+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:44:12.775+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:12 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T07:44:12.855+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:12 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:44:12.873+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:12 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T07:44:13.285+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:13 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010300__202101010400.json, range: 0-9366, partition values: [empty row]
[2023-01-31T07:44:14.881+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:14 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-01-31T07:44:14.917+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:14 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 2038 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:44:14.918+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:14 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T07:44:14.949+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:14 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 2.438 s
[2023-01-31T07:44:14.950+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:14 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:44:14.951+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T07:44:14.964+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:14 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 2.483067 s
[2023-01-31T07:44:17.458+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:17 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:44:17.665+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:17 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T07:44:17.702+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:17 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:44:21.962+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:21 INFO CodeGenerator: Code generated in 2210.974634 ms
[2023-01-31T07:44:22.184+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:22 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.7 MiB)
[2023-01-31T07:44:22.874+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:22 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.7 MiB)
[2023-01-31T07:44:22.882+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:22 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 81d5fcd0285b:37465 (size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T07:44:22.894+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:22 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T07:44:22.949+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:22 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203670 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:44:23.634+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:23 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T07:44:23.645+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:23 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T07:44:23.646+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:23 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T07:44:23.646+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:23 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:44:23.647+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:23 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:44:23.676+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:23 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T07:44:23.825+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:23 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.7 MiB)
[2023-01-31T07:44:23.960+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:23 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.6 MiB)
[2023-01-31T07:44:23.966+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:23 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 81d5fcd0285b:37465 (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T07:44:23.975+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:23 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:44:24.040+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:44:24.041+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:24 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T07:44:24.047+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:24 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:44:24.061+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:24 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T07:44:24.566+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:24 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010300__202101010400.json, range: 0-9366, partition values: [empty row]
[2023-01-31T07:44:25.051+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:25 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1729 bytes result sent to driver
[2023-01-31T07:44:25.079+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:25 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 1020 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:44:25.080+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:25 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T07:44:25.081+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:25 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 1.394 s
[2023-01-31T07:44:25.081+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:25 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:44:25.082+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T07:44:25.095+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:25 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 1.454834 s
[2023-01-31T07:44:28.012+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:28 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 81d5fcd0285b:37465 in memory (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T07:44:28.454+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:28 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 81d5fcd0285b:37465 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T07:44:28.764+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:28 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 81d5fcd0285b:37465 in memory (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T07:44:28.926+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:28 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 81d5fcd0285b:37465 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T07:44:29.061+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:29 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 81d5fcd0285b:37465 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T07:44:29.297+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:29 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 81d5fcd0285b:37465 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:44:29.431+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:29 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 81d5fcd0285b:37465 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:44:29.695+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:29 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 81d5fcd0285b:37465 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:45:05.765+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:05 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 81d5fcd0285b:37465 in memory (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T07:45:14.428+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:14 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T07:45:14.461+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T07:45:14.461+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T07:45:14.463+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:14 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T07:45:14.464+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T07:45:14.464+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T07:45:14.469+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T07:45:15.312+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:15 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-31T07:45:15.315+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:15 INFO DAGScheduler: Got job 8 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T07:45:15.316+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:15 INFO DAGScheduler: Final stage: ResultStage 8 (save at BigQueryWriteHelper.java:105)
[2023-01-31T07:45:15.316+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:15 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:45:15.316+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:15 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:45:15.323+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:15 INFO DAGScheduler: Submitting ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T07:45:15.481+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:15 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 209.9 KiB, free 434.0 MiB)
[2023-01-31T07:45:15.517+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:15 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 74.2 KiB, free 433.9 MiB)
[2023-01-31T07:45:15.525+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:15 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 81d5fcd0285b:37465 (size: 74.2 KiB, free: 434.3 MiB)
[2023-01-31T07:45:15.567+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:15 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:45:15.574+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:45:15.579+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:15 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2023-01-31T07:45:15.612+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:15 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7484 bytes) taskResourceAssignments Map()
[2023-01-31T07:45:15.621+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:15 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2023-01-31T07:45:16.075+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T07:45:16.087+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T07:45:16.088+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:16 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T07:45:16.088+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T07:45:16.089+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T07:45:16.089+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T07:45:16.133+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:16 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T07:45:16.184+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:16 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T07:45:16.391+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:16 INFO ParquetOutputFormat: Parquet block size to 134217728
[2023-01-31T07:45:16.391+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:16 INFO ParquetOutputFormat: Validation is off
[2023-01-31T07:45:16.396+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:16 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-01-31T07:45:16.401+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:16 INFO ParquetOutputFormat: Parquet properties are:
[2023-01-31T07:45:16.401+0000] {spark_submit.py:495} INFO - Parquet page size to 1048576
[2023-01-31T07:45:16.402+0000] {spark_submit.py:495} INFO - Parquet dictionary page size to 1048576
[2023-01-31T07:45:16.402+0000] {spark_submit.py:495} INFO - Dictionary is true
[2023-01-31T07:45:16.402+0000] {spark_submit.py:495} INFO - Writer version is: PARQUET_1_0
[2023-01-31T07:45:16.402+0000] {spark_submit.py:495} INFO - Page size checking is: estimated
[2023-01-31T07:45:16.403+0000] {spark_submit.py:495} INFO - Min row count for page size check is: 100
[2023-01-31T07:45:16.403+0000] {spark_submit.py:495} INFO - Max row count for page size check is: 10000
[2023-01-31T07:45:16.403+0000] {spark_submit.py:495} INFO - Truncate length for column indexes is: 64
[2023-01-31T07:45:16.403+0000] {spark_submit.py:495} INFO - Truncate length for statistics min/max  is: 2147483647
[2023-01-31T07:45:16.403+0000] {spark_submit.py:495} INFO - Bloom filter enabled: false
[2023-01-31T07:45:16.403+0000] {spark_submit.py:495} INFO - Max Bloom filter size for a column is 1048576
[2023-01-31T07:45:16.404+0000] {spark_submit.py:495} INFO - Bloom filter expected number of distinct values are: null
[2023-01-31T07:45:16.404+0000] {spark_submit.py:495} INFO - Page row count limit to 20000
[2023-01-31T07:45:16.404+0000] {spark_submit.py:495} INFO - Writing page checksums is: on
[2023-01-31T07:45:16.932+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:16 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-01-31T07:45:16.933+0000] {spark_submit.py:495} INFO - {
[2023-01-31T07:45:16.933+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-31T07:45:16.933+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-31T07:45:16.934+0000] {spark_submit.py:495} INFO - "name" : "measured_at",
[2023-01-31T07:45:16.934+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T07:45:16.934+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T07:45:16.934+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:16.935+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:16.935+0000] {spark_submit.py:495} INFO - "name" : "created_at",
[2023-01-31T07:45:16.935+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T07:45:16.936+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T07:45:16.936+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:16.936+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:16.936+0000] {spark_submit.py:495} INFO - "name" : "Biomass - Actual Aggregated",
[2023-01-31T07:45:16.936+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:16.937+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:16.941+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:16.946+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:16.951+0000] {spark_submit.py:495} INFO - "name" : "Fossil Brown coal/Lignite - Actual Aggregated",
[2023-01-31T07:45:16.951+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:16.955+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:16.955+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:16.956+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:16.957+0000] {spark_submit.py:495} INFO - "name" : "Fossil Gas - Actual Aggregated",
[2023-01-31T07:45:16.964+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:16.967+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:16.970+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:16.972+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:16.976+0000] {spark_submit.py:495} INFO - "name" : "Fossil Hard coal - Actual Aggregated",
[2023-01-31T07:45:16.977+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:16.978+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:16.984+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:16.987+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:16.991+0000] {spark_submit.py:495} INFO - "name" : "Fossil Oil - Actual Aggregated",
[2023-01-31T07:45:16.991+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:16.994+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:16.994+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:16.997+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:16.997+0000] {spark_submit.py:495} INFO - "name" : "Geothermal - Actual Aggregated",
[2023-01-31T07:45:16.999+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:17.004+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:17.021+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:17.023+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:17.026+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Aggregated",
[2023-01-31T07:45:17.029+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:17.032+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:17.035+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:17.037+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:17.040+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Consumption",
[2023-01-31T07:45:17.041+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:17.064+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:17.065+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:17.065+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:17.065+0000] {spark_submit.py:495} INFO - "name" : "Hydro Run-of-river and poundage - Actual Aggregated",
[2023-01-31T07:45:17.065+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:17.066+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:17.066+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:17.066+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:17.066+0000] {spark_submit.py:495} INFO - "name" : "Hydro Water Reservoir - Actual Aggregated",
[2023-01-31T07:45:17.067+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:17.073+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:17.073+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:17.074+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:17.074+0000] {spark_submit.py:495} INFO - "name" : "Nuclear - Actual Aggregated",
[2023-01-31T07:45:17.074+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:17.075+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:17.075+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:17.075+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:17.075+0000] {spark_submit.py:495} INFO - "name" : "Other - Actual Aggregated",
[2023-01-31T07:45:17.076+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:17.076+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:17.076+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:17.076+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:17.076+0000] {spark_submit.py:495} INFO - "name" : "Other renewable - Actual Aggregated",
[2023-01-31T07:45:17.077+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:17.077+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:17.077+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:17.077+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:17.077+0000] {spark_submit.py:495} INFO - "name" : "Solar - Actual Aggregated",
[2023-01-31T07:45:17.077+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:17.078+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:17.078+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:17.078+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:17.078+0000] {spark_submit.py:495} INFO - "name" : "Waste - Actual Aggregated",
[2023-01-31T07:45:17.078+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:17.078+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:17.078+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:17.079+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:17.079+0000] {spark_submit.py:495} INFO - "name" : "Wind Offshore - Actual Aggregated",
[2023-01-31T07:45:17.079+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:17.079+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:17.079+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:17.079+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:17.079+0000] {spark_submit.py:495} INFO - "name" : "Wind Onshore - Actual Aggregated",
[2023-01-31T07:45:17.080+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:17.080+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:17.080+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:17.080+0000] {spark_submit.py:495} INFO - } ]
[2023-01-31T07:45:17.080+0000] {spark_submit.py:495} INFO - }
[2023-01-31T07:45:17.081+0000] {spark_submit.py:495} INFO - and corresponding Parquet message type:
[2023-01-31T07:45:17.081+0000] {spark_submit.py:495} INFO - message spark_schema {
[2023-01-31T07:45:17.081+0000] {spark_submit.py:495} INFO - required int96 measured_at;
[2023-01-31T07:45:17.081+0000] {spark_submit.py:495} INFO - required int96 created_at;
[2023-01-31T07:45:17.081+0000] {spark_submit.py:495} INFO - optional double Biomass - Actual Aggregated;
[2023-01-31T07:45:17.081+0000] {spark_submit.py:495} INFO - optional double Fossil Brown coal/Lignite - Actual Aggregated;
[2023-01-31T07:45:17.081+0000] {spark_submit.py:495} INFO - optional double Fossil Gas - Actual Aggregated;
[2023-01-31T07:45:17.092+0000] {spark_submit.py:495} INFO - optional double Fossil Hard coal - Actual Aggregated;
[2023-01-31T07:45:17.093+0000] {spark_submit.py:495} INFO - optional double Fossil Oil - Actual Aggregated;
[2023-01-31T07:45:17.093+0000] {spark_submit.py:495} INFO - optional double Geothermal - Actual Aggregated;
[2023-01-31T07:45:17.093+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Aggregated;
[2023-01-31T07:45:17.093+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Consumption;
[2023-01-31T07:45:17.093+0000] {spark_submit.py:495} INFO - optional double Hydro Run-of-river and poundage - Actual Aggregated;
[2023-01-31T07:45:17.094+0000] {spark_submit.py:495} INFO - optional double Hydro Water Reservoir - Actual Aggregated;
[2023-01-31T07:45:17.094+0000] {spark_submit.py:495} INFO - optional double Nuclear - Actual Aggregated;
[2023-01-31T07:45:17.094+0000] {spark_submit.py:495} INFO - optional double Other - Actual Aggregated;
[2023-01-31T07:45:17.094+0000] {spark_submit.py:495} INFO - optional double Other renewable - Actual Aggregated;
[2023-01-31T07:45:17.094+0000] {spark_submit.py:495} INFO - optional double Solar - Actual Aggregated;
[2023-01-31T07:45:17.094+0000] {spark_submit.py:495} INFO - optional double Waste - Actual Aggregated;
[2023-01-31T07:45:17.095+0000] {spark_submit.py:495} INFO - optional double Wind Offshore - Actual Aggregated;
[2023-01-31T07:45:17.095+0000] {spark_submit.py:495} INFO - optional double Wind Onshore - Actual Aggregated;
[2023-01-31T07:45:17.095+0000] {spark_submit.py:495} INFO - }
[2023-01-31T07:45:17.095+0000] {spark_submit.py:495} INFO - 
[2023-01-31T07:45:17.095+0000] {spark_submit.py:495} INFO - 
[2023-01-31T07:45:17.618+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:17 INFO CodecPool: Got brand-new compressor [.snappy]
[2023-01-31T07:45:23.857+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:23 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675150851009-6ba7f457-c892-49e7-91ef-3df91f94e532/_temporary/0/_temporary/' directory.
[2023-01-31T07:45:23.861+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:23 INFO FileOutputCommitter: Saved output of task 'attempt_202301310745143024839106769973061_0008_m_000000_8' to gs://entsoe_temp_1009/.spark-bigquery-local-1675150851009-6ba7f457-c892-49e7-91ef-3df91f94e532/_temporary/0/task_202301310745143024839106769973061_0008_m_000000
[2023-01-31T07:45:23.869+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:23 INFO SparkHadoopMapRedUtil: attempt_202301310745143024839106769973061_0008_m_000000_8: Committed. Elapsed time: 2999 ms.
[2023-01-31T07:45:23.948+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:23 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2588 bytes result sent to driver
[2023-01-31T07:45:23.957+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:23 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 8369 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:45:23.958+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:23 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2023-01-31T07:45:23.959+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:23 INFO DAGScheduler: ResultStage 8 (save at BigQueryWriteHelper.java:105) finished in 8.638 s
[2023-01-31T07:45:23.960+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:23 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:45:23.961+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2023-01-31T07:45:23.968+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:23 INFO DAGScheduler: Job 8 finished: save at BigQueryWriteHelper.java:105, took 8.652556 s
[2023-01-31T07:45:23.996+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:23 INFO FileFormatWriter: Start to commit write Job 4eba269a-e7c4-4a7b-857d-0a5065fdaeae.
[2023-01-31T07:45:25.021+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:25 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675150851009-6ba7f457-c892-49e7-91ef-3df91f94e532/_temporary/0/task_202301310745143024839106769973061_0008_m_000000/' directory.
[2023-01-31T07:45:25.849+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:25 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675150851009-6ba7f457-c892-49e7-91ef-3df91f94e532/' directory.
[2023-01-31T07:45:26.541+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:26 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 81d5fcd0285b:37465 in memory (size: 74.2 KiB, free: 434.4 MiB)
[2023-01-31T07:45:28.097+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:28 INFO FileFormatWriter: Write Job 4eba269a-e7c4-4a7b-857d-0a5065fdaeae committed. Elapsed time: 4061 ms.
[2023-01-31T07:45:28.203+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:28 INFO FileFormatWriter: Finished processing stats for write job 4eba269a-e7c4-4a7b-857d-0a5065fdaeae.
[2023-01-31T07:45:31.172+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:31 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=entsoe_analytics_1009, projectId=rafzul-analytics-1009, tableId=TEST_total_generation_staging}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=null, ignoreUnknownValue=null, sourceUris=[gs://entsoe_temp_1009/.spark-bigquery-local-1675150851009-6ba7f457-c892-49e7-91ef-3df91f94e532/part-00000-bdda56d9-3a75-4853-8843-b107f2eb38ab-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=true, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=rafzul-analytics-1009, job=e2e36df7-6186-4f7f-beae-e3389973f95e, location=US}
[2023-01-31T07:45:37.622+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:37 INFO BigQueryClient: Done loading to rafzul-analytics-1009.entsoe_analytics_1009.TEST_total_generation_staging. jobId: JobId{project=rafzul-analytics-1009, job=e2e36df7-6186-4f7f-beae-e3389973f95e, location=US}
[2023-01-31T07:45:38.388+0000] {spark_submit.py:495} INFO - MANTEP!!!!
[2023-01-31T07:45:38.811+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:38 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-31T07:45:38.857+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:38 INFO SparkUI: Stopped Spark web UI at http://81d5fcd0285b:4042
[2023-01-31T07:45:38.910+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:38 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-31T07:45:38.975+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:38 INFO MemoryStore: MemoryStore cleared
[2023-01-31T07:45:38.977+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:38 INFO BlockManager: BlockManager stopped
[2023-01-31T07:45:38.992+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:38 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-31T07:45:39.008+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:39 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-31T07:45:39.039+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:39 INFO SparkContext: Successfully stopped SparkContext
[2023-01-31T07:45:39.041+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:39 INFO ShutdownHookManager: Shutdown hook called
[2023-01-31T07:45:39.044+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-9adc8952-75e2-41db-84df-f10c9059cc8c/pyspark-718ffcde-03b6-45a9-8ce9-241a8b03f3f9
[2023-01-31T07:45:39.064+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-1277d2eb-aeae-4116-895b-cafac3245044
[2023-01-31T07:45:39.083+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-9adc8952-75e2-41db-84df-f10c9059cc8c
[2023-01-31T07:45:39.624+0000] {taskinstance.py:1322} INFO - Marking task as SUCCESS. dag_id=entsoe-energydata-backfill, task_id=stage_total_generation, execution_date=20210101T030000, start_date=20230131T073902, end_date=20230131T074539
[2023-01-31T07:45:39.759+0000] {local_task_job.py:159} INFO - Task exited with return code 0
[2023-01-31T07:45:39.829+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T08:21:35.697+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T03:00:00+00:00 [queued]>
[2023-01-31T08:21:35.753+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T03:00:00+00:00 [queued]>
[2023-01-31T08:21:35.754+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T08:21:35.754+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T08:21:35.755+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T08:21:35.820+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 03:00:00+00:00
[2023-01-31T08:21:35.852+0000] {standard_task_runner.py:55} INFO - Started process 2080 to run task
[2023-01-31T08:21:35.871+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T03:00:00+00:00', '--job-id', '928', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpdov_7zle']
[2023-01-31T08:21:35.880+0000] {standard_task_runner.py:83} INFO - Job 928: Subtask stage_total_generation
[2023-01-31T08:21:36.081+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T03:00:00+00:00 [running]> on host 81d5fcd0285b
[2023-01-31T08:21:36.205+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T03:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T03:00:00+00:00
[2023-01-31T08:21:36.240+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T08:21:36.243+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010300 202101010400 DE_TENNET
[2023-01-31T08:21:56.709+0000] {spark_submit.py:495} INFO - {{ start_date }}
[2023-01-31T08:21:56.710+0000] {spark_submit.py:495} INFO - mulai periode: {{ start_date }}
[2023-01-31T08:21:56.710+0000] {spark_submit.py:495} INFO - selesai periode: {{ end_date }}
[2023-01-31T08:21:57.057+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:57 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T08:21:57.528+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T08:21:57.993+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:57 INFO ResourceUtils: ==============================================================
[2023-01-31T08:21:57.993+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:57 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T08:21:57.993+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:57 INFO ResourceUtils: ==============================================================
[2023-01-31T08:21:57.994+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:57 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T08:21:58.130+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:58 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T08:21:58.171+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:58 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T08:21:58.189+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:58 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T08:21:58.842+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:58 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T08:21:58.847+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:58 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T08:21:58.849+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:58 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T08:21:58.854+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:58 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T08:21:58.855+0000] {spark_submit.py:495} INFO - 23/01/31 08:21:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T08:22:00.969+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:00 INFO Utils: Successfully started service 'sparkDriver' on port 38117.
[2023-01-31T08:22:01.618+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:01 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T08:22:01.963+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:01 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T08:22:02.397+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:02 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T08:22:02.405+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:02 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T08:22:02.479+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:02 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T08:22:02.946+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:02 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-84a463fd-76b5-44af-9c5b-fda371059e15
[2023-01-31T08:22:03.105+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:03 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T08:22:03.248+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:03 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T08:22:04.698+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:04 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T08:22:04.699+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:04 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2023-01-31T08:22:04.740+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:04 INFO Utils: Successfully started service 'SparkUI' on port 4042.
[2023-01-31T08:22:05.310+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:05 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://81d5fcd0285b:38117/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675153316999
[2023-01-31T08:22:05.321+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:05 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://81d5fcd0285b:38117/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675153316999
[2023-01-31T08:22:06.954+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:06 INFO Executor: Starting executor ID driver on host 81d5fcd0285b
[2023-01-31T08:22:06.994+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:06 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T08:22:07.202+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:07 INFO Executor: Fetching spark://81d5fcd0285b:38117/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675153316999
[2023-01-31T08:22:07.628+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:07 INFO TransportClientFactory: Successfully created connection to 81d5fcd0285b/172.19.0.5:38117 after 301 ms (0 ms spent in bootstraps)
[2023-01-31T08:22:07.726+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:07 INFO Utils: Fetching spark://81d5fcd0285b:38117/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-261873cd-78fc-4a15-80cf-f8e9ee6922b8/userFiles-ecfdc479-8ced-4f1e-a4b7-7a38c0cfd94b/fetchFileTemp16864183439092771257.tmp
[2023-01-31T08:22:09.202+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:09 INFO Executor: Adding file:/tmp/spark-261873cd-78fc-4a15-80cf-f8e9ee6922b8/userFiles-ecfdc479-8ced-4f1e-a4b7-7a38c0cfd94b/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T08:22:09.203+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:09 INFO Executor: Fetching spark://81d5fcd0285b:38117/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675153316999
[2023-01-31T08:22:09.204+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:09 INFO Utils: Fetching spark://81d5fcd0285b:38117/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-261873cd-78fc-4a15-80cf-f8e9ee6922b8/userFiles-ecfdc479-8ced-4f1e-a4b7-7a38c0cfd94b/fetchFileTemp3814316569774848041.tmp
[2023-01-31T08:22:09.886+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:09 INFO Executor: Adding file:/tmp/spark-261873cd-78fc-4a15-80cf-f8e9ee6922b8/userFiles-ecfdc479-8ced-4f1e-a4b7-7a38c0cfd94b/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T08:22:10.010+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:10 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42879.
[2023-01-31T08:22:10.011+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:10 INFO NettyBlockTransferService: Server created on 81d5fcd0285b:42879
[2023-01-31T08:22:10.019+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:10 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T08:22:10.116+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:10 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 42879, None)
[2023-01-31T08:22:10.133+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:10 INFO BlockManagerMasterEndpoint: Registering block manager 81d5fcd0285b:42879 with 434.4 MiB RAM, BlockManagerId(driver, 81d5fcd0285b, 42879, None)
[2023-01-31T08:22:10.146+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:10 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 42879, None)
[2023-01-31T08:22:10.147+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:10 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 81d5fcd0285b, 42879, None)
[2023-01-31T08:22:16.366+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:16 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T08:22:16.390+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:16 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T08:22:28.889+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:28 INFO InMemoryFileIndex: It took 300 ms to list leaf files for 1 paths.
[2023-01-31T08:22:31.286+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:31 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T08:22:31.943+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:31 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T08:22:31.958+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:31 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 81d5fcd0285b:42879 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T08:22:31.978+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:31 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T08:22:34.670+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:34 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T08:22:34.734+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:34 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T08:22:34.824+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:34 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T08:22:34.947+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:34 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T08:22:34.963+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:34 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T08:22:34.970+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:34 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:22:34.984+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:34 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:22:35.052+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:35 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T08:22:35.360+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:35 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T08:22:35.377+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:35 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T08:22:35.380+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:35 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 81d5fcd0285b:42879 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T08:22:35.383+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:35 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:22:35.425+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:22:35.426+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:35 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T08:22:35.897+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:35 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T08:22:36.055+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:36 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T08:22:36.806+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:36 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010300__202101010400.json:0+9366
[2023-01-31T08:22:38.734+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:38 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3437 bytes result sent to driver
[2023-01-31T08:22:38.809+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:38 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3094 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:22:38.836+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:38 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T08:22:38.975+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:38 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 3.728 s
[2023-01-31T08:22:39.050+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:39 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:22:39.051+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T08:22:39.089+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:39 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 4.262076 s
[2023-01-31T08:22:41.944+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:41 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 81d5fcd0285b:42879 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T08:22:42.082+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:42 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 81d5fcd0285b:42879 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T08:23:01.334+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:01 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:01.335+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:01 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T08:23:01.342+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:01 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:06.295+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:06 INFO CodeGenerator: Code generated in 2787.025673 ms
[2023-01-31T08:23:06.520+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:06 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T08:23:06.658+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:06 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T08:23:06.659+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:06 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 81d5fcd0285b:42879 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T08:23:06.661+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:06 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T08:23:06.676+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:06 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203670 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:08.057+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:08 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T08:23:08.059+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:08 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T08:23:08.060+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:08 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T08:23:08.061+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:08 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:08.061+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:08 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:08.082+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:08 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T08:23:08.169+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:08 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T08:23:08.218+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:08 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T08:23:08.222+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:08 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 81d5fcd0285b:42879 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T08:23:08.230+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:08 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:08.243+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:08.244+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:08 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T08:23:08.272+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:08 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:08.294+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:08 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T08:23:09.160+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:09 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010300__202101010400.json, range: 0-9366, partition values: [empty row]
[2023-01-31T08:23:11.688+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:11 INFO CodeGenerator: Code generated in 1719.9706 ms
[2023-01-31T08:23:12.934+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:12 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1628 bytes result sent to driver
[2023-01-31T08:23:12.994+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:12 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 4729 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:12.995+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:12 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T08:23:13.019+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:13 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 4.870 s
[2023-01-31T08:23:13.019+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:13 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:13.020+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T08:23:13.022+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:13 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 4.963162 s
[2023-01-31T08:23:14.424+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:14 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:14.427+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:14 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T08:23:14.475+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:14 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:14.980+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:14 INFO CodeGenerator: Code generated in 253.300061 ms
[2023-01-31T08:23:15.082+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:15 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T08:23:15.448+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:15 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T08:23:15.473+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:15 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 81d5fcd0285b:42879 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T08:23:15.476+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:15 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T08:23:15.517+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203670 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:16.796+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:16 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T08:23:16.796+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:16 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T08:23:16.796+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:16 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T08:23:16.797+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:16 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:16.797+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:16 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:16.823+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:16 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T08:23:16.997+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:16 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T08:23:17.369+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:17 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T08:23:17.382+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:17 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 81d5fcd0285b:42879 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T08:23:17.511+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:17 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:17.512+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:17.512+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:17 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T08:23:17.574+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:17 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:17.577+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:17 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T08:23:17.935+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:17 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010300__202101010400.json, range: 0-9366, partition values: [empty row]
[2023-01-31T08:23:19.391+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:19 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1585 bytes result sent to driver
[2023-01-31T08:23:19.415+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:19 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1843 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:19.418+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:19 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T08:23:19.434+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:19 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 2.610 s
[2023-01-31T08:23:19.434+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:19 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:19.435+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T08:23:19.435+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:19 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 2.646445 s
[2023-01-31T08:23:21.286+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:21.301+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T08:23:21.354+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:25.542+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO CodeGenerator: Code generated in 2706.982016 ms
[2023-01-31T08:23:25.623+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T08:23:26.006+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T08:23:26.008+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 81d5fcd0285b:42879 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T08:23:26.037+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T08:23:26.121+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203670 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:26.564+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 81d5fcd0285b:42879 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T08:23:26.650+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T08:23:26.651+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T08:23:26.652+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T08:23:26.652+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:26.653+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:26.718+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T08:23:26.730+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.7 MiB)
[2023-01-31T08:23:26.773+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 81d5fcd0285b:42879 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T08:23:26.862+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.7 MiB)
[2023-01-31T08:23:26.864+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 81d5fcd0285b:42879 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T08:23:26.870+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:26.871+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:26.871+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T08:23:26.873+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:26.874+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T08:23:27.082+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010300__202101010400.json, range: 0-9366, partition values: [empty row]
[2023-01-31T08:23:27.815+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-01-31T08:23:27.877+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 968 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:27.879+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T08:23:27.882+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 1.153 s
[2023-01-31T08:23:27.883+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:27.883+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T08:23:27.883+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 1.230876 s
[2023-01-31T08:23:28.247+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:28.247+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T08:23:28.248+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:28.356+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO CodeGenerator: Code generated in 85.663504 ms
[2023-01-31T08:23:28.443+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.5 MiB)
[2023-01-31T08:23:28.692+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T08:23:28.695+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 81d5fcd0285b:42879 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T08:23:28.706+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T08:23:28.707+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203670 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:28.823+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T08:23:28.823+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T08:23:28.824+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T08:23:28.824+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:28.825+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:28.831+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T08:23:28.836+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-31T08:23:28.848+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-01-31T08:23:28.860+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 81d5fcd0285b:42879 (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T08:23:28.863+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:28.871+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:28.873+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T08:23:28.880+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:28.881+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:28 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T08:23:29.015+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010300__202101010400.json, range: 0-9366, partition values: [empty row]
[2023-01-31T08:23:29.377+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1586 bytes result sent to driver
[2023-01-31T08:23:29.379+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 500 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:29.380+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T08:23:29.384+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 0.553 s
[2023-01-31T08:23:29.389+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:29.390+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T08:23:29.390+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 0.564985 s
[2023-01-31T08:23:30.842+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:30 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:30.843+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:30 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T08:23:30.843+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:30 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:32.606+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO CodeGenerator: Code generated in 1304.6983 ms
[2023-01-31T08:23:32.715+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.2 MiB)
[2023-01-31T08:23:32.990+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T08:23:33.006+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 81d5fcd0285b:42879 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T08:23:33.008+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T08:23:33.056+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203670 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:33.402+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T08:23:33.434+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) with 1 output partitions
[2023-01-31T08:23:33.434+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201)
[2023-01-31T08:23:33.435+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:33.435+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:33.438+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201), which has no missing parents
[2023-01-31T08:23:33.475+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.1 MiB)
[2023-01-31T08:23:33.493+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.1 MiB)
[2023-01-31T08:23:33.505+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 81d5fcd0285b:42879 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T08:23:33.510+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:33.524+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:33.524+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T08:23:33.528+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:33.530+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T08:23:33.622+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010300__202101010400.json, range: 0-9366, partition values: [empty row]
[2023-01-31T08:23:34.070+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2515 bytes result sent to driver
[2023-01-31T08:23:34.077+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 548 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:34.078+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T08:23:34.078+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) finished in 0.631 s
[2023-01-31T08:23:34.079+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:34.079+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T08:23:34.079+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201, took 0.643595 s
[2023-01-31T08:23:34.820+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:34.821+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T08:23:34.824+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:35.502+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:35 INFO CodeGenerator: Code generated in 492.579431 ms
[2023-01-31T08:23:35.579+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:35 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 432.9 MiB)
[2023-01-31T08:23:35.679+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:35 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.9 MiB)
[2023-01-31T08:23:35.680+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:35 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 81d5fcd0285b:42879 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T08:23:35.681+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:35 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T08:23:35.704+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:35 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203670 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:36.118+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T08:23:36.154+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T08:23:36.155+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T08:23:36.155+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:36.155+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:36.158+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T08:23:36.162+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 432.9 MiB)
[2023-01-31T08:23:36.213+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.8 MiB)
[2023-01-31T08:23:36.215+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 81d5fcd0285b:42879 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T08:23:36.221+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:36.235+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:36.236+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T08:23:36.242+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:36.243+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T08:23:36.310+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010300__202101010400.json, range: 0-9366, partition values: [empty row]
[2023-01-31T08:23:36.631+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-01-31T08:23:36.644+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 392 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:36.644+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T08:23:36.645+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 0.476 s
[2023-01-31T08:23:36.645+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:36.649+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T08:23:36.650+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:36 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 0.522907 s
[2023-01-31T08:23:37.115+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:37 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:37.130+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:37 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T08:23:37.132+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:37 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:37.958+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:37 INFO CodeGenerator: Code generated in 572.564464 ms
[2023-01-31T08:23:38.046+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.7 MiB)
[2023-01-31T08:23:38.198+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.6 MiB)
[2023-01-31T08:23:38.198+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 81d5fcd0285b:42879 (size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T08:23:38.214+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T08:23:38.218+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203670 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:38.926+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T08:23:38.926+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T08:23:38.926+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T08:23:38.926+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:38.926+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:38.932+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T08:23:38.932+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 81d5fcd0285b:42879 in memory (size: 7.6 KiB, free: 434.1 MiB)
[2023-01-31T08:23:38.937+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.6 MiB)
[2023-01-31T08:23:38.941+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.6 MiB)
[2023-01-31T08:23:38.944+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 81d5fcd0285b:42879 (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T08:23:38.949+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:38.949+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:38.950+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T08:23:38.950+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:38.950+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T08:23:39.004+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 81d5fcd0285b:42879 in memory (size: 13.4 KiB, free: 434.1 MiB)
[2023-01-31T08:23:39.010+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010300__202101010400.json, range: 0-9366, partition values: [empty row]
[2023-01-31T08:23:39.564+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1729 bytes result sent to driver
[2023-01-31T08:23:39.565+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 607 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:39.571+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T08:23:39.572+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 0.628 s
[2023-01-31T08:23:39.572+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:39.572+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T08:23:39.572+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 81d5fcd0285b:42879 in memory (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T08:23:39.573+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 0.638268 s
[2023-01-31T08:23:39.764+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 81d5fcd0285b:42879 in memory (size: 12.7 KiB, free: 434.2 MiB)
