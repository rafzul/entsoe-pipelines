[2023-01-31T05:11:09.107+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [queued]>
[2023-01-31T05:11:09.182+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [queued]>
[2023-01-31T05:11:09.182+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T05:11:09.183+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T05:11:09.183+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T05:11:09.321+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 06:00:00+00:00
[2023-01-31T05:11:09.348+0000] {standard_task_runner.py:55} INFO - Started process 12528 to run task
[2023-01-31T05:11:09.368+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T06:00:00+00:00', '--job-id', '885', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpn2j849r1']
[2023-01-31T05:11:09.376+0000] {standard_task_runner.py:83} INFO - Job 885: Subtask stage_total_generation
[2023-01-31T05:11:09.900+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [running]> on host 81d5fcd0285b
[2023-01-31T05:11:10.371+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T06:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T06:00:00+00:00
[2023-01-31T05:11:10.417+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T05:11:10.421+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010600 202101010700 DE_TENNET
[2023-01-31T05:11:42.661+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:42 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T05:11:43.704+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T05:11:45.161+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:45 INFO ResourceUtils: ==============================================================
[2023-01-31T05:11:45.170+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:45 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T05:11:45.177+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:45 INFO ResourceUtils: ==============================================================
[2023-01-31T05:11:45.179+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:45 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T05:11:45.379+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:45 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T05:11:45.442+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:45 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T05:11:45.461+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:45 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T05:11:45.987+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:45 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T05:11:45.995+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:45 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T05:11:46.001+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:46 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T05:11:46.012+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:46 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T05:11:46.018+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T05:11:48.404+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:48 INFO Utils: Successfully started service 'sparkDriver' on port 39581.
[2023-01-31T05:11:48.685+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:48 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T05:11:49.321+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:49 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T05:11:49.757+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:49 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T05:11:49.773+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:49 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T05:11:49.824+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:49 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T05:11:50.331+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:50 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-368630be-9f91-445a-b7a0-479a11f3eb1a
[2023-01-31T05:11:50.597+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:50 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T05:11:50.908+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:50 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T05:11:53.017+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:53 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T05:11:53.021+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:53 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2023-01-31T05:11:53.031+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:53 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[2023-01-31T05:11:53.042+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:53 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[2023-01-31T05:11:53.052+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:53 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[2023-01-31T05:11:53.062+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:53 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
[2023-01-31T05:11:53.125+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:53 INFO Utils: Successfully started service 'SparkUI' on port 4046.
[2023-01-31T05:11:53.527+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:53 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://81d5fcd0285b:39581/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675141902601
[2023-01-31T05:11:53.538+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:53 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://81d5fcd0285b:39581/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675141902601
[2023-01-31T05:11:54.500+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:54 INFO Executor: Starting executor ID driver on host 81d5fcd0285b
[2023-01-31T05:11:54.564+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:54 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T05:11:54.740+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:54 INFO Executor: Fetching spark://81d5fcd0285b:39581/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675141902601
[2023-01-31T05:11:55.677+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:55 INFO TransportClientFactory: Successfully created connection to 81d5fcd0285b/172.19.0.9:39581 after 660 ms (0 ms spent in bootstraps)
[2023-01-31T05:11:55.737+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:55 INFO Utils: Fetching spark://81d5fcd0285b:39581/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-d7a6afd5-72c7-4f7d-9113-17e132c0fd15/userFiles-c4bf4bdc-78a7-4e0d-812d-a4b414c5bfca/fetchFileTemp17605483383629990865.tmp
[2023-01-31T05:11:58.014+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:58 INFO Executor: Adding file:/tmp/spark-d7a6afd5-72c7-4f7d-9113-17e132c0fd15/userFiles-c4bf4bdc-78a7-4e0d-812d-a4b414c5bfca/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T05:11:58.018+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:58 INFO Executor: Fetching spark://81d5fcd0285b:39581/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675141902601
[2023-01-31T05:11:58.077+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:58 INFO Utils: Fetching spark://81d5fcd0285b:39581/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-d7a6afd5-72c7-4f7d-9113-17e132c0fd15/userFiles-c4bf4bdc-78a7-4e0d-812d-a4b414c5bfca/fetchFileTemp14632098204375159938.tmp
[2023-01-31T05:11:58.792+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:58 INFO Executor: Adding file:/tmp/spark-d7a6afd5-72c7-4f7d-9113-17e132c0fd15/userFiles-c4bf4bdc-78a7-4e0d-812d-a4b414c5bfca/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T05:11:58.882+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:58 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34067.
[2023-01-31T05:11:58.883+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:58 INFO NettyBlockTransferService: Server created on 81d5fcd0285b:34067
[2023-01-31T05:11:58.892+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:58 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T05:11:58.947+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:58 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 34067, None)
[2023-01-31T05:11:58.972+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:58 INFO BlockManagerMasterEndpoint: Registering block manager 81d5fcd0285b:34067 with 434.4 MiB RAM, BlockManagerId(driver, 81d5fcd0285b, 34067, None)
[2023-01-31T05:11:59.003+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:59 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 34067, None)
[2023-01-31T05:11:59.011+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:59 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 81d5fcd0285b, 34067, None)
[2023-01-31T05:12:05.919+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:05 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T05:12:06.067+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:06 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T05:12:29.427+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:29 INFO InMemoryFileIndex: It took 784 ms to list leaf files for 1 paths.
[2023-01-31T05:12:32.921+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:32 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T05:12:34.261+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:34 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T05:12:34.280+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:34 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 81d5fcd0285b:34067 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T05:12:34.344+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:34 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T05:12:39.898+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:39 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T05:12:40.216+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:40 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T05:12:40.481+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:40 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T05:12:40.639+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:40 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T05:12:40.643+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:40 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T05:12:40.647+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:40 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:12:40.662+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:40 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:12:40.700+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:40 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T05:12:41.402+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:41 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T05:12:41.462+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:41 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T05:12:41.464+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:41 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 81d5fcd0285b:34067 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T05:12:41.470+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:41 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:12:41.615+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:12:41.618+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:41 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T05:12:41.847+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:41 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T05:12:41.984+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:41 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T05:12:42.702+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:42 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010600__202101010700.json:0+9371
[2023-01-31T05:12:44.183+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:44 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3437 bytes result sent to driver
[2023-01-31T05:12:44.288+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:44 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2487 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:12:44.297+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:44 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T05:12:44.519+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:44 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 3.607 s
[2023-01-31T05:12:44.581+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:44 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:12:44.583+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T05:12:44.610+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:44 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 4.127886 s
[2023-01-31T05:12:49.114+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:49 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 81d5fcd0285b:34067 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T05:12:49.218+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:49 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 81d5fcd0285b:34067 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T05:13:11.399+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:11 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:11.410+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:11 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:13:11.411+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:11 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:13.834+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:13 INFO CodeGenerator: Code generated in 1233.737123 ms
[2023-01-31T05:13:13.847+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:13 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T05:13:13.896+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:13 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T05:13:13.903+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:13 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 81d5fcd0285b:34067 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T05:13:13.905+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:13 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T05:13:13.945+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:13 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:14.317+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T05:13:14.317+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T05:13:14.317+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T05:13:14.317+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:14.318+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:14.318+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T05:13:14.376+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T05:13:14.413+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T05:13:14.416+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 81d5fcd0285b:34067 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T05:13:14.420+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:14.427+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:14.434+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T05:13:14.458+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:14.466+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T05:13:15.055+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T05:13:15.476+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO CodeGenerator: Code generated in 361.444883 ms
[2023-01-31T05:13:15.905+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1628 bytes result sent to driver
[2023-01-31T05:13:15.931+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1481 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:15.937+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 1.602 s
[2023-01-31T05:13:15.939+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:15.940+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T05:13:15.944+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T05:13:15.950+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 1.651254 s
[2023-01-31T05:13:16.546+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:16 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:16.546+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:16 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:13:16.551+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:16 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:16.907+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:16 INFO CodeGenerator: Code generated in 216.423158 ms
[2023-01-31T05:13:17.000+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:16 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T05:13:17.485+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T05:13:17.490+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 81d5fcd0285b:34067 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:13:17.500+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T05:13:17.507+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:17.671+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T05:13:17.676+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T05:13:17.676+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T05:13:17.677+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:17.677+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:17.684+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T05:13:17.705+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T05:13:17.729+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T05:13:17.737+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 81d5fcd0285b:34067 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T05:13:17.743+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:17.748+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:17.751+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T05:13:17.759+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:17.765+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T05:13:17.871+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T05:13:18.268+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1628 bytes result sent to driver
[2023-01-31T05:13:18.303+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 546 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:18.320+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 0.624 s
[2023-01-31T05:13:18.321+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:18.324+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T05:13:18.332+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T05:13:18.333+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 0.655656 s
[2023-01-31T05:13:19.962+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:19.965+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T05:13:19.966+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:20.870+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO CodeGenerator: Code generated in 658.917522 ms
[2023-01-31T05:13:20.886+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T05:13:20.936+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T05:13:20.938+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 81d5fcd0285b:34067 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:13:20.946+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T05:13:20.949+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:21.103+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T05:13:21.104+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T05:13:21.104+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T05:13:21.105+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:21.116+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:21.118+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T05:13:21.128+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.6 MiB)
[2023-01-31T05:13:21.153+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.6 MiB)
[2023-01-31T05:13:21.157+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 81d5fcd0285b:34067 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T05:13:21.158+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:21.163+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:21.163+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T05:13:21.167+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:21.168+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T05:13:21.212+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T05:13:21.448+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-01-31T05:13:21.460+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 293 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:21.463+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T05:13:21.469+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 0.346 s
[2023-01-31T05:13:21.469+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:21.471+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T05:13:21.476+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 0.375060 s
[2023-01-31T05:13:21.642+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:21.642+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:13:21.644+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:21.799+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO CodeGenerator: Code generated in 94.697242 ms
[2023-01-31T05:13:21.836+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.4 MiB)
[2023-01-31T05:13:22.034+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T05:13:22.037+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 81d5fcd0285b:34067 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:13:22.046+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T05:13:22.062+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:22.503+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T05:13:22.517+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T05:13:22.518+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T05:13:22.519+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:22.519+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:22.526+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T05:13:22.577+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-31T05:13:22.636+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-01-31T05:13:22.643+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 81d5fcd0285b:34067 (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T05:13:22.657+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:22.659+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:22.660+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T05:13:22.675+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:22.677+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T05:13:22.809+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T05:13:23.064+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 81d5fcd0285b:34067 in memory (size: 7.7 KiB, free: 434.2 MiB)
[2023-01-31T05:13:23.143+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 81d5fcd0285b:34067 in memory (size: 7.7 KiB, free: 434.2 MiB)
[2023-01-31T05:13:23.267+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 81d5fcd0285b:34067 in memory (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T05:13:23.326+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1629 bytes result sent to driver
[2023-01-31T05:13:23.333+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 659 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:23.334+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T05:13:23.337+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 0.797 s
[2023-01-31T05:13:23.337+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:23.339+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T05:13:23.339+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 0.831941 s
[2023-01-31T05:13:25.181+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:25.201+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T05:13:25.202+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:25.564+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO CodeGenerator: Code generated in 300.422854 ms
[2023-01-31T05:13:25.580+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.3 MiB)
[2023-01-31T05:13:25.600+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T05:13:25.606+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 81d5fcd0285b:34067 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:13:25.610+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T05:13:25.618+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:25.769+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T05:13:25.772+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) with 1 output partitions
[2023-01-31T05:13:25.772+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201)
[2023-01-31T05:13:25.773+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:25.773+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:25.780+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201), which has no missing parents
[2023-01-31T05:13:25.811+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.2 MiB)
[2023-01-31T05:13:25.835+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.2 MiB)
[2023-01-31T05:13:25.855+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 81d5fcd0285b:34067 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T05:13:25.863+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:25.872+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:25.878+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T05:13:25.893+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:25.925+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T05:13:26.036+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T05:13:26.271+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2499 bytes result sent to driver
[2023-01-31T05:13:26.286+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 384 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:26.286+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T05:13:26.287+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) finished in 0.499 s
[2023-01-31T05:13:26.287+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:26.287+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T05:13:26.287+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201, took 0.510457 s
[2023-01-31T05:13:27.165+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:27 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:27.229+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:27 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T05:13:27.229+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:27 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:28.523+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO CodeGenerator: Code generated in 746.151762 ms
[2023-01-31T05:13:28.595+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 433.0 MiB)
[2023-01-31T05:13:28.776+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.9 MiB)
[2023-01-31T05:13:28.781+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 81d5fcd0285b:34067 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:13:28.790+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T05:13:28.804+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:28.986+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T05:13:28.990+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T05:13:28.991+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T05:13:28.997+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:28.998+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:29.010+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T05:13:29.054+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 432.9 MiB)
[2023-01-31T05:13:29.083+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.9 MiB)
[2023-01-31T05:13:29.089+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 81d5fcd0285b:34067 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T05:13:29.097+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:29.107+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:29.112+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T05:13:29.124+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:29.127+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T05:13:29.186+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T05:13:29.457+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-01-31T05:13:29.500+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 362 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:29.509+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T05:13:29.513+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 0.467 s
[2023-01-31T05:13:29.514+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:29.524+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T05:13:29.524+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 0.498966 s
[2023-01-31T05:13:29.808+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:29.812+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T05:13:29.816+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:30.172+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO CodeGenerator: Code generated in 212.318126 ms
[2023-01-31T05:13:30.188+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.7 MiB)
[2023-01-31T05:13:30.240+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.7 MiB)
[2023-01-31T05:13:30.240+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 81d5fcd0285b:34067 (size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T05:13:30.256+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T05:13:30.276+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:30.435+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T05:13:30.437+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T05:13:30.437+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T05:13:30.438+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:30.438+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:30.458+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T05:13:30.477+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.6 MiB)
[2023-01-31T05:13:30.486+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.6 MiB)
[2023-01-31T05:13:30.491+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 81d5fcd0285b:34067 (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T05:13:30.494+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:30.501+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:30.503+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T05:13:30.511+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:30.512+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T05:13:30.684+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T05:13:31.287+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:31 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1729 bytes result sent to driver
[2023-01-31T05:13:31.303+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:31 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 794 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:31.305+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:31 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T05:13:31.377+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:31 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 0.851 s
[2023-01-31T05:13:31.378+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:31 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:31.426+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T05:13:31.442+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:31 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 0.992042 s
[2023-01-31T05:13:38.220+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 81d5fcd0285b:34067 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:13:38.305+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 81d5fcd0285b:34067 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T05:13:38.388+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 81d5fcd0285b:34067 in memory (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T05:13:38.467+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 81d5fcd0285b:34067 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T05:13:38.555+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 81d5fcd0285b:34067 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:13:38.641+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 81d5fcd0285b:34067 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:13:38.708+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 81d5fcd0285b:34067 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:13:38.785+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 81d5fcd0285b:34067 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:13:38.918+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 81d5fcd0285b:34067 in memory (size: 13.4 KiB, free: 434.3 MiB)
[2023-01-31T05:13:39.005+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:39 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 81d5fcd0285b:34067 in memory (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T05:14:00.152+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:00 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:14:00.190+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:14:00.191+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:14:00.195+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:00 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:14:00.200+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:14:00.201+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:14:00.211+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:14:01.488+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-31T05:14:01.490+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO DAGScheduler: Got job 8 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T05:14:01.490+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO DAGScheduler: Final stage: ResultStage 8 (save at BigQueryWriteHelper.java:105)
[2023-01-31T05:14:01.491+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:14:01.499+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:14:01.500+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO DAGScheduler: Submitting ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T05:14:01.792+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 209.9 KiB, free 434.0 MiB)
[2023-01-31T05:14:01.823+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 74.3 KiB, free 433.9 MiB)
[2023-01-31T05:14:01.830+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 81d5fcd0285b:34067 (size: 74.3 KiB, free: 434.3 MiB)
[2023-01-31T05:14:01.836+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:14:01.841+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:14:01.843+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2023-01-31T05:14:01.865+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7484 bytes) taskResourceAssignments Map()
[2023-01-31T05:14:01.887+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2023-01-31T05:14:02.961+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:14:02.967+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:14:02.984+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:02 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:14:02.988+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:14:02.991+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:14:03.014+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:14:03.131+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:03 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T05:14:03.198+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:03 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T05:14:03.378+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:03 INFO ParquetOutputFormat: Parquet block size to 134217728
[2023-01-31T05:14:03.380+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:03 INFO ParquetOutputFormat: Validation is off
[2023-01-31T05:14:03.381+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:03 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-01-31T05:14:03.387+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:03 INFO ParquetOutputFormat: Parquet properties are:
[2023-01-31T05:14:03.387+0000] {spark_submit.py:495} INFO - Parquet page size to 1048576
[2023-01-31T05:14:03.388+0000] {spark_submit.py:495} INFO - Parquet dictionary page size to 1048576
[2023-01-31T05:14:03.388+0000] {spark_submit.py:495} INFO - Dictionary is true
[2023-01-31T05:14:03.388+0000] {spark_submit.py:495} INFO - Writer version is: PARQUET_1_0
[2023-01-31T05:14:03.391+0000] {spark_submit.py:495} INFO - Page size checking is: estimated
[2023-01-31T05:14:03.391+0000] {spark_submit.py:495} INFO - Min row count for page size check is: 100
[2023-01-31T05:14:03.394+0000] {spark_submit.py:495} INFO - Max row count for page size check is: 10000
[2023-01-31T05:14:03.395+0000] {spark_submit.py:495} INFO - Truncate length for column indexes is: 64
[2023-01-31T05:14:03.397+0000] {spark_submit.py:495} INFO - Truncate length for statistics min/max  is: 2147483647
[2023-01-31T05:14:03.398+0000] {spark_submit.py:495} INFO - Bloom filter enabled: false
[2023-01-31T05:14:03.401+0000] {spark_submit.py:495} INFO - Max Bloom filter size for a column is 1048576
[2023-01-31T05:14:03.401+0000] {spark_submit.py:495} INFO - Bloom filter expected number of distinct values are: null
[2023-01-31T05:14:03.404+0000] {spark_submit.py:495} INFO - Page row count limit to 20000
[2023-01-31T05:14:03.406+0000] {spark_submit.py:495} INFO - Writing page checksums is: on
[2023-01-31T05:14:03.580+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:03 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-01-31T05:14:03.580+0000] {spark_submit.py:495} INFO - {
[2023-01-31T05:14:03.581+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-31T05:14:03.581+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-31T05:14:03.582+0000] {spark_submit.py:495} INFO - "name" : "measured_at",
[2023-01-31T05:14:03.582+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T05:14:03.583+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T05:14:03.585+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:03.586+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:03.586+0000] {spark_submit.py:495} INFO - "name" : "created_at",
[2023-01-31T05:14:03.587+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T05:14:03.588+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T05:14:03.592+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:03.593+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:03.594+0000] {spark_submit.py:495} INFO - "name" : "Biomass - Actual Aggregated",
[2023-01-31T05:14:03.594+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:03.595+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:03.595+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:03.595+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:03.596+0000] {spark_submit.py:495} INFO - "name" : "Fossil Brown coal/Lignite - Actual Aggregated",
[2023-01-31T05:14:03.596+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:03.602+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:03.603+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:03.603+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:03.604+0000] {spark_submit.py:495} INFO - "name" : "Fossil Gas - Actual Aggregated",
[2023-01-31T05:14:03.604+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:03.605+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:03.606+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:03.606+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:03.612+0000] {spark_submit.py:495} INFO - "name" : "Fossil Hard coal - Actual Aggregated",
[2023-01-31T05:14:03.614+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:03.615+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:03.615+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:03.616+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:03.616+0000] {spark_submit.py:495} INFO - "name" : "Fossil Oil - Actual Aggregated",
[2023-01-31T05:14:03.617+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:03.617+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:03.618+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:03.618+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:03.618+0000] {spark_submit.py:495} INFO - "name" : "Geothermal - Actual Aggregated",
[2023-01-31T05:14:03.619+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:03.619+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:03.620+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:03.621+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:03.623+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Aggregated",
[2023-01-31T05:14:03.627+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:03.628+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:03.628+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:03.636+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:03.637+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Consumption",
[2023-01-31T05:14:03.638+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:03.638+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:03.638+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:03.641+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:03.641+0000] {spark_submit.py:495} INFO - "name" : "Hydro Run-of-river and poundage - Actual Aggregated",
[2023-01-31T05:14:03.642+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:03.644+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:03.645+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:03.645+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:03.646+0000] {spark_submit.py:495} INFO - "name" : "Hydro Water Reservoir - Actual Aggregated",
[2023-01-31T05:14:03.653+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:03.657+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:03.658+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:03.661+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:03.662+0000] {spark_submit.py:495} INFO - "name" : "Nuclear - Actual Aggregated",
[2023-01-31T05:14:03.663+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:03.664+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:03.664+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:03.665+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:03.665+0000] {spark_submit.py:495} INFO - "name" : "Other - Actual Aggregated",
[2023-01-31T05:14:03.666+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:03.666+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:03.667+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:03.668+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:03.668+0000] {spark_submit.py:495} INFO - "name" : "Other renewable - Actual Aggregated",
[2023-01-31T05:14:03.669+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:03.669+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:03.670+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:03.670+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:03.671+0000] {spark_submit.py:495} INFO - "name" : "Solar - Actual Aggregated",
[2023-01-31T05:14:03.671+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:03.672+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:03.672+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:03.673+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:03.673+0000] {spark_submit.py:495} INFO - "name" : "Waste - Actual Aggregated",
[2023-01-31T05:14:03.673+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:03.674+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:03.674+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:03.675+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:03.675+0000] {spark_submit.py:495} INFO - "name" : "Wind Offshore - Actual Aggregated",
[2023-01-31T05:14:03.676+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:03.676+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:03.677+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:03.677+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:03.678+0000] {spark_submit.py:495} INFO - "name" : "Wind Onshore - Actual Aggregated",
[2023-01-31T05:14:03.678+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:03.679+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:03.679+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:03.679+0000] {spark_submit.py:495} INFO - } ]
[2023-01-31T05:14:03.680+0000] {spark_submit.py:495} INFO - }
[2023-01-31T05:14:03.680+0000] {spark_submit.py:495} INFO - and corresponding Parquet message type:
[2023-01-31T05:14:03.681+0000] {spark_submit.py:495} INFO - message spark_schema {
[2023-01-31T05:14:03.681+0000] {spark_submit.py:495} INFO - required int96 measured_at;
[2023-01-31T05:14:03.682+0000] {spark_submit.py:495} INFO - required int96 created_at;
[2023-01-31T05:14:03.682+0000] {spark_submit.py:495} INFO - optional double Biomass - Actual Aggregated;
[2023-01-31T05:14:03.683+0000] {spark_submit.py:495} INFO - optional double Fossil Brown coal/Lignite - Actual Aggregated;
[2023-01-31T05:14:03.683+0000] {spark_submit.py:495} INFO - optional double Fossil Gas - Actual Aggregated;
[2023-01-31T05:14:03.683+0000] {spark_submit.py:495} INFO - optional double Fossil Hard coal - Actual Aggregated;
[2023-01-31T05:14:03.684+0000] {spark_submit.py:495} INFO - optional double Fossil Oil - Actual Aggregated;
[2023-01-31T05:14:03.684+0000] {spark_submit.py:495} INFO - optional double Geothermal - Actual Aggregated;
[2023-01-31T05:14:03.685+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Aggregated;
[2023-01-31T05:14:03.685+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Consumption;
[2023-01-31T05:14:03.685+0000] {spark_submit.py:495} INFO - optional double Hydro Run-of-river and poundage - Actual Aggregated;
[2023-01-31T05:14:03.686+0000] {spark_submit.py:495} INFO - optional double Hydro Water Reservoir - Actual Aggregated;
[2023-01-31T05:14:03.686+0000] {spark_submit.py:495} INFO - optional double Nuclear - Actual Aggregated;
[2023-01-31T05:14:03.687+0000] {spark_submit.py:495} INFO - optional double Other - Actual Aggregated;
[2023-01-31T05:14:03.687+0000] {spark_submit.py:495} INFO - optional double Other renewable - Actual Aggregated;
[2023-01-31T05:14:03.688+0000] {spark_submit.py:495} INFO - optional double Solar - Actual Aggregated;
[2023-01-31T05:14:03.688+0000] {spark_submit.py:495} INFO - optional double Waste - Actual Aggregated;
[2023-01-31T05:14:03.689+0000] {spark_submit.py:495} INFO - optional double Wind Offshore - Actual Aggregated;
[2023-01-31T05:14:03.689+0000] {spark_submit.py:495} INFO - optional double Wind Onshore - Actual Aggregated;
[2023-01-31T05:14:03.690+0000] {spark_submit.py:495} INFO - }
[2023-01-31T05:14:03.690+0000] {spark_submit.py:495} INFO - 
[2023-01-31T05:14:03.694+0000] {spark_submit.py:495} INFO - 
[2023-01-31T05:14:04.287+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:04 INFO CodecPool: Got brand-new compressor [.snappy]
[2023-01-31T05:14:10.885+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:10 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675141913985-1928f463-b79d-48eb-86ff-3b3f874c1a74/_temporary/0/_temporary/' directory.
[2023-01-31T05:14:10.886+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:10 INFO FileOutputCommitter: Saved output of task 'attempt_202301310514008531838664300346989_0008_m_000000_8' to gs://entsoe_temp_1009/.spark-bigquery-local-1675141913985-1928f463-b79d-48eb-86ff-3b3f874c1a74/_temporary/0/task_202301310514008531838664300346989_0008_m_000000
[2023-01-31T05:14:10.894+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:10 INFO SparkHadoopMapRedUtil: attempt_202301310514008531838664300346989_0008_m_000000_8: Committed. Elapsed time: 1672 ms.
[2023-01-31T05:14:10.990+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:10 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2588 bytes result sent to driver
[2023-01-31T05:14:10.996+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:10 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 9148 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:14:11.000+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:10 INFO DAGScheduler: ResultStage 8 (save at BigQueryWriteHelper.java:105) finished in 9.499 s
[2023-01-31T05:14:11.001+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:10 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:14:11.003+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:11 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2023-01-31T05:14:11.005+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2023-01-31T05:14:11.009+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:11 INFO DAGScheduler: Job 8 finished: save at BigQueryWriteHelper.java:105, took 9.519713 s
[2023-01-31T05:14:11.012+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:11 INFO FileFormatWriter: Start to commit write Job a3ea308e-f977-43b3-902a-b930b0f9efb9.
[2023-01-31T05:14:11.863+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:11 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675141913985-1928f463-b79d-48eb-86ff-3b3f874c1a74/_temporary/0/task_202301310514008531838664300346989_0008_m_000000/' directory.
[2023-01-31T05:14:12.295+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:12 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675141913985-1928f463-b79d-48eb-86ff-3b3f874c1a74/' directory.
[2023-01-31T05:14:12.577+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:12 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 81d5fcd0285b:34067 in memory (size: 74.3 KiB, free: 434.4 MiB)
[2023-01-31T05:14:13.653+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:13 INFO FileFormatWriter: Write Job a3ea308e-f977-43b3-902a-b930b0f9efb9 committed. Elapsed time: 2628 ms.
[2023-01-31T05:14:13.744+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:13 INFO FileFormatWriter: Finished processing stats for write job a3ea308e-f977-43b3-902a-b930b0f9efb9.
[2023-01-31T05:14:16.639+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:16 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=entsoe_analytics_1009, projectId=rafzul-analytics-1009, tableId=TEST_total_generation_staging}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=null, ignoreUnknownValue=null, sourceUris=[gs://entsoe_temp_1009/.spark-bigquery-local-1675141913985-1928f463-b79d-48eb-86ff-3b3f874c1a74/part-00000-282c7b2a-8cc6-456c-be32-8a14b9c49951-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=true, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=rafzul-analytics-1009, job=90c22869-f7fa-43d9-b743-6862b10f80cf, location=US}
[2023-01-31T05:14:23.313+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO BigQueryClient: Done loading to rafzul-analytics-1009.entsoe_analytics_1009.TEST_total_generation_staging. jobId: JobId{project=rafzul-analytics-1009, job=90c22869-f7fa-43d9-b743-6862b10f80cf, location=US}
[2023-01-31T05:14:23.750+0000] {spark_submit.py:495} INFO - MANTEP!!!!
[2023-01-31T05:14:23.875+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-31T05:14:23.899+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO SparkUI: Stopped Spark web UI at http://81d5fcd0285b:4046
[2023-01-31T05:14:23.915+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-31T05:14:23.934+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO MemoryStore: MemoryStore cleared
[2023-01-31T05:14:23.935+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO BlockManager: BlockManager stopped
[2023-01-31T05:14:23.939+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-31T05:14:23.945+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-31T05:14:23.953+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO SparkContext: Successfully stopped SparkContext
[2023-01-31T05:14:23.953+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO ShutdownHookManager: Shutdown hook called
[2023-01-31T05:14:23.954+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-d7a6afd5-72c7-4f7d-9113-17e132c0fd15/pyspark-2bc6f4c5-7013-4beb-b593-14d49a27d692
[2023-01-31T05:14:23.960+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-d7a6afd5-72c7-4f7d-9113-17e132c0fd15
[2023-01-31T05:14:23.967+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-344a3f2d-e38b-455c-ace4-8fb5dea4a208
[2023-01-31T05:14:24.111+0000] {taskinstance.py:1322} INFO - Marking task as SUCCESS. dag_id=entsoe-energydata-backfill, task_id=stage_total_generation, execution_date=20210101T060000, start_date=20230131T051109, end_date=20230131T051424
[2023-01-31T05:14:24.158+0000] {local_task_job.py:159} INFO - Task exited with return code 0
[2023-01-31T05:14:24.178+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T05:30:47.961+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [queued]>
[2023-01-31T05:30:48.103+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [queued]>
[2023-01-31T05:30:48.105+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T05:30:48.106+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T05:30:48.107+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T05:30:48.279+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 06:00:00+00:00
[2023-01-31T05:30:48.314+0000] {standard_task_runner.py:55} INFO - Started process 15825 to run task
[2023-01-31T05:30:48.343+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T06:00:00+00:00', '--job-id', '899', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpq4zgishu']
[2023-01-31T05:30:48.356+0000] {standard_task_runner.py:83} INFO - Job 899: Subtask stage_total_generation
[2023-01-31T05:30:49.845+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [running]> on host 81d5fcd0285b
[2023-01-31T05:30:50.560+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T06:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T06:00:00+00:00
[2023-01-31T05:30:50.671+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T05:30:50.697+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010600 202101010700 DE_TENNET
[2023-01-31T05:32:11.817+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:11 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T05:32:12.336+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T05:32:15.826+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:15 INFO ResourceUtils: ==============================================================
[2023-01-31T05:32:15.852+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:15 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T05:32:15.866+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:15 INFO ResourceUtils: ==============================================================
[2023-01-31T05:32:15.868+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:15 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T05:32:17.071+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:17 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T05:32:17.120+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:17 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T05:32:17.128+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:17 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T05:32:18.035+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:18 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T05:32:18.069+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:18 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T05:32:18.088+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:18 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T05:32:18.101+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:18 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T05:32:18.115+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:18 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T05:32:25.956+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:25 INFO Utils: Successfully started service 'sparkDriver' on port 38063.
[2023-01-31T05:32:26.781+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:26 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T05:32:27.025+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:27 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T05:32:27.512+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:27 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T05:32:27.523+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:27 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T05:32:27.551+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:27 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T05:32:27.913+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:27 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-4f9253cb-84a8-4075-b181-61ee87488360
[2023-01-31T05:32:29.321+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:29 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T05:32:29.639+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:29 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T05:32:36.029+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:36 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T05:32:36.032+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:36 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2023-01-31T05:32:36.060+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:36 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[2023-01-31T05:32:36.071+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:36 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[2023-01-31T05:32:36.088+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:36 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[2023-01-31T05:32:36.173+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:36 INFO Utils: Successfully started service 'SparkUI' on port 4045.
[2023-01-31T05:32:37.325+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:37 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://81d5fcd0285b:38063/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675143131701
[2023-01-31T05:32:37.326+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:37 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://81d5fcd0285b:38063/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675143131701
[2023-01-31T05:32:39.496+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:39 INFO Executor: Starting executor ID driver on host 81d5fcd0285b
[2023-01-31T05:32:39.550+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:39 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T05:32:40.063+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:40 INFO Executor: Fetching spark://81d5fcd0285b:38063/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675143131701
[2023-01-31T05:32:41.795+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:41 INFO TransportClientFactory: Successfully created connection to 81d5fcd0285b/172.19.0.9:38063 after 1055 ms (0 ms spent in bootstraps)
[2023-01-31T05:32:42.285+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:42 INFO Utils: Fetching spark://81d5fcd0285b:38063/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-e89d0ff7-1da0-42b5-b7bf-bad23b07d899/userFiles-278161f1-4b50-46b8-ae75-a9967acf7150/fetchFileTemp14362606257490450515.tmp
[2023-01-31T05:32:50.859+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:50 INFO Executor: Adding file:/tmp/spark-e89d0ff7-1da0-42b5-b7bf-bad23b07d899/userFiles-278161f1-4b50-46b8-ae75-a9967acf7150/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T05:32:50.882+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:50 INFO Executor: Fetching spark://81d5fcd0285b:38063/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675143131701
[2023-01-31T05:32:50.930+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:50 INFO Utils: Fetching spark://81d5fcd0285b:38063/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-e89d0ff7-1da0-42b5-b7bf-bad23b07d899/userFiles-278161f1-4b50-46b8-ae75-a9967acf7150/fetchFileTemp18027940009745349618.tmp
[2023-01-31T05:32:55.199+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:55 INFO Executor: Told to re-register on heartbeat
[2023-01-31T05:32:55.206+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:55 INFO BlockManager: BlockManager null re-registering with master
[2023-01-31T05:32:55.243+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:55 INFO BlockManagerMaster: Registering BlockManager null
[2023-01-31T05:32:55.435+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:55 ERROR Inbox: Ignoring error
[2023-01-31T05:32:55.436+0000] {spark_submit.py:495} INFO - java.lang.NullPointerException
[2023-01-31T05:32:55.436+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)
[2023-01-31T05:32:55.437+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)
[2023-01-31T05:32:55.438+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-01-31T05:32:55.438+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-01-31T05:32:55.439+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-01-31T05:32:55.439+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-01-31T05:32:55.440+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-01-31T05:32:55.465+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-01-31T05:32:55.465+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-01-31T05:32:55.466+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-31T05:32:55.467+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-31T05:32:55.467+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-31T05:32:55.511+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:55 INFO Executor: Adding file:/tmp/spark-e89d0ff7-1da0-42b5-b7bf-bad23b07d899/userFiles-278161f1-4b50-46b8-ae75-a9967acf7150/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T05:32:55.516+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:55 WARN Executor: Issue communicating with driver in heartbeater
[2023-01-31T05:32:55.517+0000] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-01-31T05:32:55.518+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-01-31T05:32:55.519+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-01-31T05:32:55.519+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2023-01-31T05:32:55.520+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2023-01-31T05:32:55.549+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:79)
[2023-01-31T05:32:55.550+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:643)
[2023-01-31T05:32:55.550+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2023-01-31T05:32:55.551+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2023-01-31T05:32:55.552+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
[2023-01-31T05:32:55.573+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2023-01-31T05:32:55.574+0000] {spark_submit.py:495} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2023-01-31T05:32:55.581+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-01-31T05:32:55.582+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2023-01-31T05:32:55.583+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2023-01-31T05:32:55.584+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-31T05:32:55.584+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-31T05:32:55.610+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-31T05:32:55.610+0000] {spark_submit.py:495} INFO - Caused by: java.lang.NullPointerException
[2023-01-31T05:32:55.611+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)
[2023-01-31T05:32:55.612+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)
[2023-01-31T05:32:55.612+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-01-31T05:32:55.673+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-01-31T05:32:55.674+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-01-31T05:32:55.701+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-01-31T05:32:55.702+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-01-31T05:32:55.702+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-01-31T05:32:55.703+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-01-31T05:32:55.703+0000] {spark_submit.py:495} INFO - ... 3 more
[2023-01-31T05:32:55.825+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:55 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36307.
[2023-01-31T05:32:55.826+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:55 INFO NettyBlockTransferService: Server created on 81d5fcd0285b:36307
[2023-01-31T05:32:55.938+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:55 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T05:32:56.006+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:56 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 36307, None)
[2023-01-31T05:32:56.065+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:56 INFO BlockManagerMasterEndpoint: Registering block manager 81d5fcd0285b:36307 with 434.4 MiB RAM, BlockManagerId(driver, 81d5fcd0285b, 36307, None)
[2023-01-31T05:32:56.111+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:56 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 36307, None)
[2023-01-31T05:32:56.128+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:56 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 81d5fcd0285b, 36307, None)
[2023-01-31T05:33:15.753+0000] {spark_submit.py:495} INFO - 23/01/31 05:33:15 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T05:33:15.829+0000] {spark_submit.py:495} INFO - 23/01/31 05:33:15 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T05:34:17.633+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:17 INFO InMemoryFileIndex: It took 2848 ms to list leaf files for 1 paths.
[2023-01-31T05:34:23.026+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:23 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T05:34:25.399+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:25 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T05:34:25.506+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:25 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 81d5fcd0285b:36307 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T05:34:25.609+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:25 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T05:34:37.097+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:37 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T05:34:37.773+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:37 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T05:34:38.281+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:38 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T05:34:38.597+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:38 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T05:34:38.620+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:38 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T05:34:38.637+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:38 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:34:38.717+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:38 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:34:38.857+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:38 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T05:34:42.948+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:42 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T05:34:43.038+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:43 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T05:34:43.050+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:43 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 81d5fcd0285b:36307 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T05:34:43.132+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:43 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:34:43.560+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:34:43.658+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:43 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T05:34:45.246+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:45 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T05:34:45.405+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:45 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T05:34:48.936+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:48 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010600__202101010700.json:0+9371
[2023-01-31T05:34:56.105+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:56 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3480 bytes result sent to driver
[2023-01-31T05:34:56.590+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:56 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 11387 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:34:56.683+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:56 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T05:34:56.981+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:56 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 17.480 s
[2023-01-31T05:34:57.127+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:57 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:34:57.137+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T05:34:57.184+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:57 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 18.915667 s
[2023-01-31T05:35:19.584+0000] {spark_submit.py:495} INFO - 23/01/31 05:35:19 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 81d5fcd0285b:36307 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T05:35:19.862+0000] {spark_submit.py:495} INFO - 23/01/31 05:35:19 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 81d5fcd0285b:36307 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T05:36:38.627+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:38 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:36:38.704+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:38 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:36:38.794+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:38 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:36:50.886+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:50 INFO CodeGenerator: Code generated in 4619.696551 ms
[2023-01-31T05:36:51.417+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:51 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T05:36:52.716+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:52 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T05:36:52.770+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:52 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 81d5fcd0285b:36307 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T05:36:52.933+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:52 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T05:36:53.609+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:53 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:36:55.732+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:55 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T05:36:55.760+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:55 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T05:36:55.765+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:55 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T05:36:55.773+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:55 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:36:55.774+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:55 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:36:55.775+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:55 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T05:36:55.924+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:55 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T05:36:55.961+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:55 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T05:36:55.966+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:55 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 81d5fcd0285b:36307 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T05:36:55.980+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:55 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:36:55.994+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:36:56.002+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:56 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T05:36:56.167+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:56 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:36:56.189+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:56 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T05:36:57.845+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:57 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T05:37:01.928+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:01 INFO CodeGenerator: Code generated in 3498.614469 ms
[2023-01-31T05:37:04.165+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:04 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1628 bytes result sent to driver
[2023-01-31T05:37:04.220+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:04 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 8193 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:37:04.245+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:04 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T05:37:04.261+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:04 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 8.430 s
[2023-01-31T05:37:04.269+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:04 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:37:04.276+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T05:37:04.297+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:04 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 8.563599 s
[2023-01-31T05:37:06.715+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:06 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:37:06.728+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:06 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:37:06.817+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:06 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:37:08.307+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:08 INFO CodeGenerator: Code generated in 879.718715 ms
[2023-01-31T05:37:08.821+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:08 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T05:37:10.368+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:10 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T05:37:10.520+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:10 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 81d5fcd0285b:36307 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:37:10.590+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:10 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T05:37:10.653+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:10 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:37:11.155+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 81d5fcd0285b:36307 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T05:37:11.452+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T05:37:11.462+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T05:37:11.467+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T05:37:11.553+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:37:11.578+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:37:11.612+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T05:37:11.672+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T05:37:11.681+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T05:37:11.727+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 81d5fcd0285b:36307 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T05:37:11.777+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:37:11.879+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:37:11.901+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T05:37:11.991+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:37:12.021+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:12 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T05:37:13.173+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:13 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T05:37:17.676+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:17 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1585 bytes result sent to driver
[2023-01-31T05:37:17.755+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:17 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 5761 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:37:17.763+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:17 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T05:37:17.801+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:17 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 6.156 s
[2023-01-31T05:37:17.861+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:17 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:37:17.870+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T05:37:17.882+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:17 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 6.417992 s
[2023-01-31T05:37:21.681+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:21 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:37:21.700+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:21 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T05:37:21.746+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:21 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:37:29.004+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO CodeGenerator: Code generated in 5327.690303 ms
[2023-01-31T05:37:29.065+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T05:37:29.197+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T05:37:29.202+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 81d5fcd0285b:36307 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:37:29.213+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T05:37:29.227+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:37:29.486+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T05:37:29.491+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T05:37:29.491+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T05:37:29.495+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:37:29.496+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:37:29.503+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T05:37:29.540+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.7 MiB)
[2023-01-31T05:37:29.605+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.6 MiB)
[2023-01-31T05:37:29.608+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 81d5fcd0285b:36307 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T05:37:29.611+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:37:29.616+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:37:29.617+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T05:37:29.622+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:37:29.625+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T05:37:29.709+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T05:37:29.937+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 81d5fcd0285b:36307 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T05:37:30.461+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:30 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1755 bytes result sent to driver
[2023-01-31T05:37:30.483+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:30 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 863 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:37:30.486+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:30 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T05:37:30.487+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:30 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 0.972 s
[2023-01-31T05:37:30.489+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:30 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:37:30.490+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T05:37:30.500+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:30 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 1.009733 s
[2023-01-31T05:37:31.332+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:31 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:37:31.333+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:31 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:37:31.333+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:31 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:37:32.361+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:32 INFO CodeGenerator: Code generated in 457.923908 ms
[2023-01-31T05:37:32.558+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:32 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.5 MiB)
[2023-01-31T05:37:33.256+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:33 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T05:37:33.263+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:33 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 81d5fcd0285b:36307 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:37:33.295+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:33 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T05:37:33.386+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:37:34.447+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:34 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T05:37:34.458+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:34 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T05:37:34.462+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:34 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T05:37:34.468+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:34 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:37:34.469+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:34 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:37:34.518+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:34 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T05:37:34.839+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:34 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-31T05:37:34.935+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:34 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-01-31T05:37:34.982+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:34 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 81d5fcd0285b:36307 (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T05:37:35.023+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:34 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:37:35.085+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:37:35.087+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:35 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T05:37:35.134+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:35 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:37:35.149+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:35 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T05:37:35.590+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:35 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T05:37:36.851+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:36 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1586 bytes result sent to driver
[2023-01-31T05:37:36.876+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:36 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 1744 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:37:36.885+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:36 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 2.212 s
[2023-01-31T05:37:36.887+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:36 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T05:37:36.891+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:36 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:37:36.895+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T05:37:36.901+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:36 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 2.446376 s
[2023-01-31T05:37:44.283+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:44 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:37:44.287+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:44 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T05:37:44.290+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:44 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:37:45.211+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:45 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 81d5fcd0285b:36307 in memory (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T05:37:45.414+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:45 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 81d5fcd0285b:36307 in memory (size: 7.6 KiB, free: 434.3 MiB)
[2023-01-31T05:37:48.750+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:48 INFO CodeGenerator: Code generated in 3706.47268 ms
[2023-01-31T05:37:48.798+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:48 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.3 MiB)
[2023-01-31T05:37:49.148+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:49 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T05:37:49.167+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:49 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 81d5fcd0285b:36307 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:37:49.180+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:49 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T05:37:49.215+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:49 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:37:50.171+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:50 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T05:37:50.186+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:50 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) with 1 output partitions
[2023-01-31T05:37:50.188+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:50 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201)
[2023-01-31T05:37:50.189+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:50 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:37:50.204+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:50 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:37:50.230+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:50 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201), which has no missing parents
[2023-01-31T05:37:50.392+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:50 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.2 MiB)
[2023-01-31T05:37:50.459+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:50 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.2 MiB)
[2023-01-31T05:37:50.465+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:50 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 81d5fcd0285b:36307 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T05:37:50.496+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:50 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:37:50.500+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:37:50.533+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:50 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T05:37:50.540+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:50 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:37:50.541+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:50 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T05:37:50.866+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:50 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T05:37:51.910+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:51 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2499 bytes result sent to driver
[2023-01-31T05:37:51.924+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:51 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 1387 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:37:51.925+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:51 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T05:37:51.930+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:51 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) finished in 1.677 s
[2023-01-31T05:37:51.939+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:51 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:37:51.949+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T05:37:51.961+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:51 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201, took 1.785086 s
[2023-01-31T05:37:53.868+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:53 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:37:53.912+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:53 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T05:37:53.952+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:53 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:37:56.533+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:56 INFO CodeGenerator: Code generated in 949.63418 ms
[2023-01-31T05:37:56.757+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:56 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 433.0 MiB)
[2023-01-31T05:37:57.624+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:57 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.0 MiB)
[2023-01-31T05:37:57.638+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:57 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 81d5fcd0285b:36307 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:37:57.704+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:57 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T05:37:57.782+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:57 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:37:58.826+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:58 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T05:37:58.861+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:58 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T05:37:58.862+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:58 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T05:37:58.862+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:58 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:37:58.863+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:58 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:37:58.918+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:58 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T05:37:59.033+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:59 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 432.9 MiB)
[2023-01-31T05:37:59.126+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:59 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.9 MiB)
[2023-01-31T05:37:59.143+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:59 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 81d5fcd0285b:36307 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T05:37:59.157+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:59 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:37:59.238+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:37:59.239+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:59 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T05:37:59.266+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:59 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:37:59.270+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:59 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T05:37:59.791+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:59 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T05:38:01.096+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:01 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-01-31T05:38:01.112+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:01 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 1835 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:38:01.115+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:01 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T05:38:01.116+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:01 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 2.129 s
[2023-01-31T05:38:01.134+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:01 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:38:01.138+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T05:38:01.139+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:01 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 2.281965 s
[2023-01-31T05:38:02.380+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:02 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:38:02.413+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:02 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T05:38:02.425+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:02 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:38:05.315+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:05 INFO CodeGenerator: Code generated in 2067.130993 ms
[2023-01-31T05:38:05.418+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:05 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.7 MiB)
[2023-01-31T05:38:06.049+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:06 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.7 MiB)
[2023-01-31T05:38:06.052+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:06 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 81d5fcd0285b:36307 (size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T05:38:06.097+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:06 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T05:38:06.242+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:06 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:38:07.710+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:07 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T05:38:07.721+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:07 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T05:38:07.724+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:07 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T05:38:07.734+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:07 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:38:07.755+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:07 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:38:07.765+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:07 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T05:38:07.874+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:07 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.7 MiB)
[2023-01-31T05:38:07.948+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:07 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.6 MiB)
[2023-01-31T05:38:07.955+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:07 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 81d5fcd0285b:36307 (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T05:38:07.967+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:07 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:38:07.986+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:38:07.993+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:07 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T05:38:08.057+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:08 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:38:08.065+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:08 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T05:38:08.426+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:08 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T05:38:09.336+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:09 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1772 bytes result sent to driver
[2023-01-31T05:38:09.348+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:09 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 1290 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:38:09.350+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:09 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T05:38:09.356+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:09 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 1.547 s
[2023-01-31T05:38:09.359+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:09 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:38:09.359+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T05:38:09.363+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:09 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 1.649337 s
[2023-01-31T05:38:37.675+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:37 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 81d5fcd0285b:36307 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:38:37.721+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:37 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 81d5fcd0285b:36307 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T05:38:37.837+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:37 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 81d5fcd0285b:36307 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:38:37.935+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:37 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 81d5fcd0285b:36307 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:38:38.053+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:38 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 81d5fcd0285b:36307 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:38:38.148+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:38 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 81d5fcd0285b:36307 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:38:38.222+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:38 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 81d5fcd0285b:36307 in memory (size: 13.4 KiB, free: 434.3 MiB)
[2023-01-31T05:38:38.320+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:38 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 81d5fcd0285b:36307 in memory (size: 12.6 KiB, free: 434.3 MiB)
[2023-01-31T05:38:38.407+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:38 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 81d5fcd0285b:36307 in memory (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T05:39:08.611+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:08 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:39:08.930+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:39:08.931+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:39:08.937+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:08 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:39:08.946+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:39:08.947+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:39:08.973+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:39:11.432+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:11 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-31T05:39:11.437+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:11 INFO DAGScheduler: Got job 8 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T05:39:11.438+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:11 INFO DAGScheduler: Final stage: ResultStage 8 (save at BigQueryWriteHelper.java:105)
[2023-01-31T05:39:11.441+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:11 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:39:11.443+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:11 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:39:11.454+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:11 INFO DAGScheduler: Submitting ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T05:39:11.648+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:11 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 209.9 KiB, free 434.0 MiB)
[2023-01-31T05:39:11.723+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:11 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 74.3 KiB, free 433.9 MiB)
[2023-01-31T05:39:11.729+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:11 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 81d5fcd0285b:36307 (size: 74.3 KiB, free: 434.3 MiB)
[2023-01-31T05:39:11.746+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:11 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:39:11.766+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:39:11.770+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:11 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2023-01-31T05:39:11.845+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:11 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7484 bytes) taskResourceAssignments Map()
[2023-01-31T05:39:11.871+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:11 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2023-01-31T05:39:12.686+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:39:12.689+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:39:12.718+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:12 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:39:12.726+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:39:12.733+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:39:12.748+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:39:12.930+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:12 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T05:39:12.947+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:12 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T05:39:13.309+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:13 INFO ParquetOutputFormat: Parquet block size to 134217728
[2023-01-31T05:39:13.313+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:13 INFO ParquetOutputFormat: Validation is off
[2023-01-31T05:39:13.316+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:13 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-01-31T05:39:13.321+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:13 INFO ParquetOutputFormat: Parquet properties are:
[2023-01-31T05:39:13.322+0000] {spark_submit.py:495} INFO - Parquet page size to 1048576
[2023-01-31T05:39:13.323+0000] {spark_submit.py:495} INFO - Parquet dictionary page size to 1048576
[2023-01-31T05:39:13.324+0000] {spark_submit.py:495} INFO - Dictionary is true
[2023-01-31T05:39:13.326+0000] {spark_submit.py:495} INFO - Writer version is: PARQUET_1_0
[2023-01-31T05:39:13.326+0000] {spark_submit.py:495} INFO - Page size checking is: estimated
[2023-01-31T05:39:13.327+0000] {spark_submit.py:495} INFO - Min row count for page size check is: 100
[2023-01-31T05:39:13.327+0000] {spark_submit.py:495} INFO - Max row count for page size check is: 10000
[2023-01-31T05:39:13.328+0000] {spark_submit.py:495} INFO - Truncate length for column indexes is: 64
[2023-01-31T05:39:13.328+0000] {spark_submit.py:495} INFO - Truncate length for statistics min/max  is: 2147483647
[2023-01-31T05:39:13.337+0000] {spark_submit.py:495} INFO - Bloom filter enabled: false
[2023-01-31T05:39:13.342+0000] {spark_submit.py:495} INFO - Max Bloom filter size for a column is 1048576
[2023-01-31T05:39:13.343+0000] {spark_submit.py:495} INFO - Bloom filter expected number of distinct values are: null
[2023-01-31T05:39:13.347+0000] {spark_submit.py:495} INFO - Page row count limit to 20000
[2023-01-31T05:39:13.352+0000] {spark_submit.py:495} INFO - Writing page checksums is: on
[2023-01-31T05:39:13.864+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:13 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-01-31T05:39:13.865+0000] {spark_submit.py:495} INFO - {
[2023-01-31T05:39:13.865+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-31T05:39:13.866+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-31T05:39:13.867+0000] {spark_submit.py:495} INFO - "name" : "measured_at",
[2023-01-31T05:39:13.867+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T05:39:13.868+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T05:39:13.868+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:39:13.870+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:39:13.871+0000] {spark_submit.py:495} INFO - "name" : "created_at",
[2023-01-31T05:39:13.896+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T05:39:13.900+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T05:39:13.911+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:39:13.933+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:39:13.950+0000] {spark_submit.py:495} INFO - "name" : "Biomass - Actual Aggregated",
[2023-01-31T05:39:13.980+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:39:13.985+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:39:13.987+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:39:13.993+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:39:13.994+0000] {spark_submit.py:495} INFO - "name" : "Fossil Brown coal/Lignite - Actual Aggregated",
[2023-01-31T05:39:14.006+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:39:14.007+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:39:14.008+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:39:14.018+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:39:14.021+0000] {spark_submit.py:495} INFO - "name" : "Fossil Gas - Actual Aggregated",
[2023-01-31T05:39:14.027+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:39:14.033+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:39:14.039+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:39:14.044+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:39:14.050+0000] {spark_submit.py:495} INFO - "name" : "Fossil Hard coal - Actual Aggregated",
[2023-01-31T05:39:14.061+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:39:14.062+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:39:14.063+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:39:14.063+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:39:14.064+0000] {spark_submit.py:495} INFO - "name" : "Fossil Oil - Actual Aggregated",
[2023-01-31T05:39:14.079+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:39:14.081+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:39:14.085+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:39:14.104+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:39:14.110+0000] {spark_submit.py:495} INFO - "name" : "Geothermal - Actual Aggregated",
[2023-01-31T05:39:14.116+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:39:14.125+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:39:14.131+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:39:14.137+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:39:14.138+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Aggregated",
[2023-01-31T05:39:14.141+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:39:14.142+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:39:14.143+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:39:14.143+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:39:14.144+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Consumption",
[2023-01-31T05:39:14.144+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:39:14.149+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:39:14.155+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:39:14.160+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:39:14.165+0000] {spark_submit.py:495} INFO - "name" : "Hydro Run-of-river and poundage - Actual Aggregated",
[2023-01-31T05:39:14.168+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:39:14.172+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:39:14.176+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:39:14.180+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:39:14.184+0000] {spark_submit.py:495} INFO - "name" : "Hydro Water Reservoir - Actual Aggregated",
[2023-01-31T05:39:14.186+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:39:14.188+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:39:14.193+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:39:14.195+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:39:14.198+0000] {spark_submit.py:495} INFO - "name" : "Nuclear - Actual Aggregated",
[2023-01-31T05:39:14.206+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:39:14.215+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:39:14.216+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:39:14.224+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:39:14.225+0000] {spark_submit.py:495} INFO - "name" : "Other - Actual Aggregated",
[2023-01-31T05:39:14.226+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:39:14.227+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:39:14.227+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:39:14.230+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:39:14.235+0000] {spark_submit.py:495} INFO - "name" : "Other renewable - Actual Aggregated",
[2023-01-31T05:39:14.244+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:39:14.246+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:39:14.249+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:39:14.250+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:39:14.253+0000] {spark_submit.py:495} INFO - "name" : "Solar - Actual Aggregated",
[2023-01-31T05:39:14.261+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:39:14.266+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:39:14.268+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:39:14.271+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:39:14.274+0000] {spark_submit.py:495} INFO - "name" : "Waste - Actual Aggregated",
[2023-01-31T05:39:14.280+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:39:14.290+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:39:14.294+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:39:14.301+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:39:14.307+0000] {spark_submit.py:495} INFO - "name" : "Wind Offshore - Actual Aggregated",
[2023-01-31T05:39:14.311+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:39:14.323+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:39:14.327+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:39:14.329+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:39:14.331+0000] {spark_submit.py:495} INFO - "name" : "Wind Onshore - Actual Aggregated",
[2023-01-31T05:39:14.333+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:39:14.334+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:39:14.336+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:39:14.341+0000] {spark_submit.py:495} INFO - } ]
[2023-01-31T05:39:14.343+0000] {spark_submit.py:495} INFO - }
[2023-01-31T05:39:14.344+0000] {spark_submit.py:495} INFO - and corresponding Parquet message type:
[2023-01-31T05:39:14.347+0000] {spark_submit.py:495} INFO - message spark_schema {
[2023-01-31T05:39:14.349+0000] {spark_submit.py:495} INFO - required int96 measured_at;
[2023-01-31T05:39:14.350+0000] {spark_submit.py:495} INFO - required int96 created_at;
[2023-01-31T05:39:14.353+0000] {spark_submit.py:495} INFO - optional double Biomass - Actual Aggregated;
[2023-01-31T05:39:14.354+0000] {spark_submit.py:495} INFO - optional double Fossil Brown coal/Lignite - Actual Aggregated;
[2023-01-31T05:39:14.356+0000] {spark_submit.py:495} INFO - optional double Fossil Gas - Actual Aggregated;
[2023-01-31T05:39:14.357+0000] {spark_submit.py:495} INFO - optional double Fossil Hard coal - Actual Aggregated;
[2023-01-31T05:39:14.361+0000] {spark_submit.py:495} INFO - optional double Fossil Oil - Actual Aggregated;
[2023-01-31T05:39:14.364+0000] {spark_submit.py:495} INFO - optional double Geothermal - Actual Aggregated;
[2023-01-31T05:39:14.366+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Aggregated;
[2023-01-31T05:39:14.368+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Consumption;
[2023-01-31T05:39:14.369+0000] {spark_submit.py:495} INFO - optional double Hydro Run-of-river and poundage - Actual Aggregated;
[2023-01-31T05:39:14.371+0000] {spark_submit.py:495} INFO - optional double Hydro Water Reservoir - Actual Aggregated;
[2023-01-31T05:39:14.375+0000] {spark_submit.py:495} INFO - optional double Nuclear - Actual Aggregated;
[2023-01-31T05:39:14.375+0000] {spark_submit.py:495} INFO - optional double Other - Actual Aggregated;
[2023-01-31T05:39:14.393+0000] {spark_submit.py:495} INFO - optional double Other renewable - Actual Aggregated;
[2023-01-31T05:39:14.396+0000] {spark_submit.py:495} INFO - optional double Solar - Actual Aggregated;
[2023-01-31T05:39:14.405+0000] {spark_submit.py:495} INFO - optional double Waste - Actual Aggregated;
[2023-01-31T05:39:14.411+0000] {spark_submit.py:495} INFO - optional double Wind Offshore - Actual Aggregated;
[2023-01-31T05:39:14.416+0000] {spark_submit.py:495} INFO - optional double Wind Onshore - Actual Aggregated;
[2023-01-31T05:39:14.434+0000] {spark_submit.py:495} INFO - }
[2023-01-31T05:39:14.437+0000] {spark_submit.py:495} INFO - 
[2023-01-31T05:39:14.440+0000] {spark_submit.py:495} INFO - 
[2023-01-31T05:39:16.025+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:16 INFO CodecPool: Got brand-new compressor [.snappy]
[2023-01-31T05:39:29.329+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:29 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675143158616-81ec2dd0-39fd-4bd5-9bc9-398227d4ae7e/_temporary/0/_temporary/' directory.
[2023-01-31T05:39:29.338+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:29 INFO FileOutputCommitter: Saved output of task 'attempt_202301310539105163398628901132425_0008_m_000000_8' to gs://entsoe_temp_1009/.spark-bigquery-local-1675143158616-81ec2dd0-39fd-4bd5-9bc9-398227d4ae7e/_temporary/0/task_202301310539105163398628901132425_0008_m_000000
[2023-01-31T05:39:29.339+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:29 INFO SparkHadoopMapRedUtil: attempt_202301310539105163398628901132425_0008_m_000000_8: Committed. Elapsed time: 2856 ms.
[2023-01-31T05:39:29.687+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:29 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2588 bytes result sent to driver
[2023-01-31T05:39:29.710+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:29 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 17921 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:39:29.716+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:29 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2023-01-31T05:39:29.831+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:29 INFO DAGScheduler: ResultStage 8 (save at BigQueryWriteHelper.java:105) finished in 18.257 s
[2023-01-31T05:39:29.841+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:29 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:39:29.842+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2023-01-31T05:39:29.843+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:29 INFO DAGScheduler: Job 8 finished: save at BigQueryWriteHelper.java:105, took 18.404606 s
[2023-01-31T05:39:29.857+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:29 INFO FileFormatWriter: Start to commit write Job 274d7189-95e4-49ac-a01f-41287bcbd33a.
[2023-01-31T05:39:31.185+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:31 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675143158616-81ec2dd0-39fd-4bd5-9bc9-398227d4ae7e/_temporary/0/task_202301310539105163398628901132425_0008_m_000000/' directory.
[2023-01-31T05:39:31.584+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:31 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675143158616-81ec2dd0-39fd-4bd5-9bc9-398227d4ae7e/' directory.
[2023-01-31T05:39:32.301+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:32 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 81d5fcd0285b:36307 in memory (size: 74.3 KiB, free: 434.4 MiB)
[2023-01-31T05:39:32.486+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:32 INFO FileFormatWriter: Write Job 274d7189-95e4-49ac-a01f-41287bcbd33a committed. Elapsed time: 2615 ms.
[2023-01-31T05:39:32.516+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:32 INFO FileFormatWriter: Finished processing stats for write job 274d7189-95e4-49ac-a01f-41287bcbd33a.
[2023-01-31T05:39:34.853+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:34 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=entsoe_analytics_1009, projectId=rafzul-analytics-1009, tableId=TEST_total_generation_staging}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=null, ignoreUnknownValue=null, sourceUris=[gs://entsoe_temp_1009/.spark-bigquery-local-1675143158616-81ec2dd0-39fd-4bd5-9bc9-398227d4ae7e/part-00000-7204c14a-f28c-4ae1-8bd5-ab747fd58b87-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=true, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=rafzul-analytics-1009, job=a4b0c663-2ef9-45dc-a0fd-08e1041b5fc6, location=US}
[2023-01-31T05:39:38.695+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:38 INFO BigQueryClient: Done loading to rafzul-analytics-1009.entsoe_analytics_1009.TEST_total_generation_staging. jobId: JobId{project=rafzul-analytics-1009, job=a4b0c663-2ef9-45dc-a0fd-08e1041b5fc6, location=US}
[2023-01-31T05:39:39.750+0000] {spark_submit.py:495} INFO - MANTEP!!!!
[2023-01-31T05:39:40.020+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:40 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-31T05:39:40.044+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:40 INFO SparkUI: Stopped Spark web UI at http://81d5fcd0285b:4045
[2023-01-31T05:39:40.073+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:40 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-31T05:39:40.106+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:40 INFO MemoryStore: MemoryStore cleared
[2023-01-31T05:39:40.107+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:40 INFO BlockManager: BlockManager stopped
[2023-01-31T05:39:40.113+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:40 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-31T05:39:40.119+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:40 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-31T05:39:40.130+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:40 INFO SparkContext: Successfully stopped SparkContext
[2023-01-31T05:39:40.131+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:40 INFO ShutdownHookManager: Shutdown hook called
[2023-01-31T05:39:40.132+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:40 INFO ShutdownHookManager: Deleting directory /tmp/spark-e89d0ff7-1da0-42b5-b7bf-bad23b07d899/pyspark-67cc98ab-1eb0-44f1-acdb-cbdb7fe018f1
[2023-01-31T05:39:40.140+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:40 INFO ShutdownHookManager: Deleting directory /tmp/spark-e89d0ff7-1da0-42b5-b7bf-bad23b07d899
[2023-01-31T05:39:40.148+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:40 INFO ShutdownHookManager: Deleting directory /tmp/spark-064b2e91-e689-4562-a56e-cf4552b2fc68
[2023-01-31T05:39:40.360+0000] {taskinstance.py:1322} INFO - Marking task as SUCCESS. dag_id=entsoe-energydata-backfill, task_id=stage_total_generation, execution_date=20210101T060000, start_date=20230131T053047, end_date=20230131T053940
[2023-01-31T05:39:40.442+0000] {local_task_job.py:159} INFO - Task exited with return code 0
[2023-01-31T05:39:40.495+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T07:39:04.192+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [queued]>
[2023-01-31T07:39:04.324+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [queued]>
[2023-01-31T07:39:04.325+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T07:39:04.326+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T07:39:04.327+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T07:39:04.418+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 06:00:00+00:00
[2023-01-31T07:39:04.547+0000] {standard_task_runner.py:55} INFO - Started process 25834 to run task
[2023-01-31T07:39:04.624+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T06:00:00+00:00', '--job-id', '913', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp9vk387_y']
[2023-01-31T07:39:04.672+0000] {standard_task_runner.py:83} INFO - Job 913: Subtask stage_total_generation
[2023-01-31T07:39:05.915+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [running]> on host 81d5fcd0285b
[2023-01-31T07:39:06.862+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T06:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T06:00:00+00:00
[2023-01-31T07:39:07.091+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T07:39:07.096+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010600 202101010700 DE_TENNET
[2023-01-31T07:40:24.664+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:24 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T07:40:26.811+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T07:40:31.887+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:31 INFO ResourceUtils: ==============================================================
[2023-01-31T07:40:31.999+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:31 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T07:40:32.034+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:31 INFO ResourceUtils: ==============================================================
[2023-01-31T07:40:32.142+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:32 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T07:40:33.517+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:33 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T07:40:33.855+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:33 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T07:40:33.930+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:33 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T07:40:35.951+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:35 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T07:40:36.057+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:36 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T07:40:36.160+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:36 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T07:40:36.268+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:36 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T07:40:36.316+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:36 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T07:40:49.226+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:49 INFO Utils: Successfully started service 'sparkDriver' on port 37871.
[2023-01-31T07:40:50.524+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:50 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T07:40:52.675+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:52 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T07:40:53.625+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:53 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T07:40:53.633+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:53 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T07:40:53.658+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:53 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T07:40:54.385+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:54 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b9aea595-5404-4d69-81ca-3545d4530195
[2023-01-31T07:40:54.517+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:54 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T07:40:55.865+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:55 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T07:41:03.137+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:03 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T07:41:03.157+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:03 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2023-01-31T07:41:03.183+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:03 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[2023-01-31T07:41:03.218+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:03 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[2023-01-31T07:41:03.257+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:03 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[2023-01-31T07:41:03.258+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:03 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
[2023-01-31T07:41:03.472+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:03 INFO Utils: Successfully started service 'SparkUI' on port 4046.
[2023-01-31T07:41:04.985+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:04 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://81d5fcd0285b:37871/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675150824563
[2023-01-31T07:41:05.005+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:04 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://81d5fcd0285b:37871/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675150824563
[2023-01-31T07:41:06.414+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:06 INFO Executor: Starting executor ID driver on host 81d5fcd0285b
[2023-01-31T07:41:06.684+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:06 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T07:41:07.034+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:07 INFO Executor: Fetching spark://81d5fcd0285b:37871/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675150824563
[2023-01-31T07:41:08.374+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:08 INFO TransportClientFactory: Successfully created connection to 81d5fcd0285b/172.19.0.9:37871 after 1135 ms (0 ms spent in bootstraps)
[2023-01-31T07:41:08.580+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:08 INFO Utils: Fetching spark://81d5fcd0285b:37871/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-36349aee-2534-4650-b9b1-43890d392200/userFiles-7b500f39-df78-488f-b7ec-30402b2c2342/fetchFileTemp11253485400958233692.tmp
[2023-01-31T07:41:15.523+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:15 INFO Executor: Adding file:/tmp/spark-36349aee-2534-4650-b9b1-43890d392200/userFiles-7b500f39-df78-488f-b7ec-30402b2c2342/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T07:41:15.525+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:15 INFO Executor: Fetching spark://81d5fcd0285b:37871/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675150824563
[2023-01-31T07:41:15.540+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:15 INFO Utils: Fetching spark://81d5fcd0285b:37871/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-36349aee-2534-4650-b9b1-43890d392200/userFiles-7b500f39-df78-488f-b7ec-30402b2c2342/fetchFileTemp17250526110526649466.tmp
[2023-01-31T07:41:20.958+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:20 INFO Executor: Adding file:/tmp/spark-36349aee-2534-4650-b9b1-43890d392200/userFiles-7b500f39-df78-488f-b7ec-30402b2c2342/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T07:41:21.069+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42647.
[2023-01-31T07:41:21.069+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:21 INFO NettyBlockTransferService: Server created on 81d5fcd0285b:42647
[2023-01-31T07:41:21.093+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:21 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T07:41:21.286+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:21 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 42647, None)
[2023-01-31T07:41:21.331+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:21 INFO BlockManagerMasterEndpoint: Registering block manager 81d5fcd0285b:42647 with 434.4 MiB RAM, BlockManagerId(driver, 81d5fcd0285b, 42647, None)
[2023-01-31T07:41:21.382+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:21 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 42647, None)
[2023-01-31T07:41:21.406+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:21 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 81d5fcd0285b, 42647, None)
[2023-01-31T07:41:25.466+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:25 INFO Executor: Told to re-register on heartbeat
[2023-01-31T07:41:25.543+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:25 INFO BlockManager: BlockManager BlockManagerId(driver, 81d5fcd0285b, 42647, None) re-registering with master
[2023-01-31T07:41:25.546+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:25 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 42647, None)
[2023-01-31T07:41:25.592+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:25 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 42647, None)
[2023-01-31T07:41:25.593+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:25 INFO BlockManager: Reporting 0 blocks to the master.
[2023-01-31T07:41:39.859+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:39 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T07:41:40.032+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:40 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T07:42:22.685+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:22 INFO InMemoryFileIndex: It took 1609 ms to list leaf files for 1 paths.
[2023-01-31T07:42:28.737+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:28 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T07:42:31.266+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:31 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T07:42:31.287+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:31 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 81d5fcd0285b:42647 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T07:42:31.385+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:31 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T07:42:45.272+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:45 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T07:42:45.822+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:45 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T07:42:46.442+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:46 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T07:42:46.856+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:46 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T07:42:46.863+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:46 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T07:42:46.865+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:46 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:42:46.886+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:46 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:42:46.938+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:46 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T07:42:49.661+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:49 INFO AsyncEventQueue: Process of event SparkListenerJobStart(0,1675150966888,ArraySeq(org.apache.spark.scheduler.StageInfo@271762d9),{spark.master=local, spark.driver.port=37871, spark.submit.pyFiles=, spark.app.startTime=1675150824563, spark.rdd.compress=True, spark.executor.id=driver, spark.app.name=gcp_playground, spark.submit.deployMode=client, spark.hadoop.google.cloud.auth.service.account.json.keyfile=/.google/credentials/google_credentials.json, spark.driver.host=81d5fcd0285b, spark.app.id=local-1675150865556, spark.app.submitTime=1675150804798, spark.hadoop.fs.AbstractFileSystem.gs.impl=google.cloud.hadoop.fs.gcs.GoogleHadoopFS, spark.sql.session.timeZone=UTC, spark.hadoop.google.cloud.auth.service.account.enable=true, spark.app.initial.jar.urls=spark://81d5fcd0285b:37871/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar,spark://81d5fcd0285b:37871/jars/gcs-connector-hadoop3-latest.jar, spark.executor.extraJavaOptions=-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED, spark.jars=/opt/spark/jars/gcs-connector-hadoop3-latest.jar, /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar, spark.sql.warehouse.dir=file:/opt/***/spark-warehouse, spark.serializer.objectStreamReset=100, spark.driver.extraJavaOptions=-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED}) by listener AppStatusListener took 2.609701602s.
[2023-01-31T07:42:50.764+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:50 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T07:42:50.850+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:50 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T07:42:50.878+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:50 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 81d5fcd0285b:42647 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T07:42:50.961+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:50 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:42:51.918+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:42:51.998+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:51 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T07:42:54.774+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:54 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T07:42:55.346+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:55 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T07:42:58.061+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:58 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010600__202101010700.json:0+9371
[2023-01-31T07:43:00.405+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:00 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3480 bytes result sent to driver
[2023-01-31T07:43:00.558+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:00 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 6492 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:43:00.598+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:00 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T07:43:00.722+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:00 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 12.262 s
[2023-01-31T07:43:00.822+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:00 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:43:00.843+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T07:43:00.929+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:00 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 14.483592 s
[2023-01-31T07:43:25.304+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:25 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 81d5fcd0285b:42647 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T07:43:25.412+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:25 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 81d5fcd0285b:42647 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T07:43:27.433+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:43:27.486+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T07:43:27.592+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:43:31.776+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:31 INFO CodeGenerator: Code generated in 1697.888888 ms
[2023-01-31T07:43:31.929+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:31 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T07:43:31.965+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:31 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T07:43:31.966+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:31 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 81d5fcd0285b:42647 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T07:43:31.968+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:31 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T07:43:32.010+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:43:32.821+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T07:43:32.822+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T07:43:32.823+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T07:43:32.823+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:43:32.825+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:43:32.855+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T07:43:33.009+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T07:43:33.122+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T07:43:33.129+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 81d5fcd0285b:42647 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T07:43:33.141+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:43:33.170+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:43:33.171+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T07:43:33.280+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:43:33.294+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T07:43:34.365+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T07:43:36.012+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:36 INFO CodeGenerator: Code generated in 1296.691482 ms
[2023-01-31T07:43:37.363+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:37 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1628 bytes result sent to driver
[2023-01-31T07:43:37.407+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:37 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 4214 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:43:37.420+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:37 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 4.491 s
[2023-01-31T07:43:37.421+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:37 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T07:43:37.430+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:37 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:43:37.456+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T07:43:37.457+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:37 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 4.634064 s
[2023-01-31T07:43:38.558+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:38 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:43:38.564+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:38 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T07:43:38.572+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:38 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:43:39.073+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:39 INFO CodeGenerator: Code generated in 283.730482 ms
[2023-01-31T07:43:39.153+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:39 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T07:43:39.705+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:39 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T07:43:39.719+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:39 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 81d5fcd0285b:42647 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:43:39.744+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:39 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T07:43:39.822+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:39 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:43:40.240+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T07:43:40.245+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T07:43:40.246+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T07:43:40.248+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:43:40.249+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:43:40.273+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T07:43:40.346+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T07:43:40.391+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T07:43:40.396+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 81d5fcd0285b:42647 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T07:43:40.414+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:43:40.431+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:43:40.431+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T07:43:40.444+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:43:40.463+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T07:43:40.764+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T07:43:42.508+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:42 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1585 bytes result sent to driver
[2023-01-31T07:43:42.624+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:42 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 2181 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:43:42.648+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:42 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 2.335 s
[2023-01-31T07:43:42.650+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:42 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:43:42.664+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:42 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T07:43:42.669+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T07:43:42.677+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:42 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 2.433340 s
[2023-01-31T07:43:46.138+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:46 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:43:46.161+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:46 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T07:43:46.183+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:46 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:43:46.878+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:46 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 81d5fcd0285b:42647 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T07:43:47.113+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:47 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 81d5fcd0285b:42647 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T07:43:48.789+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:48 INFO CodeGenerator: Code generated in 464.498202 ms
[2023-01-31T07:43:48.809+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:48 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T07:43:48.856+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:48 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T07:43:48.901+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:48 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 81d5fcd0285b:42647 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:43:48.902+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:48 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T07:43:48.921+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:43:49.996+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:49 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T07:43:50.005+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T07:43:50.006+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T07:43:50.007+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:43:50.013+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:43:50.029+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T07:43:50.169+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.7 MiB)
[2023-01-31T07:43:50.247+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.7 MiB)
[2023-01-31T07:43:50.263+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 81d5fcd0285b:42647 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T07:43:50.267+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:43:50.279+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:43:50.281+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T07:43:50.302+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:43:50.307+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T07:43:50.733+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T07:43:51.841+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:51 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-01-31T07:43:51.862+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:51 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 1562 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:43:51.868+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:51 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T07:43:51.885+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:51 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 1.800 s
[2023-01-31T07:43:51.891+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:51 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:43:51.892+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T07:43:51.906+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:51 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 1.901835 s
[2023-01-31T07:43:53.395+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:53 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:43:53.418+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:53 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T07:43:53.421+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:53 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:43:54.027+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:54 INFO CodeGenerator: Code generated in 379.117003 ms
[2023-01-31T07:43:54.163+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:54 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.5 MiB)
[2023-01-31T07:43:54.733+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:54 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T07:43:54.777+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:54 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 81d5fcd0285b:42647 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:43:54.778+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:54 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T07:43:54.793+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:54 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:43:55.805+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:55 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T07:43:55.853+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:55 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T07:43:55.854+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:55 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T07:43:55.854+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:55 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:43:55.879+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:55 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:43:55.880+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:55 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T07:43:56.059+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:56 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-31T07:43:56.120+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:56 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-01-31T07:43:56.152+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:56 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 81d5fcd0285b:42647 (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T07:43:56.190+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:56 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:43:56.204+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:43:56.211+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:56 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T07:43:56.226+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:56 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:43:56.227+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:56 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T07:43:56.576+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:56 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T07:43:57.620+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:57 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1629 bytes result sent to driver
[2023-01-31T07:43:57.633+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:57 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 1416 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:43:57.644+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:57 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 1.664 s
[2023-01-31T07:43:57.645+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:57 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:43:57.649+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:57 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T07:43:57.654+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T07:43:57.663+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:57 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 1.853308 s
[2023-01-31T07:44:06.133+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:06 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:44:06.223+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:06 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T07:44:06.290+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:06 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:44:10.400+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:10 INFO CodeGenerator: Code generated in 1938.734174 ms
[2023-01-31T07:44:10.417+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:10 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.2 MiB)
[2023-01-31T07:44:10.485+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:10 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T07:44:10.492+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:10 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 81d5fcd0285b:42647 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T07:44:10.506+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:10 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T07:44:10.583+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:10 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:44:11.547+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:11 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 81d5fcd0285b:42647 in memory (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T07:44:11.893+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:11 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 81d5fcd0285b:42647 in memory (size: 12.7 KiB, free: 434.2 MiB)
[2023-01-31T07:44:11.909+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:11 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T07:44:11.920+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:11 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) with 1 output partitions
[2023-01-31T07:44:11.922+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:11 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201)
[2023-01-31T07:44:11.923+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:11 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:44:11.936+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:11 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:44:11.962+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:11 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201), which has no missing parents
[2023-01-31T07:44:12.272+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:12 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.2 MiB)
[2023-01-31T07:44:12.347+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:12 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.2 MiB)
[2023-01-31T07:44:12.369+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:12 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 81d5fcd0285b:42647 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T07:44:12.392+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:12 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:44:12.395+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:44:12.464+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:12 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T07:44:12.521+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:12 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:44:12.535+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:12 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T07:44:13.036+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:13 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T07:44:14.963+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:14 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2499 bytes result sent to driver
[2023-01-31T07:44:14.974+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:14 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 2450 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:44:14.975+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:14 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T07:44:14.986+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:14 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) finished in 2.990 s
[2023-01-31T07:44:14.995+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:14 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:44:14.996+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T07:44:14.997+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:14 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201, took 3.079531 s
[2023-01-31T07:44:17.109+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:17 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:44:17.110+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:17 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T07:44:17.111+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:17 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:44:19.262+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:19 INFO CodeGenerator: Code generated in 1034.154904 ms
[2023-01-31T07:44:19.874+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:19 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 433.0 MiB)
[2023-01-31T07:44:19.975+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:19 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.0 MiB)
[2023-01-31T07:44:19.976+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:19 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 81d5fcd0285b:42647 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T07:44:19.980+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:19 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T07:44:20.025+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:20 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:44:20.728+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:20 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T07:44:20.789+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:20 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T07:44:20.801+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:20 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T07:44:20.802+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:20 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:44:20.833+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:20 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:44:20.859+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:20 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T07:44:21.293+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:21 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 432.9 MiB)
[2023-01-31T07:44:21.357+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:21 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.9 MiB)
[2023-01-31T07:44:21.377+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:21 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 81d5fcd0285b:42647 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T07:44:21.451+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:21 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:44:21.458+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:44:21.460+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:21 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T07:44:21.549+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:21 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:44:21.561+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:21 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T07:44:21.826+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:21 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T07:44:23.017+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:22 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-01-31T07:44:23.040+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:23 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 1484 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:44:23.041+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:23 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T07:44:23.069+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:23 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 2.034 s
[2023-01-31T07:44:23.070+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:23 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:44:23.070+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T07:44:23.071+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:23 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 2.319453 s
[2023-01-31T07:44:25.389+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:25 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:44:25.390+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:25 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T07:44:25.424+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:25 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:44:29.984+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:29 INFO CodeGenerator: Code generated in 2944.339645 ms
[2023-01-31T07:44:30.262+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:30 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.7 MiB)
[2023-01-31T07:44:31.197+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:31 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.7 MiB)
[2023-01-31T07:44:31.197+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:31 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 81d5fcd0285b:42647 (size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T07:44:31.225+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:31 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T07:44:31.349+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:31 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:44:32.345+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:32 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T07:44:32.346+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:32 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T07:44:32.365+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:32 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T07:44:32.366+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:32 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:44:32.366+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:32 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:44:32.367+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:32 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T07:44:32.437+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:32 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.7 MiB)
[2023-01-31T07:44:32.478+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:32 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.6 MiB)
[2023-01-31T07:44:32.483+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:32 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 81d5fcd0285b:42647 (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T07:44:32.495+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:32 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:44:32.521+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:44:32.525+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:32 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T07:44:32.550+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:32 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:44:32.554+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:32 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T07:44:32.721+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:32 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T07:44:33.205+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:33 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1729 bytes result sent to driver
[2023-01-31T07:44:33.225+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:33 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 662 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:44:33.247+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:33 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T07:44:33.248+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:33 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 0.855 s
[2023-01-31T07:44:33.249+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:33 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:44:33.249+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T07:44:33.250+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:33 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 0.908616 s
[2023-01-31T07:44:38.613+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:38 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 81d5fcd0285b:42647 in memory (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T07:44:38.836+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:38 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 81d5fcd0285b:42647 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T07:44:39.044+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:39 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 81d5fcd0285b:42647 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T07:44:39.469+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:39 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 81d5fcd0285b:42647 in memory (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T07:44:40.012+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:40 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 81d5fcd0285b:42647 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T07:44:40.220+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:40 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 81d5fcd0285b:42647 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:44:40.297+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:40 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 81d5fcd0285b:42647 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:44:40.471+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:40 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 81d5fcd0285b:42647 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:45:17.315+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:17 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T07:45:17.420+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T07:45:17.421+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T07:45:17.428+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:17 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T07:45:17.429+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T07:45:17.429+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T07:45:17.432+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T07:45:18.151+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-31T07:45:18.154+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO DAGScheduler: Got job 8 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T07:45:18.155+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO DAGScheduler: Final stage: ResultStage 8 (save at BigQueryWriteHelper.java:105)
[2023-01-31T07:45:18.158+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:45:18.158+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:45:18.163+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO DAGScheduler: Submitting ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T07:45:18.383+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 209.9 KiB, free 433.7 MiB)
[2023-01-31T07:45:18.428+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 74.3 KiB, free 433.7 MiB)
[2023-01-31T07:45:18.431+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 81d5fcd0285b:42647 (size: 74.3 KiB, free: 434.3 MiB)
[2023-01-31T07:45:18.435+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:45:18.449+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:45:18.449+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2023-01-31T07:45:18.468+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7484 bytes) taskResourceAssignments Map()
[2023-01-31T07:45:18.476+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2023-01-31T07:45:18.616+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T07:45:18.617+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T07:45:18.618+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T07:45:18.618+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T07:45:18.619+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T07:45:18.624+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T07:45:18.640+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T07:45:18.644+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T07:45:18.736+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO ParquetOutputFormat: Parquet block size to 134217728
[2023-01-31T07:45:18.737+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO ParquetOutputFormat: Validation is off
[2023-01-31T07:45:18.737+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-01-31T07:45:18.738+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO ParquetOutputFormat: Parquet properties are:
[2023-01-31T07:45:18.738+0000] {spark_submit.py:495} INFO - Parquet page size to 1048576
[2023-01-31T07:45:18.738+0000] {spark_submit.py:495} INFO - Parquet dictionary page size to 1048576
[2023-01-31T07:45:18.738+0000] {spark_submit.py:495} INFO - Dictionary is true
[2023-01-31T07:45:18.738+0000] {spark_submit.py:495} INFO - Writer version is: PARQUET_1_0
[2023-01-31T07:45:18.739+0000] {spark_submit.py:495} INFO - Page size checking is: estimated
[2023-01-31T07:45:18.739+0000] {spark_submit.py:495} INFO - Min row count for page size check is: 100
[2023-01-31T07:45:18.739+0000] {spark_submit.py:495} INFO - Max row count for page size check is: 10000
[2023-01-31T07:45:18.739+0000] {spark_submit.py:495} INFO - Truncate length for column indexes is: 64
[2023-01-31T07:45:18.739+0000] {spark_submit.py:495} INFO - Truncate length for statistics min/max  is: 2147483647
[2023-01-31T07:45:18.739+0000] {spark_submit.py:495} INFO - Bloom filter enabled: false
[2023-01-31T07:45:18.740+0000] {spark_submit.py:495} INFO - Max Bloom filter size for a column is 1048576
[2023-01-31T07:45:18.740+0000] {spark_submit.py:495} INFO - Bloom filter expected number of distinct values are: null
[2023-01-31T07:45:18.740+0000] {spark_submit.py:495} INFO - Page row count limit to 20000
[2023-01-31T07:45:18.740+0000] {spark_submit.py:495} INFO - Writing page checksums is: on
[2023-01-31T07:45:18.926+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-01-31T07:45:18.927+0000] {spark_submit.py:495} INFO - {
[2023-01-31T07:45:18.927+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-31T07:45:18.928+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-31T07:45:18.928+0000] {spark_submit.py:495} INFO - "name" : "measured_at",
[2023-01-31T07:45:18.928+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T07:45:18.928+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T07:45:18.929+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.929+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.929+0000] {spark_submit.py:495} INFO - "name" : "created_at",
[2023-01-31T07:45:18.929+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T07:45:18.929+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T07:45:18.930+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.930+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.930+0000] {spark_submit.py:495} INFO - "name" : "Biomass - Actual Aggregated",
[2023-01-31T07:45:18.930+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.931+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.931+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.931+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.931+0000] {spark_submit.py:495} INFO - "name" : "Fossil Brown coal/Lignite - Actual Aggregated",
[2023-01-31T07:45:18.931+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.931+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.932+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.932+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.932+0000] {spark_submit.py:495} INFO - "name" : "Fossil Gas - Actual Aggregated",
[2023-01-31T07:45:18.932+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.932+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.933+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.934+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.935+0000] {spark_submit.py:495} INFO - "name" : "Fossil Hard coal - Actual Aggregated",
[2023-01-31T07:45:18.935+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.936+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.937+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.939+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.939+0000] {spark_submit.py:495} INFO - "name" : "Fossil Oil - Actual Aggregated",
[2023-01-31T07:45:18.939+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.939+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.939+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.941+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.941+0000] {spark_submit.py:495} INFO - "name" : "Geothermal - Actual Aggregated",
[2023-01-31T07:45:18.941+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.943+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.943+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.944+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.949+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Aggregated",
[2023-01-31T07:45:18.949+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.949+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.950+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.950+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.950+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Consumption",
[2023-01-31T07:45:18.950+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.950+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.950+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.951+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.951+0000] {spark_submit.py:495} INFO - "name" : "Hydro Run-of-river and poundage - Actual Aggregated",
[2023-01-31T07:45:18.951+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.951+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.951+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.951+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.952+0000] {spark_submit.py:495} INFO - "name" : "Hydro Water Reservoir - Actual Aggregated",
[2023-01-31T07:45:18.952+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.952+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.952+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.952+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.963+0000] {spark_submit.py:495} INFO - "name" : "Nuclear - Actual Aggregated",
[2023-01-31T07:45:18.963+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.963+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.963+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.964+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.964+0000] {spark_submit.py:495} INFO - "name" : "Other - Actual Aggregated",
[2023-01-31T07:45:18.964+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.964+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.964+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.965+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.965+0000] {spark_submit.py:495} INFO - "name" : "Other renewable - Actual Aggregated",
[2023-01-31T07:45:18.965+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.965+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.965+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.966+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.966+0000] {spark_submit.py:495} INFO - "name" : "Solar - Actual Aggregated",
[2023-01-31T07:45:18.966+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.966+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.966+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.966+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.967+0000] {spark_submit.py:495} INFO - "name" : "Waste - Actual Aggregated",
[2023-01-31T07:45:18.967+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.967+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.967+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.967+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.967+0000] {spark_submit.py:495} INFO - "name" : "Wind Offshore - Actual Aggregated",
[2023-01-31T07:45:18.968+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.968+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.968+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.977+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.977+0000] {spark_submit.py:495} INFO - "name" : "Wind Onshore - Actual Aggregated",
[2023-01-31T07:45:18.977+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.977+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.979+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.980+0000] {spark_submit.py:495} INFO - } ]
[2023-01-31T07:45:18.980+0000] {spark_submit.py:495} INFO - }
[2023-01-31T07:45:18.980+0000] {spark_submit.py:495} INFO - and corresponding Parquet message type:
[2023-01-31T07:45:18.980+0000] {spark_submit.py:495} INFO - message spark_schema {
[2023-01-31T07:45:18.980+0000] {spark_submit.py:495} INFO - required int96 measured_at;
[2023-01-31T07:45:18.981+0000] {spark_submit.py:495} INFO - required int96 created_at;
[2023-01-31T07:45:18.981+0000] {spark_submit.py:495} INFO - optional double Biomass - Actual Aggregated;
[2023-01-31T07:45:18.981+0000] {spark_submit.py:495} INFO - optional double Fossil Brown coal/Lignite - Actual Aggregated;
[2023-01-31T07:45:18.981+0000] {spark_submit.py:495} INFO - optional double Fossil Gas - Actual Aggregated;
[2023-01-31T07:45:18.981+0000] {spark_submit.py:495} INFO - optional double Fossil Hard coal - Actual Aggregated;
[2023-01-31T07:45:18.981+0000] {spark_submit.py:495} INFO - optional double Fossil Oil - Actual Aggregated;
[2023-01-31T07:45:18.981+0000] {spark_submit.py:495} INFO - optional double Geothermal - Actual Aggregated;
[2023-01-31T07:45:18.982+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Aggregated;
[2023-01-31T07:45:18.982+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Consumption;
[2023-01-31T07:45:18.982+0000] {spark_submit.py:495} INFO - optional double Hydro Run-of-river and poundage - Actual Aggregated;
[2023-01-31T07:45:18.983+0000] {spark_submit.py:495} INFO - optional double Hydro Water Reservoir - Actual Aggregated;
[2023-01-31T07:45:18.983+0000] {spark_submit.py:495} INFO - optional double Nuclear - Actual Aggregated;
[2023-01-31T07:45:18.983+0000] {spark_submit.py:495} INFO - optional double Other - Actual Aggregated;
[2023-01-31T07:45:18.983+0000] {spark_submit.py:495} INFO - optional double Other renewable - Actual Aggregated;
[2023-01-31T07:45:18.984+0000] {spark_submit.py:495} INFO - optional double Solar - Actual Aggregated;
[2023-01-31T07:45:18.984+0000] {spark_submit.py:495} INFO - optional double Waste - Actual Aggregated;
[2023-01-31T07:45:18.984+0000] {spark_submit.py:495} INFO - optional double Wind Offshore - Actual Aggregated;
[2023-01-31T07:45:18.984+0000] {spark_submit.py:495} INFO - optional double Wind Onshore - Actual Aggregated;
[2023-01-31T07:45:18.984+0000] {spark_submit.py:495} INFO - }
[2023-01-31T07:45:18.985+0000] {spark_submit.py:495} INFO - 
[2023-01-31T07:45:18.985+0000] {spark_submit.py:495} INFO - 
[2023-01-31T07:45:19.677+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:19 INFO CodecPool: Got brand-new compressor [.snappy]
[2023-01-31T07:45:20.765+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:20 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 81d5fcd0285b:42647 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:45:29.633+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:29 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675150865556-37aece3d-4e7d-46cd-9fdc-798c246d4773/_temporary/0/_temporary/' directory.
[2023-01-31T07:45:29.635+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:29 INFO FileOutputCommitter: Saved output of task 'attempt_202301310745181247869227893411133_0008_m_000000_8' to gs://entsoe_temp_1009/.spark-bigquery-local-1675150865556-37aece3d-4e7d-46cd-9fdc-798c246d4773/_temporary/0/task_202301310745181247869227893411133_0008_m_000000
[2023-01-31T07:45:29.636+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:29 INFO SparkHadoopMapRedUtil: attempt_202301310745181247869227893411133_0008_m_000000_8: Committed. Elapsed time: 1822 ms.
[2023-01-31T07:45:29.698+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:29 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2588 bytes result sent to driver
[2023-01-31T07:45:29.699+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:29 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 11238 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:45:29.699+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:29 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2023-01-31T07:45:29.719+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:29 INFO DAGScheduler: ResultStage 8 (save at BigQueryWriteHelper.java:105) finished in 11.535 s
[2023-01-31T07:45:29.719+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:29 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:45:29.720+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2023-01-31T07:45:29.721+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:29 INFO DAGScheduler: Job 8 finished: save at BigQueryWriteHelper.java:105, took 11.553484 s
[2023-01-31T07:45:29.733+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:29 INFO FileFormatWriter: Start to commit write Job 4bd6c209-fad9-4e01-8c96-ef4d4b6f48fc.
[2023-01-31T07:45:30.922+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:30 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675150865556-37aece3d-4e7d-46cd-9fdc-798c246d4773/_temporary/0/task_202301310745181247869227893411133_0008_m_000000/' directory.
[2023-01-31T07:45:32.198+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:32 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675150865556-37aece3d-4e7d-46cd-9fdc-798c246d4773/' directory.
[2023-01-31T07:45:33.365+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:33 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 81d5fcd0285b:42647 in memory (size: 74.3 KiB, free: 434.4 MiB)
[2023-01-31T07:45:33.768+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:33 INFO FileFormatWriter: Write Job 4bd6c209-fad9-4e01-8c96-ef4d4b6f48fc committed. Elapsed time: 4017 ms.
[2023-01-31T07:45:33.816+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:33 INFO FileFormatWriter: Finished processing stats for write job 4bd6c209-fad9-4e01-8c96-ef4d4b6f48fc.
[2023-01-31T07:45:41.101+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:41 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=entsoe_analytics_1009, projectId=rafzul-analytics-1009, tableId=TEST_total_generation_staging}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=null, ignoreUnknownValue=null, sourceUris=[gs://entsoe_temp_1009/.spark-bigquery-local-1675150865556-37aece3d-4e7d-46cd-9fdc-798c246d4773/part-00000-1ea7f152-5492-4276-af7f-ddce3159e24a-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=true, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=rafzul-analytics-1009, job=d6d9dff3-30b7-4f47-b2f0-902edbb27742, location=US}
[2023-01-31T07:45:46.105+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:46 INFO BigQueryClient: Done loading to rafzul-analytics-1009.entsoe_analytics_1009.TEST_total_generation_staging. jobId: JobId{project=rafzul-analytics-1009, job=d6d9dff3-30b7-4f47-b2f0-902edbb27742, location=US}
[2023-01-31T07:45:47.413+0000] {spark_submit.py:495} INFO - MANTEP!!!!
[2023-01-31T07:45:47.776+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:47 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-31T07:45:47.808+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:47 INFO SparkUI: Stopped Spark web UI at http://81d5fcd0285b:4046
[2023-01-31T07:45:47.845+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:47 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-31T07:45:47.894+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:47 INFO MemoryStore: MemoryStore cleared
[2023-01-31T07:45:47.895+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:47 INFO BlockManager: BlockManager stopped
[2023-01-31T07:45:47.904+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:47 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-31T07:45:47.915+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:47 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-31T07:45:47.936+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:47 INFO SparkContext: Successfully stopped SparkContext
[2023-01-31T07:45:47.937+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:47 INFO ShutdownHookManager: Shutdown hook called
[2023-01-31T07:45:47.939+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:47 INFO ShutdownHookManager: Deleting directory /tmp/spark-36349aee-2534-4650-b9b1-43890d392200/pyspark-fd3950a5-4ad6-4071-95e2-af672c105892
[2023-01-31T07:45:47.957+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:47 INFO ShutdownHookManager: Deleting directory /tmp/spark-36349aee-2534-4650-b9b1-43890d392200
[2023-01-31T07:45:47.990+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:47 INFO ShutdownHookManager: Deleting directory /tmp/spark-08c10094-4ff3-4f41-96cd-480ec1eb7c88
[2023-01-31T07:45:48.300+0000] {taskinstance.py:1322} INFO - Marking task as SUCCESS. dag_id=entsoe-energydata-backfill, task_id=stage_total_generation, execution_date=20210101T060000, start_date=20230131T073904, end_date=20230131T074548
[2023-01-31T07:45:48.369+0000] {local_task_job.py:159} INFO - Task exited with return code 0
[2023-01-31T07:45:48.427+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T08:21:37.225+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [queued]>
[2023-01-31T08:21:37.253+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [queued]>
[2023-01-31T08:21:37.254+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T08:21:37.254+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T08:21:37.254+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T08:21:37.290+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 06:00:00+00:00
[2023-01-31T08:21:37.304+0000] {standard_task_runner.py:55} INFO - Started process 2288 to run task
[2023-01-31T08:21:37.314+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T06:00:00+00:00', '--job-id', '929', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpg83wwcpz']
[2023-01-31T08:21:37.320+0000] {standard_task_runner.py:83} INFO - Job 929: Subtask stage_total_generation
[2023-01-31T08:21:37.498+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [running]> on host 81d5fcd0285b
[2023-01-31T08:21:37.717+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T06:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T06:00:00+00:00
[2023-01-31T08:21:37.839+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T08:21:37.842+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010600 202101010700 DE_TENNET
[2023-01-31T08:22:00.986+0000] {spark_submit.py:495} INFO - {{ start_date }}
[2023-01-31T08:22:00.986+0000] {spark_submit.py:495} INFO - mulai periode: {{ start_date }}
[2023-01-31T08:22:00.987+0000] {spark_submit.py:495} INFO - selesai periode: {{ end_date }}
[2023-01-31T08:22:02.018+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:02 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T08:22:02.722+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T08:22:04.237+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:04 INFO ResourceUtils: ==============================================================
[2023-01-31T08:22:04.275+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:04 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T08:22:04.284+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:04 INFO ResourceUtils: ==============================================================
[2023-01-31T08:22:04.303+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:04 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T08:22:04.707+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:04 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T08:22:04.795+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:04 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T08:22:04.809+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:04 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T08:22:05.496+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:05 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T08:22:05.519+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:05 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T08:22:05.522+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:05 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T08:22:05.557+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:05 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T08:22:05.564+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:05 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T08:22:08.744+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:08 INFO Utils: Successfully started service 'sparkDriver' on port 41523.
[2023-01-31T08:22:09.076+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:09 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T08:22:09.258+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:09 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T08:22:09.464+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:09 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T08:22:09.467+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:09 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T08:22:09.530+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:09 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T08:22:09.836+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:09 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-feb0f9b1-c8ed-4a3f-97a6-e7b03b4fa0ac
[2023-01-31T08:22:09.916+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:09 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T08:22:10.042+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:10 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T08:22:12.497+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:12 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T08:22:12.509+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:12 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2023-01-31T08:22:12.509+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:12 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[2023-01-31T08:22:12.520+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:12 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[2023-01-31T08:22:12.529+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:12 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[2023-01-31T08:22:12.567+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:12 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
[2023-01-31T08:22:12.637+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:12 INFO Utils: Successfully started service 'SparkUI' on port 4046.
[2023-01-31T08:22:13.020+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:13 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://81d5fcd0285b:41523/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675153321818
[2023-01-31T08:22:13.022+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:13 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://81d5fcd0285b:41523/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675153321818
[2023-01-31T08:22:13.660+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:13 INFO Executor: Starting executor ID driver on host 81d5fcd0285b
[2023-01-31T08:22:13.681+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:13 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T08:22:13.769+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:13 INFO Executor: Fetching spark://81d5fcd0285b:41523/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675153321818
[2023-01-31T08:22:14.031+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:14 INFO TransportClientFactory: Successfully created connection to 81d5fcd0285b/172.19.0.5:41523 after 158 ms (0 ms spent in bootstraps)
[2023-01-31T08:22:14.051+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:14 INFO Utils: Fetching spark://81d5fcd0285b:41523/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-987f480c-d346-4f2b-b445-93d153c34b87/userFiles-5daa858d-2b06-4b6d-a0cf-2a89fc9d4d08/fetchFileTemp14085226129108755483.tmp
[2023-01-31T08:22:15.493+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:15 INFO Executor: Adding file:/tmp/spark-987f480c-d346-4f2b-b445-93d153c34b87/userFiles-5daa858d-2b06-4b6d-a0cf-2a89fc9d4d08/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T08:22:15.493+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:15 INFO Executor: Fetching spark://81d5fcd0285b:41523/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675153321818
[2023-01-31T08:22:15.498+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:15 INFO Utils: Fetching spark://81d5fcd0285b:41523/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-987f480c-d346-4f2b-b445-93d153c34b87/userFiles-5daa858d-2b06-4b6d-a0cf-2a89fc9d4d08/fetchFileTemp531585759194433136.tmp
[2023-01-31T08:22:16.295+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:16 INFO Executor: Adding file:/tmp/spark-987f480c-d346-4f2b-b445-93d153c34b87/userFiles-5daa858d-2b06-4b6d-a0cf-2a89fc9d4d08/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T08:22:16.321+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:16 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44169.
[2023-01-31T08:22:16.322+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:16 INFO NettyBlockTransferService: Server created on 81d5fcd0285b:44169
[2023-01-31T08:22:16.326+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:16 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T08:22:16.418+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:16 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 44169, None)
[2023-01-31T08:22:16.424+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:16 INFO BlockManagerMasterEndpoint: Registering block manager 81d5fcd0285b:44169 with 434.4 MiB RAM, BlockManagerId(driver, 81d5fcd0285b, 44169, None)
[2023-01-31T08:22:16.458+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:16 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 44169, None)
[2023-01-31T08:22:16.461+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:16 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 81d5fcd0285b, 44169, None)
[2023-01-31T08:22:21.050+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:21 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T08:22:21.078+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:21 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T08:22:36.539+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:36 INFO InMemoryFileIndex: It took 719 ms to list leaf files for 1 paths.
[2023-01-31T08:22:38.799+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:38 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T08:22:39.381+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:39 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T08:22:39.417+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:39 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 81d5fcd0285b:44169 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T08:22:39.428+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:39 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T08:22:44.136+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:44 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T08:22:44.208+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:44 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T08:22:44.285+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:44 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T08:22:44.350+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:44 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T08:22:44.352+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:44 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T08:22:44.360+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:44 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:22:44.373+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:44 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:22:44.435+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:44 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T08:22:45.057+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:45 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T08:22:45.129+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:45 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T08:22:45.129+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:45 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 81d5fcd0285b:44169 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T08:22:45.138+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:45 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:22:45.370+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:22:45.400+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:45 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T08:22:45.818+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:45 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T08:22:45.928+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:45 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T08:22:46.728+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:46 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010600__202101010700.json:0+9371
[2023-01-31T08:22:48.406+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:48 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3437 bytes result sent to driver
[2023-01-31T08:22:48.503+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:48 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2750 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:22:48.533+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:48 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T08:22:48.634+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:48 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 4.096 s
[2023-01-31T08:22:48.718+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:48 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:22:48.725+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T08:22:48.754+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:48 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 4.465358 s
[2023-01-31T08:22:51.664+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:51 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 81d5fcd0285b:44169 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T08:22:51.731+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:51 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 81d5fcd0285b:44169 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T08:23:14.141+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:14 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:14.292+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:14 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T08:23:14.388+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:14 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:20.922+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO CodeGenerator: Code generated in 4018.634399 ms
[2023-01-31T08:23:20.942+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T08:23:20.977+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T08:23:20.978+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 81d5fcd0285b:44169 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T08:23:20.982+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T08:23:21.118+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:21.318+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T08:23:21.332+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T08:23:21.333+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T08:23:21.334+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:21.343+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:21.355+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T08:23:21.399+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T08:23:21.399+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T08:23:21.399+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 81d5fcd0285b:44169 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T08:23:21.400+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:21.428+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:21.429+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T08:23:21.449+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:21.485+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T08:23:22.659+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:22 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T08:23:24.569+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:24 INFO CodeGenerator: Code generated in 1587.683288 ms
[2023-01-31T08:23:25.299+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1628 bytes result sent to driver
[2023-01-31T08:23:25.325+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 3876 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:25.331+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 3.965 s
[2023-01-31T08:23:25.331+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:25.337+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T08:23:25.338+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T08:23:25.340+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 4.021056 s
[2023-01-31T08:23:25.822+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:25.826+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T08:23:25.830+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:26.230+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO CodeGenerator: Code generated in 272.40346 ms
[2023-01-31T08:23:26.337+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T08:23:26.571+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T08:23:26.586+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 81d5fcd0285b:44169 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T08:23:26.591+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T08:23:26.614+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:26.878+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T08:23:26.901+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T08:23:26.901+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T08:23:26.902+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:26.909+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:26.910+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T08:23:26.927+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T08:23:26.976+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T08:23:26.994+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 81d5fcd0285b:44169 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T08:23:26.997+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:26.999+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:27.000+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T08:23:27.005+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:27.006+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T08:23:27.142+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T08:23:27.508+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 81d5fcd0285b:44169 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T08:23:27.826+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1628 bytes result sent to driver
[2023-01-31T08:23:27.865+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 859 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:27.865+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T08:23:27.874+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 0.963 s
[2023-01-31T08:23:27.875+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:27.875+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T08:23:27.875+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 0.978485 s
[2023-01-31T08:23:29.442+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:29.448+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T08:23:29.458+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:30.874+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:30 INFO CodeGenerator: Code generated in 887.894502 ms
[2023-01-31T08:23:30.924+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:30 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T08:23:31.100+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T08:23:31.103+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 81d5fcd0285b:44169 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T08:23:31.118+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T08:23:31.133+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:31.515+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T08:23:31.521+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T08:23:31.521+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T08:23:31.522+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:31.523+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:31.536+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T08:23:31.556+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.7 MiB)
[2023-01-31T08:23:31.578+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.6 MiB)
[2023-01-31T08:23:31.619+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 81d5fcd0285b:44169 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T08:23:31.639+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:31.641+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:31.655+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T08:23:31.659+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:31.660+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T08:23:31.886+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T08:23:32.662+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-01-31T08:23:32.693+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 1036 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:32.695+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T08:23:32.712+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 1.179 s
[2023-01-31T08:23:32.715+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:32.715+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T08:23:32.718+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 1.197252 s
[2023-01-31T08:23:33.015+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:33.017+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T08:23:33.021+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:33.215+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO CodeGenerator: Code generated in 104.162924 ms
[2023-01-31T08:23:33.253+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.4 MiB)
[2023-01-31T08:23:33.380+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T08:23:33.386+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 81d5fcd0285b:44169 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T08:23:33.403+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T08:23:33.407+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:33.713+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T08:23:33.720+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T08:23:33.721+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T08:23:33.721+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:33.721+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:33.725+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T08:23:33.738+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-31T08:23:33.743+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-01-31T08:23:33.756+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 81d5fcd0285b:44169 (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T08:23:33.764+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:33.777+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:33.778+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T08:23:33.782+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:33.783+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T08:23:33.945+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T08:23:34.261+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1586 bytes result sent to driver
[2023-01-31T08:23:34.267+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 486 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:34.267+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T08:23:34.273+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 0.535 s
[2023-01-31T08:23:34.274+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:34.274+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T08:23:34.274+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 0.555615 s
[2023-01-31T08:23:37.887+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:37 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:37.889+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:37 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T08:23:37.951+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:37 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:38.890+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO CodeGenerator: Code generated in 521.721489 ms
[2023-01-31T08:23:38.909+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.2 MiB)
[2023-01-31T08:23:38.969+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T08:23:38.972+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 81d5fcd0285b:44169 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T08:23:38.974+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T08:23:38.977+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:39.493+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T08:23:39.495+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) with 1 output partitions
[2023-01-31T08:23:39.496+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201)
[2023-01-31T08:23:39.496+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:39.496+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:39.511+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201), which has no missing parents
[2023-01-31T08:23:39.536+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.1 MiB)
[2023-01-31T08:23:39.567+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.1 MiB)
[2023-01-31T08:23:39.570+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 81d5fcd0285b:44169 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T08:23:39.572+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:39.577+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:39.581+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T08:23:39.586+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:39.587+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T08:23:39.653+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T08:23:40.420+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:40 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2542 bytes result sent to driver
[2023-01-31T08:23:40.426+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:40 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 843 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:40.427+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:40 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T08:23:40.433+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:40 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) finished in 0.924 s
[2023-01-31T08:23:40.434+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:40 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:40.434+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T08:23:40.435+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:40 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201, took 0.935212 s
[2023-01-31T08:23:43.014+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:43 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:43.077+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:43 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T08:23:43.077+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:43 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:45.233+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:45 INFO CodeGenerator: Code generated in 1139.804635 ms
[2023-01-31T08:23:45.393+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:45 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 432.9 MiB)
[2023-01-31T08:23:45.818+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:45 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.9 MiB)
[2023-01-31T08:23:45.819+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:45 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 81d5fcd0285b:44169 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T08:23:45.831+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:45 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T08:23:45.834+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:45 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:46.174+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:46 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T08:23:46.174+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:46 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T08:23:46.175+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:46 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T08:23:46.175+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:46 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:46.176+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:46 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:46.182+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:46 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T08:23:46.224+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:46 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 432.8 MiB)
[2023-01-31T08:23:46.292+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:46 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.8 MiB)
[2023-01-31T08:23:46.298+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:46 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 81d5fcd0285b:44169 (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T08:23:46.319+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:46 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:46.327+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:46.328+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:46 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T08:23:46.357+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:46 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:46.358+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:46 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T08:23:46.444+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:46 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T08:23:47.094+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:47 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-01-31T08:23:47.104+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:47 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 741 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:47.105+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:47 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T08:23:47.105+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:47 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 0.921 s
[2023-01-31T08:23:47.106+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:47 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:47.106+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T08:23:47.106+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:47 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 0.930278 s
[2023-01-31T08:23:47.685+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:47 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:47.702+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:47 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T08:23:47.715+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:47 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:48.843+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:48 INFO CodeGenerator: Code generated in 724.11262 ms
[2023-01-31T08:23:48.872+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:48 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.6 MiB)
[2023-01-31T08:23:49.000+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.6 MiB)
[2023-01-31T08:23:49.003+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 81d5fcd0285b:44169 (size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T08:23:49.004+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T08:23:49.006+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:49.190+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T08:23:49.191+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T08:23:49.191+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T08:23:49.191+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:49.192+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:49.202+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T08:23:49.222+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.6 MiB)
[2023-01-31T08:23:49.232+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.5 MiB)
[2023-01-31T08:23:49.235+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 81d5fcd0285b:44169 (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T08:23:49.236+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:49.236+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:49.237+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T08:23:49.240+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:49.243+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T08:23:49.326+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T08:23:49.558+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1729 bytes result sent to driver
[2023-01-31T08:23:49.569+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 329 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:49.569+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T08:23:49.572+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 0.364 s
[2023-01-31T08:23:49.573+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:49.573+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T08:23:49.573+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 0.383636 s
[2023-01-31T12:01:45.512+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [queued]>
[2023-01-31T12:01:45.529+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [queued]>
[2023-01-31T12:01:45.529+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T12:01:45.529+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T12:01:45.530+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T12:01:45.549+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 06:00:00+00:00
[2023-01-31T12:01:45.561+0000] {standard_task_runner.py:55} INFO - Started process 158 to run task
[2023-01-31T12:01:45.568+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T06:00:00+00:00', '--job-id', '981', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp70t9b3ke']
[2023-01-31T12:01:45.572+0000] {standard_task_runner.py:83} INFO - Job 981: Subtask stage_total_generation
[2023-01-31T12:01:45.679+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [running]> on host 8124f810dec3
[2023-01-31T12:01:45.917+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T06:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T06:00:00+00:00
[2023-01-31T12:01:45.935+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T12:01:45.937+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 398, in submit
    spark_submit_cmd = self._build_spark_submit_command(application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 344, in _build_spark_submit_command
    self.log.info("Spark-Submit cmd: %s", self._mask_cmd(connection_cmd))
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 256, in _mask_cmd
    " ".join(connection_cmd),
TypeError: sequence item 10: expected str instance, DateTime found
[2023-01-31T12:01:45.954+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=entsoe-energydata-backfill, task_id=stage_total_generation, execution_date=20210101T060000, start_date=20230131T120145, end_date=20230131T120145
[2023-01-31T12:01:45.982+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 981 for task stage_total_generation (sequence item 10: expected str instance, DateTime found; 158)
[2023-01-31T12:01:46.035+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-31T12:01:46.084+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T12:08:53.342+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [queued]>
[2023-01-31T12:08:53.444+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [queued]>
[2023-01-31T12:08:53.450+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T12:08:53.451+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T12:08:53.452+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T12:08:53.705+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 06:00:00+00:00
[2023-01-31T12:08:53.786+0000] {standard_task_runner.py:55} INFO - Started process 668 to run task
[2023-01-31T12:08:53.840+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T06:00:00+00:00', '--job-id', '1000', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp1ftjjpho']
[2023-01-31T12:08:53.851+0000] {standard_task_runner.py:83} INFO - Job 1000: Subtask stage_total_generation
[2023-01-31T12:08:54.514+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [running]> on host 8124f810dec3
[2023-01-31T12:08:55.119+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T06:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T06:00:00+00:00
[2023-01-31T12:08:55.215+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T12:08:55.229+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010600 202101010700 DE_TENNET 202101010000 202101010600
[2023-01-31T12:09:23.922+0000] {spark_submit.py:495} INFO - mulai periode: 202101010000
[2023-01-31T12:09:23.927+0000] {spark_submit.py:495} INFO - selesai periode: 202101010600
[2023-01-31T12:09:24.412+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:24 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T12:09:25.088+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T12:09:25.819+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:25 INFO ResourceUtils: ==============================================================
[2023-01-31T12:09:25.830+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:25 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T12:09:25.831+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:25 INFO ResourceUtils: ==============================================================
[2023-01-31T12:09:25.831+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:25 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T12:09:25.921+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:25 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T12:09:25.954+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:25 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T12:09:25.961+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:25 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T12:09:26.359+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:26 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T12:09:26.366+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:26 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T12:09:26.369+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:26 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T12:09:26.371+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:26 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T12:09:26.376+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:26 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T12:09:30.398+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:30 INFO Utils: Successfully started service 'sparkDriver' on port 38201.
[2023-01-31T12:09:31.034+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:31 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T12:09:31.406+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:31 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T12:09:31.728+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:31 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T12:09:31.728+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:31 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T12:09:31.820+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:31 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T12:09:32.302+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:32 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c361dfb5-61a7-467e-9513-335476c83f2c
[2023-01-31T12:09:32.458+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:32 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T12:09:32.614+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:32 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T12:09:34.781+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:34 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T12:09:34.781+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:34 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2023-01-31T12:09:34.821+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:34 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[2023-01-31T12:09:34.871+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:34 INFO Utils: Successfully started service 'SparkUI' on port 4043.
[2023-01-31T12:09:35.137+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:35 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://8124f810dec3:38201/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675166964358
[2023-01-31T12:09:35.139+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:35 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://8124f810dec3:38201/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675166964358
[2023-01-31T12:09:36.206+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:36 INFO Executor: Starting executor ID driver on host 8124f810dec3
[2023-01-31T12:09:36.225+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:36 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T12:09:36.390+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:36 INFO Executor: Fetching spark://8124f810dec3:38201/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675166964358
[2023-01-31T12:09:37.150+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:37 INFO TransportClientFactory: Successfully created connection to 8124f810dec3/172.20.0.9:38201 after 326 ms (0 ms spent in bootstraps)
[2023-01-31T12:09:37.174+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:37 INFO Utils: Fetching spark://8124f810dec3:38201/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-2db5be7b-ef5a-4120-8c74-ccf15da16021/userFiles-a3c2d450-6ac9-421c-8ed8-2258353ac2dc/fetchFileTemp12840900895484472944.tmp
[2023-01-31T12:09:40.082+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:40 INFO Executor: Adding file:/tmp/spark-2db5be7b-ef5a-4120-8c74-ccf15da16021/userFiles-a3c2d450-6ac9-421c-8ed8-2258353ac2dc/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T12:09:40.084+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:40 INFO Executor: Fetching spark://8124f810dec3:38201/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675166964358
[2023-01-31T12:09:40.127+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:40 INFO Utils: Fetching spark://8124f810dec3:38201/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-2db5be7b-ef5a-4120-8c74-ccf15da16021/userFiles-a3c2d450-6ac9-421c-8ed8-2258353ac2dc/fetchFileTemp17206456634475009939.tmp
[2023-01-31T12:09:41.186+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:41 INFO Executor: Adding file:/tmp/spark-2db5be7b-ef5a-4120-8c74-ccf15da16021/userFiles-a3c2d450-6ac9-421c-8ed8-2258353ac2dc/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T12:09:41.249+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:41 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36903.
[2023-01-31T12:09:41.249+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:41 INFO NettyBlockTransferService: Server created on 8124f810dec3:36903
[2023-01-31T12:09:41.259+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:41 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T12:09:41.358+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:41 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:09:41.397+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:41 INFO BlockManagerMasterEndpoint: Registering block manager 8124f810dec3:36903 with 434.4 MiB RAM, BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:09:41.426+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:41 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:09:41.444+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:41 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:09:50.898+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:50 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T12:09:50.959+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:50 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T12:10:13.347+0000] {spark_submit.py:495} INFO - 23/01/31 12:10:13 INFO InMemoryFileIndex: It took 2683 ms to list leaf files for 1 paths.
[2023-01-31T12:10:30.741+0000] {spark_submit.py:495} INFO - 23/01/31 12:10:30 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T12:11:26.684+0000] {base_job.py:232} ERROR - LocalTaskJob heartbeat got an exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3361, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 325, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 888, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 491, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 271, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 386, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 684, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 210, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 680, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/jobs/base_job.py", line 228, in heartbeat
    self.heartbeat_callback(session=session)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/jobs/local_task_job.py", line 178, in heartbeat_callback
    self.task_instance.refresh_from_db()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 795, in refresh_from_db
    ti = qry.one_or_none()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2849, in one_or_none
    return self._iter().one_or_none()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2915, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1552, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3315, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3394, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3364, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2198, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 210, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3361, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 325, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 888, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 491, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 271, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 386, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 684, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 210, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 680, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-01-31T12:13:17.203+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:17 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.TimeoutException: Cannot receive any reply from 8124f810dec3:38201 in 10000 milliseconds
[2023-01-31T12:13:17.961+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:17 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T12:13:18.034+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 8124f810dec3:36903 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T12:13:18.305+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T12:13:35.326+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:35 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T12:13:35.397+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:35 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T12:13:35.785+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:35 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T12:13:36.094+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:36 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T12:13:36.095+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:36 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T12:13:36.103+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:36 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T12:13:36.227+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:36 INFO DAGScheduler: Missing parents: List()
[2023-01-31T12:13:36.380+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:36 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T12:13:40.910+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:40 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T12:13:41.333+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:41 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T12:13:41.334+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:41 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 8124f810dec3:36903 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T12:13:41.343+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:41 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T12:13:41.343+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T12:13:41.343+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:41 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T12:13:42.764+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:42 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (8124f810dec3, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T12:13:46.018+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:45 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T12:13:46.486+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:46 INFO AsyncEventQueue: Process of event SparkListenerTaskStart(0,0,org.apache.spark.scheduler.TaskInfo@175fd10a) by listener AppStatusListener took 1.142717793s.
[2023-01-31T12:14:15.847+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:15 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010600__202101010700.json:0+9371
[2023-01-31T12:14:18.430+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:18 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3437 bytes result sent to driver
[2023-01-31T12:14:18.632+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:18 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 36077 ms on 8124f810dec3 (executor driver) (1/1)
[2023-01-31T12:14:18.671+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:18 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T12:14:18.717+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:18 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 42.070 s
[2023-01-31T12:14:18.758+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:18 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T12:14:18.763+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T12:14:18.779+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:18 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 42.974771 s
[2023-01-31T12:14:19.563+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:19 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 8124f810dec3:36903 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T12:14:19.645+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:19 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 8124f810dec3:36903 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T12:15:20.981+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:20 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T12:15:21.000+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:20 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T12:15:21.021+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T12:15:24.151+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:24 INFO CodeGenerator: Code generated in 1975.532912 ms
[2023-01-31T12:15:24.345+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:24 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T12:15:24.687+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:24 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T12:15:24.700+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:24 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 8124f810dec3:36903 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T12:15:24.721+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:24 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T12:15:24.925+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:24 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T12:15:25.825+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:25 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T12:15:25.832+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:25 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T12:15:25.838+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:25 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T12:15:25.839+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:25 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T12:15:25.839+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:25 INFO DAGScheduler: Missing parents: List()
[2023-01-31T12:15:25.850+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:25 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T12:15:26.032+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:26 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T12:15:26.076+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:26 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T12:15:26.078+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:26 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 8124f810dec3:36903 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T12:15:26.085+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:26 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T12:15:26.086+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T12:15:26.086+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:26 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T12:15:26.168+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:26 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (8124f810dec3, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T12:15:26.185+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:26 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T12:15:26.917+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:26 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T12:15:28.772+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO CodeGenerator: Code generated in 1314.424471 ms
[2023-01-31T12:15:32.671+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:32 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1671 bytes result sent to driver
[2023-01-31T12:15:37.794+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:37 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,ArraySeq(),HashMap((1,0) -> org.apache.spark.executor.ExecutorMetrics@2f8a3848)) by listener AppStatusListener took 1.283246532s.
[2023-01-31T12:15:37.986+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:37 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 11323 ms on 8124f810dec3 (executor driver) (1/1)
[2023-01-31T12:15:38.078+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:37 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T12:15:38.084+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:37 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 11.780 s
[2023-01-31T12:15:38.084+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:37 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T12:15:38.085+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T12:15:38.085+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:37 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 11.984813 s
[2023-01-31T12:16:31.970+0000] {base_job.py:232} ERROR - LocalTaskJob heartbeat got an exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3361, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 325, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 888, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 491, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 271, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 386, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 684, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 210, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 680, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/jobs/base_job.py", line 204, in heartbeat
    session.merge(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in merge
    return self._merge(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3131, in _merge
    merged = self.get(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2848, in get
    return self._get_impl(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2970, in _get_impl
    return db_load_fn(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/loading.py", line 530, in load_on_pk_identity
    session.execute(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1552, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3315, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3394, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3364, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2198, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 210, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3361, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 325, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 888, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 491, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 271, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 386, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 684, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 210, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 680, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-01-31T12:20:31.153+0000] {spark_submit.py:495} INFO - 23/01/31 12:20:31 WARN NettyRpcEnv: Ignored message: HeartbeatResponse(false)
[2023-01-31T12:20:31.319+0000] {spark_submit.py:495} INFO - 23/01/31 12:20:31 WARN Executor: Issue communicating with driver in heartbeater
[2023-01-31T12:20:31.587+0000] {spark_submit.py:495} INFO - org.apache.spark.rpc.RpcTimeoutException: Future timed out after [10000 milliseconds]. This timeout is controlled by spark.executor.heartbeatInterval
[2023-01-31T12:20:31.694+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
[2023-01-31T12:20:31.695+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
[2023-01-31T12:20:31.695+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
[2023-01-31T12:20:31.695+0000] {spark_submit.py:495} INFO - at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)
[2023-01-31T12:20:31.695+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)
[2023-01-31T12:20:31.696+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2023-01-31T12:20:31.696+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1053)
[2023-01-31T12:20:31.696+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2023-01-31T12:20:31.696+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
[2023-01-31T12:20:31.696+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2023-01-31T12:20:31.697+0000] {spark_submit.py:495} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2023-01-31T12:20:31.697+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-01-31T12:20:31.700+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2023-01-31T12:20:31.700+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2023-01-31T12:20:31.700+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-31T12:20:31.701+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-31T12:20:31.701+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-31T12:20:31.701+0000] {spark_submit.py:495} INFO - Caused by: java.util.concurrent.TimeoutException: Future timed out after [10000 milliseconds]
[2023-01-31T12:20:31.701+0000] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryAwait0(Promise.scala:248)
[2023-01-31T12:20:31.701+0000] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:261)
[2023-01-31T12:20:31.702+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:293)
[2023-01-31T12:20:31.702+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-01-31T12:20:31.702+0000] {spark_submit.py:495} INFO - ... 12 more
[2023-01-31T12:20:31.705+0000] {spark_submit.py:495} INFO - 23/01/31 12:20:31 INFO AsyncEventQueue: Process of event SparkListenerTaskEnd(1,0,ResultTask,Success,org.apache.spark.scheduler.TaskInfo@e43d4b9,org.apache.spark.executor.ExecutorMetrics@96d8513,org.apache.spark.executor.TaskMetrics@13ad88f6) by listener SQLAppStatusListener took 293.083764754s.
[2023-01-31T12:21:59.791+0000] {spark_submit.py:495} INFO - 23/01/31 12:21:59 WARN Executor: Issue communicating with driver in heartbeater
[2023-01-31T12:21:59.798+0000] {spark_submit.py:495} INFO - org.apache.spark.rpc.RpcTimeoutException: Future timed out after [10000 milliseconds]. This timeout is controlled by spark.executor.heartbeatInterval
[2023-01-31T12:21:59.798+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
[2023-01-31T12:21:59.799+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
[2023-01-31T12:21:59.799+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
[2023-01-31T12:21:59.799+0000] {spark_submit.py:495} INFO - at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)
[2023-01-31T12:21:59.809+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)
[2023-01-31T12:21:59.810+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2023-01-31T12:21:59.810+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1053)
[2023-01-31T12:21:59.810+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2023-01-31T12:21:59.811+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
[2023-01-31T12:21:59.811+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2023-01-31T12:21:59.811+0000] {spark_submit.py:495} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2023-01-31T12:21:59.811+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-01-31T12:21:59.812+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2023-01-31T12:21:59.812+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2023-01-31T12:21:59.812+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-31T12:21:59.813+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-31T12:21:59.813+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-31T12:21:59.813+0000] {spark_submit.py:495} INFO - Caused by: java.util.concurrent.TimeoutException: Future timed out after [10000 milliseconds]
[2023-01-31T12:21:59.814+0000] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryAwait0(Promise.scala:248)
[2023-01-31T12:21:59.815+0000] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:261)
[2023-01-31T12:21:59.815+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:293)
[2023-01-31T12:21:59.815+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-01-31T12:21:59.816+0000] {spark_submit.py:495} INFO - ... 12 more
[2023-01-31T12:21:59.902+0000] {spark_submit.py:495} INFO - 23/01/31 12:21:59 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 271218 ms exceeds timeout 120000 ms
[2023-01-31T12:22:00.159+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 WARN NettyRpcEnv: Ignored message: HeartbeatResponse(true)
[2023-01-31T12:22:00.160+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:00.164+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:00.165+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:00.338+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 WARN SparkContext: Killing executors is not supported by current scheduler.
[2023-01-31T12:22:01.059+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:01.062+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManager: Reporting 4 blocks to the master.
[2023-01-31T12:22:01.271+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T12:22:01.346+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T12:22:01.407+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:01.408+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:01.412+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:01.440+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:01.450+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManager: Reporting 4 blocks to the master.
[2023-01-31T12:22:01.470+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T12:22:01.475+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T12:22:01.478+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:01.492+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:01.495+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:01.496+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:01.515+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManager: Reporting 4 blocks to the master.
[2023-01-31T12:22:01.515+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T12:22:01.582+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T12:22:01.583+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:01.641+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:01.647+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:01.664+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:01.664+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManager: Reporting 4 blocks to the master.
[2023-01-31T12:22:01.783+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T12:22:01.783+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T12:22:01.845+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:01.937+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:01.938+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:01.939+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:01.939+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManager: Reporting 4 blocks to the master.
[2023-01-31T12:22:02.044+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T12:22:02.076+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T12:22:02.174+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:02.175+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:02.203+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:02.213+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:02.213+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManager: Reporting 4 blocks to the master.
[2023-01-31T12:22:02.214+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T12:22:02.216+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T12:22:02.262+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:02.275+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:02.281+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:02.310+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:02.317+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManager: Reporting 4 blocks to the master.
[2023-01-31T12:22:02.318+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T12:22:02.318+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T12:22:02.319+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:02.319+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:02.319+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:02.320+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:02.321+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManager: Reporting 4 blocks to the master.
[2023-01-31T12:22:02.322+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T12:22:02.352+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T12:22:02.391+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T12:22:02.392+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T12:22:02.393+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T12:22:02.422+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:02.424+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:02.429+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:02.430+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:02.442+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManager: Reporting 4 blocks to the master.
[2023-01-31T12:22:02.451+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T12:22:02.464+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T12:22:02.536+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:02.538+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:02.551+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:02.566+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:02.568+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManager: Reporting 4 blocks to the master.
[2023-01-31T12:22:02.599+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T12:22:02.621+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T12:22:02.669+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:02.688+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:02.692+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:02.714+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO CodeGenerator: Code generated in 47.979612 ms
[2023-01-31T12:22:02.722+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:02.725+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T12:22:02.740+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManager: Reporting 5 blocks to the master.
[2023-01-31T12:22:02.744+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T12:22:02.785+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T12:22:02.857+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:02.858+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:02.858+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:02.860+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:02.888+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManager: Reporting 5 blocks to the master.
[2023-01-31T12:22:02.912+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T12:22:02.919+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T12:22:02.960+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:02.962+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:02.969+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:02.977+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:02.983+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManager: Reporting 5 blocks to the master.
[2023-01-31T12:22:02.987+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T12:22:03.022+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T12:22:03.092+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:03.104+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:03.127+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:03.136+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T12:22:03.141+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:03.141+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManager: Reporting 6 blocks to the master.
[2023-01-31T12:22:03.149+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 8124f810dec3:36903 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:03.178+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T12:22:03.184+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:03.202+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:03.206+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T12:22:03.240+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:03.272+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:03.287+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:03.296+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:03.300+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:03.326+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T12:22:03.330+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T12:22:03.331+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T12:22:03.331+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T12:22:03.333+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO DAGScheduler: Missing parents: List()
[2023-01-31T12:22:03.435+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T12:22:03.439+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManager: Reporting 6 blocks to the master.
[2023-01-31T12:22:03.510+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:03.514+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:03.560+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T12:22:03.564+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:03.578+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:03.579+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:03.579+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:03.603+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T12:22:03.622+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:03.626+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManager: Reporting 8 blocks to the master.
[2023-01-31T12:22:03.656+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 8124f810dec3:36903 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:03.662+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T12:22:03.698+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_5_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:03.720+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:03.728+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:03.830+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:03.847+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T12:22:03.849+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T12:22:03.850+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:03.851+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:03.851+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:03.858+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (8124f810dec3, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T12:22:03.861+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:03.862+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManager: Reporting 8 blocks to the master.
[2023-01-31T12:22:03.946+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_5_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:03.950+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T12:22:03.954+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:03.957+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:04.039+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:04.078+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:04.078+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:04.079+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:04.083+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:04.083+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManager: Reporting 8 blocks to the master.
[2023-01-31T12:22:04.084+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_5_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:04.092+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:04.135+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:04.140+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:04.150+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:04.152+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:04.152+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:04.205+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:04.210+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManager: Reporting 8 blocks to the master.
[2023-01-31T12:22:04.210+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_5_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:04.214+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:04.381+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:04.410+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:04.416+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:04.418+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:04.419+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:04.508+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:04.510+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManager: Reporting 8 blocks to the master.
[2023-01-31T12:22:04.516+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_5_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:05.176+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:05 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:05.279+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:05 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:05.459+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:05 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:05.474+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:05 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:05.744+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:05 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:05.745+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:05 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:05.769+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:05 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:05.769+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:05 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T12:22:05.859+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:05 INFO BlockManager: Reporting 8 blocks to the master.
[2023-01-31T12:22:05.862+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:05 INFO BlockManagerInfo: Updated broadcast_5_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:05.934+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:05 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:05.971+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:05 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:06.152+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:06.167+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:06.264+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:06.279+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:06.514+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:06.515+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO BlockManager: Reporting 8 blocks to the master.
[2023-01-31T12:22:06.617+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO BlockManagerInfo: Updated broadcast_5_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:06.630+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:06.683+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:06.807+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:06.811+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:06.817+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:06.829+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:06.919+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:06.920+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO BlockManager: Reporting 8 blocks to the master.
[2023-01-31T12:22:06.957+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO BlockManagerInfo: Updated broadcast_5_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:07.083+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:07.174+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:07.325+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:07.332+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:07.333+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:07.334+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:07.426+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:07.426+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManager: Reporting 8 blocks to the master.
[2023-01-31T12:22:07.458+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerInfo: Updated broadcast_5_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:07.624+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:07.663+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:07.667+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:07.767+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:07.771+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:07.803+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:07.806+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:07.807+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManager: Reporting 8 blocks to the master.
[2023-01-31T12:22:07.810+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerInfo: Updated broadcast_5_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:07.934+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:07.941+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:07.951+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:07.956+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:07.958+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:07.959+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:07.961+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:07.962+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManager: Reporting 8 blocks to the master.
[2023-01-31T12:22:07.965+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerInfo: Updated broadcast_5_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:07.968+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:07.988+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:07.999+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.007+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:08.009+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:08.013+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:08.019+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:08.022+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManager: Reporting 8 blocks to the master.
[2023-01-31T12:22:08.027+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_5_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.042+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.081+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.109+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.170+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:08.182+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:08.262+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:08.276+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:08.284+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManager: Reporting 8 blocks to the master.
[2023-01-31T12:22:08.285+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_5_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.295+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.417+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.526+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.530+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:08.532+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:08.532+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:08.535+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:08.535+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManager: Reporting 8 blocks to the master.
[2023-01-31T12:22:08.539+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_5_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.542+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.545+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.549+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.555+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:08.558+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:08.559+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:08.561+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:08.565+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManager: Reporting 8 blocks to the master.
[2023-01-31T12:22:08.568+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_5_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.572+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.578+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.586+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.605+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:08.606+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:08.606+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:08.635+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:08.642+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManager: Reporting 8 blocks to the master.
[2023-01-31T12:22:08.677+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_5_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.709+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.776+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.863+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.874+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:08.876+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:08.892+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:08.975+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:08.976+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManager: Reporting 8 blocks to the master.
[2023-01-31T12:22:08.978+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_5_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:09.033+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:09.057+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1628 bytes result sent to driver
[2023-01-31T12:22:09.067+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:09.070+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:09.093+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 5236 ms on 8124f810dec3 (executor driver) (1/1)
[2023-01-31T12:22:09.093+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T12:22:09.096+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:09.097+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:09.097+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 5.593 s
[2023-01-31T12:22:09.097+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:09.098+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T12:22:09.113+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T12:22:09.114+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 5.770560 s
[2023-01-31T12:22:09.153+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:09.154+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManager: Reporting 8 blocks to the master.
[2023-01-31T12:22:09.157+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerInfo: Updated broadcast_5_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:09.177+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:09.177+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:09.215+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:09.221+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:09.222+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:09.223+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:09.228+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:09.232+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManager: Reporting 8 blocks to the master.
[2023-01-31T12:22:09.236+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerInfo: Updated broadcast_5_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:09.245+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:09.252+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:09.262+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:09.269+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:09.270+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:09.270+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:09.303+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:09.326+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManager: Reporting 8 blocks to the master.
[2023-01-31T12:22:09.368+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerInfo: Updated broadcast_5_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:09.449+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:09.477+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:09.554+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:09.564+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:09.565+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:09.566+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:09.600+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:09.600+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManager: Reporting 8 blocks to the master.
[2023-01-31T12:22:09.625+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerInfo: Updated broadcast_5_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:09.644+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:09.700+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:09.741+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:09.745+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:09.747+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:09.747+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:09.779+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:09.780+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManager: Reporting 8 blocks to the master.
[2023-01-31T12:22:09.795+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerInfo: Updated broadcast_5_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:09.802+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:09.831+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:10.010+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:12.263+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:12 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T12:22:12.270+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:12 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T12:22:12.272+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:12 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T12:22:13.280+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:13 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:13.281+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:13 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:13.283+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:13 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:13.300+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:13 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:13.308+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:13 INFO BlockManager: Reporting 8 blocks to the master.
[2023-01-31T12:22:13.322+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:13 INFO BlockManagerInfo: Updated broadcast_5_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:13.350+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:13 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:13.416+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:13 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:13.499+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:13 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:15.236+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:15 INFO CodeGenerator: Code generated in 2496.636191 ms
[2023-01-31T12:22:15.292+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:15 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T12:22:15.555+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:15 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T12:22:15.561+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:15 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 8124f810dec3:36903 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:15.616+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:15 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T12:22:15.764+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T12:22:16.197+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:16 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 8124f810dec3:36903 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:16.524+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:16 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T12:22:16.539+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:16 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T12:22:16.540+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:16 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T12:22:16.541+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:16 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T12:22:16.541+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:16 INFO DAGScheduler: Missing parents: List()
[2023-01-31T12:22:16.550+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:16 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T12:22:16.619+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:16 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.7 MiB)
[2023-01-31T12:22:16.637+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:16 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.6 MiB)
[2023-01-31T12:22:16.678+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:16 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 8124f810dec3:36903 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:16.706+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:16 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T12:22:16.716+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T12:22:16.722+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:16 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T12:22:16.745+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:16 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (8124f810dec3, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T12:22:16.748+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:16 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T12:22:16.887+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:16 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T12:22:17.465+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:17 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-01-31T12:22:17.481+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:17 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 741 ms on 8124f810dec3 (executor driver) (1/1)
[2023-01-31T12:22:17.491+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:17 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 0.921 s
[2023-01-31T12:22:17.493+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:17 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T12:22:17.494+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:17 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T12:22:17.495+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T12:22:17.502+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:17 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 0.972237 s
[2023-01-31T12:22:17.858+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:17 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T12:22:17.863+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:17 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T12:22:17.870+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:17 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T12:22:18.088+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:18 INFO CodeGenerator: Code generated in 167.285984 ms
[2023-01-31T12:22:18.157+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:18 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.4 MiB)
[2023-01-31T12:22:18.409+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:18 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T12:22:18.428+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:18 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 8124f810dec3:36903 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:18.454+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:18 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T12:22:18.487+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:18 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T12:22:18.991+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:18 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T12:22:18.996+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:18 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T12:22:18.996+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:18 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T12:22:18.997+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:18 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T12:22:18.997+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:18 INFO DAGScheduler: Missing parents: List()
[2023-01-31T12:22:19.033+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:19 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T12:22:19.116+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:19 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-31T12:22:19.202+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:19 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-01-31T12:22:19.210+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:19 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 8124f810dec3:36903 (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T12:22:19.212+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:19 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T12:22:19.232+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T12:22:19.233+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:19 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T12:22:19.258+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:19 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (8124f810dec3, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T12:22:19.259+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:19 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T12:22:19.440+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:19 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T12:22:19.839+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:19 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1586 bytes result sent to driver
[2023-01-31T12:22:19.844+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:19 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 587 ms on 8124f810dec3 (executor driver) (1/1)
[2023-01-31T12:22:19.850+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:19 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T12:22:19.851+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:19 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 0.816 s
[2023-01-31T12:22:19.852+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:19 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T12:22:19.852+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T12:22:19.854+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:19 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 0.854744 s
[2023-01-31T12:22:22.111+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:22 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 8124f810dec3:36903 in memory (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T12:22:22.126+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:22 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 8124f810dec3:36903 in memory (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:22.483+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:22 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T12:22:22.485+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:22 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T12:22:22.486+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:22 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T12:22:23.212+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO CodeGenerator: Code generated in 420.040685 ms
[2023-01-31T12:22:23.222+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.3 MiB)
[2023-01-31T12:22:23.263+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T12:22:23.265+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:23.266+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:23.267+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 8124f810dec3:36903 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:23.268+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:23.270+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T12:22:23.271+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:23.273+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO BlockManager: Reporting 12 blocks to the master.
[2023-01-31T12:22:23.300+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO BlockManagerInfo: Updated broadcast_10_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:23.308+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T12:22:23.333+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.2 MiB)
[2023-01-31T12:22:23.356+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:23.364+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO BlockManagerInfo: Updated broadcast_8_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:23.403+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO BlockManagerInfo: Updated broadcast_6_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:23.424+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:23.514+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T12:22:23.515+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T12:22:23.519+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T12:22:23.524+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T12:22:23.525+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO DAGScheduler: Missing parents: List()
[2023-01-31T12:22:23.526+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T12:22:23.542+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.2 MiB)
[2023-01-31T12:22:23.550+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.2 MiB)
[2023-01-31T12:22:23.555+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 8124f810dec3:36903 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T12:22:23.557+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T12:22:23.560+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T12:22:23.562+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T12:22:23.575+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (8124f810dec3, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T12:22:23.578+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T12:22:23.624+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T12:22:23.899+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2499 bytes result sent to driver
[2023-01-31T12:22:23.904+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 335 ms on 8124f810dec3 (executor driver) (1/1)
[2023-01-31T12:22:23.905+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T12:22:23.908+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 0.382 s
[2023-01-31T12:22:23.909+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T12:22:23.909+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T12:22:23.910+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 0.393507 s
[2023-01-31T12:22:24.185+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:24 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T12:22:24.190+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:24 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T12:22:24.192+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:24 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T12:22:24.696+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:24 INFO CodeGenerator: Code generated in 353.552339 ms
[2023-01-31T12:22:24.722+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:24 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 433.0 MiB)
[2023-01-31T12:22:24.820+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:24 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.9 MiB)
[2023-01-31T12:22:24.824+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:24 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 8124f810dec3:36903 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:24.825+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:24 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T12:22:24.833+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:24 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T12:22:24.936+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:24 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 8124f810dec3:36903 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:24.997+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:24 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 8124f810dec3:36903 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:25.023+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T12:22:25.025+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T12:22:25.025+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T12:22:25.026+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T12:22:25.026+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO DAGScheduler: Missing parents: List()
[2023-01-31T12:22:25.029+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T12:22:25.038+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 433.4 MiB)
[2023-01-31T12:22:25.050+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 433.4 MiB)
[2023-01-31T12:22:25.062+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 8124f810dec3:36903 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:25.065+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 8124f810dec3:36903 (size: 12.6 KiB, free: 434.3 MiB)
[2023-01-31T12:22:25.067+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T12:22:25.069+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T12:22:25.070+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T12:22:25.074+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (8124f810dec3, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T12:22:25.075+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T12:22:25.113+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T12:22:25.124+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 8124f810dec3:36903 in memory (size: 13.4 KiB, free: 434.3 MiB)
[2023-01-31T12:22:25.257+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-01-31T12:22:25.265+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 192 ms on 8124f810dec3 (executor driver) (1/1)
[2023-01-31T12:22:25.265+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T12:22:25.268+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 0.233 s
[2023-01-31T12:22:25.269+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T12:22:25.269+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T12:22:25.269+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 0.243503 s
[2023-01-31T12:22:25.442+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T12:22:25.444+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T12:22:25.446+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T12:22:25.781+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO CodeGenerator: Code generated in 238.4051 ms
[2023-01-31T12:22:25.794+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 433.4 MiB)
[2023-01-31T12:22:25.832+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T12:22:25.833+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 8124f810dec3:36903 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:25.835+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:204
[2023-01-31T12:22:25.841+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T12:22:25.937+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:204
[2023-01-31T12:22:25.939+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:204) with 1 output partitions
[2023-01-31T12:22:25.940+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:204)
[2023-01-31T12:22:25.940+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T12:22:25.940+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO DAGScheduler: Missing parents: List()
[2023-01-31T12:22:25.950+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:204), which has no missing parents
[2023-01-31T12:22:25.954+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 433.4 MiB)
[2023-01-31T12:22:25.957+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 433.4 MiB)
[2023-01-31T12:22:25.963+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 8124f810dec3:36903 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T12:22:25.967+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T12:22:25.968+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:204) (first 15 tasks are for partitions Vector(0))
[2023-01-31T12:22:25.969+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T12:22:25.972+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (8124f810dec3, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T12:22:25.974+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T12:22:25.997+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T12:22:26.132+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:26 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1729 bytes result sent to driver
[2023-01-31T12:22:26.160+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:26 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 189 ms on 8124f810dec3 (executor driver) (1/1)
[2023-01-31T12:22:26.164+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:26 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T12:22:26.164+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:26 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:204) finished in 0.213 s
[2023-01-31T12:22:26.164+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:26 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T12:22:26.164+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T12:22:26.165+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:26 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:204, took 0.224836 s
[2023-01-31T12:22:26.850+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:26 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 8124f810dec3:36903 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T12:22:26.855+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:26 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 8124f810dec3:36903 in memory (size: 12.6 KiB, free: 434.3 MiB)
[2023-01-31T12:22:28.774+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:28 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 8124f810dec3:36903 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:28.804+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:28 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 8124f810dec3:36903 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:33.264+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:33 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:33.264+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:33 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:33.265+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:33 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:33.266+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:33 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:33.266+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:33 INFO BlockManager: Reporting 6 blocks to the master.
[2023-01-31T12:22:33.268+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:33 INFO BlockManagerInfo: Updated broadcast_10_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:33.271+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:33 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:33.274+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:33 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:36.499+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:36 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T12:22:36.562+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T12:22:36.564+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T12:22:36.567+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:36 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T12:22:36.568+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T12:22:36.568+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T12:22:36.578+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T12:22:36.914+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:36 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-31T12:22:36.915+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:36 INFO DAGScheduler: Got job 8 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T12:22:36.915+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:36 INFO DAGScheduler: Final stage: ResultStage 8 (save at BigQueryWriteHelper.java:105)
[2023-01-31T12:22:36.915+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:36 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T12:22:36.915+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:36 INFO DAGScheduler: Missing parents: List()
[2023-01-31T12:22:36.916+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:36 INFO DAGScheduler: Submitting ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T12:22:36.946+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:36 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 209.9 KiB, free 433.7 MiB)
[2023-01-31T12:22:36.948+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:36 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 74.3 KiB, free 433.6 MiB)
[2023-01-31T12:22:36.948+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:36 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 8124f810dec3:36903 (size: 74.3 KiB, free: 434.3 MiB)
[2023-01-31T12:22:36.949+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:36 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513
[2023-01-31T12:22:36.949+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T12:22:36.950+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:36 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2023-01-31T12:22:36.953+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:36 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (8124f810dec3, executor driver, partition 0, PROCESS_LOCAL, 7484 bytes) taskResourceAssignments Map()
[2023-01-31T12:22:36.954+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:36 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2023-01-31T12:22:37.025+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T12:22:37.025+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T12:22:37.026+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:37 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T12:22:37.026+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T12:22:37.027+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T12:22:37.027+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T12:22:37.032+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:37 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T12:22:37.035+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:37 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T12:22:37.064+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:37 INFO ParquetOutputFormat: Parquet block size to 134217728
[2023-01-31T12:22:37.064+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:37 INFO ParquetOutputFormat: Validation is off
[2023-01-31T12:22:37.065+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:37 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-01-31T12:22:37.065+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:37 INFO ParquetOutputFormat: Parquet properties are:
[2023-01-31T12:22:37.065+0000] {spark_submit.py:495} INFO - Parquet page size to 1048576
[2023-01-31T12:22:37.065+0000] {spark_submit.py:495} INFO - Parquet dictionary page size to 1048576
[2023-01-31T12:22:37.065+0000] {spark_submit.py:495} INFO - Dictionary is true
[2023-01-31T12:22:37.065+0000] {spark_submit.py:495} INFO - Writer version is: PARQUET_1_0
[2023-01-31T12:22:37.066+0000] {spark_submit.py:495} INFO - Page size checking is: estimated
[2023-01-31T12:22:37.066+0000] {spark_submit.py:495} INFO - Min row count for page size check is: 100
[2023-01-31T12:22:37.066+0000] {spark_submit.py:495} INFO - Max row count for page size check is: 10000
[2023-01-31T12:22:37.066+0000] {spark_submit.py:495} INFO - Truncate length for column indexes is: 64
[2023-01-31T12:22:37.067+0000] {spark_submit.py:495} INFO - Truncate length for statistics min/max  is: 2147483647
[2023-01-31T12:22:37.067+0000] {spark_submit.py:495} INFO - Bloom filter enabled: false
[2023-01-31T12:22:37.067+0000] {spark_submit.py:495} INFO - Max Bloom filter size for a column is 1048576
[2023-01-31T12:22:37.067+0000] {spark_submit.py:495} INFO - Bloom filter expected number of distinct values are: null
[2023-01-31T12:22:37.067+0000] {spark_submit.py:495} INFO - Page row count limit to 20000
[2023-01-31T12:22:37.068+0000] {spark_submit.py:495} INFO - Writing page checksums is: on
[2023-01-31T12:22:37.099+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:37 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-01-31T12:22:37.099+0000] {spark_submit.py:495} INFO - {
[2023-01-31T12:22:37.099+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-31T12:22:37.100+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-31T12:22:37.100+0000] {spark_submit.py:495} INFO - "name" : "measured_at",
[2023-01-31T12:22:37.100+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T12:22:37.100+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T12:22:37.101+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:37.101+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:37.101+0000] {spark_submit.py:495} INFO - "name" : "created_at",
[2023-01-31T12:22:37.102+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T12:22:37.102+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T12:22:37.102+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:37.102+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:37.103+0000] {spark_submit.py:495} INFO - "name" : "Biomass - Actual Aggregated",
[2023-01-31T12:22:37.103+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:37.103+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:37.103+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:37.103+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:37.103+0000] {spark_submit.py:495} INFO - "name" : "Fossil Brown coal/Lignite - Actual Aggregated",
[2023-01-31T12:22:37.103+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:37.104+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:37.104+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:37.104+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:37.104+0000] {spark_submit.py:495} INFO - "name" : "Fossil Gas - Actual Aggregated",
[2023-01-31T12:22:37.104+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:37.104+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:37.105+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:37.105+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:37.105+0000] {spark_submit.py:495} INFO - "name" : "Fossil Hard coal - Actual Aggregated",
[2023-01-31T12:22:37.105+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:37.105+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:37.105+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:37.105+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:37.106+0000] {spark_submit.py:495} INFO - "name" : "Fossil Oil - Actual Aggregated",
[2023-01-31T12:22:37.107+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:37.108+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:37.108+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:37.108+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:37.108+0000] {spark_submit.py:495} INFO - "name" : "Geothermal - Actual Aggregated",
[2023-01-31T12:22:37.108+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:37.108+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:37.109+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:37.109+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:37.109+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Aggregated",
[2023-01-31T12:22:37.109+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:37.109+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:37.109+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:37.109+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:37.109+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Consumption",
[2023-01-31T12:22:37.110+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:37.110+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:37.110+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:37.110+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:37.110+0000] {spark_submit.py:495} INFO - "name" : "Hydro Run-of-river and poundage - Actual Aggregated",
[2023-01-31T12:22:37.110+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:37.110+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:37.111+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:37.111+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:37.111+0000] {spark_submit.py:495} INFO - "name" : "Hydro Water Reservoir - Actual Aggregated",
[2023-01-31T12:22:37.111+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:37.111+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:37.111+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:37.112+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:37.112+0000] {spark_submit.py:495} INFO - "name" : "Nuclear - Actual Aggregated",
[2023-01-31T12:22:37.112+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:37.112+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:37.112+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:37.112+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:37.112+0000] {spark_submit.py:495} INFO - "name" : "Other - Actual Aggregated",
[2023-01-31T12:22:37.113+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:37.113+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:37.113+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:37.113+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:37.113+0000] {spark_submit.py:495} INFO - "name" : "Other renewable - Actual Aggregated",
[2023-01-31T12:22:37.113+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:37.113+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:37.114+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:37.114+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:37.114+0000] {spark_submit.py:495} INFO - "name" : "Solar - Actual Aggregated",
[2023-01-31T12:22:37.114+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:37.114+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:37.114+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:37.114+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:37.115+0000] {spark_submit.py:495} INFO - "name" : "Waste - Actual Aggregated",
[2023-01-31T12:22:37.115+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:37.115+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:37.115+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:37.115+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:37.115+0000] {spark_submit.py:495} INFO - "name" : "Wind Offshore - Actual Aggregated",
[2023-01-31T12:22:37.115+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:37.116+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:37.116+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:37.116+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:37.116+0000] {spark_submit.py:495} INFO - "name" : "Wind Onshore - Actual Aggregated",
[2023-01-31T12:22:37.116+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:37.116+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:37.116+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:37.117+0000] {spark_submit.py:495} INFO - } ]
[2023-01-31T12:22:37.117+0000] {spark_submit.py:495} INFO - }
[2023-01-31T12:22:37.117+0000] {spark_submit.py:495} INFO - and corresponding Parquet message type:
[2023-01-31T12:22:37.117+0000] {spark_submit.py:495} INFO - message spark_schema {
[2023-01-31T12:22:37.117+0000] {spark_submit.py:495} INFO - required int96 measured_at;
[2023-01-31T12:22:37.117+0000] {spark_submit.py:495} INFO - required int96 created_at;
[2023-01-31T12:22:37.117+0000] {spark_submit.py:495} INFO - optional double Biomass - Actual Aggregated;
[2023-01-31T12:22:37.117+0000] {spark_submit.py:495} INFO - optional double Fossil Brown coal/Lignite - Actual Aggregated;
[2023-01-31T12:22:37.118+0000] {spark_submit.py:495} INFO - optional double Fossil Gas - Actual Aggregated;
[2023-01-31T12:22:37.118+0000] {spark_submit.py:495} INFO - optional double Fossil Hard coal - Actual Aggregated;
[2023-01-31T12:22:37.118+0000] {spark_submit.py:495} INFO - optional double Fossil Oil - Actual Aggregated;
[2023-01-31T12:22:37.118+0000] {spark_submit.py:495} INFO - optional double Geothermal - Actual Aggregated;
[2023-01-31T12:22:37.118+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Aggregated;
[2023-01-31T12:22:37.118+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Consumption;
[2023-01-31T12:22:37.119+0000] {spark_submit.py:495} INFO - optional double Hydro Run-of-river and poundage - Actual Aggregated;
[2023-01-31T12:22:37.119+0000] {spark_submit.py:495} INFO - optional double Hydro Water Reservoir - Actual Aggregated;
[2023-01-31T12:22:37.119+0000] {spark_submit.py:495} INFO - optional double Nuclear - Actual Aggregated;
[2023-01-31T12:22:37.119+0000] {spark_submit.py:495} INFO - optional double Other - Actual Aggregated;
[2023-01-31T12:22:37.119+0000] {spark_submit.py:495} INFO - optional double Other renewable - Actual Aggregated;
[2023-01-31T12:22:37.119+0000] {spark_submit.py:495} INFO - optional double Solar - Actual Aggregated;
[2023-01-31T12:22:37.120+0000] {spark_submit.py:495} INFO - optional double Waste - Actual Aggregated;
[2023-01-31T12:22:37.120+0000] {spark_submit.py:495} INFO - optional double Wind Offshore - Actual Aggregated;
[2023-01-31T12:22:37.120+0000] {spark_submit.py:495} INFO - optional double Wind Onshore - Actual Aggregated;
[2023-01-31T12:22:37.120+0000] {spark_submit.py:495} INFO - }
[2023-01-31T12:22:37.120+0000] {spark_submit.py:495} INFO - 
[2023-01-31T12:22:37.120+0000] {spark_submit.py:495} INFO - 
[2023-01-31T12:22:37.373+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:37 INFO CodecPool: Got brand-new compressor [.snappy]
[2023-01-31T12:22:37.629+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:37 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 8124f810dec3:36903 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:39.236+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:39 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675166976081-c3a76610-54ae-4c28-bf0c-abcfa660f1e1/_temporary/0/_temporary/' directory.
[2023-01-31T12:22:39.236+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:39 INFO FileOutputCommitter: Saved output of task 'attempt_20230131122236328934217227920603_0008_m_000000_8' to gs://entsoe_temp_1009/.spark-bigquery-local-1675166976081-c3a76610-54ae-4c28-bf0c-abcfa660f1e1/_temporary/0/task_20230131122236328934217227920603_0008_m_000000
[2023-01-31T12:22:39.237+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:39 INFO SparkHadoopMapRedUtil: attempt_20230131122236328934217227920603_0008_m_000000_8: Committed. Elapsed time: 858 ms.
[2023-01-31T12:22:39.243+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:39 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2588 bytes result sent to driver
[2023-01-31T12:22:39.244+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:39 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 2294 ms on 8124f810dec3 (executor driver) (1/1)
[2023-01-31T12:22:39.244+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:39 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2023-01-31T12:22:39.244+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:39 INFO DAGScheduler: ResultStage 8 (save at BigQueryWriteHelper.java:105) finished in 2.327 s
[2023-01-31T12:22:39.245+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:39 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T12:22:39.245+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2023-01-31T12:22:39.245+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:39 INFO DAGScheduler: Job 8 finished: save at BigQueryWriteHelper.java:105, took 2.331003 s
[2023-01-31T12:22:39.247+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:39 INFO FileFormatWriter: Start to commit write Job 3566e665-a387-4f46-b631-57b5b748baf9.
[2023-01-31T12:22:39.910+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:39 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675166976081-c3a76610-54ae-4c28-bf0c-abcfa660f1e1/_temporary/0/task_20230131122236328934217227920603_0008_m_000000/' directory.
[2023-01-31T12:22:41.258+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:41 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675166976081-c3a76610-54ae-4c28-bf0c-abcfa660f1e1/' directory.
[2023-01-31T12:22:41.409+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:41 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 8124f810dec3:36903 in memory (size: 74.3 KiB, free: 434.3 MiB)
[2023-01-31T12:22:41.454+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:41 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 8124f810dec3:36903 in memory (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T12:22:41.851+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:41 INFO FileFormatWriter: Write Job 3566e665-a387-4f46-b631-57b5b748baf9 committed. Elapsed time: 2603 ms.
[2023-01-31T12:22:41.856+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:41 INFO FileFormatWriter: Finished processing stats for write job 3566e665-a387-4f46-b631-57b5b748baf9.
[2023-01-31T12:22:41.909+0000] {local_task_job.py:223} WARNING - State of this instance has been externally set to up_for_retry. Terminating instance.
[2023-01-31T12:22:41.912+0000] {process_utils.py:129} INFO - Sending Signals.SIGTERM to group 668. PIDs of all processes in the group: [673, 960, 668]
[2023-01-31T12:22:41.912+0000] {process_utils.py:84} INFO - Sending the signal Signals.SIGTERM to group 668
[2023-01-31T12:22:41.912+0000] {taskinstance.py:1483} ERROR - Received SIGTERM. Terminating subprocesses.
[2023-01-31T12:22:41.913+0000] {spark_submit.py:620} INFO - Sending kill signal to spark-submit
[2023-01-31T12:22:41.926+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 414, in submit
    self._process_spark_submit_log(iter(self._submit_sp.stdout))  # type: ignore
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 463, in _process_spark_submit_log
    for line in itr:
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 1485, in signal_handler
    raise AirflowException("Task received SIGTERM signal")
airflow.exceptions.AirflowException: Task received SIGTERM signal
[2023-01-31T12:22:41.932+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=entsoe-energydata-backfill, task_id=stage_total_generation, execution_date=20210101T060000, start_date=20230131T120853, end_date=20230131T122241
[2023-01-31T12:22:41.947+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 1000 for task stage_total_generation (Task received SIGTERM signal; 668)
[2023-01-31T12:22:42.005+0000] {process_utils.py:79} INFO - Process psutil.Process(pid=673, status='terminated', started='12:08:54') (673) terminated with exit code None
[2023-01-31T12:22:42.005+0000] {process_utils.py:79} INFO - Process psutil.Process(pid=668, status='terminated', exitcode=1, started='12:08:53') (668) terminated with exit code 1
[2023-01-31T12:22:42.006+0000] {process_utils.py:79} INFO - Process psutil.Process(pid=960, status='terminated', started='12:09:17') (960) terminated with exit code None
[2023-01-31T13:03:43.057+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [queued]>
[2023-01-31T13:03:43.121+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [queued]>
[2023-01-31T13:03:43.122+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T13:03:43.123+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T13:03:43.124+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T13:03:43.179+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 06:00:00+00:00
[2023-01-31T13:03:43.194+0000] {standard_task_runner.py:55} INFO - Started process 227 to run task
[2023-01-31T13:03:43.200+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T06:00:00+00:00', '--job-id', '1029', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpis74emdt']
[2023-01-31T13:03:43.213+0000] {standard_task_runner.py:83} INFO - Job 1029: Subtask stage_total_generation
[2023-01-31T13:03:43.392+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [running]> on host b37fe3cbf330
[2023-01-31T13:03:43.733+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T06:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T06:00:00+00:00
[2023-01-31T13:03:43.781+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T13:03:43.782+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010600 202101010700 DE_TENNET 202101010000 202101010600
[2023-01-31T13:04:16.962+0000] {spark_submit.py:495} INFO - mulai periode: 202101010000
[2023-01-31T13:04:16.964+0000] {spark_submit.py:495} INFO - selesai periode: 202101010600
[2023-01-31T13:04:18.218+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:18 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T13:04:20.608+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T13:04:22.227+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:22 INFO ResourceUtils: ==============================================================
[2023-01-31T13:04:22.229+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:22 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T13:04:22.232+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:22 INFO ResourceUtils: ==============================================================
[2023-01-31T13:04:22.234+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:22 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T13:04:22.404+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:22 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T13:04:22.435+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:22 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T13:04:22.443+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:22 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T13:04:22.864+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:22 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T13:04:22.869+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:22 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T13:04:22.873+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:22 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T13:04:22.875+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:22 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T13:04:22.883+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:22 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T13:04:28.836+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:28 INFO Utils: Successfully started service 'sparkDriver' on port 34829.
[2023-01-31T13:04:30.298+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:30 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T13:04:31.085+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:31 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T13:04:31.957+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:31 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T13:04:31.982+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:31 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T13:04:32.056+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:32 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T13:04:32.297+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:32 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f00e830c-6644-4013-ac6a-a26b369ec280
[2023-01-31T13:04:32.493+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:32 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T13:04:32.765+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:32 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T13:04:35.075+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:35 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-01-31T13:04:35.410+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:35 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://b37fe3cbf330:34829/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675170258000
[2023-01-31T13:04:35.420+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:35 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://b37fe3cbf330:34829/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675170258000
[2023-01-31T13:04:36.710+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:36 INFO Executor: Starting executor ID driver on host b37fe3cbf330
[2023-01-31T13:04:37.070+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:37 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T13:04:37.575+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:37 INFO Executor: Fetching spark://b37fe3cbf330:34829/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675170258000
[2023-01-31T13:04:39.132+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:39 INFO TransportClientFactory: Successfully created connection to b37fe3cbf330/172.19.0.8:34829 after 902 ms (0 ms spent in bootstraps)
[2023-01-31T13:04:39.226+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:39 INFO Utils: Fetching spark://b37fe3cbf330:34829/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-e1437171-d55c-4105-9c19-ebea9dd1a587/userFiles-cfa397b1-ee31-4748-89fe-dabd80386d2b/fetchFileTemp4307517289469770544.tmp
[2023-01-31T13:04:45.223+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:45 INFO Executor: Adding file:/tmp/spark-e1437171-d55c-4105-9c19-ebea9dd1a587/userFiles-cfa397b1-ee31-4748-89fe-dabd80386d2b/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T13:04:45.246+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:45 INFO Executor: Fetching spark://b37fe3cbf330:34829/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675170258000
[2023-01-31T13:04:45.272+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:45 INFO Utils: Fetching spark://b37fe3cbf330:34829/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-e1437171-d55c-4105-9c19-ebea9dd1a587/userFiles-cfa397b1-ee31-4748-89fe-dabd80386d2b/fetchFileTemp5204228240187704032.tmp
[2023-01-31T13:04:48.454+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:48 INFO Executor: Adding file:/tmp/spark-e1437171-d55c-4105-9c19-ebea9dd1a587/userFiles-cfa397b1-ee31-4748-89fe-dabd80386d2b/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T13:04:48.851+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:48 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41543.
[2023-01-31T13:04:48.852+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:48 INFO NettyBlockTransferService: Server created on b37fe3cbf330:41543
[2023-01-31T13:04:48.894+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:48 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T13:04:48.959+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:48 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, b37fe3cbf330, 41543, None)
[2023-01-31T13:04:49.026+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:48 INFO Executor: Told to re-register on heartbeat
[2023-01-31T13:04:49.028+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:49 INFO BlockManagerMasterEndpoint: Registering block manager b37fe3cbf330:41543 with 434.4 MiB RAM, BlockManagerId(driver, b37fe3cbf330, 41543, None)
[2023-01-31T13:04:49.044+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:49 INFO BlockManager: BlockManager null re-registering with master
[2023-01-31T13:04:49.046+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:49 INFO BlockManagerMaster: Registering BlockManager null
[2023-01-31T13:04:49.118+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:49 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, b37fe3cbf330, 41543, None)
[2023-01-31T13:04:49.126+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:49 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, b37fe3cbf330, 41543, None)
[2023-01-31T13:04:49.207+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:49 WARN Executor: Issue communicating with driver in heartbeater
[2023-01-31T13:04:49.208+0000] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-01-31T13:04:49.208+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-01-31T13:04:49.210+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-01-31T13:04:49.210+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2023-01-31T13:04:49.210+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2023-01-31T13:04:49.211+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:79)
[2023-01-31T13:04:49.211+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:643)
[2023-01-31T13:04:49.212+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2023-01-31T13:04:49.212+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2023-01-31T13:04:49.212+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
[2023-01-31T13:04:49.212+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2023-01-31T13:04:49.213+0000] {spark_submit.py:495} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2023-01-31T13:04:49.213+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-01-31T13:04:49.213+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2023-01-31T13:04:49.214+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2023-01-31T13:04:49.270+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-31T13:04:49.271+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-31T13:04:49.271+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-31T13:04:49.272+0000] {spark_submit.py:495} INFO - Caused by: java.lang.NullPointerException
[2023-01-31T13:04:49.272+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)
[2023-01-31T13:04:49.273+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)
[2023-01-31T13:04:49.273+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-01-31T13:04:49.273+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-01-31T13:04:49.274+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-01-31T13:04:49.274+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-01-31T13:04:49.329+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-01-31T13:04:49.330+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-01-31T13:04:49.331+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-01-31T13:04:49.331+0000] {spark_submit.py:495} INFO - ... 3 more
[2023-01-31T13:04:49.331+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:49 ERROR Inbox: Ignoring error
[2023-01-31T13:04:49.332+0000] {spark_submit.py:495} INFO - java.lang.NullPointerException
[2023-01-31T13:04:49.332+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)
[2023-01-31T13:04:49.333+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)
[2023-01-31T13:04:49.333+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-01-31T13:04:49.333+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-01-31T13:04:49.334+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-01-31T13:04:49.334+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-01-31T13:04:49.335+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-01-31T13:04:49.335+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-01-31T13:04:49.335+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-01-31T13:04:49.336+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-31T13:04:49.336+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-31T13:04:49.337+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-31T13:05:03.062+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:03 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T13:05:03.358+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:03 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T13:05:30.559+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:30 INFO InMemoryFileIndex: It took 292 ms to list leaf files for 1 paths.
[2023-01-31T13:05:30.936+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:30 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T13:05:31.382+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:31 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T13:05:31.390+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:31 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on b37fe3cbf330:41543 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T13:05:31.458+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:31 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T13:05:35.465+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:35 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T13:05:35.934+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:35 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T13:05:36.766+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:36 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T13:05:37.049+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:37 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T13:05:37.051+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:37 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T13:05:37.053+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:37 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T13:05:37.060+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:37 INFO DAGScheduler: Missing parents: List()
[2023-01-31T13:05:37.098+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:37 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T13:05:38.068+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:38 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T13:05:38.126+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:38 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T13:05:38.142+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:38 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on b37fe3cbf330:41543 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T13:05:38.155+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:38 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T13:05:38.585+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T13:05:38.586+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:38 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T13:05:39.896+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:39 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T13:05:40.060+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:40 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T13:05:42.178+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:42 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010600__202101010700.json:0+9371
[2023-01-31T13:05:45.616+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:45 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3480 bytes result sent to driver
[2023-01-31T13:05:45.881+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:45 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 6467 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T13:05:45.933+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:45 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T13:05:45.970+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:45 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 8.508 s
[2023-01-31T13:05:46.027+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:46 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T13:05:46.034+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T13:05:46.060+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:46 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 9.291058 s
[2023-01-31T13:05:57.132+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:57 INFO BlockManagerInfo: Removed broadcast_1_piece0 on b37fe3cbf330:41543 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T13:05:57.226+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:57 INFO BlockManagerInfo: Removed broadcast_0_piece0 on b37fe3cbf330:41543 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T13:06:21.054+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:21 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T13:06:21.089+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:21 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T13:06:21.135+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:21 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T13:06:24.019+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO CodeGenerator: Code generated in 1498.545649 ms
[2023-01-31T13:06:24.049+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T13:06:24.128+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T13:06:24.130+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on b37fe3cbf330:41543 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T13:06:24.133+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T13:06:24.182+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T13:06:24.673+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T13:06:24.680+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T13:06:24.680+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T13:06:24.681+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T13:06:24.681+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO DAGScheduler: Missing parents: List()
[2023-01-31T13:06:24.695+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T13:06:24.761+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T13:06:24.797+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T13:06:24.802+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on b37fe3cbf330:41543 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T13:06:24.806+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T13:06:24.810+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T13:06:24.811+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T13:06:24.836+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T13:06:24.840+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T13:06:25.456+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:25 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T13:06:26.077+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO CodeGenerator: Code generated in 488.472135 ms
[2023-01-31T13:06:26.541+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1628 bytes result sent to driver
[2023-01-31T13:06:26.555+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1734 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T13:06:26.559+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 1.825 s
[2023-01-31T13:06:26.560+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T13:06:26.561+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T13:06:26.564+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T13:06:26.571+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 1.893884 s
[2023-01-31T13:06:26.825+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T13:06:26.826+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T13:06:26.828+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T13:06:26.940+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO CodeGenerator: Code generated in 71.294155 ms
[2023-01-31T13:06:26.980+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T13:06:27.027+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T13:06:27.035+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on b37fe3cbf330:41543 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T13:06:27.041+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T13:06:27.050+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T13:06:27.191+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T13:06:27.197+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T13:06:27.197+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T13:06:27.198+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T13:06:27.198+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO DAGScheduler: Missing parents: List()
[2023-01-31T13:06:27.204+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T13:06:27.256+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T13:06:27.312+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T13:06:27.318+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on b37fe3cbf330:41543 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T13:06:27.320+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T13:06:27.338+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T13:06:27.340+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T13:06:27.359+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T13:06:27.374+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T13:06:27.494+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T13:06:28.016+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1628 bytes result sent to driver
[2023-01-31T13:06:28.037+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 677 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T13:06:28.039+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T13:06:28.046+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 0.817 s
[2023-01-31T13:06:28.047+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T13:06:28.047+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T13:06:28.047+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 0.854658 s
[2023-01-31T13:06:28.902+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T13:06:28.908+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T13:06:28.913+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T13:06:29.532+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO CodeGenerator: Code generated in 393.710611 ms
[2023-01-31T13:06:29.574+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T13:06:29.626+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T13:06:29.626+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on b37fe3cbf330:41543 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T13:06:29.627+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T13:06:29.627+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T13:06:29.778+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T13:06:29.781+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T13:06:29.781+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T13:06:29.781+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T13:06:29.782+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO DAGScheduler: Missing parents: List()
[2023-01-31T13:06:29.794+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T13:06:29.805+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.6 MiB)
[2023-01-31T13:06:29.818+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.6 MiB)
[2023-01-31T13:06:29.820+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on b37fe3cbf330:41543 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T13:06:29.823+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T13:06:29.835+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T13:06:29.836+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T13:06:29.841+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T13:06:29.842+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T13:06:29.906+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T13:06:30.050+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-01-31T13:06:30.064+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 224 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T13:06:30.066+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 0.279 s
[2023-01-31T13:06:30.066+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T13:06:30.067+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T13:06:30.067+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T13:06:30.078+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 0.291997 s
[2023-01-31T13:06:30.228+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T13:06:30.228+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T13:06:30.229+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T13:06:30.558+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO CodeGenerator: Code generated in 225.380948 ms
[2023-01-31T13:06:30.578+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.4 MiB)
[2023-01-31T13:06:30.629+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T13:06:30.633+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on b37fe3cbf330:41543 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T13:06:30.634+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T13:06:30.636+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T13:06:30.777+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T13:06:30.779+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T13:06:30.780+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T13:06:30.780+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T13:06:30.780+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO DAGScheduler: Missing parents: List()
[2023-01-31T13:06:30.784+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T13:06:30.808+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-31T13:06:30.826+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-01-31T13:06:30.828+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on b37fe3cbf330:41543 (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T13:06:30.832+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T13:06:30.841+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T13:06:30.843+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T13:06:30.846+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T13:06:30.853+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T13:06:30.920+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T13:06:31.095+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:31 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1585 bytes result sent to driver
[2023-01-31T13:06:31.261+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:31 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 415 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T13:06:31.261+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:31 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T13:06:31.267+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:31 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 0.474 s
[2023-01-31T13:06:31.268+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:31 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T13:06:31.268+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T13:06:31.268+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:31 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 0.489725 s
[2023-01-31T13:06:31.290+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:31 INFO BlockManagerInfo: Removed broadcast_5_piece0 on b37fe3cbf330:41543 in memory (size: 7.7 KiB, free: 434.2 MiB)
[2023-01-31T13:06:31.344+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:31 INFO BlockManagerInfo: Removed broadcast_7_piece0 on b37fe3cbf330:41543 in memory (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T13:06:31.436+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:31 INFO BlockManagerInfo: Removed broadcast_3_piece0 on b37fe3cbf330:41543 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T13:06:32.676+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:32 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T13:06:32.695+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:32 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T13:06:32.697+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:32 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T13:06:33.422+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:33 INFO CodeGenerator: Code generated in 464.927155 ms
[2023-01-31T13:06:33.456+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:33 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.3 MiB)
[2023-01-31T13:06:33.659+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:33 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T13:06:33.660+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:33 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on b37fe3cbf330:41543 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T13:06:33.671+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:33 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T13:06:33.681+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T13:06:33.926+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:33 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T13:06:33.926+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:33 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T13:06:33.927+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:33 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T13:06:33.927+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:33 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T13:06:33.927+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:33 INFO DAGScheduler: Missing parents: List()
[2023-01-31T13:06:33.934+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:33 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T13:06:33.981+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:33 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.2 MiB)
[2023-01-31T13:06:34.001+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:33 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.2 MiB)
[2023-01-31T13:06:34.004+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on b37fe3cbf330:41543 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T13:06:34.011+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T13:06:34.020+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T13:06:34.024+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T13:06:34.031+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T13:06:34.034+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T13:06:34.082+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T13:06:34.334+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2499 bytes result sent to driver
[2023-01-31T13:06:34.338+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 304 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T13:06:34.339+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T13:06:34.339+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 0.406 s
[2023-01-31T13:06:34.342+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T13:06:34.343+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T13:06:34.343+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 0.416040 s
[2023-01-31T13:06:34.664+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T13:06:34.665+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T13:06:34.666+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T13:06:35.311+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO CodeGenerator: Code generated in 362.193857 ms
[2023-01-31T13:06:35.345+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 433.0 MiB)
[2023-01-31T13:06:35.439+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.9 MiB)
[2023-01-31T13:06:35.443+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on b37fe3cbf330:41543 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T13:06:35.454+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T13:06:35.464+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T13:06:35.652+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T13:06:35.653+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T13:06:35.653+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T13:06:35.653+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T13:06:35.653+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO DAGScheduler: Missing parents: List()
[2023-01-31T13:06:35.664+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T13:06:35.683+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 432.9 MiB)
[2023-01-31T13:06:35.728+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.9 MiB)
[2023-01-31T13:06:35.729+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on b37fe3cbf330:41543 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T13:06:35.730+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T13:06:35.751+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T13:06:35.751+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T13:06:35.754+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T13:06:35.758+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T13:06:35.795+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T13:06:36.012+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:36 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-01-31T13:06:36.020+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:36 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 267 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T13:06:36.021+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:36 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T13:06:36.024+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:36 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 0.363 s
[2023-01-31T13:06:36.029+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:36 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T13:06:36.029+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T13:06:36.029+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:36 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 0.375015 s
[2023-01-31T13:06:36.354+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:36 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T13:06:36.355+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:36 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T13:06:36.363+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:36 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T13:06:37.029+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO CodeGenerator: Code generated in 292.461027 ms
[2023-01-31T13:06:37.061+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.7 MiB)
[2023-01-31T13:06:37.161+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.7 MiB)
[2023-01-31T13:06:37.162+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on b37fe3cbf330:41543 (size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T13:06:37.169+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:204
[2023-01-31T13:06:37.177+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T13:06:37.306+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:204
[2023-01-31T13:06:37.309+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:204) with 1 output partitions
[2023-01-31T13:06:37.309+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:204)
[2023-01-31T13:06:37.309+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T13:06:37.310+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO DAGScheduler: Missing parents: List()
[2023-01-31T13:06:37.313+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:204), which has no missing parents
[2023-01-31T13:06:37.319+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.6 MiB)
[2023-01-31T13:06:37.325+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.6 MiB)
[2023-01-31T13:06:37.331+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on b37fe3cbf330:41543 (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T13:06:37.333+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T13:06:37.335+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:204) (first 15 tasks are for partitions Vector(0))
[2023-01-31T13:06:37.336+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T13:06:37.338+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T13:06:37.340+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T13:06:37.363+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T13:06:37.500+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1729 bytes result sent to driver
[2023-01-31T13:06:37.505+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 165 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T13:06:37.505+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T13:06:37.506+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:204) finished in 0.192 s
[2023-01-31T13:06:37.506+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T13:06:37.506+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T13:06:37.506+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:204, took 0.198604 s
[2023-01-31T13:06:41.238+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:41 INFO BlockManagerInfo: Removed broadcast_4_piece0 on b37fe3cbf330:41543 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T13:06:41.301+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:41 INFO BlockManagerInfo: Removed broadcast_13_piece0 on b37fe3cbf330:41543 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T13:06:41.409+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:41 INFO BlockManagerInfo: Removed broadcast_12_piece0 on b37fe3cbf330:41543 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T13:06:41.464+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:41 INFO BlockManagerInfo: Removed broadcast_2_piece0 on b37fe3cbf330:41543 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T13:06:41.510+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:41 INFO BlockManagerInfo: Removed broadcast_9_piece0 on b37fe3cbf330:41543 in memory (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T13:06:41.574+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:41 INFO BlockManagerInfo: Removed broadcast_15_piece0 on b37fe3cbf330:41543 in memory (size: 12.6 KiB, free: 434.3 MiB)
[2023-01-31T13:06:41.647+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:41 INFO BlockManagerInfo: Removed broadcast_11_piece0 on b37fe3cbf330:41543 in memory (size: 13.4 KiB, free: 434.3 MiB)
[2023-01-31T13:06:41.684+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:41 INFO BlockManagerInfo: Removed broadcast_14_piece0 on b37fe3cbf330:41543 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T13:06:41.740+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:41 INFO BlockManagerInfo: Removed broadcast_6_piece0 on b37fe3cbf330:41543 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T13:06:49.622+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:49 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T13:06:49.719+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T13:06:49.725+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T13:06:49.726+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:49 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T13:06:49.726+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T13:06:49.726+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T13:06:49.743+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T13:06:50.530+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-31T13:06:50.530+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO DAGScheduler: Got job 8 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T13:06:50.531+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO DAGScheduler: Final stage: ResultStage 8 (save at BigQueryWriteHelper.java:105)
[2023-01-31T13:06:50.531+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T13:06:50.531+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO DAGScheduler: Missing parents: List()
[2023-01-31T13:06:50.536+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO DAGScheduler: Submitting ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T13:06:50.754+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 209.9 KiB, free 433.7 MiB)
[2023-01-31T13:06:50.759+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 74.3 KiB, free 433.7 MiB)
[2023-01-31T13:06:50.759+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on b37fe3cbf330:41543 (size: 74.3 KiB, free: 434.3 MiB)
[2023-01-31T13:06:50.762+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513
[2023-01-31T13:06:50.762+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T13:06:50.762+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2023-01-31T13:06:50.782+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7484 bytes) taskResourceAssignments Map()
[2023-01-31T13:06:50.788+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2023-01-31T13:06:51.199+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T13:06:51.199+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T13:06:51.201+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T13:06:51.202+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T13:06:51.202+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T13:06:51.205+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T13:06:51.219+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T13:06:51.228+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T13:06:51.331+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO ParquetOutputFormat: Parquet block size to 134217728
[2023-01-31T13:06:51.331+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO ParquetOutputFormat: Validation is off
[2023-01-31T13:06:51.342+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-01-31T13:06:51.343+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO ParquetOutputFormat: Parquet properties are:
[2023-01-31T13:06:51.343+0000] {spark_submit.py:495} INFO - Parquet page size to 1048576
[2023-01-31T13:06:51.343+0000] {spark_submit.py:495} INFO - Parquet dictionary page size to 1048576
[2023-01-31T13:06:51.343+0000] {spark_submit.py:495} INFO - Dictionary is true
[2023-01-31T13:06:51.345+0000] {spark_submit.py:495} INFO - Writer version is: PARQUET_1_0
[2023-01-31T13:06:51.345+0000] {spark_submit.py:495} INFO - Page size checking is: estimated
[2023-01-31T13:06:51.346+0000] {spark_submit.py:495} INFO - Min row count for page size check is: 100
[2023-01-31T13:06:51.346+0000] {spark_submit.py:495} INFO - Max row count for page size check is: 10000
[2023-01-31T13:06:51.346+0000] {spark_submit.py:495} INFO - Truncate length for column indexes is: 64
[2023-01-31T13:06:51.347+0000] {spark_submit.py:495} INFO - Truncate length for statistics min/max  is: 2147483647
[2023-01-31T13:06:51.347+0000] {spark_submit.py:495} INFO - Bloom filter enabled: false
[2023-01-31T13:06:51.348+0000] {spark_submit.py:495} INFO - Max Bloom filter size for a column is 1048576
[2023-01-31T13:06:51.350+0000] {spark_submit.py:495} INFO - Bloom filter expected number of distinct values are: null
[2023-01-31T13:06:51.353+0000] {spark_submit.py:495} INFO - Page row count limit to 20000
[2023-01-31T13:06:51.353+0000] {spark_submit.py:495} INFO - Writing page checksums is: on
[2023-01-31T13:06:51.427+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-01-31T13:06:51.428+0000] {spark_submit.py:495} INFO - {
[2023-01-31T13:06:51.428+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-31T13:06:51.428+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-31T13:06:51.428+0000] {spark_submit.py:495} INFO - "name" : "measured_at",
[2023-01-31T13:06:51.428+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T13:06:51.429+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T13:06:51.429+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.429+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.429+0000] {spark_submit.py:495} INFO - "name" : "created_at",
[2023-01-31T13:06:51.429+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T13:06:51.436+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T13:06:51.436+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.449+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.472+0000] {spark_submit.py:495} INFO - "name" : "Biomass - Actual Aggregated",
[2023-01-31T13:06:51.475+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.475+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.475+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.476+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.476+0000] {spark_submit.py:495} INFO - "name" : "Fossil Brown coal/Lignite - Actual Aggregated",
[2023-01-31T13:06:51.476+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.476+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.476+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.476+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.476+0000] {spark_submit.py:495} INFO - "name" : "Fossil Gas - Actual Aggregated",
[2023-01-31T13:06:51.477+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.477+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.477+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.477+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.477+0000] {spark_submit.py:495} INFO - "name" : "Fossil Hard coal - Actual Aggregated",
[2023-01-31T13:06:51.477+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.477+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.478+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.478+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.478+0000] {spark_submit.py:495} INFO - "name" : "Fossil Oil - Actual Aggregated",
[2023-01-31T13:06:51.484+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.484+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.484+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.485+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.485+0000] {spark_submit.py:495} INFO - "name" : "Geothermal - Actual Aggregated",
[2023-01-31T13:06:51.488+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.490+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.491+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.491+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.491+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Aggregated",
[2023-01-31T13:06:51.492+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.494+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.494+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.496+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.498+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Consumption",
[2023-01-31T13:06:51.499+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.502+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.503+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.504+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.504+0000] {spark_submit.py:495} INFO - "name" : "Hydro Run-of-river and poundage - Actual Aggregated",
[2023-01-31T13:06:51.505+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.506+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.507+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.508+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.508+0000] {spark_submit.py:495} INFO - "name" : "Hydro Water Reservoir - Actual Aggregated",
[2023-01-31T13:06:51.508+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.509+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.509+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.510+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.510+0000] {spark_submit.py:495} INFO - "name" : "Nuclear - Actual Aggregated",
[2023-01-31T13:06:51.511+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.511+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.514+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.515+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.515+0000] {spark_submit.py:495} INFO - "name" : "Other - Actual Aggregated",
[2023-01-31T13:06:51.520+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.521+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.522+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.523+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.523+0000] {spark_submit.py:495} INFO - "name" : "Other renewable - Actual Aggregated",
[2023-01-31T13:06:51.524+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.524+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.525+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.526+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.527+0000] {spark_submit.py:495} INFO - "name" : "Solar - Actual Aggregated",
[2023-01-31T13:06:51.527+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.528+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.528+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.529+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.529+0000] {spark_submit.py:495} INFO - "name" : "Waste - Actual Aggregated",
[2023-01-31T13:06:51.530+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.530+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.531+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.531+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.531+0000] {spark_submit.py:495} INFO - "name" : "Wind Offshore - Actual Aggregated",
[2023-01-31T13:06:51.533+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.533+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.533+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.534+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.534+0000] {spark_submit.py:495} INFO - "name" : "Wind Onshore - Actual Aggregated",
[2023-01-31T13:06:51.535+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.537+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.537+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.537+0000] {spark_submit.py:495} INFO - } ]
[2023-01-31T13:06:51.538+0000] {spark_submit.py:495} INFO - }
[2023-01-31T13:06:51.538+0000] {spark_submit.py:495} INFO - and corresponding Parquet message type:
[2023-01-31T13:06:51.539+0000] {spark_submit.py:495} INFO - message spark_schema {
[2023-01-31T13:06:51.539+0000] {spark_submit.py:495} INFO - required int96 measured_at;
[2023-01-31T13:06:51.540+0000] {spark_submit.py:495} INFO - required int96 created_at;
[2023-01-31T13:06:51.540+0000] {spark_submit.py:495} INFO - optional double Biomass - Actual Aggregated;
[2023-01-31T13:06:51.541+0000] {spark_submit.py:495} INFO - optional double Fossil Brown coal/Lignite - Actual Aggregated;
[2023-01-31T13:06:51.541+0000] {spark_submit.py:495} INFO - optional double Fossil Gas - Actual Aggregated;
[2023-01-31T13:06:51.544+0000] {spark_submit.py:495} INFO - optional double Fossil Hard coal - Actual Aggregated;
[2023-01-31T13:06:51.545+0000] {spark_submit.py:495} INFO - optional double Fossil Oil - Actual Aggregated;
[2023-01-31T13:06:51.545+0000] {spark_submit.py:495} INFO - optional double Geothermal - Actual Aggregated;
[2023-01-31T13:06:51.547+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Aggregated;
[2023-01-31T13:06:51.548+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Consumption;
[2023-01-31T13:06:51.548+0000] {spark_submit.py:495} INFO - optional double Hydro Run-of-river and poundage - Actual Aggregated;
[2023-01-31T13:06:51.548+0000] {spark_submit.py:495} INFO - optional double Hydro Water Reservoir - Actual Aggregated;
[2023-01-31T13:06:51.549+0000] {spark_submit.py:495} INFO - optional double Nuclear - Actual Aggregated;
[2023-01-31T13:06:51.549+0000] {spark_submit.py:495} INFO - optional double Other - Actual Aggregated;
[2023-01-31T13:06:51.549+0000] {spark_submit.py:495} INFO - optional double Other renewable - Actual Aggregated;
[2023-01-31T13:06:51.549+0000] {spark_submit.py:495} INFO - optional double Solar - Actual Aggregated;
[2023-01-31T13:06:51.550+0000] {spark_submit.py:495} INFO - optional double Waste - Actual Aggregated;
[2023-01-31T13:06:51.550+0000] {spark_submit.py:495} INFO - optional double Wind Offshore - Actual Aggregated;
[2023-01-31T13:06:51.553+0000] {spark_submit.py:495} INFO - optional double Wind Onshore - Actual Aggregated;
[2023-01-31T13:06:51.554+0000] {spark_submit.py:495} INFO - }
[2023-01-31T13:06:51.554+0000] {spark_submit.py:495} INFO - 
[2023-01-31T13:06:51.554+0000] {spark_submit.py:495} INFO - 
[2023-01-31T13:06:52.080+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:52 INFO CodecPool: Got brand-new compressor [.snappy]
[2023-01-31T13:06:56.631+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:56 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675170276333-b447705c-2eab-43e4-b856-492fc35b0eeb/_temporary/0/_temporary/' directory.
[2023-01-31T13:06:56.631+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:56 INFO FileOutputCommitter: Saved output of task 'attempt_202301311306507982672693499282250_0008_m_000000_8' to gs://entsoe_temp_1009/.spark-bigquery-local-1675170276333-b447705c-2eab-43e4-b856-492fc35b0eeb/_temporary/0/task_202301311306507982672693499282250_0008_m_000000
[2023-01-31T13:06:56.632+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:56 INFO SparkHadoopMapRedUtil: attempt_202301311306507982672693499282250_0008_m_000000_8: Committed. Elapsed time: 1611 ms.
[2023-01-31T13:06:56.648+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:56 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2588 bytes result sent to driver
[2023-01-31T13:06:56.650+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:56 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 5887 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T13:06:56.651+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:56 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2023-01-31T13:06:56.654+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:56 INFO DAGScheduler: ResultStage 8 (save at BigQueryWriteHelper.java:105) finished in 6.114 s
[2023-01-31T13:06:56.654+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:56 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T13:06:56.654+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2023-01-31T13:06:56.654+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:56 INFO DAGScheduler: Job 8 finished: save at BigQueryWriteHelper.java:105, took 6.124829 s
[2023-01-31T13:06:56.655+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:56 INFO FileFormatWriter: Start to commit write Job 14c06971-6c49-445b-9ace-fce2abe2c2fe.
[2023-01-31T13:06:57.798+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:57 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675170276333-b447705c-2eab-43e4-b856-492fc35b0eeb/_temporary/0/task_202301311306507982672693499282250_0008_m_000000/' directory.
[2023-01-31T13:06:58.096+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:58 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675170276333-b447705c-2eab-43e4-b856-492fc35b0eeb/' directory.
[2023-01-31T13:06:58.249+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:58 INFO BlockManagerInfo: Removed broadcast_16_piece0 on b37fe3cbf330:41543 in memory (size: 74.3 KiB, free: 434.3 MiB)
[2023-01-31T13:06:58.328+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:58 INFO BlockManagerInfo: Removed broadcast_8_piece0 on b37fe3cbf330:41543 in memory (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T13:06:58.860+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:58 INFO FileFormatWriter: Write Job 14c06971-6c49-445b-9ace-fce2abe2c2fe committed. Elapsed time: 2203 ms.
[2023-01-31T13:06:58.863+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:58 INFO FileFormatWriter: Finished processing stats for write job 14c06971-6c49-445b-9ace-fce2abe2c2fe.
[2023-01-31T13:07:00.866+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:00 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=entsoe_analytics_1009, projectId=rafzul-analytics-1009, tableId=TEST_total_generation_staging}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=null, ignoreUnknownValue=null, sourceUris=[gs://entsoe_temp_1009/.spark-bigquery-local-1675170276333-b447705c-2eab-43e4-b856-492fc35b0eeb/part-00000-d658df12-be8f-4c4d-a940-1d3f0a576451-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=true, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=rafzul-analytics-1009, job=922eaaa0-dc35-4a57-aa05-a208751a0cc1, location=US}
[2023-01-31T13:07:06.025+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:06 INFO BigQueryClient: Done loading to rafzul-analytics-1009.entsoe_analytics_1009.TEST_total_generation_staging. jobId: JobId{project=rafzul-analytics-1009, job=922eaaa0-dc35-4a57-aa05-a208751a0cc1, location=US}
[2023-01-31T13:07:06.995+0000] {spark_submit.py:495} INFO - MANTEP!!!!
[2023-01-31T13:07:07.510+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:07 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-31T13:07:07.587+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:07 INFO SparkUI: Stopped Spark web UI at http://b37fe3cbf330:4040
[2023-01-31T13:07:07.680+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:07 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-31T13:07:07.916+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:07 INFO MemoryStore: MemoryStore cleared
[2023-01-31T13:07:07.930+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:07 INFO BlockManager: BlockManager stopped
[2023-01-31T13:07:07.954+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:07 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-31T13:07:07.980+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:07 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-31T13:07:08.126+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:08 INFO SparkContext: Successfully stopped SparkContext
[2023-01-31T13:07:08.134+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:08 INFO ShutdownHookManager: Shutdown hook called
[2023-01-31T13:07:08.157+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:08 INFO ShutdownHookManager: Deleting directory /tmp/spark-abcabe63-9517-496b-a92c-1a4af2224a91
[2023-01-31T13:07:08.204+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:08 INFO ShutdownHookManager: Deleting directory /tmp/spark-e1437171-d55c-4105-9c19-ebea9dd1a587
[2023-01-31T13:07:08.228+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:08 INFO ShutdownHookManager: Deleting directory /tmp/spark-e1437171-d55c-4105-9c19-ebea9dd1a587/pyspark-0b0b4dc1-de45-4065-bb8b-062f61b6598f
[2023-01-31T13:07:08.587+0000] {taskinstance.py:1322} INFO - Marking task as SUCCESS. dag_id=entsoe-energydata-backfill, task_id=stage_total_generation, execution_date=20210101T060000, start_date=20230131T130343, end_date=20230131T130708
[2023-01-31T13:07:08.669+0000] {local_task_job.py:159} INFO - Task exited with return code 0
[2023-01-31T13:07:08.725+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T14:09:44.043+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [queued]>
[2023-01-31T14:09:44.072+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [queued]>
[2023-01-31T14:09:44.074+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T14:09:44.075+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T14:09:44.076+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T14:09:44.125+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 06:00:00+00:00
[2023-01-31T14:09:44.141+0000] {standard_task_runner.py:55} INFO - Started process 6107 to run task
[2023-01-31T14:09:44.172+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T06:00:00+00:00', '--job-id', '1043', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpdd_xhif8']
[2023-01-31T14:09:44.186+0000] {standard_task_runner.py:83} INFO - Job 1043: Subtask stage_total_generation
[2023-01-31T14:09:44.429+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [running]> on host b37fe3cbf330
[2023-01-31T14:09:44.832+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T06:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T06:00:00+00:00
[2023-01-31T14:09:44.887+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T14:09:44.893+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 398, in submit
    spark_submit_cmd = self._build_spark_submit_command(application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 344, in _build_spark_submit_command
    self.log.info("Spark-Submit cmd: %s", self._mask_cmd(connection_cmd))
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 256, in _mask_cmd
    " ".join(connection_cmd),
TypeError: sequence item 10: expected str instance, DateTime found
[2023-01-31T14:09:44.913+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=entsoe-energydata-backfill, task_id=stage_total_generation, execution_date=20210101T060000, start_date=20230131T140944, end_date=20230131T140944
[2023-01-31T14:09:44.978+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 1043 for task stage_total_generation (sequence item 10: expected str instance, DateTime found; 6107)
[2023-01-31T14:09:45.031+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-31T14:09:45.176+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T14:10:53.494+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [queued]>
[2023-01-31T14:10:53.553+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [queued]>
[2023-01-31T14:10:53.554+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T14:10:53.554+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T14:10:53.555+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T14:10:53.624+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 06:00:00+00:00
[2023-01-31T14:10:53.654+0000] {standard_task_runner.py:55} INFO - Started process 6210 to run task
[2023-01-31T14:10:53.672+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T06:00:00+00:00', '--job-id', '1056', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp_rxnxc4b']
[2023-01-31T14:10:53.684+0000] {standard_task_runner.py:83} INFO - Job 1056: Subtask stage_total_generation
[2023-01-31T14:10:54.040+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [running]> on host b37fe3cbf330
[2023-01-31T14:10:54.429+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T06:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T06:00:00+00:00
[2023-01-31T14:10:54.479+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T14:10:54.487+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 398, in submit
    spark_submit_cmd = self._build_spark_submit_command(application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 344, in _build_spark_submit_command
    self.log.info("Spark-Submit cmd: %s", self._mask_cmd(connection_cmd))
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 256, in _mask_cmd
    " ".join(connection_cmd),
TypeError: sequence item 10: expected str instance, DateTime found
[2023-01-31T14:10:54.529+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=entsoe-energydata-backfill, task_id=stage_total_generation, execution_date=20210101T060000, start_date=20230131T141053, end_date=20230131T141054
[2023-01-31T14:10:54.615+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 1056 for task stage_total_generation (sequence item 10: expected str instance, DateTime found; 6210)
[2023-01-31T14:10:54.680+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-31T14:10:54.771+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T14:28:10.165+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [queued]>
[2023-01-31T14:28:10.234+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [queued]>
[2023-01-31T14:28:10.246+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T14:28:10.247+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T14:28:10.247+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T14:28:10.435+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 06:00:00+00:00
[2023-01-31T14:28:10.481+0000] {standard_task_runner.py:55} INFO - Started process 9421 to run task
[2023-01-31T14:28:10.496+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T06:00:00+00:00', '--job-id', '1078', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp9qi3pmv1']
[2023-01-31T14:28:10.507+0000] {standard_task_runner.py:83} INFO - Job 1078: Subtask stage_total_generation
[2023-01-31T14:28:11.124+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [running]> on host b37fe3cbf330
[2023-01-31T14:28:11.621+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T06:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T06:00:00+00:00
[2023-01-31T14:28:11.671+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T14:28:11.690+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010600 202101010700 DE_TENNET 2021-01-01 00:00:00 2021-01-01 06:00:00
[2023-01-31T14:28:37.084+0000] {spark_submit.py:495} INFO - mulai periode: 2021-01-01 00:00:00
[2023-01-31T14:28:37.085+0000] {spark_submit.py:495} INFO - selesai periode: 2021-01-01 06:00:00
[2023-01-31T14:28:37.321+0000] {spark_submit.py:495} INFO - 23/01/31 14:28:37 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T14:28:37.671+0000] {spark_submit.py:495} INFO - 23/01/31 14:28:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T14:28:38.199+0000] {spark_submit.py:495} INFO - 23/01/31 14:28:38 INFO ResourceUtils: ==============================================================
[2023-01-31T14:28:38.203+0000] {spark_submit.py:495} INFO - 23/01/31 14:28:38 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T14:28:38.204+0000] {spark_submit.py:495} INFO - 23/01/31 14:28:38 INFO ResourceUtils: ==============================================================
[2023-01-31T14:28:38.211+0000] {spark_submit.py:495} INFO - 23/01/31 14:28:38 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T14:28:38.333+0000] {spark_submit.py:495} INFO - 23/01/31 14:28:38 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T14:28:38.347+0000] {spark_submit.py:495} INFO - 23/01/31 14:28:38 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T14:28:38.349+0000] {spark_submit.py:495} INFO - 23/01/31 14:28:38 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T14:28:38.514+0000] {spark_submit.py:495} INFO - 23/01/31 14:28:38 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T14:28:38.519+0000] {spark_submit.py:495} INFO - 23/01/31 14:28:38 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T14:28:38.524+0000] {spark_submit.py:495} INFO - 23/01/31 14:28:38 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T14:28:38.526+0000] {spark_submit.py:495} INFO - 23/01/31 14:28:38 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T14:28:38.536+0000] {spark_submit.py:495} INFO - 23/01/31 14:28:38 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T14:28:40.509+0000] {spark_submit.py:495} INFO - 23/01/31 14:28:40 INFO Utils: Successfully started service 'sparkDriver' on port 38793.
[2023-01-31T14:28:40.799+0000] {spark_submit.py:495} INFO - 23/01/31 14:28:40 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T14:28:41.057+0000] {spark_submit.py:495} INFO - 23/01/31 14:28:41 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T14:28:41.328+0000] {spark_submit.py:495} INFO - 23/01/31 14:28:41 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T14:28:41.336+0000] {spark_submit.py:495} INFO - 23/01/31 14:28:41 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T14:28:41.383+0000] {spark_submit.py:495} INFO - 23/01/31 14:28:41 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T14:28:41.716+0000] {spark_submit.py:495} INFO - 23/01/31 14:28:41 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1cc19125-541d-44ba-8f01-8100509ded6a
[2023-01-31T14:28:41.942+0000] {spark_submit.py:495} INFO - 23/01/31 14:28:41 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T14:28:42.176+0000] {spark_submit.py:495} INFO - 23/01/31 14:28:42 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T14:28:43.789+0000] {spark_submit.py:495} INFO - 23/01/31 14:28:43 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T14:28:43.801+0000] {spark_submit.py:495} INFO - 23/01/31 14:28:43 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2023-01-31T14:28:43.802+0000] {spark_submit.py:495} INFO - 23/01/31 14:28:43 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[2023-01-31T14:28:43.803+0000] {spark_submit.py:495} INFO - 23/01/31 14:28:43 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[2023-01-31T14:28:43.908+0000] {spark_submit.py:495} INFO - 23/01/31 14:28:43 INFO Utils: Successfully started service 'SparkUI' on port 4044.
[2023-01-31T14:28:44.183+0000] {spark_submit.py:495} INFO - 23/01/31 14:28:44 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://b37fe3cbf330:38793/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675175317302
[2023-01-31T14:28:44.184+0000] {spark_submit.py:495} INFO - 23/01/31 14:28:44 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://b37fe3cbf330:38793/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675175317302
[2023-01-31T14:28:44.494+0000] {spark_submit.py:495} INFO - 23/01/31 14:28:44 INFO Executor: Starting executor ID driver on host b37fe3cbf330
[2023-01-31T14:28:44.526+0000] {spark_submit.py:495} INFO - 23/01/31 14:28:44 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T14:28:44.584+0000] {spark_submit.py:495} INFO - 23/01/31 14:28:44 INFO Executor: Fetching spark://b37fe3cbf330:38793/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675175317302
[2023-01-31T14:28:44.917+0000] {spark_submit.py:495} INFO - 23/01/31 14:28:44 INFO TransportClientFactory: Successfully created connection to b37fe3cbf330/172.19.0.8:38793 after 147 ms (0 ms spent in bootstraps)
[2023-01-31T14:28:44.976+0000] {spark_submit.py:495} INFO - 23/01/31 14:28:44 INFO Utils: Fetching spark://b37fe3cbf330:38793/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-95bacd20-a0ef-43f9-89f1-b564aa4dcb50/userFiles-0c508006-5418-4aab-96f5-db0716ebbfed/fetchFileTemp826088932954013610.tmp
[2023-01-31T14:28:45.784+0000] {spark_submit.py:495} INFO - 23/01/31 14:28:45 INFO Executor: Adding file:/tmp/spark-95bacd20-a0ef-43f9-89f1-b564aa4dcb50/userFiles-0c508006-5418-4aab-96f5-db0716ebbfed/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T14:28:45.786+0000] {spark_submit.py:495} INFO - 23/01/31 14:28:45 INFO Executor: Fetching spark://b37fe3cbf330:38793/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675175317302
[2023-01-31T14:28:45.798+0000] {spark_submit.py:495} INFO - 23/01/31 14:28:45 INFO Utils: Fetching spark://b37fe3cbf330:38793/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-95bacd20-a0ef-43f9-89f1-b564aa4dcb50/userFiles-0c508006-5418-4aab-96f5-db0716ebbfed/fetchFileTemp13934949908706375810.tmp
[2023-01-31T14:28:46.446+0000] {spark_submit.py:495} INFO - 23/01/31 14:28:46 INFO Executor: Adding file:/tmp/spark-95bacd20-a0ef-43f9-89f1-b564aa4dcb50/userFiles-0c508006-5418-4aab-96f5-db0716ebbfed/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T14:28:46.518+0000] {spark_submit.py:495} INFO - 23/01/31 14:28:46 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38267.
[2023-01-31T14:28:46.521+0000] {spark_submit.py:495} INFO - 23/01/31 14:28:46 INFO NettyBlockTransferService: Server created on b37fe3cbf330:38267
[2023-01-31T14:28:46.526+0000] {spark_submit.py:495} INFO - 23/01/31 14:28:46 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T14:28:46.592+0000] {spark_submit.py:495} INFO - 23/01/31 14:28:46 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None)
[2023-01-31T14:28:46.625+0000] {spark_submit.py:495} INFO - 23/01/31 14:28:46 INFO BlockManagerMasterEndpoint: Registering block manager b37fe3cbf330:38267 with 434.4 MiB RAM, BlockManagerId(driver, b37fe3cbf330, 38267, None)
[2023-01-31T14:28:46.663+0000] {spark_submit.py:495} INFO - 23/01/31 14:28:46 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None)
[2023-01-31T14:28:46.695+0000] {spark_submit.py:495} INFO - 23/01/31 14:28:46 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, b37fe3cbf330, 38267, None)
[2023-01-31T14:28:49.985+0000] {spark_submit.py:495} INFO - 23/01/31 14:28:49 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T14:28:50.014+0000] {spark_submit.py:495} INFO - 23/01/31 14:28:50 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T14:29:04.468+0000] {spark_submit.py:495} INFO - 23/01/31 14:29:04 INFO InMemoryFileIndex: It took 1072 ms to list leaf files for 1 paths.
[2023-01-31T14:29:10.175+0000] {spark_submit.py:495} INFO - 23/01/31 14:29:10 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T14:29:11.411+0000] {spark_submit.py:495} INFO - 23/01/31 14:29:11 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T14:29:11.455+0000] {spark_submit.py:495} INFO - 23/01/31 14:29:11 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on b37fe3cbf330:38267 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T14:29:11.516+0000] {spark_submit.py:495} INFO - 23/01/31 14:29:11 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T14:29:16.347+0000] {spark_submit.py:495} INFO - 23/01/31 14:29:16 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T14:29:16.432+0000] {spark_submit.py:495} INFO - 23/01/31 14:29:16 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T14:29:16.532+0000] {spark_submit.py:495} INFO - 23/01/31 14:29:16 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T14:29:16.725+0000] {spark_submit.py:495} INFO - 23/01/31 14:29:16 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T14:29:16.733+0000] {spark_submit.py:495} INFO - 23/01/31 14:29:16 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T14:29:16.734+0000] {spark_submit.py:495} INFO - 23/01/31 14:29:16 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T14:29:16.785+0000] {spark_submit.py:495} INFO - 23/01/31 14:29:16 INFO DAGScheduler: Missing parents: List()
[2023-01-31T14:29:16.987+0000] {spark_submit.py:495} INFO - 23/01/31 14:29:16 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T14:29:17.875+0000] {spark_submit.py:495} INFO - 23/01/31 14:29:17 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T14:29:17.918+0000] {spark_submit.py:495} INFO - 23/01/31 14:29:17 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T14:29:17.919+0000] {spark_submit.py:495} INFO - 23/01/31 14:29:17 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on b37fe3cbf330:38267 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T14:29:17.942+0000] {spark_submit.py:495} INFO - 23/01/31 14:29:17 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T14:29:18.283+0000] {spark_submit.py:495} INFO - 23/01/31 14:29:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T14:29:18.292+0000] {spark_submit.py:495} INFO - 23/01/31 14:29:18 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T14:29:19.174+0000] {spark_submit.py:495} INFO - 23/01/31 14:29:19 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T14:29:19.475+0000] {spark_submit.py:495} INFO - 23/01/31 14:29:19 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T14:29:20.384+0000] {spark_submit.py:495} INFO - 23/01/31 14:29:20 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010600__202101010700.json:0+9371
[2023-01-31T14:29:23.198+0000] {spark_submit.py:495} INFO - 23/01/31 14:29:23 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3437 bytes result sent to driver
[2023-01-31T14:29:23.326+0000] {spark_submit.py:495} INFO - 23/01/31 14:29:23 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 4418 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T14:29:23.362+0000] {spark_submit.py:495} INFO - 23/01/31 14:29:23 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T14:29:23.460+0000] {spark_submit.py:495} INFO - 23/01/31 14:29:23 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 6.084 s
[2023-01-31T14:29:23.529+0000] {spark_submit.py:495} INFO - 23/01/31 14:29:23 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T14:29:23.536+0000] {spark_submit.py:495} INFO - 23/01/31 14:29:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T14:29:23.555+0000] {spark_submit.py:495} INFO - 23/01/31 14:29:23 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 7.022815 s
[2023-01-31T14:29:25.011+0000] {spark_submit.py:495} INFO - 23/01/31 14:29:25 INFO BlockManagerInfo: Removed broadcast_1_piece0 on b37fe3cbf330:38267 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T14:29:53.538+0000] {spark_submit.py:495} INFO - 23/01/31 14:29:53 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T14:29:53.543+0000] {spark_submit.py:495} INFO - 23/01/31 14:29:53 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T14:29:53.549+0000] {spark_submit.py:495} INFO - 23/01/31 14:29:53 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T14:29:56.547+0000] {spark_submit.py:495} INFO - 23/01/31 14:29:56 INFO CodeGenerator: Code generated in 1928.048475 ms
[2023-01-31T14:29:56.583+0000] {spark_submit.py:495} INFO - 23/01/31 14:29:56 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T14:29:56.852+0000] {spark_submit.py:495} INFO - 23/01/31 14:29:56 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T14:29:56.861+0000] {spark_submit.py:495} INFO - 23/01/31 14:29:56 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on b37fe3cbf330:38267 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T14:29:56.870+0000] {spark_submit.py:495} INFO - 23/01/31 14:29:56 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T14:29:56.981+0000] {spark_submit.py:495} INFO - 23/01/31 14:29:56 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T14:29:58.669+0000] {spark_submit.py:495} INFO - 23/01/31 14:29:58 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T14:29:58.670+0000] {spark_submit.py:495} INFO - 23/01/31 14:29:58 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T14:29:58.670+0000] {spark_submit.py:495} INFO - 23/01/31 14:29:58 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T14:29:58.670+0000] {spark_submit.py:495} INFO - 23/01/31 14:29:58 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T14:29:58.671+0000] {spark_submit.py:495} INFO - 23/01/31 14:29:58 INFO DAGScheduler: Missing parents: List()
[2023-01-31T14:29:58.671+0000] {spark_submit.py:495} INFO - 23/01/31 14:29:58 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T14:29:58.785+0000] {spark_submit.py:495} INFO - 23/01/31 14:29:58 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T14:29:58.861+0000] {spark_submit.py:495} INFO - 23/01/31 14:29:58 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T14:29:58.906+0000] {spark_submit.py:495} INFO - 23/01/31 14:29:58 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on b37fe3cbf330:38267 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T14:29:58.910+0000] {spark_submit.py:495} INFO - 23/01/31 14:29:58 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T14:29:58.914+0000] {spark_submit.py:495} INFO - 23/01/31 14:29:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T14:29:58.929+0000] {spark_submit.py:495} INFO - 23/01/31 14:29:58 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T14:29:59.146+0000] {spark_submit.py:495} INFO - 23/01/31 14:29:59 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T14:29:59.172+0000] {spark_submit.py:495} INFO - 23/01/31 14:29:59 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T14:30:01.088+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:01 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T14:30:03.929+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:03 INFO CodeGenerator: Code generated in 2682.505783 ms
[2023-01-31T14:30:04.555+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:04 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1628 bytes result sent to driver
[2023-01-31T14:30:04.586+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:04 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 5626 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T14:30:04.596+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:04 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 5.848 s
[2023-01-31T14:30:04.597+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:04 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T14:30:04.616+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:04 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T14:30:04.618+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T14:30:04.619+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:04 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 5.961000 s
[2023-01-31T14:30:05.112+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:05 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T14:30:05.112+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:05 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T14:30:05.115+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:05 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T14:30:05.474+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:05 INFO CodeGenerator: Code generated in 204.702783 ms
[2023-01-31T14:30:05.552+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:05 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T14:30:05.646+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:05 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T14:30:05.648+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:05 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on b37fe3cbf330:38267 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T14:30:05.651+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:05 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T14:30:05.656+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:05 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T14:30:06.061+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:06 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T14:30:06.070+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:06 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T14:30:06.072+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:06 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T14:30:06.073+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:06 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T14:30:06.074+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:06 INFO DAGScheduler: Missing parents: List()
[2023-01-31T14:30:06.083+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:06 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T14:30:06.099+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:06 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.7 MiB)
[2023-01-31T14:30:06.123+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:06 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.7 MiB)
[2023-01-31T14:30:06.127+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:06 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on b37fe3cbf330:38267 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T14:30:06.132+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:06 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T14:30:06.137+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T14:30:06.139+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:06 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T14:30:06.149+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:06 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T14:30:06.151+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:06 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T14:30:06.209+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:06 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T14:30:06.530+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:06 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1628 bytes result sent to driver
[2023-01-31T14:30:06.560+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:06 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 413 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T14:30:06.568+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:06 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 0.479 s
[2023-01-31T14:30:06.570+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:06 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T14:30:06.574+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:06 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T14:30:06.576+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T14:30:06.582+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:06 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 0.512301 s
[2023-01-31T14:30:07.704+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:07 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T14:30:07.715+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:07 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T14:30:07.717+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:07 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T14:30:09.621+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:09 INFO BlockManagerInfo: Removed broadcast_5_piece0 on b37fe3cbf330:38267 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T14:30:09.890+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:09 INFO BlockManagerInfo: Removed broadcast_3_piece0 on b37fe3cbf330:38267 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T14:30:11.286+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:11 INFO CodeGenerator: Code generated in 1878.045493 ms
[2023-01-31T14:30:11.312+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:11 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.5 MiB)
[2023-01-31T14:30:11.505+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:11 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.5 MiB)
[2023-01-31T14:30:11.512+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:11 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on b37fe3cbf330:38267 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T14:30:11.513+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:11 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T14:30:11.530+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:11 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T14:30:11.919+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:11 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T14:30:11.922+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:11 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T14:30:11.923+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:11 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T14:30:11.923+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:11 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T14:30:11.924+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:11 INFO DAGScheduler: Missing parents: List()
[2023-01-31T14:30:11.939+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:11 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T14:30:11.985+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:11 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.4 MiB)
[2023-01-31T14:30:12.036+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:12 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.4 MiB)
[2023-01-31T14:30:12.040+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:12 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on b37fe3cbf330:38267 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T14:30:12.080+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:12 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T14:30:12.089+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T14:30:12.090+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:12 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T14:30:12.094+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:12 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T14:30:12.098+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:12 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T14:30:12.162+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:12 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T14:30:12.580+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:12 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-01-31T14:30:12.607+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:12 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 508 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T14:30:12.617+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:12 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T14:30:12.618+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:12 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 0.664 s
[2023-01-31T14:30:12.622+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:12 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T14:30:12.646+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T14:30:12.647+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:12 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 0.703765 s
[2023-01-31T14:30:13.042+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:13 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T14:30:13.047+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:13 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T14:30:13.050+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:13 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T14:30:13.259+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:13 INFO CodeGenerator: Code generated in 76.396919 ms
[2023-01-31T14:30:13.304+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:13 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.2 MiB)
[2023-01-31T14:30:13.405+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:13 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T14:30:13.412+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:13 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on b37fe3cbf330:38267 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T14:30:13.414+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:13 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T14:30:13.420+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:13 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T14:30:13.501+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:13 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T14:30:13.505+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:13 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T14:30:13.506+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:13 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T14:30:13.507+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:13 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T14:30:13.507+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:13 INFO DAGScheduler: Missing parents: List()
[2023-01-31T14:30:13.518+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:13 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T14:30:13.537+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:13 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.2 MiB)
[2023-01-31T14:30:13.551+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:13 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.2 MiB)
[2023-01-31T14:30:13.554+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:13 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on b37fe3cbf330:38267 (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T14:30:13.559+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:13 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T14:30:13.564+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T14:30:13.565+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:13 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T14:30:13.572+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:13 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T14:30:13.572+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:13 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T14:30:13.645+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:13 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T14:30:14.040+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:14 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1586 bytes result sent to driver
[2023-01-31T14:30:14.058+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:14 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 480 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T14:30:14.059+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:14 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 0.540 s
[2023-01-31T14:30:14.060+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:14 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T14:30:14.062+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:14 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T14:30:14.064+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T14:30:14.068+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:14 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 0.562367 s
[2023-01-31T14:30:16.002+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:16 INFO BlockManagerInfo: Removed broadcast_9_piece0 on b37fe3cbf330:38267 in memory (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T14:30:16.156+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:16 INFO BlockManagerInfo: Removed broadcast_7_piece0 on b37fe3cbf330:38267 in memory (size: 12.7 KiB, free: 434.2 MiB)
[2023-01-31T14:30:16.777+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:16 INFO BlockManagerInfo: Removed broadcast_0_piece0 on b37fe3cbf330:38267 in memory (size: 34.6 KiB, free: 434.3 MiB)
[2023-01-31T14:30:17.613+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:17 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T14:30:17.614+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:17 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T14:30:17.626+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:17 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T14:30:18.992+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:18 INFO CodeGenerator: Code generated in 976.01096 ms
[2023-01-31T14:30:19.076+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:19 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.3 MiB)
[2023-01-31T14:30:19.285+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:19 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T14:30:19.287+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:19 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on b37fe3cbf330:38267 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T14:30:19.307+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:19 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T14:30:19.323+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:19 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T14:30:19.650+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:19 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T14:30:19.651+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:19 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T14:30:19.666+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:19 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T14:30:19.667+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:19 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T14:30:19.667+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:19 INFO DAGScheduler: Missing parents: List()
[2023-01-31T14:30:19.668+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:19 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T14:30:19.699+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:19 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.2 MiB)
[2023-01-31T14:30:19.718+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:19 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.2 MiB)
[2023-01-31T14:30:19.723+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:19 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on b37fe3cbf330:38267 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T14:30:19.726+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:19 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T14:30:19.729+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T14:30:19.745+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:19 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T14:30:19.754+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:19 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T14:30:19.755+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:19 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T14:30:19.780+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:19 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T14:30:20.822+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:20 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2499 bytes result sent to driver
[2023-01-31T14:30:20.842+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:20 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 1073 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T14:30:20.844+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:20 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 1.166 s
[2023-01-31T14:30:20.850+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:20 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T14:30:20.852+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:20 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T14:30:20.856+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T14:30:20.859+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:20 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 1.214489 s
[2023-01-31T14:30:23.147+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:23 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T14:30:23.150+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:23 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T14:30:23.151+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:23 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T14:30:23.644+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:23 INFO CodeGenerator: Code generated in 346.255808 ms
[2023-01-31T14:30:23.701+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:23 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 433.0 MiB)
[2023-01-31T14:30:23.927+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:23 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.0 MiB)
[2023-01-31T14:30:23.931+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:23 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on b37fe3cbf330:38267 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T14:30:23.938+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:23 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T14:30:23.965+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:23 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T14:30:24.678+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:24 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T14:30:24.680+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:24 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T14:30:24.680+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:24 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T14:30:24.681+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:24 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T14:30:24.681+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:24 INFO DAGScheduler: Missing parents: List()
[2023-01-31T14:30:24.695+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:24 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T14:30:24.748+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:24 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 432.9 MiB)
[2023-01-31T14:30:24.857+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:24 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.9 MiB)
[2023-01-31T14:30:24.860+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:24 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on b37fe3cbf330:38267 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T14:30:24.884+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:24 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T14:30:24.904+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T14:30:24.905+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:24 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T14:30:24.940+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:24 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T14:30:24.950+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:24 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T14:30:25.008+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:25 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T14:30:25.886+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:25 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-01-31T14:30:25.887+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:25 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 948 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T14:30:25.889+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:25 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 1.199 s
[2023-01-31T14:30:25.889+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:25 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T14:30:25.892+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:25 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T14:30:25.893+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T14:30:25.898+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:25 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 1.220047 s
[2023-01-31T14:30:26.131+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:26 INFO BlockManagerInfo: Removed broadcast_6_piece0 on b37fe3cbf330:38267 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T14:30:26.180+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:26 INFO BlockManagerInfo: Removed broadcast_8_piece0 on b37fe3cbf330:38267 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T14:30:26.240+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:26 INFO BlockManagerInfo: Removed broadcast_13_piece0 on b37fe3cbf330:38267 in memory (size: 12.6 KiB, free: 434.3 MiB)
[2023-01-31T14:30:26.294+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:26 INFO BlockManagerInfo: Removed broadcast_11_piece0 on b37fe3cbf330:38267 in memory (size: 13.4 KiB, free: 434.3 MiB)
[2023-01-31T14:30:26.425+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:26 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T14:30:26.425+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:26 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T14:30:26.428+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:26 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T14:30:26.454+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:26 INFO BlockManagerInfo: Removed broadcast_10_piece0 on b37fe3cbf330:38267 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T14:30:27.285+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:27 INFO CodeGenerator: Code generated in 274.710628 ms
[2023-01-31T14:30:27.349+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:27 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 433.5 MiB)
[2023-01-31T14:30:28.154+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:28 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.5 MiB)
[2023-01-31T14:30:28.164+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:28 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on b37fe3cbf330:38267 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T14:30:28.169+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:28 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:204
[2023-01-31T14:30:28.179+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:28 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T14:30:28.358+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:28 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:204
[2023-01-31T14:30:28.382+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:28 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:204) with 1 output partitions
[2023-01-31T14:30:28.383+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:28 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:204)
[2023-01-31T14:30:28.383+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:28 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T14:30:28.384+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:28 INFO DAGScheduler: Missing parents: List()
[2023-01-31T14:30:28.482+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:28 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:204), which has no missing parents
[2023-01-31T14:30:28.605+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:28 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 433.4 MiB)
[2023-01-31T14:30:28.615+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:28 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 433.4 MiB)
[2023-01-31T14:30:28.617+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:28 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on b37fe3cbf330:38267 (size: 12.6 KiB, free: 434.3 MiB)
[2023-01-31T14:30:28.619+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:28 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T14:30:28.628+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:204) (first 15 tasks are for partitions Vector(0))
[2023-01-31T14:30:28.629+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:28 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T14:30:28.633+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:28 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T14:30:28.634+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:28 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T14:30:28.722+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:28 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T14:30:29.008+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:29 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1729 bytes result sent to driver
[2023-01-31T14:30:29.040+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:29 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 401 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T14:30:29.066+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:29 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:204) finished in 0.449 s
[2023-01-31T14:30:29.067+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:29 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T14:30:29.067+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:29 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T14:30:29.068+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T14:30:29.083+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:29 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:204, took 0.713022 s
[2023-01-31T14:30:36.699+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:36 INFO BlockManagerInfo: Removed broadcast_15_piece0 on b37fe3cbf330:38267 in memory (size: 12.6 KiB, free: 434.3 MiB)
[2023-01-31T14:30:36.769+0000] {spark_submit.py:495} INFO - 23/01/31 14:30:36 INFO BlockManagerInfo: Removed broadcast_14_piece0 on b37fe3cbf330:38267 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T14:34:45.316+0000] {base_job.py:232} ERROR - LocalTaskJob heartbeat got an exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3361, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 325, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 888, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 491, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 271, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 386, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 684, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 210, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 680, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.19.0.2), port 5432 failed: server closed the connection unexpectedly
	This probably means the server terminated abnormally
	before or while processing the request.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/jobs/base_job.py", line 228, in heartbeat
    self.heartbeat_callback(session=session)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/jobs/local_task_job.py", line 178, in heartbeat_callback
    self.task_instance.refresh_from_db()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 795, in refresh_from_db
    ti = qry.one_or_none()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2849, in one_or_none
    return self._iter().one_or_none()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2915, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1552, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3315, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3394, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3364, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2198, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 210, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3361, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 325, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 888, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 491, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 271, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 386, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 684, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 210, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 680, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.19.0.2), port 5432 failed: server closed the connection unexpectedly
	This probably means the server terminated abnormally
	before or while processing the request.

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-01-31T14:36:00.379+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:00 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T14:36:00.680+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T14:36:00.686+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T14:36:00.727+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:00 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T14:36:00.730+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T14:36:00.731+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T14:36:00.731+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T14:36:02.771+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:02 INFO CodeGenerator: Code generated in 95.123294 ms
[2023-01-31T14:36:02.888+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:02 INFO DAGScheduler: Registering RDD 118 (save at BigQueryWriteHelper.java:105) as input to shuffle 0
[2023-01-31T14:36:02.910+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:02 INFO DAGScheduler: Got map stage job 8 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T14:36:02.915+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:02 INFO DAGScheduler: Final stage: ShuffleMapStage 8 (save at BigQueryWriteHelper.java:105)
[2023-01-31T14:36:02.916+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:02 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T14:36:02.924+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:02 INFO DAGScheduler: Missing parents: List()
[2023-01-31T14:36:02.932+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:02 INFO DAGScheduler: Submitting ShuffleMapStage 8 (MapPartitionsRDD[118] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T14:36:02.988+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:02 INFO CodeGenerator: Code generated in 78.636322 ms
[2023-01-31T14:36:03.145+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO CodeGenerator: Code generated in 91.571304 ms
[2023-01-31T14:36:03.214+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 11.1 KiB, free 433.7 MiB)
[2023-01-31T14:36:03.232+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 5.7 KiB, free 433.7 MiB)
[2023-01-31T14:36:03.239+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on b37fe3cbf330:38267 (size: 5.7 KiB, free: 434.3 MiB)
[2023-01-31T14:36:03.240+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513
[2023-01-31T14:36:03.259+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[118] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T14:36:03.260+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2023-01-31T14:36:03.267+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO CodeGenerator: Code generated in 62.615313 ms
[2023-01-31T14:36:03.276+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO DAGScheduler: Registering RDD 120 (save at BigQueryWriteHelper.java:105) as input to shuffle 1
[2023-01-31T14:36:03.279+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO DAGScheduler: Got map stage job 9 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T14:36:03.283+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7660 bytes) taskResourceAssignments Map()
[2023-01-31T14:36:03.283+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO DAGScheduler: Final stage: ShuffleMapStage 9 (save at BigQueryWriteHelper.java:105)
[2023-01-31T14:36:03.283+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T14:36:03.284+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO DAGScheduler: Missing parents: List()
[2023-01-31T14:36:03.295+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2023-01-31T14:36:03.299+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[120] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T14:36:03.378+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO CodeGenerator: Code generated in 41.199168 ms
[2023-01-31T14:36:03.474+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO CodeGenerator: Code generated in 61.315206 ms
[2023-01-31T14:36:03.512+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 15.6 KiB, free 433.7 MiB)
[2023-01-31T14:36:03.528+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.7 MiB)
[2023-01-31T14:36:03.531+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on b37fe3cbf330:38267 (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T14:36:03.531+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1513
[2023-01-31T14:36:03.532+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[120] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T14:36:03.532+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
[2023-01-31T14:36:03.540+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO DAGScheduler: Registering RDD 122 (save at BigQueryWriteHelper.java:105) as input to shuffle 2
[2023-01-31T14:36:03.547+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO DAGScheduler: Got map stage job 10 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T14:36:03.548+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO DAGScheduler: Final stage: ShuffleMapStage 10 (save at BigQueryWriteHelper.java:105)
[2023-01-31T14:36:03.548+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T14:36:03.551+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO DAGScheduler: Missing parents: List()
[2023-01-31T14:36:03.556+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO DAGScheduler: Submitting ShuffleMapStage 10 (MapPartitionsRDD[122] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T14:36:03.557+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO CodeGenerator: Code generated in 111.55986 ms
[2023-01-31T14:36:03.597+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO CodeGenerator: Code generated in 80.626558 ms
[2023-01-31T14:36:03.600+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 15.7 KiB, free 433.7 MiB)
[2023-01-31T14:36:03.629+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.6 MiB)
[2023-01-31T14:36:03.630+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on b37fe3cbf330:38267 (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T14:36:03.632+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1513
[2023-01-31T14:36:03.639+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[122] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T14:36:03.640+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0
[2023-01-31T14:36:03.651+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO DAGScheduler: Registering RDD 124 (save at BigQueryWriteHelper.java:105) as input to shuffle 3
[2023-01-31T14:36:03.652+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO DAGScheduler: Got map stage job 11 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T14:36:03.652+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO DAGScheduler: Final stage: ShuffleMapStage 11 (save at BigQueryWriteHelper.java:105)
[2023-01-31T14:36:03.652+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T14:36:03.658+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO DAGScheduler: Missing parents: List()
[2023-01-31T14:36:03.663+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[124] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T14:36:03.685+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 15.7 KiB, free 433.6 MiB)
[2023-01-31T14:36:03.690+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.6 MiB)
[2023-01-31T14:36:03.693+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on b37fe3cbf330:38267 (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T14:36:03.696+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1513
[2023-01-31T14:36:03.701+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[124] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T14:36:03.702+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
[2023-01-31T14:36:03.708+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO DAGScheduler: Registering RDD 126 (save at BigQueryWriteHelper.java:105) as input to shuffle 4
[2023-01-31T14:36:03.709+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO DAGScheduler: Got map stage job 12 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T14:36:03.709+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO DAGScheduler: Final stage: ShuffleMapStage 12 (save at BigQueryWriteHelper.java:105)
[2023-01-31T14:36:03.709+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T14:36:03.711+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO DAGScheduler: Missing parents: List()
[2023-01-31T14:36:03.726+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO CodeGenerator: Code generated in 33.307477 ms
[2023-01-31T14:36:03.726+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO DAGScheduler: Submitting ShuffleMapStage 12 (MapPartitionsRDD[126] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T14:36:03.731+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 15.7 KiB, free 433.6 MiB)
[2023-01-31T14:36:03.743+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.6 MiB)
[2023-01-31T14:36:03.744+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on b37fe3cbf330:38267 (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T14:36:03.745+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1513
[2023-01-31T14:36:03.750+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[126] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T14:36:03.751+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
[2023-01-31T14:36:03.754+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO DAGScheduler: Registering RDD 128 (save at BigQueryWriteHelper.java:105) as input to shuffle 5
[2023-01-31T14:36:03.757+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO DAGScheduler: Got map stage job 13 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T14:36:03.757+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO DAGScheduler: Final stage: ShuffleMapStage 13 (save at BigQueryWriteHelper.java:105)
[2023-01-31T14:36:03.757+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T14:36:03.758+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO DAGScheduler: Missing parents: List()
[2023-01-31T14:36:03.760+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO DAGScheduler: Submitting ShuffleMapStage 13 (MapPartitionsRDD[128] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T14:36:03.778+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 15.7 KiB, free 433.6 MiB)
[2023-01-31T14:36:03.780+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.6 MiB)
[2023-01-31T14:36:03.782+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on b37fe3cbf330:38267 (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T14:36:03.787+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1513
[2023-01-31T14:36:03.792+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 13 (MapPartitionsRDD[128] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T14:36:03.792+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0
[2023-01-31T14:36:03.798+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO DAGScheduler: Registering RDD 130 (save at BigQueryWriteHelper.java:105) as input to shuffle 6
[2023-01-31T14:36:03.802+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO DAGScheduler: Got map stage job 14 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T14:36:03.802+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO DAGScheduler: Final stage: ShuffleMapStage 14 (save at BigQueryWriteHelper.java:105)
[2023-01-31T14:36:03.802+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T14:36:03.803+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO DAGScheduler: Missing parents: List()
[2023-01-31T14:36:03.807+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO CodeGenerator: Code generated in 50.069838 ms
[2023-01-31T14:36:03.808+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO DAGScheduler: Submitting ShuffleMapStage 14 (MapPartitionsRDD[130] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T14:36:03.836+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 15.7 KiB, free 433.6 MiB)
[2023-01-31T14:36:03.838+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.6 MiB)
[2023-01-31T14:36:03.840+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on b37fe3cbf330:38267 (size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:36:03.846+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1513
[2023-01-31T14:36:03.851+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[130] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T14:36:03.852+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0
[2023-01-31T14:36:03.861+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO DAGScheduler: Registering RDD 132 (save at BigQueryWriteHelper.java:105) as input to shuffle 7
[2023-01-31T14:36:03.861+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO DAGScheduler: Got map stage job 15 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T14:36:03.861+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO DAGScheduler: Final stage: ShuffleMapStage 15 (save at BigQueryWriteHelper.java:105)
[2023-01-31T14:36:03.862+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T14:36:03.862+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO DAGScheduler: Missing parents: List()
[2023-01-31T14:36:03.870+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[132] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T14:36:03.874+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO CodeGenerator: Code generated in 35.610025 ms
[2023-01-31T14:36:03.902+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 15.7 KiB, free 433.5 MiB)
[2023-01-31T14:36:03.905+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.5 MiB)
[2023-01-31T14:36:03.912+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on b37fe3cbf330:38267 (size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:36:03.914+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1513
[2023-01-31T14:36:03.922+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[132] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T14:36:03.923+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
[2023-01-31T14:36:03.924+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO DAGScheduler: Registering RDD 134 (save at BigQueryWriteHelper.java:105) as input to shuffle 8
[2023-01-31T14:36:03.931+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO DAGScheduler: Got map stage job 16 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T14:36:03.931+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO DAGScheduler: Final stage: ShuffleMapStage 16 (save at BigQueryWriteHelper.java:105)
[2023-01-31T14:36:03.932+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T14:36:03.933+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO DAGScheduler: Missing parents: List()
[2023-01-31T14:36:03.938+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO DAGScheduler: Submitting ShuffleMapStage 16 (MapPartitionsRDD[134] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T14:36:03.965+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO CodeGenerator: Code generated in 46.094782 ms
[2023-01-31T14:36:03.986+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2130 bytes result sent to driver
[2023-01-31T14:36:03.987+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 15.7 KiB, free 433.5 MiB)
[2023-01-31T14:36:03.987+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T14:36:03.989+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 725 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T14:36:03.990+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2023-01-31T14:36:03.990+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)
[2023-01-31T14:36:03.997+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.5 MiB)
[2023-01-31T14:36:03.997+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on b37fe3cbf330:38267 (size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:36:03.998+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:03 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1513
[2023-01-31T14:36:04.001+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 16 (MapPartitionsRDD[134] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T14:36:04.006+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0
[2023-01-31T14:36:04.008+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: Registering RDD 136 (save at BigQueryWriteHelper.java:105) as input to shuffle 9
[2023-01-31T14:36:04.010+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: Got map stage job 17 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T14:36:04.012+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: Final stage: ShuffleMapStage 17 (save at BigQueryWriteHelper.java:105)
[2023-01-31T14:36:04.013+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T14:36:04.014+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: Missing parents: List()
[2023-01-31T14:36:04.032+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: Submitting ShuffleMapStage 17 (MapPartitionsRDD[136] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T14:36:04.041+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO CodeGenerator: Code generated in 40.652991 ms
[2023-01-31T14:36:04.051+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 15.7 KiB, free 433.5 MiB)
[2023-01-31T14:36:04.059+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.5 MiB)
[2023-01-31T14:36:04.061+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on b37fe3cbf330:38267 (size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:36:04.063+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1513
[2023-01-31T14:36:04.073+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 17 (MapPartitionsRDD[136] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T14:36:04.073+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0
[2023-01-31T14:36:04.090+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: ShuffleMapStage 8 (save at BigQueryWriteHelper.java:105) finished in 1.126 s
[2023-01-31T14:36:04.099+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T14:36:04.126+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: running: HashSet(ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 9, ShuffleMapStage 10, ShuffleMapStage 11, ShuffleMapStage 12, ShuffleMapStage 13, ShuffleMapStage 14, ShuffleMapStage 15)
[2023-01-31T14:36:04.127+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T14:36:04.127+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: failed: HashSet()
[2023-01-31T14:36:04.184+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: Registering RDD 138 (save at BigQueryWriteHelper.java:105) as input to shuffle 10
[2023-01-31T14:36:04.194+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: Got map stage job 18 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T14:36:04.194+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: Final stage: ShuffleMapStage 18 (save at BigQueryWriteHelper.java:105)
[2023-01-31T14:36:04.199+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T14:36:04.199+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: Missing parents: List()
[2023-01-31T14:36:04.199+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[138] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T14:36:04.208+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO CodeGenerator: Code generated in 60.222357 ms
[2023-01-31T14:36:04.208+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 15.7 KiB, free 433.5 MiB)
[2023-01-31T14:36:04.229+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.5 MiB)
[2023-01-31T14:36:04.229+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on b37fe3cbf330:38267 (size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:36:04.232+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1513
[2023-01-31T14:36:04.243+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[138] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T14:36:04.243+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0
[2023-01-31T14:36:04.248+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: Registering RDD 140 (save at BigQueryWriteHelper.java:105) as input to shuffle 11
[2023-01-31T14:36:04.248+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: Got map stage job 19 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T14:36:04.251+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: Final stage: ShuffleMapStage 19 (save at BigQueryWriteHelper.java:105)
[2023-01-31T14:36:04.256+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T14:36:04.256+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: Missing parents: List()
[2023-01-31T14:36:04.267+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: Submitting ShuffleMapStage 19 (MapPartitionsRDD[140] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T14:36:04.273+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 15.7 KiB, free 433.4 MiB)
[2023-01-31T14:36:04.277+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.4 MiB)
[2023-01-31T14:36:04.280+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on b37fe3cbf330:38267 (size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:36:04.281+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1513
[2023-01-31T14:36:04.284+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 19 (MapPartitionsRDD[140] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T14:36:04.285+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0
[2023-01-31T14:36:04.289+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: Registering RDD 142 (save at BigQueryWriteHelper.java:105) as input to shuffle 12
[2023-01-31T14:36:04.292+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: Got map stage job 20 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T14:36:04.293+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: Final stage: ShuffleMapStage 20 (save at BigQueryWriteHelper.java:105)
[2023-01-31T14:36:04.294+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T14:36:04.295+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO CodeGenerator: Code generated in 28.937624 ms
[2023-01-31T14:36:04.296+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: Missing parents: List()
[2023-01-31T14:36:04.300+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: Submitting ShuffleMapStage 20 (MapPartitionsRDD[142] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T14:36:04.316+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 15.7 KiB, free 433.4 MiB)
[2023-01-31T14:36:04.323+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.4 MiB)
[2023-01-31T14:36:04.325+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on b37fe3cbf330:38267 (size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:36:04.327+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1513
[2023-01-31T14:36:04.329+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[142] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T14:36:04.330+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks resource profile 0
[2023-01-31T14:36:04.337+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: Registering RDD 144 (save at BigQueryWriteHelper.java:105) as input to shuffle 13
[2023-01-31T14:36:04.350+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: Got map stage job 21 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T14:36:04.351+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: Final stage: ShuffleMapStage 21 (save at BigQueryWriteHelper.java:105)
[2023-01-31T14:36:04.351+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T14:36:04.352+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: Missing parents: List()
[2023-01-31T14:36:04.359+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: Submitting ShuffleMapStage 21 (MapPartitionsRDD[144] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T14:36:04.361+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO CodeGenerator: Code generated in 27.88159 ms
[2023-01-31T14:36:04.376+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 15.7 KiB, free 433.4 MiB)
[2023-01-31T14:36:04.376+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.4 MiB)
[2023-01-31T14:36:04.380+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on b37fe3cbf330:38267 (size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:36:04.381+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1513
[2023-01-31T14:36:04.382+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 21 (MapPartitionsRDD[144] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T14:36:04.382+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0
[2023-01-31T14:36:04.388+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: Registering RDD 146 (save at BigQueryWriteHelper.java:105) as input to shuffle 14
[2023-01-31T14:36:04.389+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: Got map stage job 22 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T14:36:04.389+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: Final stage: ShuffleMapStage 22 (save at BigQueryWriteHelper.java:105)
[2023-01-31T14:36:04.389+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T14:36:04.390+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: Missing parents: List()
[2023-01-31T14:36:04.395+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: Submitting ShuffleMapStage 22 (MapPartitionsRDD[146] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T14:36:04.408+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 15.7 KiB, free 433.4 MiB)
[2023-01-31T14:36:04.413+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.4 MiB)
[2023-01-31T14:36:04.415+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on b37fe3cbf330:38267 (size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:36:04.417+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1513
[2023-01-31T14:36:04.419+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 22 (MapPartitionsRDD[146] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T14:36:04.420+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0
[2023-01-31T14:36:04.429+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO CodeGenerator: Code generated in 34.969466 ms
[2023-01-31T14:36:04.442+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: Registering RDD 148 (save at BigQueryWriteHelper.java:105) as input to shuffle 15
[2023-01-31T14:36:04.442+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: Got map stage job 23 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T14:36:04.442+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: Final stage: ShuffleMapStage 23 (save at BigQueryWriteHelper.java:105)
[2023-01-31T14:36:04.443+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T14:36:04.443+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: Missing parents: List()
[2023-01-31T14:36:04.448+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: Submitting ShuffleMapStage 23 (MapPartitionsRDD[148] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T14:36:04.464+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 15.7 KiB, free 433.4 MiB)
[2023-01-31T14:36:04.464+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.3 MiB)
[2023-01-31T14:36:04.465+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on b37fe3cbf330:38267 (size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:36:04.467+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1513
[2023-01-31T14:36:04.471+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 23 (MapPartitionsRDD[148] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T14:36:04.472+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks resource profile 0
[2023-01-31T14:36:04.486+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO CodeGenerator: Code generated in 27.763115 ms
[2023-01-31T14:36:04.499+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: Registering RDD 150 (save at BigQueryWriteHelper.java:105) as input to shuffle 16
[2023-01-31T14:36:04.500+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: Got map stage job 24 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T14:36:04.500+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: Final stage: ShuffleMapStage 24 (save at BigQueryWriteHelper.java:105)
[2023-01-31T14:36:04.500+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T14:36:04.500+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: Missing parents: List()
[2023-01-31T14:36:04.504+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: Submitting ShuffleMapStage 24 (MapPartitionsRDD[150] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T14:36:04.509+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 15.7 KiB, free 433.3 MiB)
[2023-01-31T14:36:04.518+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.3 MiB)
[2023-01-31T14:36:04.520+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on b37fe3cbf330:38267 (size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:36:04.521+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1513
[2023-01-31T14:36:04.522+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 24 (MapPartitionsRDD[150] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T14:36:04.523+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks resource profile 0
[2023-01-31T14:36:04.812+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO BlockManagerInfo: Removed broadcast_16_piece0 on b37fe3cbf330:38267 in memory (size: 5.7 KiB, free: 434.2 MiB)
[2023-01-31T14:36:04.971+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:04 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2023-01-31T14:36:05.284+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:05 INFO CodeGenerator: Code generated in 40.583254 ms
[2023-01-31T14:36:05.387+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:05 INFO CodeGenerator: Code generated in 47.957042 ms
[2023-01-31T14:36:05.817+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:05 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2023-01-31T14:36:05.821+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:05 INFO DAGScheduler: Got job 25 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
[2023-01-31T14:36:05.822+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:05 INFO DAGScheduler: Final stage: ResultStage 26 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2023-01-31T14:36:05.822+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:05 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 25)
[2023-01-31T14:36:05.822+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:05 INFO DAGScheduler: Missing parents: List()
[2023-01-31T14:36:05.845+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:05 INFO DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[155] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2023-01-31T14:36:06.033+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:06 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 23.8 KiB, free 433.3 MiB)
[2023-01-31T14:36:06.044+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:06 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 10.8 KiB, free 433.3 MiB)
[2023-01-31T14:36:06.047+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:06 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on b37fe3cbf330:38267 (size: 10.8 KiB, free: 434.2 MiB)
[2023-01-31T14:36:06.050+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:06 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1513
[2023-01-31T14:36:06.056+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[155] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
[2023-01-31T14:36:06.056+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:06 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks resource profile 0
[2023-01-31T14:36:13.974+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:13 INFO CodeGenerator: Code generated in 185.762034 ms
[2023-01-31T14:36:17.154+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:17 INFO PythonRunner: Times: total = 9990, boot = 7808, init = 2146, finish = 36
[2023-01-31T14:36:17.178+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:17 INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 2521 bytes result sent to driver
[2023-01-31T14:36:17.183+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:17 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T14:36:17.184+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:17 INFO Executor: Running task 0.0 in stage 10.0 (TID 10)
[2023-01-31T14:36:17.187+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:17 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 13201 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T14:36:17.187+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:17 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
[2023-01-31T14:36:17.207+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:17 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 41797
[2023-01-31T14:36:17.289+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:17 INFO DAGScheduler: ShuffleMapStage 9 (save at BigQueryWriteHelper.java:105) finished in 13.987 s
[2023-01-31T14:36:17.290+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:17 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T14:36:17.291+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:17 INFO DAGScheduler: running: HashSet(ShuffleMapStage 10, ShuffleMapStage 11, ShuffleMapStage 12, ShuffleMapStage 13, ShuffleMapStage 14, ShuffleMapStage 15, ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 18, ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T14:36:17.291+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:17 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T14:36:17.291+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:17 INFO DAGScheduler: failed: HashSet()
[2023-01-31T14:36:18.239+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:18 INFO PythonRunner: Times: total = 888, boot = -2288, init = 3176, finish = 0
[2023-01-31T14:36:18.308+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:18 INFO Executor: Finished task 0.0 in stage 10.0 (TID 10). 2478 bytes result sent to driver
[2023-01-31T14:36:18.315+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:18 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 11) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T14:36:18.323+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:18 INFO Executor: Running task 0.0 in stage 11.0 (TID 11)
[2023-01-31T14:36:18.325+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:18 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 1145 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T14:36:18.326+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:18 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool
[2023-01-31T14:36:18.328+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:18 INFO DAGScheduler: ShuffleMapStage 10 (save at BigQueryWriteHelper.java:105) finished in 14.765 s
[2023-01-31T14:36:18.328+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:18 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T14:36:18.330+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:18 INFO DAGScheduler: running: HashSet(ShuffleMapStage 11, ShuffleMapStage 12, ShuffleMapStage 13, ShuffleMapStage 14, ShuffleMapStage 15, ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 18, ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T14:36:18.338+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:18 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T14:36:18.345+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:18 INFO DAGScheduler: failed: HashSet()
[2023-01-31T14:36:19.730+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:19 INFO PythonRunner: Times: total = 1285, boot = -139, init = 1424, finish = 0
[2023-01-31T14:36:19.780+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:19 INFO Executor: Finished task 0.0 in stage 11.0 (TID 11). 2478 bytes result sent to driver
[2023-01-31T14:36:19.781+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:19 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 12) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T14:36:19.782+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:19 INFO Executor: Running task 0.0 in stage 12.0 (TID 12)
[2023-01-31T14:36:19.784+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:19 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 11) in 1468 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T14:36:19.785+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:19 INFO DAGScheduler: ShuffleMapStage 11 (save at BigQueryWriteHelper.java:105) finished in 16.119 s
[2023-01-31T14:36:19.785+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:19 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T14:36:19.785+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:19 INFO DAGScheduler: running: HashSet(ShuffleMapStage 12, ShuffleMapStage 13, ShuffleMapStage 14, ShuffleMapStage 15, ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 18, ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T14:36:19.786+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:19 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T14:36:19.786+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:19 INFO DAGScheduler: failed: HashSet()
[2023-01-31T14:36:19.802+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:19 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool
[2023-01-31T14:36:20.511+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:20 INFO PythonRunner: Times: total = 624, boot = -43, init = 667, finish = 0
[2023-01-31T14:36:20.549+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:20 INFO Executor: Finished task 0.0 in stage 12.0 (TID 12). 2521 bytes result sent to driver
[2023-01-31T14:36:20.552+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:20 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 13) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T14:36:20.553+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:20 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 12) in 772 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T14:36:20.553+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:20 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool
[2023-01-31T14:36:20.554+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:20 INFO DAGScheduler: ShuffleMapStage 12 (save at BigQueryWriteHelper.java:105) finished in 16.840 s
[2023-01-31T14:36:20.554+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:20 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T14:36:20.555+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:20 INFO DAGScheduler: running: HashSet(ShuffleMapStage 13, ShuffleMapStage 14, ShuffleMapStage 15, ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 18, ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T14:36:20.555+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:20 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T14:36:20.555+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:20 INFO DAGScheduler: failed: HashSet()
[2023-01-31T14:36:20.561+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:20 INFO Executor: Running task 0.0 in stage 13.0 (TID 13)
[2023-01-31T14:36:21.240+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:21 INFO PythonRunner: Times: total = 612, boot = -5, init = 616, finish = 1
[2023-01-31T14:36:21.271+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:21 INFO Executor: Finished task 0.0 in stage 13.0 (TID 13). 2478 bytes result sent to driver
[2023-01-31T14:36:21.276+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:21 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 14) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T14:36:21.281+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:21 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 13) in 728 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T14:36:21.282+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:21 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool
[2023-01-31T14:36:21.282+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:21 INFO DAGScheduler: ShuffleMapStage 13 (save at BigQueryWriteHelper.java:105) finished in 17.519 s
[2023-01-31T14:36:21.282+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:21 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T14:36:21.282+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:21 INFO DAGScheduler: running: HashSet(ShuffleMapStage 14, ShuffleMapStage 15, ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 18, ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T14:36:21.283+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:21 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T14:36:21.286+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:21 INFO DAGScheduler: failed: HashSet()
[2023-01-31T14:36:21.304+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:21 INFO Executor: Running task 0.0 in stage 14.0 (TID 14)
[2023-01-31T14:36:21.824+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:21 INFO PythonRunner: Times: total = 441, boot = -25, init = 466, finish = 0
[2023-01-31T14:36:21.877+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:21 INFO Executor: Finished task 0.0 in stage 14.0 (TID 14). 2478 bytes result sent to driver
[2023-01-31T14:36:21.883+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:21 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 15) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T14:36:21.884+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:21 INFO Executor: Running task 0.0 in stage 15.0 (TID 15)
[2023-01-31T14:36:21.888+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:21 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 14) in 607 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T14:36:21.889+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:21 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool
[2023-01-31T14:36:21.890+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:21 INFO DAGScheduler: ShuffleMapStage 14 (save at BigQueryWriteHelper.java:105) finished in 18.074 s
[2023-01-31T14:36:21.900+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:21 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T14:36:21.900+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:21 INFO DAGScheduler: running: HashSet(ShuffleMapStage 15, ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 18, ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T14:36:21.906+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:21 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T14:36:21.907+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:21 INFO DAGScheduler: failed: HashSet()
[2023-01-31T14:36:22.751+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:22 INFO PythonRunner: Times: total = 759, boot = -66, init = 825, finish = 0
[2023-01-31T14:36:22.809+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:22 INFO Executor: Finished task 0.0 in stage 15.0 (TID 15). 2521 bytes result sent to driver
[2023-01-31T14:36:22.815+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:22 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 16) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T14:36:22.816+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:22 INFO Executor: Running task 0.0 in stage 16.0 (TID 16)
[2023-01-31T14:36:22.823+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:22 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 15) in 938 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T14:36:22.826+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:22 INFO DAGScheduler: ShuffleMapStage 15 (save at BigQueryWriteHelper.java:105) finished in 18.947 s
[2023-01-31T14:36:22.838+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:22 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T14:36:22.841+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:22 INFO DAGScheduler: running: HashSet(ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 18, ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T14:36:22.845+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:22 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T14:36:22.852+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:22 INFO DAGScheduler: failed: HashSet()
[2023-01-31T14:36:22.852+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:22 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool
[2023-01-31T14:36:23.618+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:23 INFO PythonRunner: Times: total = 682, boot = -50, init = 731, finish = 1
[2023-01-31T14:36:23.658+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:23 INFO Executor: Finished task 0.0 in stage 16.0 (TID 16). 2478 bytes result sent to driver
[2023-01-31T14:36:23.665+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:23 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 17) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T14:36:23.667+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:23 INFO Executor: Running task 0.0 in stage 17.0 (TID 17)
[2023-01-31T14:36:23.674+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:23 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 16) in 857 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T14:36:23.674+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:23 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool
[2023-01-31T14:36:23.675+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:23 INFO DAGScheduler: ShuffleMapStage 16 (save at BigQueryWriteHelper.java:105) finished in 19.732 s
[2023-01-31T14:36:23.675+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:23 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T14:36:23.675+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:23 INFO DAGScheduler: running: HashSet(ShuffleMapStage 17, ShuffleMapStage 18, ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T14:36:23.676+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:23 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T14:36:23.676+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:23 INFO DAGScheduler: failed: HashSet()
[2023-01-31T14:36:24.681+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:24 INFO PythonRunner: Times: total = 945, boot = -44, init = 989, finish = 0
[2023-01-31T14:36:24.692+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:24 INFO Executor: Finished task 0.0 in stage 17.0 (TID 17). 2478 bytes result sent to driver
[2023-01-31T14:36:24.696+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:24 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 18) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T14:36:24.697+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:24 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 17) in 1034 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T14:36:24.697+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:24 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool
[2023-01-31T14:36:24.698+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:24 INFO Executor: Running task 0.0 in stage 18.0 (TID 18)
[2023-01-31T14:36:24.718+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:24 INFO DAGScheduler: ShuffleMapStage 17 (save at BigQueryWriteHelper.java:105) finished in 20.685 s
[2023-01-31T14:36:24.719+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:24 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T14:36:24.719+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:24 INFO DAGScheduler: running: HashSet(ShuffleMapStage 18, ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T14:36:24.742+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:24 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T14:36:24.743+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:24 INFO DAGScheduler: failed: HashSet()
[2023-01-31T14:36:25.501+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:25 INFO PythonRunner: Times: total = 670, boot = -32, init = 702, finish = 0
[2023-01-31T14:36:25.569+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:25 INFO Executor: Finished task 0.0 in stage 18.0 (TID 18). 2478 bytes result sent to driver
[2023-01-31T14:36:25.578+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:25 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 19) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T14:36:25.579+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:25 INFO Executor: Running task 0.0 in stage 19.0 (TID 19)
[2023-01-31T14:36:25.580+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:25 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 18) in 880 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T14:36:25.580+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:25 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool
[2023-01-31T14:36:25.581+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:25 INFO DAGScheduler: ShuffleMapStage 18 (save at BigQueryWriteHelper.java:105) finished in 21.384 s
[2023-01-31T14:36:25.585+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:25 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T14:36:25.592+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:25 INFO DAGScheduler: running: HashSet(ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T14:36:25.593+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:25 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T14:36:25.594+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:25 INFO DAGScheduler: failed: HashSet()
[2023-01-31T14:36:27.282+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:27 INFO PythonRunner: Times: total = 1610, boot = -183, init = 1792, finish = 1
[2023-01-31T14:36:27.334+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:27 INFO Executor: Finished task 0.0 in stage 19.0 (TID 19). 2521 bytes result sent to driver
[2023-01-31T14:36:27.346+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:27 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 20) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T14:36:27.353+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:27 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 19) in 1778 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T14:36:27.353+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:27 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool
[2023-01-31T14:36:27.358+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:27 INFO Executor: Running task 0.0 in stage 20.0 (TID 20)
[2023-01-31T14:36:27.359+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:27 INFO DAGScheduler: ShuffleMapStage 19 (save at BigQueryWriteHelper.java:105) finished in 23.084 s
[2023-01-31T14:36:27.361+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:27 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T14:36:27.367+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:27 INFO DAGScheduler: running: HashSet(ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T14:36:27.376+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:27 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T14:36:27.381+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:27 INFO DAGScheduler: failed: HashSet()
[2023-01-31T14:36:28.678+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:28 INFO PythonRunner: Times: total = 1171, boot = 32, init = 1139, finish = 0
[2023-01-31T14:36:28.736+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:28 INFO Executor: Finished task 0.0 in stage 20.0 (TID 20). 2478 bytes result sent to driver
[2023-01-31T14:36:28.743+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:28 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 21) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T14:36:28.750+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:28 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 20) in 1399 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T14:36:28.751+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:28 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool
[2023-01-31T14:36:28.752+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:28 INFO DAGScheduler: ShuffleMapStage 20 (save at BigQueryWriteHelper.java:105) finished in 24.442 s
[2023-01-31T14:36:28.755+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:28 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T14:36:28.761+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:28 INFO DAGScheduler: running: HashSet(ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T14:36:28.762+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:28 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T14:36:28.784+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:28 INFO DAGScheduler: failed: HashSet()
[2023-01-31T14:36:28.785+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:28 INFO Executor: Running task 0.0 in stage 21.0 (TID 21)
[2023-01-31T14:36:29.636+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:29 INFO PythonRunner: Times: total = 756, boot = -128, init = 884, finish = 0
[2023-01-31T14:36:29.682+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:29 INFO Executor: Finished task 0.0 in stage 21.0 (TID 21). 2478 bytes result sent to driver
[2023-01-31T14:36:29.690+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:29 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 22) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T14:36:29.695+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:29 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 21) in 952 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T14:36:29.697+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:29 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool
[2023-01-31T14:36:29.700+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:29 INFO DAGScheduler: ShuffleMapStage 21 (save at BigQueryWriteHelper.java:105) finished in 25.333 s
[2023-01-31T14:36:29.701+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:29 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T14:36:29.709+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:29 INFO DAGScheduler: running: HashSet(ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T14:36:29.710+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:29 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T14:36:29.721+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:29 INFO DAGScheduler: failed: HashSet()
[2023-01-31T14:36:29.725+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:29 INFO Executor: Running task 0.0 in stage 22.0 (TID 22)
[2023-01-31T14:36:30.872+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:30 INFO PythonRunner: Times: total = 1054, boot = -154, init = 1207, finish = 1
[2023-01-31T14:36:30.928+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:30 INFO Executor: Finished task 0.0 in stage 22.0 (TID 22). 2478 bytes result sent to driver
[2023-01-31T14:36:30.933+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:30 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 23) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T14:36:30.940+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:30 INFO Executor: Running task 0.0 in stage 23.0 (TID 23)
[2023-01-31T14:36:30.978+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:30 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 22) in 1251 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T14:36:30.979+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:30 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool
[2023-01-31T14:36:30.995+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:30 INFO DAGScheduler: ShuffleMapStage 22 (save at BigQueryWriteHelper.java:105) finished in 26.543 s
[2023-01-31T14:36:31.002+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:30 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T14:36:31.016+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:31 INFO DAGScheduler: running: HashSet(ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T14:36:31.033+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:31 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T14:36:31.042+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:31 INFO DAGScheduler: failed: HashSet()
[2023-01-31T14:36:32.269+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:32 INFO PythonRunner: Times: total = 1043, boot = -169, init = 1212, finish = 0
[2023-01-31T14:36:32.308+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:32 INFO Executor: Finished task 0.0 in stage 23.0 (TID 23). 2521 bytes result sent to driver
[2023-01-31T14:36:32.317+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:32 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 24) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T14:36:32.321+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:32 INFO Executor: Running task 0.0 in stage 24.0 (TID 24)
[2023-01-31T14:36:32.322+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:32 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 23) in 1382 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T14:36:32.324+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:32 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool
[2023-01-31T14:36:32.325+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:32 INFO DAGScheduler: ShuffleMapStage 23 (save at BigQueryWriteHelper.java:105) finished in 27.873 s
[2023-01-31T14:36:32.333+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:32 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T14:36:32.333+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:32 INFO DAGScheduler: running: HashSet(ShuffleMapStage 24, ResultStage 26)
[2023-01-31T14:36:32.334+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:32 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T14:36:32.334+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:32 INFO DAGScheduler: failed: HashSet()
[2023-01-31T14:36:33.242+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:33 INFO PythonRunner: Times: total = 827, boot = -162, init = 988, finish = 1
[2023-01-31T14:36:33.262+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:33 INFO Executor: Finished task 0.0 in stage 24.0 (TID 24). 2478 bytes result sent to driver
[2023-01-31T14:36:33.286+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:33 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 25) (b37fe3cbf330, executor driver, partition 0, NODE_LOCAL, 7399 bytes) taskResourceAssignments Map()
[2023-01-31T14:36:33.309+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:33 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 24) in 997 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T14:36:33.322+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:33 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool
[2023-01-31T14:36:33.326+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:33 INFO DAGScheduler: ShuffleMapStage 24 (save at BigQueryWriteHelper.java:105) finished in 28.809 s
[2023-01-31T14:36:33.327+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:33 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T14:36:33.328+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:33 INFO DAGScheduler: running: HashSet(ResultStage 26)
[2023-01-31T14:36:33.331+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:33 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T14:36:33.334+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:33 INFO DAGScheduler: failed: HashSet()
[2023-01-31T14:36:33.342+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:33 INFO Executor: Running task 0.0 in stage 26.0 (TID 25)
[2023-01-31T14:36:34.797+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:34 INFO ShuffleBlockFetcherIterator: Getting 1 (97.0 B) non-empty blocks including 1 (97.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T14:36:34.819+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 194 ms
[2023-01-31T14:36:35.582+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:35 INFO CodeGenerator: Code generated in 555.771009 ms
[2023-01-31T14:36:39.249+0000] {spark_submit.py:495} INFO - 23/01/31 14:36:39 INFO CodeGenerator: Code generated in 2468.42066 ms
[2023-01-31T14:39:14.089+0000] {base_job.py:232} ERROR - LocalTaskJob heartbeat got an exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3361, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 325, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 888, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 491, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 271, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 386, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 684, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 210, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 680, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/jobs/base_job.py", line 204, in heartbeat
    session.merge(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in merge
    return self._merge(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3131, in _merge
    merged = self.get(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2848, in get
    return self._get_impl(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2970, in _get_impl
    return db_load_fn(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/loading.py", line 530, in load_on_pk_identity
    session.execute(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1552, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3315, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3394, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3364, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2198, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 210, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3361, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 325, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 888, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 491, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 271, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 386, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 684, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 210, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 680, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-01-31T14:39:29.370+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:29 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 167213 ms exceeds timeout 120000 ms
[2023-01-31T14:39:29.748+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:29 INFO Executor: Told to re-register on heartbeat
[2023-01-31T14:39:29.753+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:29 INFO BlockManager: BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None) re-registering with master
[2023-01-31T14:39:29.754+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:29 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None)
[2023-01-31T14:39:29.870+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:29 WARN SparkContext: Killing executors is not supported by current scheduler.
[2023-01-31T14:39:30.202+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:30 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None)
[2023-01-31T14:39:30.206+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:30 INFO BlockManager: Reporting 40 blocks to the master.
[2023-01-31T14:39:30.221+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:30 INFO CodeGenerator: Code generated in 241.938985 ms
[2023-01-31T14:39:30.236+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:30 INFO CodeGenerator: Code generated in 11.593446 ms
[2023-01-31T14:39:30.284+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:30 INFO BlockManagerInfo: Updated broadcast_20_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:30.288+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:30 INFO BlockManagerInfo: Updated broadcast_24_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:32.957+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:32 INFO BlockManagerInfo: Updated broadcast_18_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:33.019+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:33 INFO BlockManagerInfo: Updated broadcast_29_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:33.172+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:33 INFO BlockManagerInfo: Updated broadcast_23_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:33.320+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:33 INFO BlockManagerInfo: Updated broadcast_25_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:33.342+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:33 INFO BlockManagerInfo: Updated broadcast_30_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:33.635+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:33 INFO BlockManagerInfo: Updated broadcast_28_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:33.637+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:33 INFO BlockManagerInfo: Updated broadcast_17_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:33.651+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:33 INFO BlockManagerInfo: Updated broadcast_22_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:33.996+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:33 INFO CodeGenerator: Code generated in 5.718555 ms
[2023-01-31T14:39:34.010+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:33 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T14:39:34.011+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:34 INFO BlockManagerInfo: Updated broadcast_21_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:34.027+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:34 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T14:39:34.099+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:34 INFO BlockManagerInfo: Updated broadcast_32_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:34.111+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:34 INFO CodeGenerator: Code generated in 18.477594 ms
[2023-01-31T14:39:34.130+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:34 INFO CodeGenerator: Code generated in 14.332071 ms
[2023-01-31T14:39:34.142+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:34 INFO BlockManagerInfo: Updated broadcast_27_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:34.209+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:34 INFO BlockManagerInfo: Updated broadcast_31_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:34.454+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:34 INFO Executor: Finished task 0.0 in stage 26.0 (TID 25). 3200 bytes result sent to driver
[2023-01-31T14:39:34.522+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:34 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 25) in 181203 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T14:39:34.523+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:34 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool
[2023-01-31T14:39:34.523+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:34 INFO DAGScheduler: ResultStage 26 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 208.505 s
[2023-01-31T14:39:34.523+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:34 INFO DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T14:39:34.535+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 26: Stage finished
[2023-01-31T14:39:34.535+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:34 INFO DAGScheduler: Job 25 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 208.672728 s
[2023-01-31T14:39:34.755+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:34 INFO BlockManagerInfo: Updated broadcast_26_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:34.815+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:34 INFO BlockManagerInfo: Updated broadcast_33_piece0 in memory on b37fe3cbf330:38267 (current size: 10.8 KiB, original size: 10.8 KiB, free: 434.2 MiB)
[2023-01-31T14:39:34.823+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:34 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T14:39:34.832+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:34 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 24.0 B, free 433.3 MiB)
[2023-01-31T14:39:34.845+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:34 INFO BlockManagerInfo: Updated broadcast_19_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:34.851+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:34 INFO Executor: Told to re-register on heartbeat
[2023-01-31T14:39:34.851+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:34 INFO BlockManager: BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None) re-registering with master
[2023-01-31T14:39:34.851+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:34 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None)
[2023-01-31T14:39:34.855+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:34 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 204.0 B, free 433.3 MiB)
[2023-01-31T14:39:34.856+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:34 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on b37fe3cbf330:38267 (size: 204.0 B, free: 434.2 MiB)
[2023-01-31T14:39:34.857+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:34 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None)
[2023-01-31T14:39:34.858+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:34 INFO BlockManager: Reporting 42 blocks to the master.
[2023-01-31T14:39:34.893+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:34 INFO SparkContext: Created broadcast 34 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2023-01-31T14:39:34.977+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:34 INFO BlockManagerInfo: Updated broadcast_20_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:34.979+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:34 INFO BlockManagerInfo: Updated broadcast_24_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:35.030+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:35 INFO BlockManagerInfo: Updated broadcast_18_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:35.060+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:35 INFO BlockManagerInfo: Updated broadcast_29_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:35.062+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:35 INFO BlockManagerInfo: Updated broadcast_23_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:35.119+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:35 INFO BlockManagerInfo: Updated broadcast_25_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:35.121+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:35 INFO BlockManagerInfo: Updated broadcast_30_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:35.166+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:35 INFO BlockManagerInfo: Updated broadcast_28_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:35.193+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:35 INFO BlockManagerInfo: Updated broadcast_17_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:35.194+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:35 INFO BlockManagerInfo: Updated broadcast_22_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:35.195+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:35 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T14:39:35.196+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:35 INFO BlockManagerInfo: Updated broadcast_21_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:35.232+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:35 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T14:39:35.233+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:35 INFO BlockManagerInfo: Updated broadcast_32_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:35.259+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:35 INFO BlockManagerInfo: Updated broadcast_27_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:35.366+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:35 INFO BlockManagerInfo: Updated broadcast_31_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:35.367+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:35 INFO BlockManagerInfo: Updated broadcast_26_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:35.446+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:35 INFO BlockManagerInfo: Updated broadcast_34_piece0 in memory on b37fe3cbf330:38267 (current size: 204.0 B, original size: 204.0 B, free: 434.2 MiB)
[2023-01-31T14:39:35.449+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:35 INFO BlockManagerInfo: Updated broadcast_33_piece0 in memory on b37fe3cbf330:38267 (current size: 10.8 KiB, original size: 10.8 KiB, free: 434.2 MiB)
[2023-01-31T14:39:35.450+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:35 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T14:39:35.452+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:35 INFO BlockManagerInfo: Updated broadcast_19_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:35.453+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:35 INFO Executor: Told to re-register on heartbeat
[2023-01-31T14:39:35.454+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:35 INFO BlockManager: BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None) re-registering with master
[2023-01-31T14:39:35.454+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:35 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None)
[2023-01-31T14:39:35.454+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:35 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None)
[2023-01-31T14:39:35.455+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:35 INFO BlockManager: Reporting 42 blocks to the master.
[2023-01-31T14:39:35.456+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:35 INFO BlockManagerInfo: Updated broadcast_20_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:35.457+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:35 INFO BlockManagerInfo: Updated broadcast_24_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:35.458+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:35 INFO BlockManagerInfo: Updated broadcast_18_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:35.464+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:35 INFO BlockManagerInfo: Updated broadcast_29_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:35.499+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:35 INFO BlockManagerInfo: Updated broadcast_23_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:35.517+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:35 INFO BlockManagerInfo: Updated broadcast_25_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:35.756+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:35 INFO BlockManagerInfo: Updated broadcast_30_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:35.861+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:35 INFO BlockManagerInfo: Updated broadcast_28_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:35.888+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:35 INFO BlockManagerInfo: Updated broadcast_17_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:35.991+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:35 INFO BlockManagerInfo: Updated broadcast_22_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:36.026+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:36 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T14:39:36.065+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:36 INFO BlockManagerInfo: Updated broadcast_21_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:36.198+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:36 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T14:39:36.359+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:36 INFO BlockManagerInfo: Updated broadcast_32_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:36.645+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:36 INFO BlockManagerInfo: Updated broadcast_27_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:36.647+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:36 INFO BlockManagerInfo: Updated broadcast_31_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:36.660+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:36 INFO BlockManagerInfo: Updated broadcast_26_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:36.664+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:36 INFO BlockManagerInfo: Updated broadcast_34_piece0 in memory on b37fe3cbf330:38267 (current size: 204.0 B, original size: 204.0 B, free: 434.2 MiB)
[2023-01-31T14:39:36.761+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:36 INFO BlockManagerInfo: Updated broadcast_33_piece0 in memory on b37fe3cbf330:38267 (current size: 10.8 KiB, original size: 10.8 KiB, free: 434.2 MiB)
[2023-01-31T14:39:36.795+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:36 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T14:39:36.855+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:36 INFO BlockManagerInfo: Updated broadcast_19_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:36.897+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:36 INFO Executor: Told to re-register on heartbeat
[2023-01-31T14:39:36.897+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:36 INFO BlockManager: BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None) re-registering with master
[2023-01-31T14:39:36.897+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:36 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None)
[2023-01-31T14:39:36.931+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:36 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None)
[2023-01-31T14:39:36.932+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:36 INFO BlockManager: Reporting 42 blocks to the master.
[2023-01-31T14:39:36.940+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:36 INFO BlockManagerInfo: Updated broadcast_20_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:36.990+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:36 INFO BlockManagerInfo: Updated broadcast_24_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:36.995+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:36 INFO BlockManagerInfo: Updated broadcast_18_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.026+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_29_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.028+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_23_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.039+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_25_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.042+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_30_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.046+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_28_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.061+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_17_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.072+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_22_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.077+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.079+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_21_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.094+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.103+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_32_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.112+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_27_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.117+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_31_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.119+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_26_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.122+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_34_piece0 in memory on b37fe3cbf330:38267 (current size: 204.0 B, original size: 204.0 B, free: 434.2 MiB)
[2023-01-31T14:39:37.123+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_33_piece0 in memory on b37fe3cbf330:38267 (current size: 10.8 KiB, original size: 10.8 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.127+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.133+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_19_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.140+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO Executor: Told to re-register on heartbeat
[2023-01-31T14:39:37.141+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManager: BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None) re-registering with master
[2023-01-31T14:39:37.144+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None)
[2023-01-31T14:39:37.178+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None)
[2023-01-31T14:39:37.178+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManager: Reporting 42 blocks to the master.
[2023-01-31T14:39:37.178+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_20_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.187+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_24_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.190+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_18_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.191+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_29_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.192+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_23_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.198+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_25_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.202+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_30_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.203+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_28_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.203+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_17_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.204+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_22_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.204+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.207+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_21_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.212+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.216+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_32_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.219+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_27_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.223+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_31_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.227+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_26_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.229+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_34_piece0 in memory on b37fe3cbf330:38267 (current size: 204.0 B, original size: 204.0 B, free: 434.2 MiB)
[2023-01-31T14:39:37.237+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_33_piece0 in memory on b37fe3cbf330:38267 (current size: 10.8 KiB, original size: 10.8 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.240+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.247+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_19_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.251+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO Executor: Told to re-register on heartbeat
[2023-01-31T14:39:37.251+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManager: BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None) re-registering with master
[2023-01-31T14:39:37.251+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None)
[2023-01-31T14:39:37.254+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None)
[2023-01-31T14:39:37.255+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManager: Reporting 42 blocks to the master.
[2023-01-31T14:39:37.259+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_20_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.263+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_24_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.264+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_18_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.265+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_29_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.271+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_23_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.289+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_25_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.293+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_30_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.300+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_28_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.305+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_17_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.309+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_22_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.313+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.319+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_21_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.321+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.327+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_32_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.331+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_27_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.336+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_31_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.342+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_26_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.343+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_34_piece0 in memory on b37fe3cbf330:38267 (current size: 204.0 B, original size: 204.0 B, free: 434.2 MiB)
[2023-01-31T14:39:37.344+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_33_piece0 in memory on b37fe3cbf330:38267 (current size: 10.8 KiB, original size: 10.8 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.349+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.355+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_19_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.387+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO Executor: Told to re-register on heartbeat
[2023-01-31T14:39:37.387+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManager: BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None) re-registering with master
[2023-01-31T14:39:37.387+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None)
[2023-01-31T14:39:37.393+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None)
[2023-01-31T14:39:37.395+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManager: Reporting 42 blocks to the master.
[2023-01-31T14:39:37.396+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_20_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.403+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_24_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.404+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_18_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.415+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_29_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.416+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_23_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.439+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_25_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.446+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_30_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.454+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_28_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.458+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_17_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.467+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_22_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.469+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.472+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-31T14:39:37.479+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO DAGScheduler: Got job 26 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T14:39:37.480+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO DAGScheduler: Final stage: ResultStage 27 (save at BigQueryWriteHelper.java:105)
[2023-01-31T14:39:37.480+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T14:39:37.480+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO DAGScheduler: Missing parents: List()
[2023-01-31T14:39:37.642+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO DAGScheduler: Submitting ResultStage 27 (ParallelCollectionRDD[158] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T14:39:37.658+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_21_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.660+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.661+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_32_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.662+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_27_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.665+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_31_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.666+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_26_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.670+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_34_piece0 in memory on b37fe3cbf330:38267 (current size: 204.0 B, original size: 204.0 B, free: 434.2 MiB)
[2023-01-31T14:39:37.672+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_33_piece0 in memory on b37fe3cbf330:38267 (current size: 10.8 KiB, original size: 10.8 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.682+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.694+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_19_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.694+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO Executor: Told to re-register on heartbeat
[2023-01-31T14:39:37.695+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManager: BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None) re-registering with master
[2023-01-31T14:39:37.695+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None)
[2023-01-31T14:39:37.701+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None)
[2023-01-31T14:39:37.703+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManager: Reporting 42 blocks to the master.
[2023-01-31T14:39:37.706+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_20_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.718+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_24_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.728+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_18_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.738+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_29_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.748+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_23_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.760+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_25_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.773+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_30_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.774+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_28_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.785+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_17_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.798+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_22_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.809+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.820+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_21_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.824+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.834+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_32_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.836+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_27_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.852+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_31_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.862+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_26_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.880+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_34_piece0 in memory on b37fe3cbf330:38267 (current size: 204.0 B, original size: 204.0 B, free: 434.2 MiB)
[2023-01-31T14:39:37.894+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_33_piece0 in memory on b37fe3cbf330:38267 (current size: 10.8 KiB, original size: 10.8 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.905+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.919+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_19_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:37.930+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO Executor: Told to re-register on heartbeat
[2023-01-31T14:39:37.936+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManager: BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None) re-registering with master
[2023-01-31T14:39:37.940+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None)
[2023-01-31T14:39:37.941+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 209.9 KiB, free 433.1 MiB)
[2023-01-31T14:39:37.962+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 74.3 KiB, free 433.0 MiB)
[2023-01-31T14:39:37.978+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None)
[2023-01-31T14:39:37.983+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on b37fe3cbf330:38267 (size: 74.3 KiB, free: 434.1 MiB)
[2023-01-31T14:39:37.988+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManager: Reporting 44 blocks to the master.
[2023-01-31T14:39:37.990+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_20_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:37.990+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_24_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:38.023+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1513
[2023-01-31T14:39:38.159+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_18_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:38.160+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_29_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:38.161+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_23_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:38.163+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_25_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:38.164+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO BlockManagerInfo: Updated broadcast_30_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:38.165+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 27 (ParallelCollectionRDD[158] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T14:39:38.165+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:37 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks resource profile 0
[2023-01-31T14:39:38.166+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:38 INFO BlockManagerInfo: Updated broadcast_28_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:38.201+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:38 INFO BlockManagerInfo: Updated broadcast_17_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:38.202+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:38 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 26) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7484 bytes) taskResourceAssignments Map()
[2023-01-31T14:39:38.268+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:38 INFO BlockManagerInfo: Updated broadcast_22_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:38.269+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:38 INFO Executor: Running task 0.0 in stage 27.0 (TID 26)
[2023-01-31T14:39:38.396+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:38 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T14:39:38.397+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:38 INFO BlockManagerInfo: Updated broadcast_21_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:38.454+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:38 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T14:39:38.455+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:38 INFO BlockManagerInfo: Updated broadcast_32_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:38.477+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:38 INFO BlockManagerInfo: Updated broadcast_27_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:38.483+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:38 INFO BlockManagerInfo: Updated broadcast_31_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:38.497+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:38 INFO BlockManagerInfo: Updated broadcast_26_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:38.759+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:38 INFO BlockManagerInfo: Updated broadcast_34_piece0 in memory on b37fe3cbf330:38267 (current size: 204.0 B, original size: 204.0 B, free: 434.1 MiB)
[2023-01-31T14:39:38.761+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:38 INFO BlockManagerInfo: Updated broadcast_35_piece0 in memory on b37fe3cbf330:38267 (current size: 74.3 KiB, original size: 74.3 KiB, free: 434.1 MiB)
[2023-01-31T14:39:38.859+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:38 INFO BlockManagerInfo: Updated broadcast_33_piece0 in memory on b37fe3cbf330:38267 (current size: 10.8 KiB, original size: 10.8 KiB, free: 434.1 MiB)
[2023-01-31T14:39:38.860+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:38 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T14:39:39.131+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:38 INFO BlockManagerInfo: Updated broadcast_19_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:39.132+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:38 INFO Executor: Told to re-register on heartbeat
[2023-01-31T14:39:39.133+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:38 INFO BlockManager: BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None) re-registering with master
[2023-01-31T14:39:39.230+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:38 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None)
[2023-01-31T14:39:39.259+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:38 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None)
[2023-01-31T14:39:39.305+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:38 INFO BlockManager: Reporting 44 blocks to the master.
[2023-01-31T14:39:39.307+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:38 INFO BlockManagerInfo: Updated broadcast_20_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:39.449+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:38 INFO BlockManagerInfo: Updated broadcast_24_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:39.451+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:38 INFO BlockManagerInfo: Updated broadcast_18_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:39.452+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:38 INFO BlockManagerInfo: Updated broadcast_29_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:39.453+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:38 INFO BlockManagerInfo: Updated broadcast_23_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:39.453+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:38 INFO BlockManagerInfo: Updated broadcast_25_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:39.454+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:38 INFO BlockManagerInfo: Updated broadcast_30_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:39.454+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:38 INFO BlockManagerInfo: Updated broadcast_28_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:39.454+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:38 INFO BlockManagerInfo: Updated broadcast_17_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:39.454+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:39 INFO BlockManagerInfo: Updated broadcast_22_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:39.454+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:39 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T14:39:39.455+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:39 INFO BlockManagerInfo: Updated broadcast_21_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:39.459+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:39 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T14:39:39.832+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:39 INFO BlockManagerInfo: Updated broadcast_32_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:39.833+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:39 INFO BlockManagerInfo: Updated broadcast_27_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:39.883+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:39 INFO BlockManagerInfo: Updated broadcast_31_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:39.884+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:39 INFO BlockManagerInfo: Updated broadcast_26_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:39.885+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:39 INFO BlockManagerInfo: Updated broadcast_34_piece0 in memory on b37fe3cbf330:38267 (current size: 204.0 B, original size: 204.0 B, free: 434.1 MiB)
[2023-01-31T14:39:39.886+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:39 INFO BlockManagerInfo: Updated broadcast_35_piece0 in memory on b37fe3cbf330:38267 (current size: 74.3 KiB, original size: 74.3 KiB, free: 434.1 MiB)
[2023-01-31T14:39:39.922+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:39 INFO BlockManagerInfo: Updated broadcast_33_piece0 in memory on b37fe3cbf330:38267 (current size: 10.8 KiB, original size: 10.8 KiB, free: 434.1 MiB)
[2023-01-31T14:39:39.947+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:39 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T14:39:39.948+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:39 INFO BlockManagerInfo: Updated broadcast_19_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:40.257+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:40 INFO Executor: Told to re-register on heartbeat
[2023-01-31T14:39:40.262+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:40 INFO BlockManager: BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None) re-registering with master
[2023-01-31T14:39:40.277+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:40 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None)
[2023-01-31T14:39:40.285+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:40 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None)
[2023-01-31T14:39:40.326+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:40 INFO BlockManager: Reporting 44 blocks to the master.
[2023-01-31T14:39:40.328+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:40 INFO BlockManagerInfo: Updated broadcast_20_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:40.332+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:40 INFO BlockManagerInfo: Updated broadcast_24_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:40.364+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:40 INFO BlockManagerInfo: Updated broadcast_18_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:40.445+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:40 INFO BlockManagerInfo: Updated broadcast_29_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:40.446+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:40 INFO BlockManagerInfo: Updated broadcast_23_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:40.553+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:40 INFO BlockManagerInfo: Updated broadcast_25_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:40.621+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:40 INFO BlockManagerInfo: Updated broadcast_30_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:40.713+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:40 INFO BlockManagerInfo: Updated broadcast_28_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:41.573+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:41 INFO BlockManagerInfo: Updated broadcast_17_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:41.733+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:41 INFO BlockManagerInfo: Updated broadcast_22_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:41.926+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:41 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T14:39:42.012+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:42 INFO BlockManagerInfo: Updated broadcast_21_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:42.346+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:42 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T14:39:42.707+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:42 INFO BlockManagerInfo: Updated broadcast_32_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:42.712+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:42 INFO BlockManagerInfo: Updated broadcast_27_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:42.718+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:42 INFO BlockManagerInfo: Updated broadcast_31_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:42.738+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:42 INFO BlockManagerInfo: Updated broadcast_26_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:42.998+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:42 INFO BlockManagerInfo: Updated broadcast_34_piece0 in memory on b37fe3cbf330:38267 (current size: 204.0 B, original size: 204.0 B, free: 434.1 MiB)
[2023-01-31T14:39:43.002+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:43 INFO BlockManagerInfo: Updated broadcast_35_piece0 in memory on b37fe3cbf330:38267 (current size: 74.3 KiB, original size: 74.3 KiB, free: 434.1 MiB)
[2023-01-31T14:39:43.007+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:43 INFO BlockManagerInfo: Updated broadcast_33_piece0 in memory on b37fe3cbf330:38267 (current size: 10.8 KiB, original size: 10.8 KiB, free: 434.1 MiB)
[2023-01-31T14:39:43.215+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:43 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T14:39:43.961+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:43 INFO BlockManagerInfo: Updated broadcast_19_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:43.979+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:43 INFO Executor: Told to re-register on heartbeat
[2023-01-31T14:39:43.985+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:43 INFO BlockManager: BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None) re-registering with master
[2023-01-31T14:39:43.985+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:43 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None)
[2023-01-31T14:39:43.989+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:43 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None)
[2023-01-31T14:39:43.999+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:43 INFO BlockManager: Reporting 44 blocks to the master.
[2023-01-31T14:39:43.999+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:43 INFO BlockManagerInfo: Updated broadcast_20_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.007+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_24_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.012+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_18_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.014+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_29_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.017+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_23_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.018+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_25_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.028+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_30_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.033+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_28_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.035+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_17_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.038+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_22_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.040+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.042+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_21_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.045+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.066+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_32_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.083+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_27_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.087+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_31_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.096+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_26_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.115+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_34_piece0 in memory on b37fe3cbf330:38267 (current size: 204.0 B, original size: 204.0 B, free: 434.1 MiB)
[2023-01-31T14:39:44.123+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_35_piece0 in memory on b37fe3cbf330:38267 (current size: 74.3 KiB, original size: 74.3 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.160+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_33_piece0 in memory on b37fe3cbf330:38267 (current size: 10.8 KiB, original size: 10.8 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.171+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.171+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_19_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.175+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO Executor: Told to re-register on heartbeat
[2023-01-31T14:39:44.176+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManager: BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None) re-registering with master
[2023-01-31T14:39:44.176+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None)
[2023-01-31T14:39:44.202+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None)
[2023-01-31T14:39:44.202+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManager: Reporting 44 blocks to the master.
[2023-01-31T14:39:44.209+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_20_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.217+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_24_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.221+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_18_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.236+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_29_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.237+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_23_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.237+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_25_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.239+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_30_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.239+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_28_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.239+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_17_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.244+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_22_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.283+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.285+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_21_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.286+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.288+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_32_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.289+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_27_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.342+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_31_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.344+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_26_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.346+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_34_piece0 in memory on b37fe3cbf330:38267 (current size: 204.0 B, original size: 204.0 B, free: 434.1 MiB)
[2023-01-31T14:39:44.349+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_35_piece0 in memory on b37fe3cbf330:38267 (current size: 74.3 KiB, original size: 74.3 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.350+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_33_piece0 in memory on b37fe3cbf330:38267 (current size: 10.8 KiB, original size: 10.8 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.352+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.387+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_19_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.390+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO Executor: Told to re-register on heartbeat
[2023-01-31T14:39:44.391+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManager: BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None) re-registering with master
[2023-01-31T14:39:44.391+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None)
[2023-01-31T14:39:44.402+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None)
[2023-01-31T14:39:44.403+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManager: Reporting 44 blocks to the master.
[2023-01-31T14:39:44.403+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T14:39:44.404+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T14:39:44.404+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T14:39:44.404+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T14:39:44.404+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T14:39:44.405+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T14:39:44.420+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_20_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.422+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_24_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.433+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_18_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.433+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_29_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.444+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_23_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.461+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_25_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.464+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_30_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.500+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_28_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.500+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_17_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.517+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_22_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.521+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.521+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_21_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.523+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.526+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_32_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.527+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_27_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.527+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_31_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.530+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_26_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.531+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_34_piece0 in memory on b37fe3cbf330:38267 (current size: 204.0 B, original size: 204.0 B, free: 434.1 MiB)
[2023-01-31T14:39:44.535+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_35_piece0 in memory on b37fe3cbf330:38267 (current size: 74.3 KiB, original size: 74.3 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.537+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_33_piece0 in memory on b37fe3cbf330:38267 (current size: 10.8 KiB, original size: 10.8 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.539+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.541+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_19_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.555+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO Executor: Told to re-register on heartbeat
[2023-01-31T14:39:44.556+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManager: BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None) re-registering with master
[2023-01-31T14:39:44.556+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None)
[2023-01-31T14:39:44.563+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None)
[2023-01-31T14:39:44.563+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManager: Reporting 44 blocks to the master.
[2023-01-31T14:39:44.579+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_20_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.580+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_24_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.580+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_18_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.582+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_29_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.582+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_23_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.585+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_25_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.585+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_30_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.586+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_28_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.602+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T14:39:44.622+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_17_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.634+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_22_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.637+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.639+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_21_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.641+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.683+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_32_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.684+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_27_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.686+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_31_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.688+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_26_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.690+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_34_piece0 in memory on b37fe3cbf330:38267 (current size: 204.0 B, original size: 204.0 B, free: 434.1 MiB)
[2023-01-31T14:39:44.691+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_35_piece0 in memory on b37fe3cbf330:38267 (current size: 74.3 KiB, original size: 74.3 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.693+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_33_piece0 in memory on b37fe3cbf330:38267 (current size: 10.8 KiB, original size: 10.8 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.715+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.717+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T14:39:44.717+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_19_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.720+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO Executor: Told to re-register on heartbeat
[2023-01-31T14:39:44.720+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManager: BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None) re-registering with master
[2023-01-31T14:39:44.721+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None)
[2023-01-31T14:39:44.731+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None)
[2023-01-31T14:39:44.731+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManager: Reporting 44 blocks to the master.
[2023-01-31T14:39:44.735+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_20_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.742+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_24_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.753+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_18_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.755+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_29_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.756+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_23_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.766+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_25_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.767+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_30_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.767+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_28_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.767+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_17_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.784+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_22_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.785+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.787+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_21_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.790+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.792+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_32_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.815+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_27_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.817+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_31_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.819+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_26_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.820+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_34_piece0 in memory on b37fe3cbf330:38267 (current size: 204.0 B, original size: 204.0 B, free: 434.1 MiB)
[2023-01-31T14:39:44.822+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_35_piece0 in memory on b37fe3cbf330:38267 (current size: 74.3 KiB, original size: 74.3 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.824+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_33_piece0 in memory on b37fe3cbf330:38267 (current size: 10.8 KiB, original size: 10.8 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.825+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.840+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_19_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.859+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO Executor: Told to re-register on heartbeat
[2023-01-31T14:39:44.860+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManager: BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None) re-registering with master
[2023-01-31T14:39:44.860+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None)
[2023-01-31T14:39:44.863+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None)
[2023-01-31T14:39:44.863+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManager: Reporting 44 blocks to the master.
[2023-01-31T14:39:44.863+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_20_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.866+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_24_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.867+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_18_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.869+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_29_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.872+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_23_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.875+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_25_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.879+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_30_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.883+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_28_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.887+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_17_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.888+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_22_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.890+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.894+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_21_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.903+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.904+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_32_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.904+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_27_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.905+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_31_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.905+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_26_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.929+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_34_piece0 in memory on b37fe3cbf330:38267 (current size: 204.0 B, original size: 204.0 B, free: 434.1 MiB)
[2023-01-31T14:39:44.930+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_35_piece0 in memory on b37fe3cbf330:38267 (current size: 74.3 KiB, original size: 74.3 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.930+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_33_piece0 in memory on b37fe3cbf330:38267 (current size: 10.8 KiB, original size: 10.8 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.940+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.966+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_19_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.967+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO Executor: Told to re-register on heartbeat
[2023-01-31T14:39:44.967+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManager: BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None) re-registering with master
[2023-01-31T14:39:44.968+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None)
[2023-01-31T14:39:44.986+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None)
[2023-01-31T14:39:44.986+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManager: Reporting 44 blocks to the master.
[2023-01-31T14:39:44.987+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_20_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.992+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_24_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.993+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_18_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:44.998+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_29_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:45.004+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:44 INFO BlockManagerInfo: Updated broadcast_23_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:45.021+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:45 INFO BlockManagerInfo: Updated broadcast_25_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:45.025+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:45 INFO BlockManagerInfo: Updated broadcast_30_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:45.026+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:45 INFO BlockManagerInfo: Updated broadcast_28_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:45.026+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:45 INFO BlockManagerInfo: Updated broadcast_17_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:45.036+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:45 INFO BlockManagerInfo: Updated broadcast_22_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:45.043+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:45 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T14:39:45.047+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:45 INFO BlockManagerInfo: Updated broadcast_21_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:45.051+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:45 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T14:39:45.052+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:45 INFO BlockManagerInfo: Updated broadcast_32_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:45.074+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:45 INFO BlockManagerInfo: Updated broadcast_27_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:45.106+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:45 INFO BlockManagerInfo: Updated broadcast_31_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:45.107+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:45 INFO BlockManagerInfo: Updated broadcast_26_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:45.109+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:45 INFO BlockManagerInfo: Updated broadcast_34_piece0 in memory on b37fe3cbf330:38267 (current size: 204.0 B, original size: 204.0 B, free: 434.1 MiB)
[2023-01-31T14:39:45.113+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:45 INFO BlockManagerInfo: Updated broadcast_35_piece0 in memory on b37fe3cbf330:38267 (current size: 74.3 KiB, original size: 74.3 KiB, free: 434.1 MiB)
[2023-01-31T14:39:45.115+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:45 INFO BlockManagerInfo: Updated broadcast_33_piece0 in memory on b37fe3cbf330:38267 (current size: 10.8 KiB, original size: 10.8 KiB, free: 434.1 MiB)
[2023-01-31T14:39:45.116+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:45 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T14:39:45.118+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:45 INFO BlockManagerInfo: Updated broadcast_19_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:45.305+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:45 INFO ParquetOutputFormat: Parquet block size to 134217728
[2023-01-31T14:39:45.305+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:45 INFO ParquetOutputFormat: Validation is off
[2023-01-31T14:39:45.306+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:45 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-01-31T14:39:45.306+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:45 INFO ParquetOutputFormat: Parquet properties are:
[2023-01-31T14:39:45.307+0000] {spark_submit.py:495} INFO - Parquet page size to 1048576
[2023-01-31T14:39:45.307+0000] {spark_submit.py:495} INFO - Parquet dictionary page size to 1048576
[2023-01-31T14:39:45.307+0000] {spark_submit.py:495} INFO - Dictionary is true
[2023-01-31T14:39:45.308+0000] {spark_submit.py:495} INFO - Writer version is: PARQUET_1_0
[2023-01-31T14:39:45.308+0000] {spark_submit.py:495} INFO - Page size checking is: estimated
[2023-01-31T14:39:45.308+0000] {spark_submit.py:495} INFO - Min row count for page size check is: 100
[2023-01-31T14:39:45.309+0000] {spark_submit.py:495} INFO - Max row count for page size check is: 10000
[2023-01-31T14:39:45.309+0000] {spark_submit.py:495} INFO - Truncate length for column indexes is: 64
[2023-01-31T14:39:45.309+0000] {spark_submit.py:495} INFO - Truncate length for statistics min/max  is: 2147483647
[2023-01-31T14:39:45.310+0000] {spark_submit.py:495} INFO - Bloom filter enabled: false
[2023-01-31T14:39:45.310+0000] {spark_submit.py:495} INFO - Max Bloom filter size for a column is 1048576
[2023-01-31T14:39:45.452+0000] {spark_submit.py:495} INFO - Bloom filter expected number of distinct values are: null
[2023-01-31T14:39:45.453+0000] {spark_submit.py:495} INFO - Page row count limit to 20000
[2023-01-31T14:39:45.453+0000] {spark_submit.py:495} INFO - Writing page checksums is: on
[2023-01-31T14:39:46.926+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:46 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-01-31T14:39:46.927+0000] {spark_submit.py:495} INFO - {
[2023-01-31T14:39:46.927+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-31T14:39:46.928+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-31T14:39:46.928+0000] {spark_submit.py:495} INFO - "name" : "measured_at",
[2023-01-31T14:39:46.928+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T14:39:46.929+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T14:39:46.929+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T14:39:46.930+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T14:39:46.930+0000] {spark_submit.py:495} INFO - "name" : "created_at",
[2023-01-31T14:39:46.974+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T14:39:46.974+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T14:39:46.975+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T14:39:46.975+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T14:39:46.975+0000] {spark_submit.py:495} INFO - "name" : "Biomass - Actual Aggregated",
[2023-01-31T14:39:46.976+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T14:39:46.976+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T14:39:46.977+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T14:39:46.977+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T14:39:46.977+0000] {spark_submit.py:495} INFO - "name" : "Fossil Brown coal/Lignite - Actual Aggregated",
[2023-01-31T14:39:46.978+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T14:39:46.978+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T14:39:46.978+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T14:39:46.979+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T14:39:46.979+0000] {spark_submit.py:495} INFO - "name" : "Fossil Gas - Actual Aggregated",
[2023-01-31T14:39:46.980+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T14:39:46.980+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T14:39:46.980+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T14:39:46.981+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T14:39:46.981+0000] {spark_submit.py:495} INFO - "name" : "Fossil Hard coal - Actual Aggregated",
[2023-01-31T14:39:46.981+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T14:39:46.982+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T14:39:46.982+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T14:39:46.982+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T14:39:46.983+0000] {spark_submit.py:495} INFO - "name" : "Fossil Oil - Actual Aggregated",
[2023-01-31T14:39:46.983+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T14:39:46.983+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T14:39:46.985+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T14:39:46.985+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T14:39:46.986+0000] {spark_submit.py:495} INFO - "name" : "Geothermal - Actual Aggregated",
[2023-01-31T14:39:46.986+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T14:39:46.986+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T14:39:46.986+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T14:39:46.987+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T14:39:46.987+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Aggregated",
[2023-01-31T14:39:46.987+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T14:39:46.988+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T14:39:46.988+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T14:39:46.988+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T14:39:46.989+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Consumption",
[2023-01-31T14:39:46.989+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T14:39:46.989+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T14:39:46.990+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T14:39:46.990+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T14:39:46.991+0000] {spark_submit.py:495} INFO - "name" : "Hydro Run-of-river and poundage - Actual Aggregated",
[2023-01-31T14:39:46.991+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T14:39:46.991+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T14:39:46.992+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T14:39:46.992+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T14:39:46.992+0000] {spark_submit.py:495} INFO - "name" : "Hydro Water Reservoir - Actual Aggregated",
[2023-01-31T14:39:46.993+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T14:39:46.993+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T14:39:46.993+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T14:39:46.994+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T14:39:46.994+0000] {spark_submit.py:495} INFO - "name" : "Nuclear - Actual Aggregated",
[2023-01-31T14:39:47.055+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T14:39:47.056+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T14:39:47.056+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T14:39:47.056+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T14:39:47.057+0000] {spark_submit.py:495} INFO - "name" : "Other - Actual Aggregated",
[2023-01-31T14:39:47.057+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T14:39:47.057+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T14:39:47.058+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T14:39:47.099+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T14:39:47.099+0000] {spark_submit.py:495} INFO - "name" : "Other renewable - Actual Aggregated",
[2023-01-31T14:39:47.150+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T14:39:47.151+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T14:39:47.151+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T14:39:47.152+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T14:39:47.152+0000] {spark_submit.py:495} INFO - "name" : "Solar - Actual Aggregated",
[2023-01-31T14:39:47.152+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T14:39:47.153+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T14:39:47.153+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T14:39:47.153+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T14:39:47.154+0000] {spark_submit.py:495} INFO - "name" : "Waste - Actual Aggregated",
[2023-01-31T14:39:47.230+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T14:39:47.259+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T14:39:47.260+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T14:39:47.260+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T14:39:47.261+0000] {spark_submit.py:495} INFO - "name" : "Wind Offshore - Actual Aggregated",
[2023-01-31T14:39:47.261+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T14:39:47.261+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T14:39:47.262+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T14:39:47.262+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T14:39:47.320+0000] {spark_submit.py:495} INFO - "name" : "Wind Onshore - Actual Aggregated",
[2023-01-31T14:39:47.330+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T14:39:47.339+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T14:39:47.340+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T14:39:47.340+0000] {spark_submit.py:495} INFO - } ]
[2023-01-31T14:39:47.341+0000] {spark_submit.py:495} INFO - }
[2023-01-31T14:39:47.350+0000] {spark_submit.py:495} INFO - and corresponding Parquet message type:
[2023-01-31T14:39:47.351+0000] {spark_submit.py:495} INFO - message spark_schema {
[2023-01-31T14:39:47.351+0000] {spark_submit.py:495} INFO - required int96 measured_at;
[2023-01-31T14:39:47.351+0000] {spark_submit.py:495} INFO - required int96 created_at;
[2023-01-31T14:39:47.352+0000] {spark_submit.py:495} INFO - optional double Biomass - Actual Aggregated;
[2023-01-31T14:39:47.352+0000] {spark_submit.py:495} INFO - optional double Fossil Brown coal/Lignite - Actual Aggregated;
[2023-01-31T14:39:47.352+0000] {spark_submit.py:495} INFO - optional double Fossil Gas - Actual Aggregated;
[2023-01-31T14:39:47.353+0000] {spark_submit.py:495} INFO - optional double Fossil Hard coal - Actual Aggregated;
[2023-01-31T14:39:47.353+0000] {spark_submit.py:495} INFO - optional double Fossil Oil - Actual Aggregated;
[2023-01-31T14:39:47.389+0000] {spark_submit.py:495} INFO - optional double Geothermal - Actual Aggregated;
[2023-01-31T14:39:47.390+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Aggregated;
[2023-01-31T14:39:47.390+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Consumption;
[2023-01-31T14:39:47.403+0000] {spark_submit.py:495} INFO - optional double Hydro Run-of-river and poundage - Actual Aggregated;
[2023-01-31T14:39:47.404+0000] {spark_submit.py:495} INFO - optional double Hydro Water Reservoir - Actual Aggregated;
[2023-01-31T14:39:47.411+0000] {spark_submit.py:495} INFO - optional double Nuclear - Actual Aggregated;
[2023-01-31T14:39:47.411+0000] {spark_submit.py:495} INFO - optional double Other - Actual Aggregated;
[2023-01-31T14:39:47.412+0000] {spark_submit.py:495} INFO - optional double Other renewable - Actual Aggregated;
[2023-01-31T14:39:47.432+0000] {spark_submit.py:495} INFO - optional double Solar - Actual Aggregated;
[2023-01-31T14:39:47.433+0000] {spark_submit.py:495} INFO - optional double Waste - Actual Aggregated;
[2023-01-31T14:39:47.433+0000] {spark_submit.py:495} INFO - optional double Wind Offshore - Actual Aggregated;
[2023-01-31T14:39:47.465+0000] {spark_submit.py:495} INFO - optional double Wind Onshore - Actual Aggregated;
[2023-01-31T14:39:47.465+0000] {spark_submit.py:495} INFO - }
[2023-01-31T14:39:47.466+0000] {spark_submit.py:495} INFO - 
[2023-01-31T14:39:47.479+0000] {spark_submit.py:495} INFO - 
[2023-01-31T14:39:48.859+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:48 INFO Executor: Told to re-register on heartbeat
[2023-01-31T14:39:48.860+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:48 INFO BlockManager: BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None) re-registering with master
[2023-01-31T14:39:48.861+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:48 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None)
[2023-01-31T14:39:48.864+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:48 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None)
[2023-01-31T14:39:48.865+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:48 INFO BlockManager: Reporting 44 blocks to the master.
[2023-01-31T14:39:48.865+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:48 INFO BlockManagerInfo: Updated broadcast_20_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:48.867+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:48 INFO BlockManagerInfo: Updated broadcast_24_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:48.871+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:48 INFO BlockManagerInfo: Updated broadcast_18_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:48.876+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:48 INFO BlockManagerInfo: Updated broadcast_29_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:48.876+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:48 INFO BlockManagerInfo: Updated broadcast_23_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:48.884+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:48 INFO BlockManagerInfo: Updated broadcast_25_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:48.888+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:48 INFO BlockManagerInfo: Updated broadcast_30_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:48.890+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:48 INFO BlockManagerInfo: Updated broadcast_28_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:48.892+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:48 INFO BlockManagerInfo: Updated broadcast_17_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:48.893+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:48 INFO BlockManagerInfo: Updated broadcast_22_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:48.895+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:48 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T14:39:48.899+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:48 INFO BlockManagerInfo: Updated broadcast_21_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:48.900+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:48 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T14:39:48.901+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:48 INFO BlockManagerInfo: Updated broadcast_32_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:48.903+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:48 INFO BlockManagerInfo: Updated broadcast_27_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:48.905+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:48 INFO BlockManagerInfo: Updated broadcast_31_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:48.913+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:48 INFO BlockManagerInfo: Updated broadcast_26_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:48.928+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:48 INFO BlockManagerInfo: Updated broadcast_34_piece0 in memory on b37fe3cbf330:38267 (current size: 204.0 B, original size: 204.0 B, free: 434.1 MiB)
[2023-01-31T14:39:48.936+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:48 INFO BlockManagerInfo: Updated broadcast_35_piece0 in memory on b37fe3cbf330:38267 (current size: 74.3 KiB, original size: 74.3 KiB, free: 434.1 MiB)
[2023-01-31T14:39:48.937+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:48 INFO BlockManagerInfo: Updated broadcast_33_piece0 in memory on b37fe3cbf330:38267 (current size: 10.8 KiB, original size: 10.8 KiB, free: 434.1 MiB)
[2023-01-31T14:39:48.938+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:48 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T14:39:48.939+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:48 INFO BlockManagerInfo: Updated broadcast_19_piece0 in memory on b37fe3cbf330:38267 (current size: 8.1 KiB, original size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:50.028+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:50 INFO CodecPool: Got brand-new compressor [.snappy]
[2023-01-31T14:39:51.966+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:51 INFO BlockManagerInfo: Removed broadcast_28_piece0 on b37fe3cbf330:38267 in memory (size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:52.075+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:52 INFO BlockManagerInfo: Removed broadcast_18_piece0 on b37fe3cbf330:38267 in memory (size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:52.473+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:52 INFO BlockManagerInfo: Removed broadcast_29_piece0 on b37fe3cbf330:38267 in memory (size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:52.613+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:52 INFO BlockManagerInfo: Removed broadcast_23_piece0 on b37fe3cbf330:38267 in memory (size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:52.711+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:52 INFO BlockManagerInfo: Removed broadcast_21_piece0 on b37fe3cbf330:38267 in memory (size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:52.864+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:52 INFO BlockManagerInfo: Removed broadcast_19_piece0 on b37fe3cbf330:38267 in memory (size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:53.232+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:53 INFO BlockManagerInfo: Removed broadcast_17_piece0 on b37fe3cbf330:38267 in memory (size: 8.1 KiB, free: 434.1 MiB)
[2023-01-31T14:39:54.278+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:54 INFO BlockManagerInfo: Removed broadcast_32_piece0 on b37fe3cbf330:38267 in memory (size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:54.401+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:54 INFO BlockManagerInfo: Removed broadcast_30_piece0 on b37fe3cbf330:38267 in memory (size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:54.547+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:54 INFO BlockManagerInfo: Removed broadcast_4_piece0 on b37fe3cbf330:38267 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T14:39:54.752+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:54 INFO BlockManagerInfo: Removed broadcast_2_piece0 on b37fe3cbf330:38267 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T14:39:55.176+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:55 INFO BlockManagerInfo: Removed broadcast_20_piece0 on b37fe3cbf330:38267 in memory (size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:55.754+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:55 INFO BlockManagerInfo: Removed broadcast_22_piece0 on b37fe3cbf330:38267 in memory (size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T14:39:56.291+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:56 INFO BlockManagerInfo: Removed broadcast_24_piece0 on b37fe3cbf330:38267 in memory (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T14:39:56.541+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:56 INFO BlockManagerInfo: Removed broadcast_27_piece0 on b37fe3cbf330:38267 in memory (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T14:39:56.715+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:56 INFO BlockManagerInfo: Removed broadcast_31_piece0 on b37fe3cbf330:38267 in memory (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T14:39:57.034+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:57 INFO BlockManagerInfo: Removed broadcast_25_piece0 on b37fe3cbf330:38267 in memory (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T14:39:57.314+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:57 INFO BlockManagerInfo: Removed broadcast_26_piece0 on b37fe3cbf330:38267 in memory (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T14:39:58.863+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:58 INFO Executor: Told to re-register on heartbeat
[2023-01-31T14:39:58.865+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:58 INFO BlockManager: BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None) re-registering with master
[2023-01-31T14:39:58.865+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:58 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None)
[2023-01-31T14:39:58.867+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:58 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None)
[2023-01-31T14:39:58.867+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:58 INFO BlockManager: Reporting 8 blocks to the master.
[2023-01-31T14:39:58.869+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:58 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T14:39:58.878+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:58 INFO BlockManagerInfo: Updated broadcast_34_piece0 in memory on b37fe3cbf330:38267 (current size: 204.0 B, original size: 204.0 B, free: 434.3 MiB)
[2023-01-31T14:39:58.881+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:58 INFO BlockManagerInfo: Updated broadcast_35_piece0 in memory on b37fe3cbf330:38267 (current size: 74.3 KiB, original size: 74.3 KiB, free: 434.3 MiB)
[2023-01-31T14:39:58.885+0000] {spark_submit.py:495} INFO - 23/01/31 14:39:58 INFO BlockManagerInfo: Updated broadcast_33_piece0 in memory on b37fe3cbf330:38267 (current size: 10.8 KiB, original size: 10.8 KiB, free: 434.3 MiB)
[2023-01-31T14:40:08.880+0000] {spark_submit.py:495} INFO - 23/01/31 14:40:08 INFO Executor: Told to re-register on heartbeat
[2023-01-31T14:40:08.883+0000] {spark_submit.py:495} INFO - 23/01/31 14:40:08 INFO BlockManager: BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None) re-registering with master
[2023-01-31T14:40:08.891+0000] {spark_submit.py:495} INFO - 23/01/31 14:40:08 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None)
[2023-01-31T14:40:08.894+0000] {spark_submit.py:495} INFO - 23/01/31 14:40:08 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None)
[2023-01-31T14:40:08.900+0000] {spark_submit.py:495} INFO - 23/01/31 14:40:08 INFO BlockManager: Reporting 8 blocks to the master.
[2023-01-31T14:40:08.913+0000] {spark_submit.py:495} INFO - 23/01/31 14:40:08 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T14:40:08.916+0000] {spark_submit.py:495} INFO - 23/01/31 14:40:08 INFO BlockManagerInfo: Updated broadcast_34_piece0 in memory on b37fe3cbf330:38267 (current size: 204.0 B, original size: 204.0 B, free: 434.3 MiB)
[2023-01-31T14:40:08.945+0000] {spark_submit.py:495} INFO - 23/01/31 14:40:08 INFO BlockManagerInfo: Updated broadcast_35_piece0 in memory on b37fe3cbf330:38267 (current size: 74.3 KiB, original size: 74.3 KiB, free: 434.3 MiB)
[2023-01-31T14:40:09.002+0000] {spark_submit.py:495} INFO - 23/01/31 14:40:08 INFO BlockManagerInfo: Updated broadcast_33_piece0 in memory on b37fe3cbf330:38267 (current size: 10.8 KiB, original size: 10.8 KiB, free: 434.3 MiB)
[2023-01-31T14:40:10.554+0000] {spark_submit.py:495} INFO - 23/01/31 14:40:10 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675175324300-d3cff982-8a74-4384-b72c-8fc3b4165c5d/_temporary/0/_temporary/' directory.
[2023-01-31T14:40:10.555+0000] {spark_submit.py:495} INFO - 23/01/31 14:40:10 INFO FileOutputCommitter: Saved output of task 'attempt_202301311439368428457758485571999_0027_m_000000_26' to gs://entsoe_temp_1009/.spark-bigquery-local-1675175324300-d3cff982-8a74-4384-b72c-8fc3b4165c5d/_temporary/0/task_202301311439368428457758485571999_0027_m_000000
[2023-01-31T14:40:10.766+0000] {spark_submit.py:495} INFO - 23/01/31 14:40:10 INFO SparkHadoopMapRedUtil: attempt_202301311439368428457758485571999_0027_m_000000_26: Committed. Elapsed time: 2991 ms.
[2023-01-31T14:40:11.010+0000] {spark_submit.py:495} INFO - 23/01/31 14:40:10 INFO Executor: Finished task 0.0 in stage 27.0 (TID 26). 2588 bytes result sent to driver
[2023-01-31T14:40:11.018+0000] {spark_submit.py:495} INFO - 23/01/31 14:40:11 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 26) in 32987 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T14:40:11.019+0000] {spark_submit.py:495} INFO - 23/01/31 14:40:11 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool
[2023-01-31T14:40:11.019+0000] {spark_submit.py:495} INFO - 23/01/31 14:40:11 INFO DAGScheduler: ResultStage 27 (save at BigQueryWriteHelper.java:105) finished in 33.377 s
[2023-01-31T14:40:11.019+0000] {spark_submit.py:495} INFO - 23/01/31 14:40:11 INFO DAGScheduler: Job 26 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T14:40:11.020+0000] {spark_submit.py:495} INFO - 23/01/31 14:40:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 27: Stage finished
[2023-01-31T14:40:11.026+0000] {spark_submit.py:495} INFO - 23/01/31 14:40:11 INFO DAGScheduler: Job 26 finished: save at BigQueryWriteHelper.java:105, took 33.554695 s
[2023-01-31T14:40:11.140+0000] {spark_submit.py:495} INFO - 23/01/31 14:40:11 INFO FileFormatWriter: Start to commit write Job b6fe327c-6265-410d-90ed-282f88e0b8e2.
[2023-01-31T14:40:12.644+0000] {spark_submit.py:495} INFO - 23/01/31 14:40:12 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675175324300-d3cff982-8a74-4384-b72c-8fc3b4165c5d/_temporary/0/task_202301311439368428457758485571999_0027_m_000000/' directory.
[2023-01-31T14:40:13.888+0000] {spark_submit.py:495} INFO - 23/01/31 14:40:13 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675175324300-d3cff982-8a74-4384-b72c-8fc3b4165c5d/' directory.
[2023-01-31T14:40:15.342+0000] {spark_submit.py:495} INFO - 23/01/31 14:40:15 INFO FileFormatWriter: Write Job b6fe327c-6265-410d-90ed-282f88e0b8e2 committed. Elapsed time: 4200 ms.
[2023-01-31T14:40:15.353+0000] {spark_submit.py:495} INFO - 23/01/31 14:40:15 INFO FileFormatWriter: Finished processing stats for write job b6fe327c-6265-410d-90ed-282f88e0b8e2.
[2023-01-31T14:40:15.399+0000] {spark_submit.py:495} INFO - 23/01/31 14:40:15 INFO BlockManagerInfo: Removed broadcast_33_piece0 on b37fe3cbf330:38267 in memory (size: 10.8 KiB, free: 434.3 MiB)
[2023-01-31T14:40:15.557+0000] {spark_submit.py:495} INFO - 23/01/31 14:40:15 INFO BlockManagerInfo: Removed broadcast_35_piece0 on b37fe3cbf330:38267 in memory (size: 74.3 KiB, free: 434.4 MiB)
[2023-01-31T14:40:18.555+0000] {spark_submit.py:495} INFO - 23/01/31 14:40:18 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=entsoe_analytics_1009, projectId=rafzul-analytics-1009, tableId=TEST_total_generation_staging}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=null, ignoreUnknownValue=null, sourceUris=[gs://entsoe_temp_1009/.spark-bigquery-local-1675175324300-d3cff982-8a74-4384-b72c-8fc3b4165c5d/part-00000-480e5cc5-6582-4bc7-99b4-3c793031c505-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=true, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=rafzul-analytics-1009, job=c634f85e-fac7-4af4-aa00-328705a5cae6, location=US}
[2023-01-31T14:40:18.851+0000] {spark_submit.py:495} INFO - 23/01/31 14:40:18 INFO Executor: Told to re-register on heartbeat
[2023-01-31T14:40:18.853+0000] {spark_submit.py:495} INFO - 23/01/31 14:40:18 INFO BlockManager: BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None) re-registering with master
[2023-01-31T14:40:18.853+0000] {spark_submit.py:495} INFO - 23/01/31 14:40:18 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None)
[2023-01-31T14:40:18.854+0000] {spark_submit.py:495} INFO - 23/01/31 14:40:18 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None)
[2023-01-31T14:40:18.855+0000] {spark_submit.py:495} INFO - 23/01/31 14:40:18 INFO BlockManager: Reporting 4 blocks to the master.
[2023-01-31T14:40:18.861+0000] {spark_submit.py:495} INFO - 23/01/31 14:40:18 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T14:40:18.870+0000] {spark_submit.py:495} INFO - 23/01/31 14:40:18 INFO BlockManagerInfo: Updated broadcast_34_piece0 in memory on b37fe3cbf330:38267 (current size: 204.0 B, original size: 204.0 B, free: 434.4 MiB)
[2023-01-31T14:40:22.790+0000] {spark_submit.py:495} INFO - 23/01/31 14:40:22 INFO BigQueryClient: Done loading to rafzul-analytics-1009.entsoe_analytics_1009.TEST_total_generation_staging. jobId: JobId{project=rafzul-analytics-1009, job=c634f85e-fac7-4af4-aa00-328705a5cae6, location=US}
[2023-01-31T14:40:25.192+0000] {spark_submit.py:495} INFO - MANTEP!!!!
[2023-01-31T14:40:29.186+0000] {spark_submit.py:495} INFO - 23/01/31 14:40:29 INFO Executor: Told to re-register on heartbeat
[2023-01-31T14:40:29.192+0000] {spark_submit.py:495} INFO - 23/01/31 14:40:29 INFO BlockManager: BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None) re-registering with master
[2023-01-31T14:40:29.193+0000] {spark_submit.py:495} INFO - 23/01/31 14:40:29 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None)
[2023-01-31T14:40:29.193+0000] {spark_submit.py:495} INFO - 23/01/31 14:40:29 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, b37fe3cbf330, 38267, None)
[2023-01-31T14:40:29.194+0000] {spark_submit.py:495} INFO - 23/01/31 14:40:29 INFO BlockManager: Reporting 4 blocks to the master.
[2023-01-31T14:40:29.207+0000] {spark_submit.py:495} INFO - 23/01/31 14:40:29 INFO BlockManagerInfo: Updated broadcast_12_piece0 in memory on b37fe3cbf330:38267 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T14:40:29.209+0000] {spark_submit.py:495} INFO - 23/01/31 14:40:29 INFO BlockManagerInfo: Updated broadcast_34_piece0 in memory on b37fe3cbf330:38267 (current size: 204.0 B, original size: 204.0 B, free: 434.4 MiB)
[2023-01-31T14:40:29.888+0000] {spark_submit.py:495} INFO - 23/01/31 14:40:29 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-31T14:40:32.979+0000] {spark_submit.py:495} INFO - 23/01/31 14:40:32 INFO SparkUI: Stopped Spark web UI at http://b37fe3cbf330:4044
[2023-01-31T14:40:36.030+0000] {spark_submit.py:495} INFO - 23/01/31 14:40:36 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-31T14:40:46.886+0000] {spark_submit.py:495} INFO - 23/01/31 14:40:46 INFO MemoryStore: MemoryStore cleared
[2023-01-31T14:40:47.128+0000] {spark_submit.py:495} INFO - 23/01/31 14:40:46 INFO BlockManager: BlockManager stopped
[2023-01-31T14:40:47.136+0000] {spark_submit.py:495} INFO - 23/01/31 14:40:47 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-31T14:40:47.152+0000] {spark_submit.py:495} INFO - 23/01/31 14:40:47 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-31T14:40:47.701+0000] {spark_submit.py:495} INFO - 23/01/31 14:40:47 INFO SparkContext: Successfully stopped SparkContext
[2023-01-31T14:40:47.703+0000] {spark_submit.py:495} INFO - 23/01/31 14:40:47 INFO ShutdownHookManager: Shutdown hook called
[2023-01-31T14:40:47.714+0000] {spark_submit.py:495} INFO - 23/01/31 14:40:47 INFO ShutdownHookManager: Deleting directory /tmp/spark-95bacd20-a0ef-43f9-89f1-b564aa4dcb50/pyspark-f026e133-176c-489b-bfc7-b833e77dce43
[2023-01-31T14:40:48.110+0000] {spark_submit.py:495} INFO - 23/01/31 14:40:48 INFO ShutdownHookManager: Deleting directory /tmp/spark-dce0c331-1da0-485b-acba-93ac10969b51
[2023-01-31T14:40:48.220+0000] {spark_submit.py:495} INFO - 23/01/31 14:40:48 INFO ShutdownHookManager: Deleting directory /tmp/spark-95bacd20-a0ef-43f9-89f1-b564aa4dcb50
[2023-01-31T14:40:48.951+0000] {taskinstance.py:1322} INFO - Marking task as SUCCESS. dag_id=entsoe-energydata-backfill, task_id=stage_total_generation, execution_date=20210101T060000, start_date=20230131T142810, end_date=20230131T144048
[2023-01-31T14:40:49.024+0000] {local_task_job.py:159} INFO - Task exited with return code 0
[2023-01-31T14:40:49.072+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-02-08T01:27:46.966+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [queued]>
[2023-02-08T01:27:46.990+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [queued]>
[2023-02-08T01:27:46.990+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-02-08T01:27:46.990+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-02-08T01:27:46.991+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-02-08T01:27:47.013+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 06:00:00+00:00
[2023-02-08T01:27:47.028+0000] {standard_task_runner.py:55} INFO - Started process 3343 to run task
[2023-02-08T01:27:47.036+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T06:00:00+00:00', '--job-id', '1109', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpd97jpm3y']
[2023-02-08T01:27:47.042+0000] {standard_task_runner.py:83} INFO - Job 1109: Subtask stage_total_generation
[2023-02-08T01:27:47.226+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [running]> on host 4e853e3e6d16
[2023-02-08T01:27:47.404+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T06:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T06:00:00+00:00
[2023-02-08T01:27:47.421+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-02-08T01:27:47.424+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010600 202101010700 DE_TENNET 2021-01-01 00:00:00 2021-01-01 06:00:00
[2023-02-08T01:27:53.726+0000] {spark_submit.py:495} INFO - mulai periode: 2021-01-01 00:00:00
[2023-02-08T01:27:53.726+0000] {spark_submit.py:495} INFO - selesai periode: 2021-01-01 06:00:00
[2023-02-08T01:27:53.928+0000] {spark_submit.py:495} INFO - 23/02/08 01:27:53 INFO SparkContext: Running Spark version 3.3.1
[2023-02-08T01:27:54.054+0000] {spark_submit.py:495} INFO - 23/02/08 01:27:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-02-08T01:27:54.197+0000] {spark_submit.py:495} INFO - 23/02/08 01:27:54 INFO ResourceUtils: ==============================================================
[2023-02-08T01:27:54.198+0000] {spark_submit.py:495} INFO - 23/02/08 01:27:54 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-02-08T01:27:54.198+0000] {spark_submit.py:495} INFO - 23/02/08 01:27:54 INFO ResourceUtils: ==============================================================
[2023-02-08T01:27:54.199+0000] {spark_submit.py:495} INFO - 23/02/08 01:27:54 INFO SparkContext: Submitted application: gcp_playground
[2023-02-08T01:27:54.226+0000] {spark_submit.py:495} INFO - 23/02/08 01:27:54 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-02-08T01:27:54.232+0000] {spark_submit.py:495} INFO - 23/02/08 01:27:54 INFO ResourceProfile: Limiting resource is cpu
[2023-02-08T01:27:54.234+0000] {spark_submit.py:495} INFO - 23/02/08 01:27:54 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-02-08T01:27:54.306+0000] {spark_submit.py:495} INFO - 23/02/08 01:27:54 INFO SecurityManager: Changing view acls to: ***
[2023-02-08T01:27:54.308+0000] {spark_submit.py:495} INFO - 23/02/08 01:27:54 INFO SecurityManager: Changing modify acls to: ***
[2023-02-08T01:27:54.309+0000] {spark_submit.py:495} INFO - 23/02/08 01:27:54 INFO SecurityManager: Changing view acls groups to:
[2023-02-08T01:27:54.310+0000] {spark_submit.py:495} INFO - 23/02/08 01:27:54 INFO SecurityManager: Changing modify acls groups to:
[2023-02-08T01:27:54.312+0000] {spark_submit.py:495} INFO - 23/02/08 01:27:54 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-02-08T01:27:54.802+0000] {spark_submit.py:495} INFO - 23/02/08 01:27:54 INFO Utils: Successfully started service 'sparkDriver' on port 42485.
[2023-02-08T01:27:54.839+0000] {spark_submit.py:495} INFO - 23/02/08 01:27:54 INFO SparkEnv: Registering MapOutputTracker
[2023-02-08T01:27:54.910+0000] {spark_submit.py:495} INFO - 23/02/08 01:27:54 INFO SparkEnv: Registering BlockManagerMaster
[2023-02-08T01:27:55.032+0000] {spark_submit.py:495} INFO - 23/02/08 01:27:55 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-02-08T01:27:55.038+0000] {spark_submit.py:495} INFO - 23/02/08 01:27:55 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-02-08T01:27:55.058+0000] {spark_submit.py:495} INFO - 23/02/08 01:27:55 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-02-08T01:27:55.124+0000] {spark_submit.py:495} INFO - 23/02/08 01:27:55 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-950b8c27-a326-4961-8e7b-2628ed82f9de
[2023-02-08T01:27:55.171+0000] {spark_submit.py:495} INFO - 23/02/08 01:27:55 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-02-08T01:27:55.238+0000] {spark_submit.py:495} INFO - 23/02/08 01:27:55 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-02-08T01:27:55.625+0000] {spark_submit.py:495} INFO - 23/02/08 01:27:55 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-02-08T01:27:55.699+0000] {spark_submit.py:495} INFO - 23/02/08 01:27:55 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://4e853e3e6d16:42485/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675819673914
[2023-02-08T01:27:55.700+0000] {spark_submit.py:495} INFO - 23/02/08 01:27:55 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://4e853e3e6d16:42485/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675819673914
[2023-02-08T01:27:55.825+0000] {spark_submit.py:495} INFO - 23/02/08 01:27:55 INFO Executor: Starting executor ID driver on host 4e853e3e6d16
[2023-02-08T01:27:55.835+0000] {spark_submit.py:495} INFO - 23/02/08 01:27:55 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-02-08T01:27:55.865+0000] {spark_submit.py:495} INFO - 23/02/08 01:27:55 INFO Executor: Fetching spark://4e853e3e6d16:42485/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675819673914
[2023-02-08T01:27:56.019+0000] {spark_submit.py:495} INFO - 23/02/08 01:27:56 INFO TransportClientFactory: Successfully created connection to 4e853e3e6d16/172.20.0.4:42485 after 95 ms (0 ms spent in bootstraps)
[2023-02-08T01:27:56.034+0000] {spark_submit.py:495} INFO - 23/02/08 01:27:56 INFO Utils: Fetching spark://4e853e3e6d16:42485/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-a643f247-bf04-4f23-a6a4-3be0279a13ee/userFiles-a8097bbe-3523-4eb1-9915-c94edb78b8e5/fetchFileTemp10938932477757705166.tmp
[2023-02-08T01:27:56.308+0000] {spark_submit.py:495} INFO - 23/02/08 01:27:56 INFO Executor: Adding file:/tmp/spark-a643f247-bf04-4f23-a6a4-3be0279a13ee/userFiles-a8097bbe-3523-4eb1-9915-c94edb78b8e5/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-02-08T01:27:56.309+0000] {spark_submit.py:495} INFO - 23/02/08 01:27:56 INFO Executor: Fetching spark://4e853e3e6d16:42485/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675819673914
[2023-02-08T01:27:56.311+0000] {spark_submit.py:495} INFO - 23/02/08 01:27:56 INFO Utils: Fetching spark://4e853e3e6d16:42485/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-a643f247-bf04-4f23-a6a4-3be0279a13ee/userFiles-a8097bbe-3523-4eb1-9915-c94edb78b8e5/fetchFileTemp27710552682488692.tmp
[2023-02-08T01:27:56.488+0000] {spark_submit.py:495} INFO - 23/02/08 01:27:56 INFO Executor: Adding file:/tmp/spark-a643f247-bf04-4f23-a6a4-3be0279a13ee/userFiles-a8097bbe-3523-4eb1-9915-c94edb78b8e5/gcs-connector-hadoop3-latest.jar to class loader
[2023-02-08T01:27:56.503+0000] {spark_submit.py:495} INFO - 23/02/08 01:27:56 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39783.
[2023-02-08T01:27:56.504+0000] {spark_submit.py:495} INFO - 23/02/08 01:27:56 INFO NettyBlockTransferService: Server created on 4e853e3e6d16:39783
[2023-02-08T01:27:56.508+0000] {spark_submit.py:495} INFO - 23/02/08 01:27:56 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-02-08T01:27:56.522+0000] {spark_submit.py:495} INFO - 23/02/08 01:27:56 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 4e853e3e6d16, 39783, None)
[2023-02-08T01:27:56.529+0000] {spark_submit.py:495} INFO - 23/02/08 01:27:56 INFO BlockManagerMasterEndpoint: Registering block manager 4e853e3e6d16:39783 with 434.4 MiB RAM, BlockManagerId(driver, 4e853e3e6d16, 39783, None)
[2023-02-08T01:27:56.535+0000] {spark_submit.py:495} INFO - 23/02/08 01:27:56 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 4e853e3e6d16, 39783, None)
[2023-02-08T01:27:56.537+0000] {spark_submit.py:495} INFO - 23/02/08 01:27:56 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 4e853e3e6d16, 39783, None)
[2023-02-08T01:27:57.893+0000] {spark_submit.py:495} INFO - 23/02/08 01:27:57 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-02-08T01:27:57.905+0000] {spark_submit.py:495} INFO - 23/02/08 01:27:57 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-02-08T01:28:01.009+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:01 INFO InMemoryFileIndex: It took 143 ms to list leaf files for 1 paths.
[2023-02-08T01:28:01.387+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:01 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-02-08T01:28:01.569+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:01 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-02-08T01:28:01.590+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:01 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 4e853e3e6d16:39783 (size: 34.6 KiB, free: 434.4 MiB)
[2023-02-08T01:28:01.602+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:01 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-02-08T01:28:02.881+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:02 INFO FileInputFormat: Total input files to process : 1
[2023-02-08T01:28:02.933+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:02 INFO FileInputFormat: Total input files to process : 1
[2023-02-08T01:28:02.958+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:02 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-02-08T01:28:02.986+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:02 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-02-08T01:28:02.987+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:02 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-02-08T01:28:02.989+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:02 INFO DAGScheduler: Parents of final stage: List()
[2023-02-08T01:28:02.993+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:02 INFO DAGScheduler: Missing parents: List()
[2023-02-08T01:28:03.006+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:03 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-02-08T01:28:03.145+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:03 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-02-08T01:28:03.153+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:03 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-02-08T01:28:03.155+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:03 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 4e853e3e6d16:39783 (size: 4.6 KiB, free: 434.4 MiB)
[2023-02-08T01:28:03.156+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:03 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-02-08T01:28:03.182+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-02-08T01:28:03.183+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:03 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-02-08T01:28:03.286+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:03 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (4e853e3e6d16, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-02-08T01:28:03.315+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:03 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-02-08T01:28:03.501+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:03 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010600__202101010700.json:0+9371
[2023-02-08T01:28:03.902+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:03 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3437 bytes result sent to driver
[2023-02-08T01:28:03.917+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:03 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 661 ms on 4e853e3e6d16 (executor driver) (1/1)
[2023-02-08T01:28:03.920+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:03 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-02-08T01:28:03.929+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:03 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 0.871 s
[2023-02-08T01:28:03.934+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:03 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-02-08T01:28:03.935+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-02-08T01:28:03.938+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:03 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 0.979995 s
[2023-02-08T01:28:04.622+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:04 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 4e853e3e6d16:39783 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-02-08T01:28:04.628+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:04 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 4e853e3e6d16:39783 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-02-08T01:28:09.460+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:09 INFO FileSourceStrategy: Pushed Filters:
[2023-02-08T01:28:09.462+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:09 INFO FileSourceStrategy: Post-Scan Filters:
[2023-02-08T01:28:09.464+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:09 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-02-08T01:28:10.130+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:10 INFO CodeGenerator: Code generated in 283.867136 ms
[2023-02-08T01:28:10.143+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:10 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-02-08T01:28:10.171+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:10 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-02-08T01:28:10.173+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:10 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 4e853e3e6d16:39783 (size: 34.5 KiB, free: 434.4 MiB)
[2023-02-08T01:28:10.175+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:10 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-02-08T01:28:10.196+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:10 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-02-08T01:28:10.288+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:10 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-02-08T01:28:10.289+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:10 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-02-08T01:28:10.290+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:10 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-02-08T01:28:10.290+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:10 INFO DAGScheduler: Parents of final stage: List()
[2023-02-08T01:28:10.291+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:10 INFO DAGScheduler: Missing parents: List()
[2023-02-08T01:28:10.292+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:10 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-02-08T01:28:10.303+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:10 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-02-08T01:28:10.311+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:10 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-02-08T01:28:10.312+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:10 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 4e853e3e6d16:39783 (size: 7.7 KiB, free: 434.4 MiB)
[2023-02-08T01:28:10.313+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:10 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-02-08T01:28:10.314+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-02-08T01:28:10.314+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:10 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-02-08T01:28:10.321+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:10 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (4e853e3e6d16, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-02-08T01:28:10.322+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:10 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-02-08T01:28:10.415+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:10 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-02-08T01:28:10.651+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:10 INFO CodeGenerator: Code generated in 196.398869 ms
[2023-02-08T01:28:10.968+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:10 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1628 bytes result sent to driver
[2023-02-08T01:28:10.973+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:10 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 656 ms on 4e853e3e6d16 (executor driver) (1/1)
[2023-02-08T01:28:10.974+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:10 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-02-08T01:28:10.977+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:10 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 0.678 s
[2023-02-08T01:28:10.978+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:10 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-02-08T01:28:10.979+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-02-08T01:28:10.981+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:10 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 0.690810 s
[2023-02-08T01:28:11.086+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:11 INFO FileSourceStrategy: Pushed Filters:
[2023-02-08T01:28:11.086+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:11 INFO FileSourceStrategy: Post-Scan Filters:
[2023-02-08T01:28:11.087+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:11 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-02-08T01:28:11.162+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:11 INFO CodeGenerator: Code generated in 53.127829 ms
[2023-02-08T01:28:11.183+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:11 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-02-08T01:28:11.237+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:11 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-02-08T01:28:11.239+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:11 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 4e853e3e6d16:39783 (size: 34.5 KiB, free: 434.3 MiB)
[2023-02-08T01:28:11.242+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:11 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-02-08T01:28:11.245+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:11 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-02-08T01:28:11.285+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:11 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-02-08T01:28:11.287+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:11 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-02-08T01:28:11.287+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:11 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-02-08T01:28:11.288+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:11 INFO DAGScheduler: Parents of final stage: List()
[2023-02-08T01:28:11.288+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:11 INFO DAGScheduler: Missing parents: List()
[2023-02-08T01:28:11.292+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:11 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-02-08T01:28:11.297+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:11 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-02-08T01:28:11.307+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:11 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-02-08T01:28:11.309+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:11 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 4e853e3e6d16:39783 (size: 7.7 KiB, free: 434.3 MiB)
[2023-02-08T01:28:11.317+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:11 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-02-08T01:28:11.317+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-02-08T01:28:11.319+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:11 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-02-08T01:28:11.322+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:11 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (4e853e3e6d16, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-02-08T01:28:11.325+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:11 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-02-08T01:28:11.342+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:11 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-02-08T01:28:11.495+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:11 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1585 bytes result sent to driver
[2023-02-08T01:28:11.503+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:11 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 181 ms on 4e853e3e6d16 (executor driver) (1/1)
[2023-02-08T01:28:11.505+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:11 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-02-08T01:28:11.508+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:11 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 0.215 s
[2023-02-08T01:28:11.508+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:11 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-02-08T01:28:11.509+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-02-08T01:28:11.511+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:11 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 0.223807 s
[2023-02-08T01:28:11.849+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:11 INFO FileSourceStrategy: Pushed Filters:
[2023-02-08T01:28:11.851+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:11 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-02-08T01:28:11.852+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:11 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-02-08T01:28:11.964+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:11 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 4e853e3e6d16:39783 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-02-08T01:28:11.975+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:11 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 4e853e3e6d16:39783 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-02-08T01:28:12.178+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:12 INFO CodeGenerator: Code generated in 184.166572 ms
[2023-02-08T01:28:12.189+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:12 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-02-08T01:28:12.222+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:12 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-02-08T01:28:12.223+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:12 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 4e853e3e6d16:39783 (size: 34.5 KiB, free: 434.3 MiB)
[2023-02-08T01:28:12.229+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:12 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-02-08T01:28:12.236+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-02-08T01:28:12.290+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:12 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-02-08T01:28:12.290+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:12 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-02-08T01:28:12.291+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:12 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-02-08T01:28:12.292+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:12 INFO DAGScheduler: Parents of final stage: List()
[2023-02-08T01:28:12.292+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:12 INFO DAGScheduler: Missing parents: List()
[2023-02-08T01:28:12.301+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:12 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-02-08T01:28:12.308+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:12 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.7 MiB)
[2023-02-08T01:28:12.312+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:12 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.7 MiB)
[2023-02-08T01:28:12.313+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:12 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 4e853e3e6d16:39783 (size: 12.7 KiB, free: 434.3 MiB)
[2023-02-08T01:28:12.314+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:12 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-02-08T01:28:12.319+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-02-08T01:28:12.319+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:12 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-02-08T01:28:12.320+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:12 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (4e853e3e6d16, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-02-08T01:28:12.321+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:12 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-02-08T01:28:12.339+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:12 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-02-08T01:28:12.517+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:12 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-02-08T01:28:12.555+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:12 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 229 ms on 4e853e3e6d16 (executor driver) (1/1)
[2023-02-08T01:28:12.556+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:12 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 0.251 s
[2023-02-08T01:28:12.556+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:12 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-02-08T01:28:12.557+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:12 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-02-08T01:28:12.562+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-02-08T01:28:12.566+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:12 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 0.275984 s
[2023-02-08T01:28:12.670+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:12 INFO FileSourceStrategy: Pushed Filters:
[2023-02-08T01:28:12.670+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:12 INFO FileSourceStrategy: Post-Scan Filters:
[2023-02-08T01:28:12.671+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:12 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-02-08T01:28:12.766+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:12 INFO CodeGenerator: Code generated in 68.068522 ms
[2023-02-08T01:28:12.787+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:12 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.5 MiB)
[2023-02-08T01:28:12.821+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:12 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-02-08T01:28:12.822+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:12 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 4e853e3e6d16:39783 (size: 34.5 KiB, free: 434.3 MiB)
[2023-02-08T01:28:12.822+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:12 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-02-08T01:28:12.825+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-02-08T01:28:12.910+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:12 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-02-08T01:28:12.914+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:12 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-02-08T01:28:12.915+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:12 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-02-08T01:28:12.915+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:12 INFO DAGScheduler: Parents of final stage: List()
[2023-02-08T01:28:12.915+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:12 INFO DAGScheduler: Missing parents: List()
[2023-02-08T01:28:12.918+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:12 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-02-08T01:28:12.939+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:12 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-02-08T01:28:12.941+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:12 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-02-08T01:28:12.943+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:12 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 4e853e3e6d16:39783 (size: 7.6 KiB, free: 434.2 MiB)
[2023-02-08T01:28:12.946+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:12 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-02-08T01:28:12.949+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-02-08T01:28:12.950+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:12 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-02-08T01:28:12.954+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:12 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (4e853e3e6d16, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-02-08T01:28:12.954+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:12 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-02-08T01:28:12.975+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:12 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-02-08T01:28:13.105+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:13 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1586 bytes result sent to driver
[2023-02-08T01:28:13.107+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:13 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 155 ms on 4e853e3e6d16 (executor driver) (1/1)
[2023-02-08T01:28:13.107+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:13 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-02-08T01:28:13.108+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:13 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 0.189 s
[2023-02-08T01:28:13.113+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:13 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-02-08T01:28:13.114+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-02-08T01:28:13.114+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:13 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 0.200565 s
[2023-02-08T01:28:13.654+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:13 INFO FileSourceStrategy: Pushed Filters:
[2023-02-08T01:28:13.656+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:13 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-02-08T01:28:13.657+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:13 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-02-08T01:28:13.986+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:13 INFO CodeGenerator: Code generated in 265.272311 ms
[2023-02-08T01:28:13.998+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:13 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.2 MiB)
[2023-02-08T01:28:14.043+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:14 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-02-08T01:28:14.050+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:14 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 4e853e3e6d16:39783 (size: 34.5 KiB, free: 434.2 MiB)
[2023-02-08T01:28:14.051+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:14 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:204
[2023-02-08T01:28:14.058+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:14 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-02-08T01:28:14.107+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:14 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:204
[2023-02-08T01:28:14.107+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:14 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:204) with 1 output partitions
[2023-02-08T01:28:14.108+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:14 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:204)
[2023-02-08T01:28:14.108+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:14 INFO DAGScheduler: Parents of final stage: List()
[2023-02-08T01:28:14.108+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:14 INFO DAGScheduler: Missing parents: List()
[2023-02-08T01:28:14.118+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:14 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:204), which has no missing parents
[2023-02-08T01:28:14.123+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:14 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.1 MiB)
[2023-02-08T01:28:14.130+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:14 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.1 MiB)
[2023-02-08T01:28:14.134+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:14 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 4e853e3e6d16:39783 (size: 13.4 KiB, free: 434.2 MiB)
[2023-02-08T01:28:14.139+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:14 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-02-08T01:28:14.139+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:204) (first 15 tasks are for partitions Vector(0))
[2023-02-08T01:28:14.140+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:14 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-02-08T01:28:14.150+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:14 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (4e853e3e6d16, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-02-08T01:28:14.151+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:14 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-02-08T01:28:14.176+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:14 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-02-08T01:28:14.397+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:14 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2499 bytes result sent to driver
[2023-02-08T01:28:14.402+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:14 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 254 ms on 4e853e3e6d16 (executor driver) (1/1)
[2023-02-08T01:28:14.402+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:14 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-02-08T01:28:14.415+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:14 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:204) finished in 0.295 s
[2023-02-08T01:28:14.416+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:14 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-02-08T01:28:14.418+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-02-08T01:28:14.418+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:14 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:204, took 0.306271 s
[2023-02-08T01:28:14.691+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:14 INFO FileSourceStrategy: Pushed Filters:
[2023-02-08T01:28:14.692+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:14 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-02-08T01:28:14.694+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:14 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-02-08T01:28:14.904+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:14 INFO CodeGenerator: Code generated in 146.03983 ms
[2023-02-08T01:28:14.918+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:14 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 432.9 MiB)
[2023-02-08T01:28:14.945+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:14 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.9 MiB)
[2023-02-08T01:28:14.947+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:14 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 4e853e3e6d16:39783 (size: 34.5 KiB, free: 434.2 MiB)
[2023-02-08T01:28:14.950+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:14 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:205
[2023-02-08T01:28:14.953+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:14 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-02-08T01:28:14.978+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:14 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:205
[2023-02-08T01:28:14.981+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:14 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:205) with 1 output partitions
[2023-02-08T01:28:14.981+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:14 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:205)
[2023-02-08T01:28:14.981+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:14 INFO DAGScheduler: Parents of final stage: List()
[2023-02-08T01:28:14.982+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:14 INFO DAGScheduler: Missing parents: List()
[2023-02-08T01:28:14.990+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:14 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:205), which has no missing parents
[2023-02-08T01:28:14.996+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:14 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 432.9 MiB)
[2023-02-08T01:28:15.001+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:15 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.8 MiB)
[2023-02-08T01:28:15.002+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:15 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 4e853e3e6d16:39783 (size: 12.6 KiB, free: 434.2 MiB)
[2023-02-08T01:28:15.003+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:15 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-02-08T01:28:15.005+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:205) (first 15 tasks are for partitions Vector(0))
[2023-02-08T01:28:15.006+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:15 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-02-08T01:28:15.009+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:15 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (4e853e3e6d16, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-02-08T01:28:15.014+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:15 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-02-08T01:28:15.034+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:15 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-02-08T01:28:15.169+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:15 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-02-08T01:28:15.171+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:15 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 163 ms on 4e853e3e6d16 (executor driver) (1/1)
[2023-02-08T01:28:15.171+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:15 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-02-08T01:28:15.173+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:15 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:205) finished in 0.181 s
[2023-02-08T01:28:15.173+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:15 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-02-08T01:28:15.174+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-02-08T01:28:15.174+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:15 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:205, took 0.194864 s
[2023-02-08T01:28:15.240+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:15 INFO FileSourceStrategy: Pushed Filters:
[2023-02-08T01:28:15.241+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:15 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-02-08T01:28:15.243+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:15 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-02-08T01:28:15.351+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:15 INFO CodeGenerator: Code generated in 74.130413 ms
[2023-02-08T01:28:15.359+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:15 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.7 MiB)
[2023-02-08T01:28:15.382+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:15 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.6 MiB)
[2023-02-08T01:28:15.383+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:15 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 4e853e3e6d16:39783 (size: 34.5 KiB, free: 434.1 MiB)
[2023-02-08T01:28:15.384+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:15 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:206
[2023-02-08T01:28:15.387+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-02-08T01:28:15.409+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:15 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:206
[2023-02-08T01:28:15.410+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:15 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:206) with 1 output partitions
[2023-02-08T01:28:15.411+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:15 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:206)
[2023-02-08T01:28:15.411+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:15 INFO DAGScheduler: Parents of final stage: List()
[2023-02-08T01:28:15.411+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:15 INFO DAGScheduler: Missing parents: List()
[2023-02-08T01:28:15.412+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:15 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:206), which has no missing parents
[2023-02-08T01:28:15.416+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:15 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.6 MiB)
[2023-02-08T01:28:15.418+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:15 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.6 MiB)
[2023-02-08T01:28:15.420+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:15 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 4e853e3e6d16:39783 (size: 12.6 KiB, free: 434.1 MiB)
[2023-02-08T01:28:15.421+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:15 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-02-08T01:28:15.421+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:206) (first 15 tasks are for partitions Vector(0))
[2023-02-08T01:28:15.422+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:15 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-02-08T01:28:15.428+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:15 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (4e853e3e6d16, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-02-08T01:28:15.430+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:15 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-02-08T01:28:15.456+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:15 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-02-08T01:28:15.582+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:15 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1729 bytes result sent to driver
[2023-02-08T01:28:15.585+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:15 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 157 ms on 4e853e3e6d16 (executor driver) (1/1)
[2023-02-08T01:28:15.586+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:15 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-02-08T01:28:15.589+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:15 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:206) finished in 0.173 s
[2023-02-08T01:28:15.590+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:15 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-02-08T01:28:15.591+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-02-08T01:28:15.592+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:15 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:206, took 0.179676 s
[2023-02-08T01:28:16.107+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:16 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 4e853e3e6d16:39783 in memory (size: 12.7 KiB, free: 434.1 MiB)
[2023-02-08T01:28:16.114+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:16 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 4e853e3e6d16:39783 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-02-08T01:28:16.133+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:16 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 4e853e3e6d16:39783 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-02-08T01:28:16.141+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:16 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 4e853e3e6d16:39783 in memory (size: 13.4 KiB, free: 434.2 MiB)
[2023-02-08T01:28:16.150+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:16 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 4e853e3e6d16:39783 in memory (size: 7.6 KiB, free: 434.2 MiB)
[2023-02-08T01:28:16.161+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:16 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 4e853e3e6d16:39783 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-02-08T01:28:16.174+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:16 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 4e853e3e6d16:39783 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-02-08T01:28:16.223+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:16 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 4e853e3e6d16:39783 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-02-08T01:28:16.297+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:16 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 4e853e3e6d16:39783 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-02-08T01:28:16.369+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:16 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 4e853e3e6d16:39783 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-02-08T01:28:18.476+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:18 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 4e853e3e6d16:39783 in memory (size: 34.5 KiB, free: 434.4 MiB)
[2023-02-08T01:28:21.568+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:21 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-02-08T01:28:21.596+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-02-08T01:28:21.597+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-02-08T01:28:21.599+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:21 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-02-08T01:28:21.600+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-02-08T01:28:21.600+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-02-08T01:28:21.610+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-02-08T01:28:22.265+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO CodeGenerator: Code generated in 23.120487 ms
[2023-02-08T01:28:22.362+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Registering RDD 118 (save at BigQueryWriteHelper.java:105) as input to shuffle 0
[2023-02-08T01:28:22.371+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Got map stage job 8 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-02-08T01:28:22.372+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Final stage: ShuffleMapStage 8 (save at BigQueryWriteHelper.java:105)
[2023-02-08T01:28:22.375+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Parents of final stage: List()
[2023-02-08T01:28:22.375+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Missing parents: List()
[2023-02-08T01:28:22.382+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Submitting ShuffleMapStage 8 (MapPartitionsRDD[118] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-02-08T01:28:22.394+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO CodeGenerator: Code generated in 28.334342 ms
[2023-02-08T01:28:22.451+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO CodeGenerator: Code generated in 28.843012 ms
[2023-02-08T01:28:22.466+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 11.1 KiB, free 434.2 MiB)
[2023-02-08T01:28:22.468+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 5.7 KiB, free 434.2 MiB)
[2023-02-08T01:28:22.469+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 4e853e3e6d16:39783 (size: 5.7 KiB, free: 434.4 MiB)
[2023-02-08T01:28:22.474+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513
[2023-02-08T01:28:22.475+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[118] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-02-08T01:28:22.475+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2023-02-08T01:28:22.507+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO CodeGenerator: Code generated in 14.783887 ms
[2023-02-08T01:28:22.507+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Registering RDD 120 (save at BigQueryWriteHelper.java:105) as input to shuffle 1
[2023-02-08T01:28:22.508+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Got map stage job 9 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-02-08T01:28:22.508+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Final stage: ShuffleMapStage 9 (save at BigQueryWriteHelper.java:105)
[2023-02-08T01:28:22.508+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Parents of final stage: List()
[2023-02-08T01:28:22.508+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Missing parents: List()
[2023-02-08T01:28:22.509+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (4e853e3e6d16, executor driver, partition 0, PROCESS_LOCAL, 7660 bytes) taskResourceAssignments Map()
[2023-02-08T01:28:22.509+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2023-02-08T01:28:22.536+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[120] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-02-08T01:28:22.578+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO CodeGenerator: Code generated in 33.76097 ms
[2023-02-08T01:28:22.625+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 15.6 KiB, free 434.1 MiB)
[2023-02-08T01:28:22.626+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO CodeGenerator: Code generated in 21.447462 ms
[2023-02-08T01:28:22.629+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 434.1 MiB)
[2023-02-08T01:28:22.631+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 4e853e3e6d16:39783 (size: 8.1 KiB, free: 434.4 MiB)
[2023-02-08T01:28:22.638+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1513
[2023-02-08T01:28:22.638+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[120] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-02-08T01:28:22.639+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
[2023-02-08T01:28:22.639+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Registering RDD 122 (save at BigQueryWriteHelper.java:105) as input to shuffle 2
[2023-02-08T01:28:22.641+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Got map stage job 10 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-02-08T01:28:22.641+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Final stage: ShuffleMapStage 10 (save at BigQueryWriteHelper.java:105)
[2023-02-08T01:28:22.641+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Parents of final stage: List()
[2023-02-08T01:28:22.642+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Missing parents: List()
[2023-02-08T01:28:22.665+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Submitting ShuffleMapStage 10 (MapPartitionsRDD[122] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-02-08T01:28:22.672+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO CodeGenerator: Code generated in 23.866206 ms
[2023-02-08T01:28:22.738+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 15.7 KiB, free 434.1 MiB)
[2023-02-08T01:28:22.764+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 434.1 MiB)
[2023-02-08T01:28:22.774+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 4e853e3e6d16:39783 (size: 8.1 KiB, free: 434.3 MiB)
[2023-02-08T01:28:22.779+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1513
[2023-02-08T01:28:22.780+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[122] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-02-08T01:28:22.791+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0
[2023-02-08T01:28:22.791+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Registering RDD 124 (save at BigQueryWriteHelper.java:105) as input to shuffle 3
[2023-02-08T01:28:22.792+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Got map stage job 11 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-02-08T01:28:22.792+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Final stage: ShuffleMapStage 11 (save at BigQueryWriteHelper.java:105)
[2023-02-08T01:28:22.792+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Parents of final stage: List()
[2023-02-08T01:28:22.792+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Missing parents: List()
[2023-02-08T01:28:22.809+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[124] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-02-08T01:28:22.814+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO CodeGenerator: Code generated in 96.517365 ms
[2023-02-08T01:28:22.827+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 15.7 KiB, free 434.1 MiB)
[2023-02-08T01:28:22.830+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 434.1 MiB)
[2023-02-08T01:28:22.835+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO CodeGenerator: Code generated in 57.4091 ms
[2023-02-08T01:28:22.842+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 4e853e3e6d16:39783 (size: 8.1 KiB, free: 434.3 MiB)
[2023-02-08T01:28:22.849+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1513
[2023-02-08T01:28:22.850+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[124] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-02-08T01:28:22.850+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
[2023-02-08T01:28:22.853+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Registering RDD 126 (save at BigQueryWriteHelper.java:105) as input to shuffle 4
[2023-02-08T01:28:22.853+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Got map stage job 12 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-02-08T01:28:22.853+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Final stage: ShuffleMapStage 12 (save at BigQueryWriteHelper.java:105)
[2023-02-08T01:28:22.854+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Parents of final stage: List()
[2023-02-08T01:28:22.854+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Missing parents: List()
[2023-02-08T01:28:22.864+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Submitting ShuffleMapStage 12 (MapPartitionsRDD[126] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-02-08T01:28:22.873+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 15.7 KiB, free 434.1 MiB)
[2023-02-08T01:28:22.873+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 434.1 MiB)
[2023-02-08T01:28:22.874+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 4e853e3e6d16:39783 (size: 8.1 KiB, free: 434.3 MiB)
[2023-02-08T01:28:22.876+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1513
[2023-02-08T01:28:22.879+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[126] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-02-08T01:28:22.879+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
[2023-02-08T01:28:22.882+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Registering RDD 128 (save at BigQueryWriteHelper.java:105) as input to shuffle 5
[2023-02-08T01:28:22.883+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Got map stage job 13 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-02-08T01:28:22.883+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Final stage: ShuffleMapStage 13 (save at BigQueryWriteHelper.java:105)
[2023-02-08T01:28:22.884+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Parents of final stage: List()
[2023-02-08T01:28:22.884+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Missing parents: List()
[2023-02-08T01:28:22.888+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Submitting ShuffleMapStage 13 (MapPartitionsRDD[128] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-02-08T01:28:22.896+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 15.7 KiB, free 434.0 MiB)
[2023-02-08T01:28:22.897+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 434.0 MiB)
[2023-02-08T01:28:22.898+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO CodeGenerator: Code generated in 26.953866 ms
[2023-02-08T01:28:22.901+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 4e853e3e6d16:39783 (size: 8.1 KiB, free: 434.3 MiB)
[2023-02-08T01:28:22.903+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1513
[2023-02-08T01:28:22.906+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 13 (MapPartitionsRDD[128] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-02-08T01:28:22.907+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0
[2023-02-08T01:28:22.911+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Registering RDD 130 (save at BigQueryWriteHelper.java:105) as input to shuffle 6
[2023-02-08T01:28:22.912+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Got map stage job 14 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-02-08T01:28:22.912+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Final stage: ShuffleMapStage 14 (save at BigQueryWriteHelper.java:105)
[2023-02-08T01:28:22.913+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Parents of final stage: List()
[2023-02-08T01:28:22.913+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Missing parents: List()
[2023-02-08T01:28:22.917+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Submitting ShuffleMapStage 14 (MapPartitionsRDD[130] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-02-08T01:28:22.938+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 15.7 KiB, free 434.0 MiB)
[2023-02-08T01:28:22.938+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO CodeGenerator: Code generated in 24.750928 ms
[2023-02-08T01:28:22.939+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 434.0 MiB)
[2023-02-08T01:28:22.941+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 4e853e3e6d16:39783 (size: 8.1 KiB, free: 434.3 MiB)
[2023-02-08T01:28:22.944+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1513
[2023-02-08T01:28:22.945+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[130] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-02-08T01:28:22.945+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0
[2023-02-08T01:28:22.951+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Registering RDD 132 (save at BigQueryWriteHelper.java:105) as input to shuffle 7
[2023-02-08T01:28:22.954+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Got map stage job 15 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-02-08T01:28:22.954+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Final stage: ShuffleMapStage 15 (save at BigQueryWriteHelper.java:105)
[2023-02-08T01:28:22.955+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Parents of final stage: List()
[2023-02-08T01:28:22.955+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Missing parents: List()
[2023-02-08T01:28:22.960+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[132] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-02-08T01:28:22.970+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 15.7 KiB, free 434.0 MiB)
[2023-02-08T01:28:22.973+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 434.0 MiB)
[2023-02-08T01:28:22.982+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 4e853e3e6d16:39783 (size: 8.1 KiB, free: 434.3 MiB)
[2023-02-08T01:28:22.983+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO CodeGenerator: Code generated in 22.525495 ms
[2023-02-08T01:28:22.986+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1513
[2023-02-08T01:28:22.987+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[132] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-02-08T01:28:22.990+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
[2023-02-08T01:28:22.996+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Registering RDD 134 (save at BigQueryWriteHelper.java:105) as input to shuffle 8
[2023-02-08T01:28:22.997+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Got map stage job 16 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-02-08T01:28:22.997+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Final stage: ShuffleMapStage 16 (save at BigQueryWriteHelper.java:105)
[2023-02-08T01:28:22.998+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Parents of final stage: List()
[2023-02-08T01:28:22.998+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Missing parents: List()
[2023-02-08T01:28:23.002+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:22 INFO DAGScheduler: Submitting ShuffleMapStage 16 (MapPartitionsRDD[134] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-02-08T01:28:23.003+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2087 bytes result sent to driver
[2023-02-08T01:28:23.008+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9) (4e853e3e6d16, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-02-08T01:28:23.012+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)
[2023-02-08T01:28:23.013+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 15.7 KiB, free 434.0 MiB)
[2023-02-08T01:28:23.013+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 519 ms on 4e853e3e6d16 (executor driver) (1/1)
[2023-02-08T01:28:23.013+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2023-02-08T01:28:23.014+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 434.0 MiB)
[2023-02-08T01:28:23.016+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 4e853e3e6d16:39783 (size: 8.1 KiB, free: 434.3 MiB)
[2023-02-08T01:28:23.019+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1513
[2023-02-08T01:28:23.020+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 16 (MapPartitionsRDD[134] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-02-08T01:28:23.020+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0
[2023-02-08T01:28:23.022+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: Registering RDD 136 (save at BigQueryWriteHelper.java:105) as input to shuffle 9
[2023-02-08T01:28:23.023+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: Got map stage job 17 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-02-08T01:28:23.023+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: Final stage: ShuffleMapStage 17 (save at BigQueryWriteHelper.java:105)
[2023-02-08T01:28:23.024+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: Parents of final stage: List()
[2023-02-08T01:28:23.025+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: Missing parents: List()
[2023-02-08T01:28:23.031+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: Submitting ShuffleMapStage 17 (MapPartitionsRDD[136] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-02-08T01:28:23.039+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO CodeGenerator: Code generated in 36.742926 ms
[2023-02-08T01:28:23.044+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 15.7 KiB, free 434.0 MiB)
[2023-02-08T01:28:23.055+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.9 MiB)
[2023-02-08T01:28:23.058+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 4e853e3e6d16:39783 (size: 8.1 KiB, free: 434.3 MiB)
[2023-02-08T01:28:23.060+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1513
[2023-02-08T01:28:23.061+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 17 (MapPartitionsRDD[136] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-02-08T01:28:23.066+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0
[2023-02-08T01:28:23.067+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: Registering RDD 138 (save at BigQueryWriteHelper.java:105) as input to shuffle 10
[2023-02-08T01:28:23.067+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: Got map stage job 18 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-02-08T01:28:23.067+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: Final stage: ShuffleMapStage 18 (save at BigQueryWriteHelper.java:105)
[2023-02-08T01:28:23.068+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: Parents of final stage: List()
[2023-02-08T01:28:23.068+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: Missing parents: List()
[2023-02-08T01:28:23.070+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[138] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-02-08T01:28:23.076+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO CodeGenerator: Code generated in 13.659553 ms
[2023-02-08T01:28:23.079+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 15.7 KiB, free 433.9 MiB)
[2023-02-08T01:28:23.140+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.9 MiB)
[2023-02-08T01:28:23.141+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 4e853e3e6d16:39783 (size: 8.1 KiB, free: 434.3 MiB)
[2023-02-08T01:28:23.146+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1513
[2023-02-08T01:28:23.147+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[138] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-02-08T01:28:23.147+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0
[2023-02-08T01:28:23.153+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: ShuffleMapStage 8 (save at BigQueryWriteHelper.java:105) finished in 0.768 s
[2023-02-08T01:28:23.154+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: looking for newly runnable stages
[2023-02-08T01:28:23.157+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: running: HashSet(ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 18, ShuffleMapStage 9, ShuffleMapStage 10, ShuffleMapStage 11, ShuffleMapStage 12, ShuffleMapStage 13, ShuffleMapStage 14, ShuffleMapStage 15)
[2023-02-08T01:28:23.159+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: waiting: HashSet()
[2023-02-08T01:28:23.161+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: failed: HashSet()
[2023-02-08T01:28:23.178+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO CodeGenerator: Code generated in 21.020749 ms
[2023-02-08T01:28:23.190+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: Registering RDD 140 (save at BigQueryWriteHelper.java:105) as input to shuffle 11
[2023-02-08T01:28:23.190+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: Got map stage job 19 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-02-08T01:28:23.191+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: Final stage: ShuffleMapStage 19 (save at BigQueryWriteHelper.java:105)
[2023-02-08T01:28:23.191+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: Parents of final stage: List()
[2023-02-08T01:28:23.191+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: Missing parents: List()
[2023-02-08T01:28:23.198+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: Submitting ShuffleMapStage 19 (MapPartitionsRDD[140] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-02-08T01:28:23.210+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 15.7 KiB, free 433.9 MiB)
[2023-02-08T01:28:23.211+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.9 MiB)
[2023-02-08T01:28:23.211+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 4e853e3e6d16:39783 (size: 8.1 KiB, free: 434.3 MiB)
[2023-02-08T01:28:23.213+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1513
[2023-02-08T01:28:23.235+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO CodeGenerator: Code generated in 32.108362 ms
[2023-02-08T01:28:23.242+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 19 (MapPartitionsRDD[140] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-02-08T01:28:23.242+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0
[2023-02-08T01:28:23.246+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: Registering RDD 142 (save at BigQueryWriteHelper.java:105) as input to shuffle 12
[2023-02-08T01:28:23.246+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: Got map stage job 20 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-02-08T01:28:23.249+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: Final stage: ShuffleMapStage 20 (save at BigQueryWriteHelper.java:105)
[2023-02-08T01:28:23.250+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: Parents of final stage: List()
[2023-02-08T01:28:23.251+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: Missing parents: List()
[2023-02-08T01:28:23.253+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: Submitting ShuffleMapStage 20 (MapPartitionsRDD[142] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-02-08T01:28:23.318+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 15.7 KiB, free 433.9 MiB)
[2023-02-08T01:28:23.319+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO CodeGenerator: Code generated in 58.778359 ms
[2023-02-08T01:28:23.319+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.9 MiB)
[2023-02-08T01:28:23.320+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 4e853e3e6d16:39783 (size: 8.1 KiB, free: 434.3 MiB)
[2023-02-08T01:28:23.334+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1513
[2023-02-08T01:28:23.335+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[142] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-02-08T01:28:23.335+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks resource profile 0
[2023-02-08T01:28:23.343+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: Registering RDD 144 (save at BigQueryWriteHelper.java:105) as input to shuffle 13
[2023-02-08T01:28:23.344+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: Got map stage job 21 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-02-08T01:28:23.344+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: Final stage: ShuffleMapStage 21 (save at BigQueryWriteHelper.java:105)
[2023-02-08T01:28:23.345+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: Parents of final stage: List()
[2023-02-08T01:28:23.345+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: Missing parents: List()
[2023-02-08T01:28:23.345+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: Submitting ShuffleMapStage 21 (MapPartitionsRDD[144] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-02-08T01:28:23.349+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO CodeGenerator: Code generated in 16.449903 ms
[2023-02-08T01:28:23.364+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 15.7 KiB, free 433.9 MiB)
[2023-02-08T01:28:23.364+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.9 MiB)
[2023-02-08T01:28:23.370+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 4e853e3e6d16:39783 (size: 8.1 KiB, free: 434.3 MiB)
[2023-02-08T01:28:23.374+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1513
[2023-02-08T01:28:23.374+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 21 (MapPartitionsRDD[144] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-02-08T01:28:23.375+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0
[2023-02-08T01:28:23.375+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: Registering RDD 146 (save at BigQueryWriteHelper.java:105) as input to shuffle 14
[2023-02-08T01:28:23.375+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: Got map stage job 22 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-02-08T01:28:23.376+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: Final stage: ShuffleMapStage 22 (save at BigQueryWriteHelper.java:105)
[2023-02-08T01:28:23.380+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: Parents of final stage: List()
[2023-02-08T01:28:23.381+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: Missing parents: List()
[2023-02-08T01:28:23.382+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: Submitting ShuffleMapStage 22 (MapPartitionsRDD[146] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-02-08T01:28:23.390+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 15.7 KiB, free 433.8 MiB)
[2023-02-08T01:28:23.390+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.8 MiB)
[2023-02-08T01:28:23.392+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 4e853e3e6d16:39783 (size: 8.1 KiB, free: 434.3 MiB)
[2023-02-08T01:28:23.396+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1513
[2023-02-08T01:28:23.397+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 22 (MapPartitionsRDD[146] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-02-08T01:28:23.397+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0
[2023-02-08T01:28:23.398+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: Registering RDD 148 (save at BigQueryWriteHelper.java:105) as input to shuffle 15
[2023-02-08T01:28:23.398+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: Got map stage job 23 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-02-08T01:28:23.398+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: Final stage: ShuffleMapStage 23 (save at BigQueryWriteHelper.java:105)
[2023-02-08T01:28:23.398+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: Parents of final stage: List()
[2023-02-08T01:28:23.399+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: Missing parents: List()
[2023-02-08T01:28:23.402+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: Submitting ShuffleMapStage 23 (MapPartitionsRDD[148] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-02-08T01:28:23.406+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 15.7 KiB, free 433.8 MiB)
[2023-02-08T01:28:23.407+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.8 MiB)
[2023-02-08T01:28:23.409+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 4e853e3e6d16:39783 (size: 8.1 KiB, free: 434.2 MiB)
[2023-02-08T01:28:23.410+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1513
[2023-02-08T01:28:23.411+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 23 (MapPartitionsRDD[148] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-02-08T01:28:23.411+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks resource profile 0
[2023-02-08T01:28:23.413+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: Registering RDD 150 (save at BigQueryWriteHelper.java:105) as input to shuffle 16
[2023-02-08T01:28:23.413+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: Got map stage job 24 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-02-08T01:28:23.414+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: Final stage: ShuffleMapStage 24 (save at BigQueryWriteHelper.java:105)
[2023-02-08T01:28:23.417+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: Parents of final stage: List()
[2023-02-08T01:28:23.418+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: Missing parents: List()
[2023-02-08T01:28:23.418+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: Submitting ShuffleMapStage 24 (MapPartitionsRDD[150] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-02-08T01:28:23.423+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 15.7 KiB, free 433.8 MiB)
[2023-02-08T01:28:23.426+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.8 MiB)
[2023-02-08T01:28:23.431+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 4e853e3e6d16:39783 (size: 8.1 KiB, free: 434.2 MiB)
[2023-02-08T01:28:23.436+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1513
[2023-02-08T01:28:23.436+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 24 (MapPartitionsRDD[150] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-02-08T01:28:23.436+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks resource profile 0
[2023-02-08T01:28:23.582+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2023-02-08T01:28:23.850+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO CodeGenerator: Code generated in 16.580361 ms
[2023-02-08T01:28:23.932+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:23 INFO CodeGenerator: Code generated in 39.807476 ms
[2023-02-08T01:28:24.048+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:24 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2023-02-08T01:28:24.049+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:24 INFO DAGScheduler: Got job 25 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
[2023-02-08T01:28:24.049+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:24 INFO DAGScheduler: Final stage: ResultStage 26 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2023-02-08T01:28:24.050+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:24 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 25)
[2023-02-08T01:28:24.050+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:24 INFO DAGScheduler: Missing parents: List()
[2023-02-08T01:28:24.051+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:24 INFO DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[155] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2023-02-08T01:28:24.090+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:24 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 23.8 KiB, free 433.8 MiB)
[2023-02-08T01:28:24.097+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:24 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 10.8 KiB, free 433.7 MiB)
[2023-02-08T01:28:24.098+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:24 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 4e853e3e6d16:39783 (size: 10.8 KiB, free: 434.2 MiB)
[2023-02-08T01:28:24.102+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:24 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1513
[2023-02-08T01:28:24.105+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[155] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
[2023-02-08T01:28:24.107+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:24 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks resource profile 0
[2023-02-08T01:28:25.368+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:25 INFO CodeGenerator: Code generated in 15.501476 ms
[2023-02-08T01:28:25.446+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:25 INFO PythonRunner: Times: total = 2251, boot = 2145, init = 105, finish = 1
[2023-02-08T01:28:25.458+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:25 INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 2521 bytes result sent to driver
[2023-02-08T01:28:25.460+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:25 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10) (4e853e3e6d16, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-02-08T01:28:25.461+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:25 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 2455 ms on 4e853e3e6d16 (executor driver) (1/1)
[2023-02-08T01:28:25.461+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:25 INFO Executor: Running task 0.0 in stage 10.0 (TID 10)
[2023-02-08T01:28:25.461+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:25 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
[2023-02-08T01:28:25.462+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:25 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 45819
[2023-02-08T01:28:25.464+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:25 INFO DAGScheduler: ShuffleMapStage 9 (save at BigQueryWriteHelper.java:105) finished in 2.931 s
[2023-02-08T01:28:25.465+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:25 INFO DAGScheduler: looking for newly runnable stages
[2023-02-08T01:28:25.466+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:25 INFO DAGScheduler: running: HashSet(ShuffleMapStage 10, ShuffleMapStage 11, ShuffleMapStage 12, ShuffleMapStage 13, ShuffleMapStage 14, ShuffleMapStage 15, ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 18, ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-02-08T01:28:25.466+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:25 INFO DAGScheduler: waiting: HashSet()
[2023-02-08T01:28:25.466+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:25 INFO DAGScheduler: failed: HashSet()
[2023-02-08T01:28:25.621+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:25 INFO PythonRunner: Times: total = 141, boot = -16, init = 157, finish = 0
[2023-02-08T01:28:25.629+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:25 INFO Executor: Finished task 0.0 in stage 10.0 (TID 10). 2478 bytes result sent to driver
[2023-02-08T01:28:25.631+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:25 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 11) (4e853e3e6d16, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-02-08T01:28:25.632+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:25 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 172 ms on 4e853e3e6d16 (executor driver) (1/1)
[2023-02-08T01:28:25.632+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:25 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool
[2023-02-08T01:28:25.632+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:25 INFO Executor: Running task 0.0 in stage 11.0 (TID 11)
[2023-02-08T01:28:25.634+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:25 INFO DAGScheduler: ShuffleMapStage 10 (save at BigQueryWriteHelper.java:105) finished in 2.971 s
[2023-02-08T01:28:25.634+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:25 INFO DAGScheduler: looking for newly runnable stages
[2023-02-08T01:28:25.635+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:25 INFO DAGScheduler: running: HashSet(ShuffleMapStage 11, ShuffleMapStage 12, ShuffleMapStage 13, ShuffleMapStage 14, ShuffleMapStage 15, ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 18, ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-02-08T01:28:25.635+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:25 INFO DAGScheduler: waiting: HashSet()
[2023-02-08T01:28:25.635+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:25 INFO DAGScheduler: failed: HashSet()
[2023-02-08T01:28:25.791+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:25 INFO PythonRunner: Times: total = 142, boot = -14, init = 156, finish = 0
[2023-02-08T01:28:25.799+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:25 INFO Executor: Finished task 0.0 in stage 11.0 (TID 11). 2478 bytes result sent to driver
[2023-02-08T01:28:25.800+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:25 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 12) (4e853e3e6d16, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-02-08T01:28:25.801+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:25 INFO Executor: Running task 0.0 in stage 12.0 (TID 12)
[2023-02-08T01:28:25.802+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:25 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 11) in 171 ms on 4e853e3e6d16 (executor driver) (1/1)
[2023-02-08T01:28:25.802+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:25 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool
[2023-02-08T01:28:25.803+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:25 INFO DAGScheduler: ShuffleMapStage 11 (save at BigQueryWriteHelper.java:105) finished in 2.995 s
[2023-02-08T01:28:25.804+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:25 INFO DAGScheduler: looking for newly runnable stages
[2023-02-08T01:28:25.804+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:25 INFO DAGScheduler: running: HashSet(ShuffleMapStage 12, ShuffleMapStage 13, ShuffleMapStage 14, ShuffleMapStage 15, ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 18, ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-02-08T01:28:25.805+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:25 INFO DAGScheduler: waiting: HashSet()
[2023-02-08T01:28:25.805+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:25 INFO DAGScheduler: failed: HashSet()
[2023-02-08T01:28:25.940+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:25 INFO PythonRunner: Times: total = 120, boot = -12, init = 132, finish = 0
[2023-02-08T01:28:25.948+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:25 INFO Executor: Finished task 0.0 in stage 12.0 (TID 12). 2478 bytes result sent to driver
[2023-02-08T01:28:25.950+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:25 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 13) (4e853e3e6d16, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-02-08T01:28:25.951+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:25 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 12) in 151 ms on 4e853e3e6d16 (executor driver) (1/1)
[2023-02-08T01:28:25.952+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:25 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool
[2023-02-08T01:28:25.953+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:25 INFO Executor: Running task 0.0 in stage 13.0 (TID 13)
[2023-02-08T01:28:25.953+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:25 INFO DAGScheduler: ShuffleMapStage 12 (save at BigQueryWriteHelper.java:105) finished in 3.087 s
[2023-02-08T01:28:25.954+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:25 INFO DAGScheduler: looking for newly runnable stages
[2023-02-08T01:28:25.954+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:25 INFO DAGScheduler: running: HashSet(ShuffleMapStage 13, ShuffleMapStage 14, ShuffleMapStage 15, ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 18, ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-02-08T01:28:25.954+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:25 INFO DAGScheduler: waiting: HashSet()
[2023-02-08T01:28:25.955+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:25 INFO DAGScheduler: failed: HashSet()
[2023-02-08T01:28:26.098+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:26 INFO PythonRunner: Times: total = 124, boot = -15, init = 139, finish = 0
[2023-02-08T01:28:26.102+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:26 INFO Executor: Finished task 0.0 in stage 13.0 (TID 13). 2478 bytes result sent to driver
[2023-02-08T01:28:26.103+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:26 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 14) (4e853e3e6d16, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-02-08T01:28:26.104+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:26 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 13) in 154 ms on 4e853e3e6d16 (executor driver) (1/1)
[2023-02-08T01:28:26.104+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:26 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool
[2023-02-08T01:28:26.106+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:26 INFO DAGScheduler: ShuffleMapStage 13 (save at BigQueryWriteHelper.java:105) finished in 3.219 s
[2023-02-08T01:28:26.107+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:26 INFO DAGScheduler: looking for newly runnable stages
[2023-02-08T01:28:26.107+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:26 INFO DAGScheduler: running: HashSet(ShuffleMapStage 14, ShuffleMapStage 15, ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 18, ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-02-08T01:28:26.107+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:26 INFO DAGScheduler: waiting: HashSet()
[2023-02-08T01:28:26.108+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:26 INFO DAGScheduler: failed: HashSet()
[2023-02-08T01:28:26.114+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:26 INFO Executor: Running task 0.0 in stage 14.0 (TID 14)
[2023-02-08T01:28:26.373+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:26 INFO PythonRunner: Times: total = 239, boot = -10, init = 249, finish = 0
[2023-02-08T01:28:26.380+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:26 INFO Executor: Finished task 0.0 in stage 14.0 (TID 14). 2478 bytes result sent to driver
[2023-02-08T01:28:26.382+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:26 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 15) (4e853e3e6d16, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-02-08T01:28:26.385+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:26 INFO Executor: Running task 0.0 in stage 15.0 (TID 15)
[2023-02-08T01:28:26.385+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:26 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 14) in 281 ms on 4e853e3e6d16 (executor driver) (1/1)
[2023-02-08T01:28:26.386+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:26 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool
[2023-02-08T01:28:26.387+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:26 INFO DAGScheduler: ShuffleMapStage 14 (save at BigQueryWriteHelper.java:105) finished in 3.470 s
[2023-02-08T01:28:26.387+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:26 INFO DAGScheduler: looking for newly runnable stages
[2023-02-08T01:28:26.388+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:26 INFO DAGScheduler: running: HashSet(ShuffleMapStage 15, ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 18, ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-02-08T01:28:26.388+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:26 INFO DAGScheduler: waiting: HashSet()
[2023-02-08T01:28:26.389+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:26 INFO DAGScheduler: failed: HashSet()
[2023-02-08T01:28:26.594+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:26 INFO PythonRunner: Times: total = 192, boot = -11, init = 203, finish = 0
[2023-02-08T01:28:26.606+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:26 INFO Executor: Finished task 0.0 in stage 15.0 (TID 15). 2478 bytes result sent to driver
[2023-02-08T01:28:26.607+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:26 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 16) (4e853e3e6d16, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-02-08T01:28:26.607+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:26 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 15) in 221 ms on 4e853e3e6d16 (executor driver) (1/1)
[2023-02-08T01:28:26.607+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:26 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool
[2023-02-08T01:28:26.608+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:26 INFO Executor: Running task 0.0 in stage 16.0 (TID 16)
[2023-02-08T01:28:26.608+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:26 INFO DAGScheduler: ShuffleMapStage 15 (save at BigQueryWriteHelper.java:105) finished in 3.647 s
[2023-02-08T01:28:26.608+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:26 INFO DAGScheduler: looking for newly runnable stages
[2023-02-08T01:28:26.608+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:26 INFO DAGScheduler: running: HashSet(ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 18, ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-02-08T01:28:26.609+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:26 INFO DAGScheduler: waiting: HashSet()
[2023-02-08T01:28:26.609+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:26 INFO DAGScheduler: failed: HashSet()
[2023-02-08T01:28:26.807+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:26 INFO PythonRunner: Times: total = 181, boot = -8, init = 189, finish = 0
[2023-02-08T01:28:26.819+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:26 INFO Executor: Finished task 0.0 in stage 16.0 (TID 16). 2478 bytes result sent to driver
[2023-02-08T01:28:26.820+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:26 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 17) (4e853e3e6d16, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-02-08T01:28:26.822+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:26 INFO Executor: Running task 0.0 in stage 17.0 (TID 17)
[2023-02-08T01:28:26.822+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:26 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 16) in 220 ms on 4e853e3e6d16 (executor driver) (1/1)
[2023-02-08T01:28:26.822+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:26 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool
[2023-02-08T01:28:26.824+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:26 INFO DAGScheduler: ShuffleMapStage 16 (save at BigQueryWriteHelper.java:105) finished in 3.823 s
[2023-02-08T01:28:26.824+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:26 INFO DAGScheduler: looking for newly runnable stages
[2023-02-08T01:28:26.825+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:26 INFO DAGScheduler: running: HashSet(ShuffleMapStage 17, ShuffleMapStage 18, ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-02-08T01:28:26.825+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:26 INFO DAGScheduler: waiting: HashSet()
[2023-02-08T01:28:26.825+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:26 INFO DAGScheduler: failed: HashSet()
[2023-02-08T01:28:27.016+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO PythonRunner: Times: total = 157, boot = -12, init = 169, finish = 0
[2023-02-08T01:28:27.031+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO Executor: Finished task 0.0 in stage 17.0 (TID 17). 2478 bytes result sent to driver
[2023-02-08T01:28:27.038+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 18) (4e853e3e6d16, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-02-08T01:28:27.042+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 17) in 221 ms on 4e853e3e6d16 (executor driver) (1/1)
[2023-02-08T01:28:27.042+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool
[2023-02-08T01:28:27.048+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO DAGScheduler: ShuffleMapStage 17 (save at BigQueryWriteHelper.java:105) finished in 4.010 s
[2023-02-08T01:28:27.049+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO DAGScheduler: looking for newly runnable stages
[2023-02-08T01:28:27.050+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO DAGScheduler: running: HashSet(ShuffleMapStage 18, ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-02-08T01:28:27.050+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO DAGScheduler: waiting: HashSet()
[2023-02-08T01:28:27.050+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO DAGScheduler: failed: HashSet()
[2023-02-08T01:28:27.051+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO Executor: Running task 0.0 in stage 18.0 (TID 18)
[2023-02-08T01:28:27.238+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 4e853e3e6d16:39783 in memory (size: 8.1 KiB, free: 434.2 MiB)
[2023-02-08T01:28:27.256+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 4e853e3e6d16:39783 in memory (size: 8.1 KiB, free: 434.2 MiB)
[2023-02-08T01:28:27.273+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 4e853e3e6d16:39783 in memory (size: 8.1 KiB, free: 434.2 MiB)
[2023-02-08T01:28:27.283+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 4e853e3e6d16:39783 in memory (size: 8.1 KiB, free: 434.3 MiB)
[2023-02-08T01:28:27.293+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 4e853e3e6d16:39783 in memory (size: 8.1 KiB, free: 434.3 MiB)
[2023-02-08T01:28:27.300+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 4e853e3e6d16:39783 in memory (size: 8.1 KiB, free: 434.3 MiB)
[2023-02-08T01:28:27.319+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 4e853e3e6d16:39783 in memory (size: 8.1 KiB, free: 434.3 MiB)
[2023-02-08T01:28:27.322+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO PythonRunner: Times: total = 218, boot = -65, init = 283, finish = 0
[2023-02-08T01:28:27.334+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO Executor: Finished task 0.0 in stage 18.0 (TID 18). 2521 bytes result sent to driver
[2023-02-08T01:28:27.340+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 19) (4e853e3e6d16, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-02-08T01:28:27.342+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 18) in 307 ms on 4e853e3e6d16 (executor driver) (1/1)
[2023-02-08T01:28:27.343+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool
[2023-02-08T01:28:27.345+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO Executor: Running task 0.0 in stage 19.0 (TID 19)
[2023-02-08T01:28:27.349+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO DAGScheduler: ShuffleMapStage 18 (save at BigQueryWriteHelper.java:105) finished in 4.271 s
[2023-02-08T01:28:27.350+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO DAGScheduler: looking for newly runnable stages
[2023-02-08T01:28:27.351+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO DAGScheduler: running: HashSet(ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-02-08T01:28:27.351+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO DAGScheduler: waiting: HashSet()
[2023-02-08T01:28:27.352+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO DAGScheduler: failed: HashSet()
[2023-02-08T01:28:27.352+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 4e853e3e6d16:39783 in memory (size: 8.1 KiB, free: 434.3 MiB)
[2023-02-08T01:28:27.374+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 4e853e3e6d16:39783 in memory (size: 8.1 KiB, free: 434.3 MiB)
[2023-02-08T01:28:27.386+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 4e853e3e6d16:39783 in memory (size: 5.7 KiB, free: 434.3 MiB)
[2023-02-08T01:28:27.538+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO PythonRunner: Times: total = 154, boot = -40, init = 194, finish = 0
[2023-02-08T01:28:27.544+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO Executor: Finished task 0.0 in stage 19.0 (TID 19). 2478 bytes result sent to driver
[2023-02-08T01:28:27.553+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 20) (4e853e3e6d16, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-02-08T01:28:27.556+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 19) in 218 ms on 4e853e3e6d16 (executor driver) (1/1)
[2023-02-08T01:28:27.557+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO Executor: Running task 0.0 in stage 20.0 (TID 20)
[2023-02-08T01:28:27.557+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool
[2023-02-08T01:28:27.561+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO DAGScheduler: ShuffleMapStage 19 (save at BigQueryWriteHelper.java:105) finished in 4.358 s
[2023-02-08T01:28:27.561+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO DAGScheduler: looking for newly runnable stages
[2023-02-08T01:28:27.562+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO DAGScheduler: running: HashSet(ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-02-08T01:28:27.562+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO DAGScheduler: waiting: HashSet()
[2023-02-08T01:28:27.562+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO DAGScheduler: failed: HashSet()
[2023-02-08T01:28:27.734+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO PythonRunner: Times: total = 156, boot = -21, init = 177, finish = 0
[2023-02-08T01:28:27.740+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO Executor: Finished task 0.0 in stage 20.0 (TID 20). 2478 bytes result sent to driver
[2023-02-08T01:28:27.742+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 21) (4e853e3e6d16, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-02-08T01:28:27.743+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 20) in 193 ms on 4e853e3e6d16 (executor driver) (1/1)
[2023-02-08T01:28:27.744+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO Executor: Running task 0.0 in stage 21.0 (TID 21)
[2023-02-08T01:28:27.744+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool
[2023-02-08T01:28:27.746+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO DAGScheduler: ShuffleMapStage 20 (save at BigQueryWriteHelper.java:105) finished in 4.491 s
[2023-02-08T01:28:27.747+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO DAGScheduler: looking for newly runnable stages
[2023-02-08T01:28:27.747+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO DAGScheduler: running: HashSet(ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-02-08T01:28:27.747+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO DAGScheduler: waiting: HashSet()
[2023-02-08T01:28:27.748+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO DAGScheduler: failed: HashSet()
[2023-02-08T01:28:27.972+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO PythonRunner: Times: total = 187, boot = -11, init = 197, finish = 1
[2023-02-08T01:28:27.990+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO Executor: Finished task 0.0 in stage 21.0 (TID 21). 2478 bytes result sent to driver
[2023-02-08T01:28:27.994+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 22) (4e853e3e6d16, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-02-08T01:28:27.995+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 21) in 254 ms on 4e853e3e6d16 (executor driver) (1/1)
[2023-02-08T01:28:27.996+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool
[2023-02-08T01:28:27.996+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO Executor: Running task 0.0 in stage 22.0 (TID 22)
[2023-02-08T01:28:27.999+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO DAGScheduler: ShuffleMapStage 21 (save at BigQueryWriteHelper.java:105) finished in 4.655 s
[2023-02-08T01:28:28.000+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO DAGScheduler: looking for newly runnable stages
[2023-02-08T01:28:28.000+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO DAGScheduler: running: HashSet(ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-02-08T01:28:28.001+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO DAGScheduler: waiting: HashSet()
[2023-02-08T01:28:28.001+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:27 INFO DAGScheduler: failed: HashSet()
[2023-02-08T01:28:28.283+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:28 INFO PythonRunner: Times: total = 264, boot = -16, init = 280, finish = 0
[2023-02-08T01:28:28.296+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:28 INFO Executor: Finished task 0.0 in stage 22.0 (TID 22). 2478 bytes result sent to driver
[2023-02-08T01:28:28.298+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:28 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 23) (4e853e3e6d16, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-02-08T01:28:28.299+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:28 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 22) in 306 ms on 4e853e3e6d16 (executor driver) (1/1)
[2023-02-08T01:28:28.299+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:28 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool
[2023-02-08T01:28:28.300+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:28 INFO Executor: Running task 0.0 in stage 23.0 (TID 23)
[2023-02-08T01:28:28.301+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:28 INFO DAGScheduler: ShuffleMapStage 22 (save at BigQueryWriteHelper.java:105) finished in 4.917 s
[2023-02-08T01:28:28.302+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:28 INFO DAGScheduler: looking for newly runnable stages
[2023-02-08T01:28:28.302+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:28 INFO DAGScheduler: running: HashSet(ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-02-08T01:28:28.320+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:28 INFO DAGScheduler: waiting: HashSet()
[2023-02-08T01:28:28.320+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:28 INFO DAGScheduler: failed: HashSet()
[2023-02-08T01:28:28.507+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:28 INFO PythonRunner: Times: total = 191, boot = -17, init = 208, finish = 0
[2023-02-08T01:28:28.514+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:28 INFO Executor: Finished task 0.0 in stage 23.0 (TID 23). 2478 bytes result sent to driver
[2023-02-08T01:28:28.521+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:28 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 24) (4e853e3e6d16, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-02-08T01:28:28.521+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:28 INFO Executor: Running task 0.0 in stage 24.0 (TID 24)
[2023-02-08T01:28:28.521+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:28 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 23) in 221 ms on 4e853e3e6d16 (executor driver) (1/1)
[2023-02-08T01:28:28.522+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:28 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool
[2023-02-08T01:28:28.522+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:28 INFO DAGScheduler: ShuffleMapStage 23 (save at BigQueryWriteHelper.java:105) finished in 5.118 s
[2023-02-08T01:28:28.522+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:28 INFO DAGScheduler: looking for newly runnable stages
[2023-02-08T01:28:28.522+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:28 INFO DAGScheduler: running: HashSet(ShuffleMapStage 24, ResultStage 26)
[2023-02-08T01:28:28.522+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:28 INFO DAGScheduler: waiting: HashSet()
[2023-02-08T01:28:28.523+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:28 INFO DAGScheduler: failed: HashSet()
[2023-02-08T01:28:28.726+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:28 INFO PythonRunner: Times: total = 187, boot = -7, init = 193, finish = 1
[2023-02-08T01:28:28.735+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:28 INFO Executor: Finished task 0.0 in stage 24.0 (TID 24). 2478 bytes result sent to driver
[2023-02-08T01:28:28.745+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:28 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 25) (4e853e3e6d16, executor driver, partition 0, NODE_LOCAL, 7399 bytes) taskResourceAssignments Map()
[2023-02-08T01:28:28.751+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:28 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 24) in 231 ms on 4e853e3e6d16 (executor driver) (1/1)
[2023-02-08T01:28:28.752+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:28 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool
[2023-02-08T01:28:28.753+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:28 INFO Executor: Running task 0.0 in stage 26.0 (TID 25)
[2023-02-08T01:28:28.753+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:28 INFO DAGScheduler: ShuffleMapStage 24 (save at BigQueryWriteHelper.java:105) finished in 5.332 s
[2023-02-08T01:28:28.753+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:28 INFO DAGScheduler: looking for newly runnable stages
[2023-02-08T01:28:28.754+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:28 INFO DAGScheduler: running: HashSet(ResultStage 26)
[2023-02-08T01:28:28.754+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:28 INFO DAGScheduler: waiting: HashSet()
[2023-02-08T01:28:28.755+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:28 INFO DAGScheduler: failed: HashSet()
[2023-02-08T01:28:28.900+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:28 INFO ShuffleBlockFetcherIterator: Getting 1 (97.0 B) non-empty blocks including 1 (97.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-02-08T01:28:28.906+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 43 ms
[2023-02-08T01:28:29.010+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:29 INFO CodeGenerator: Code generated in 69.050505 ms
[2023-02-08T01:28:29.055+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:29 INFO CodeGenerator: Code generated in 22.676923 ms
[2023-02-08T01:28:29.102+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:29 INFO CodeGenerator: Code generated in 12.835998 ms
[2023-02-08T01:28:29.121+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:29 INFO CodeGenerator: Code generated in 11.391414 ms
[2023-02-08T01:28:29.213+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:29 INFO CodeGenerator: Code generated in 8.977155 ms
[2023-02-08T01:28:29.229+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:29 INFO CodeGenerator: Code generated in 10.341122 ms
[2023-02-08T01:28:29.240+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:29 INFO CodeGenerator: Code generated in 8.241271 ms
[2023-02-08T01:28:29.249+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:29 INFO Executor: Finished task 0.0 in stage 26.0 (TID 25). 3200 bytes result sent to driver
[2023-02-08T01:28:29.251+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:29 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 25) in 515 ms on 4e853e3e6d16 (executor driver) (1/1)
[2023-02-08T01:28:29.251+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:29 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool
[2023-02-08T01:28:29.252+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:29 INFO DAGScheduler: ResultStage 26 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 5.170 s
[2023-02-08T01:28:29.253+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:29 INFO DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-02-08T01:28:29.253+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 26: Stage finished
[2023-02-08T01:28:29.253+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:29 INFO DAGScheduler: Job 25 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 5.206654 s
[2023-02-08T01:28:29.262+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:29 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 24.0 B, free 434.0 MiB)
[2023-02-08T01:28:29.267+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:29 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 204.0 B, free 434.0 MiB)
[2023-02-08T01:28:29.268+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:29 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 4e853e3e6d16:39783 (size: 204.0 B, free: 434.3 MiB)
[2023-02-08T01:28:29.268+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:29 INFO SparkContext: Created broadcast 34 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2023-02-08T01:28:29.387+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:29 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-02-08T01:28:29.389+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:29 INFO DAGScheduler: Got job 26 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-02-08T01:28:29.390+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:29 INFO DAGScheduler: Final stage: ResultStage 27 (save at BigQueryWriteHelper.java:105)
[2023-02-08T01:28:29.392+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:29 INFO DAGScheduler: Parents of final stage: List()
[2023-02-08T01:28:29.393+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:29 INFO DAGScheduler: Missing parents: List()
[2023-02-08T01:28:29.395+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:29 INFO DAGScheduler: Submitting ResultStage 27 (ParallelCollectionRDD[158] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-02-08T01:28:29.453+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:29 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 209.9 KiB, free 433.8 MiB)
[2023-02-08T01:28:29.459+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:29 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 74.3 KiB, free 433.7 MiB)
[2023-02-08T01:28:29.460+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:29 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 4e853e3e6d16:39783 (size: 74.3 KiB, free: 434.2 MiB)
[2023-02-08T01:28:29.462+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:29 INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1513
[2023-02-08T01:28:29.463+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 27 (ParallelCollectionRDD[158] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-02-08T01:28:29.463+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:29 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks resource profile 0
[2023-02-08T01:28:29.465+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:29 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 26) (4e853e3e6d16, executor driver, partition 0, PROCESS_LOCAL, 7484 bytes) taskResourceAssignments Map()
[2023-02-08T01:28:29.467+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:29 INFO Executor: Running task 0.0 in stage 27.0 (TID 26)
[2023-02-08T01:28:29.534+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-02-08T01:28:29.534+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-02-08T01:28:29.535+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:29 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-02-08T01:28:29.535+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-02-08T01:28:29.535+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-02-08T01:28:29.536+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-02-08T01:28:29.547+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:29 INFO CodecConfig: Compression: SNAPPY
[2023-02-08T01:28:29.553+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:29 INFO CodecConfig: Compression: SNAPPY
[2023-02-08T01:28:29.601+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:29 INFO ParquetOutputFormat: Parquet block size to 134217728
[2023-02-08T01:28:29.602+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:29 INFO ParquetOutputFormat: Validation is off
[2023-02-08T01:28:29.602+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:29 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-02-08T01:28:29.602+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:29 INFO ParquetOutputFormat: Parquet properties are:
[2023-02-08T01:28:29.603+0000] {spark_submit.py:495} INFO - Parquet page size to 1048576
[2023-02-08T01:28:29.603+0000] {spark_submit.py:495} INFO - Parquet dictionary page size to 1048576
[2023-02-08T01:28:29.603+0000] {spark_submit.py:495} INFO - Dictionary is true
[2023-02-08T01:28:29.604+0000] {spark_submit.py:495} INFO - Writer version is: PARQUET_1_0
[2023-02-08T01:28:29.604+0000] {spark_submit.py:495} INFO - Page size checking is: estimated
[2023-02-08T01:28:29.604+0000] {spark_submit.py:495} INFO - Min row count for page size check is: 100
[2023-02-08T01:28:29.604+0000] {spark_submit.py:495} INFO - Max row count for page size check is: 10000
[2023-02-08T01:28:29.605+0000] {spark_submit.py:495} INFO - Truncate length for column indexes is: 64
[2023-02-08T01:28:29.605+0000] {spark_submit.py:495} INFO - Truncate length for statistics min/max  is: 2147483647
[2023-02-08T01:28:29.605+0000] {spark_submit.py:495} INFO - Bloom filter enabled: false
[2023-02-08T01:28:29.605+0000] {spark_submit.py:495} INFO - Max Bloom filter size for a column is 1048576
[2023-02-08T01:28:29.608+0000] {spark_submit.py:495} INFO - Bloom filter expected number of distinct values are: null
[2023-02-08T01:28:29.608+0000] {spark_submit.py:495} INFO - Page row count limit to 20000
[2023-02-08T01:28:29.609+0000] {spark_submit.py:495} INFO - Writing page checksums is: on
[2023-02-08T01:28:29.679+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:29 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-02-08T01:28:29.679+0000] {spark_submit.py:495} INFO - {
[2023-02-08T01:28:29.679+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-02-08T01:28:29.680+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-02-08T01:28:29.680+0000] {spark_submit.py:495} INFO - "name" : "measured_at",
[2023-02-08T01:28:29.680+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-02-08T01:28:29.681+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-02-08T01:28:29.681+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-02-08T01:28:29.681+0000] {spark_submit.py:495} INFO - }, {
[2023-02-08T01:28:29.682+0000] {spark_submit.py:495} INFO - "name" : "created_at",
[2023-02-08T01:28:29.682+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-02-08T01:28:29.682+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-02-08T01:28:29.682+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-02-08T01:28:29.683+0000] {spark_submit.py:495} INFO - }, {
[2023-02-08T01:28:29.683+0000] {spark_submit.py:495} INFO - "name" : "Biomass - Actual Aggregated",
[2023-02-08T01:28:29.683+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-02-08T01:28:29.683+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-02-08T01:28:29.684+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-02-08T01:28:29.684+0000] {spark_submit.py:495} INFO - }, {
[2023-02-08T01:28:29.684+0000] {spark_submit.py:495} INFO - "name" : "Fossil Brown coal/Lignite - Actual Aggregated",
[2023-02-08T01:28:29.684+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-02-08T01:28:29.685+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-02-08T01:28:29.685+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-02-08T01:28:29.685+0000] {spark_submit.py:495} INFO - }, {
[2023-02-08T01:28:29.685+0000] {spark_submit.py:495} INFO - "name" : "Fossil Gas - Actual Aggregated",
[2023-02-08T01:28:29.685+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-02-08T01:28:29.686+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-02-08T01:28:29.686+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-02-08T01:28:29.686+0000] {spark_submit.py:495} INFO - }, {
[2023-02-08T01:28:29.686+0000] {spark_submit.py:495} INFO - "name" : "Fossil Hard coal - Actual Aggregated",
[2023-02-08T01:28:29.686+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-02-08T01:28:29.687+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-02-08T01:28:29.687+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-02-08T01:28:29.687+0000] {spark_submit.py:495} INFO - }, {
[2023-02-08T01:28:29.687+0000] {spark_submit.py:495} INFO - "name" : "Fossil Oil - Actual Aggregated",
[2023-02-08T01:28:29.688+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-02-08T01:28:29.688+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-02-08T01:28:29.688+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-02-08T01:28:29.689+0000] {spark_submit.py:495} INFO - }, {
[2023-02-08T01:28:29.689+0000] {spark_submit.py:495} INFO - "name" : "Geothermal - Actual Aggregated",
[2023-02-08T01:28:29.690+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-02-08T01:28:29.690+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-02-08T01:28:29.690+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-02-08T01:28:29.691+0000] {spark_submit.py:495} INFO - }, {
[2023-02-08T01:28:29.691+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Aggregated",
[2023-02-08T01:28:29.691+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-02-08T01:28:29.692+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-02-08T01:28:29.692+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-02-08T01:28:29.692+0000] {spark_submit.py:495} INFO - }, {
[2023-02-08T01:28:29.693+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Consumption",
[2023-02-08T01:28:29.693+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-02-08T01:28:29.693+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-02-08T01:28:29.693+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-02-08T01:28:29.694+0000] {spark_submit.py:495} INFO - }, {
[2023-02-08T01:28:29.694+0000] {spark_submit.py:495} INFO - "name" : "Hydro Run-of-river and poundage - Actual Aggregated",
[2023-02-08T01:28:29.694+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-02-08T01:28:29.695+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-02-08T01:28:29.696+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-02-08T01:28:29.696+0000] {spark_submit.py:495} INFO - }, {
[2023-02-08T01:28:29.696+0000] {spark_submit.py:495} INFO - "name" : "Hydro Water Reservoir - Actual Aggregated",
[2023-02-08T01:28:29.697+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-02-08T01:28:29.697+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-02-08T01:28:29.697+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-02-08T01:28:29.697+0000] {spark_submit.py:495} INFO - }, {
[2023-02-08T01:28:29.698+0000] {spark_submit.py:495} INFO - "name" : "Nuclear - Actual Aggregated",
[2023-02-08T01:28:29.698+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-02-08T01:28:29.698+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-02-08T01:28:29.698+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-02-08T01:28:29.699+0000] {spark_submit.py:495} INFO - }, {
[2023-02-08T01:28:29.699+0000] {spark_submit.py:495} INFO - "name" : "Other - Actual Aggregated",
[2023-02-08T01:28:29.699+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-02-08T01:28:29.699+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-02-08T01:28:29.700+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-02-08T01:28:29.700+0000] {spark_submit.py:495} INFO - }, {
[2023-02-08T01:28:29.700+0000] {spark_submit.py:495} INFO - "name" : "Other renewable - Actual Aggregated",
[2023-02-08T01:28:29.700+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-02-08T01:28:29.701+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-02-08T01:28:29.701+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-02-08T01:28:29.701+0000] {spark_submit.py:495} INFO - }, {
[2023-02-08T01:28:29.701+0000] {spark_submit.py:495} INFO - "name" : "Solar - Actual Aggregated",
[2023-02-08T01:28:29.702+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-02-08T01:28:29.702+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-02-08T01:28:29.703+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-02-08T01:28:29.703+0000] {spark_submit.py:495} INFO - }, {
[2023-02-08T01:28:29.704+0000] {spark_submit.py:495} INFO - "name" : "Waste - Actual Aggregated",
[2023-02-08T01:28:29.704+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-02-08T01:28:29.704+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-02-08T01:28:29.705+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-02-08T01:28:29.705+0000] {spark_submit.py:495} INFO - }, {
[2023-02-08T01:28:29.705+0000] {spark_submit.py:495} INFO - "name" : "Wind Offshore - Actual Aggregated",
[2023-02-08T01:28:29.705+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-02-08T01:28:29.706+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-02-08T01:28:29.706+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-02-08T01:28:29.706+0000] {spark_submit.py:495} INFO - }, {
[2023-02-08T01:28:29.706+0000] {spark_submit.py:495} INFO - "name" : "Wind Onshore - Actual Aggregated",
[2023-02-08T01:28:29.707+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-02-08T01:28:29.707+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-02-08T01:28:29.707+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-02-08T01:28:29.708+0000] {spark_submit.py:495} INFO - } ]
[2023-02-08T01:28:29.708+0000] {spark_submit.py:495} INFO - }
[2023-02-08T01:28:29.708+0000] {spark_submit.py:495} INFO - and corresponding Parquet message type:
[2023-02-08T01:28:29.708+0000] {spark_submit.py:495} INFO - message spark_schema {
[2023-02-08T01:28:29.709+0000] {spark_submit.py:495} INFO - required int96 measured_at;
[2023-02-08T01:28:29.709+0000] {spark_submit.py:495} INFO - required int96 created_at;
[2023-02-08T01:28:29.710+0000] {spark_submit.py:495} INFO - optional double Biomass - Actual Aggregated;
[2023-02-08T01:28:29.710+0000] {spark_submit.py:495} INFO - optional double Fossil Brown coal/Lignite - Actual Aggregated;
[2023-02-08T01:28:29.711+0000] {spark_submit.py:495} INFO - optional double Fossil Gas - Actual Aggregated;
[2023-02-08T01:28:29.711+0000] {spark_submit.py:495} INFO - optional double Fossil Hard coal - Actual Aggregated;
[2023-02-08T01:28:29.711+0000] {spark_submit.py:495} INFO - optional double Fossil Oil - Actual Aggregated;
[2023-02-08T01:28:29.712+0000] {spark_submit.py:495} INFO - optional double Geothermal - Actual Aggregated;
[2023-02-08T01:28:29.712+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Aggregated;
[2023-02-08T01:28:29.712+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Consumption;
[2023-02-08T01:28:29.713+0000] {spark_submit.py:495} INFO - optional double Hydro Run-of-river and poundage - Actual Aggregated;
[2023-02-08T01:28:29.713+0000] {spark_submit.py:495} INFO - optional double Hydro Water Reservoir - Actual Aggregated;
[2023-02-08T01:28:29.713+0000] {spark_submit.py:495} INFO - optional double Nuclear - Actual Aggregated;
[2023-02-08T01:28:29.714+0000] {spark_submit.py:495} INFO - optional double Other - Actual Aggregated;
[2023-02-08T01:28:29.714+0000] {spark_submit.py:495} INFO - optional double Other renewable - Actual Aggregated;
[2023-02-08T01:28:29.715+0000] {spark_submit.py:495} INFO - optional double Solar - Actual Aggregated;
[2023-02-08T01:28:29.715+0000] {spark_submit.py:495} INFO - optional double Waste - Actual Aggregated;
[2023-02-08T01:28:29.715+0000] {spark_submit.py:495} INFO - optional double Wind Offshore - Actual Aggregated;
[2023-02-08T01:28:29.716+0000] {spark_submit.py:495} INFO - optional double Wind Onshore - Actual Aggregated;
[2023-02-08T01:28:29.716+0000] {spark_submit.py:495} INFO - }
[2023-02-08T01:28:29.717+0000] {spark_submit.py:495} INFO - 
[2023-02-08T01:28:29.717+0000] {spark_submit.py:495} INFO - 
[2023-02-08T01:28:30.240+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:30 INFO CodecPool: Got brand-new compressor [.snappy]
[2023-02-08T01:28:30.325+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:30 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 4e853e3e6d16:39783 in memory (size: 8.1 KiB, free: 434.2 MiB)
[2023-02-08T01:28:30.334+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:30 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 4e853e3e6d16:39783 in memory (size: 8.1 KiB, free: 434.2 MiB)
[2023-02-08T01:28:30.340+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:30 INFO BlockManagerInfo: Removed broadcast_32_piece0 on 4e853e3e6d16:39783 in memory (size: 8.1 KiB, free: 434.3 MiB)
[2023-02-08T01:28:30.412+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:30 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 4e853e3e6d16:39783 in memory (size: 8.1 KiB, free: 434.3 MiB)
[2023-02-08T01:28:30.443+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:30 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 4e853e3e6d16:39783 in memory (size: 8.1 KiB, free: 434.3 MiB)
[2023-02-08T01:28:30.466+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:30 INFO BlockManagerInfo: Removed broadcast_33_piece0 on 4e853e3e6d16:39783 in memory (size: 10.8 KiB, free: 434.3 MiB)
[2023-02-08T01:28:30.484+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:30 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 4e853e3e6d16:39783 in memory (size: 8.1 KiB, free: 434.3 MiB)
[2023-02-08T01:28:30.507+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:30 INFO BlockManagerInfo: Removed broadcast_31_piece0 on 4e853e3e6d16:39783 in memory (size: 8.1 KiB, free: 434.3 MiB)
[2023-02-08T01:28:32.173+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:32 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675819675753-3fbf8d3f-dac3-4ff0-a90d-56d517dce4f7/_temporary/0/_temporary/' directory.
[2023-02-08T01:28:32.174+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:32 INFO FileOutputCommitter: Saved output of task 'attempt_202302080128297254348117363591048_0027_m_000000_26' to gs://entsoe_temp_1009/.spark-bigquery-local-1675819675753-3fbf8d3f-dac3-4ff0-a90d-56d517dce4f7/_temporary/0/task_202302080128297254348117363591048_0027_m_000000
[2023-02-08T01:28:32.174+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:32 INFO SparkHadoopMapRedUtil: attempt_202302080128297254348117363591048_0027_m_000000_26: Committed. Elapsed time: 646 ms.
[2023-02-08T01:28:32.190+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:32 INFO Executor: Finished task 0.0 in stage 27.0 (TID 26). 2588 bytes result sent to driver
[2023-02-08T01:28:32.193+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:32 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 26) in 2729 ms on 4e853e3e6d16 (executor driver) (1/1)
[2023-02-08T01:28:32.194+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:32 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool
[2023-02-08T01:28:32.199+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:32 INFO DAGScheduler: ResultStage 27 (save at BigQueryWriteHelper.java:105) finished in 2.801 s
[2023-02-08T01:28:32.205+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:32 INFO DAGScheduler: Job 26 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-02-08T01:28:32.205+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 27: Stage finished
[2023-02-08T01:28:32.205+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:32 INFO DAGScheduler: Job 26 finished: save at BigQueryWriteHelper.java:105, took 2.815200 s
[2023-02-08T01:28:32.206+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:32 INFO FileFormatWriter: Start to commit write Job 8fe00c9e-0ae4-4326-a638-970d4748b5c7.
[2023-02-08T01:28:32.819+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:32 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675819675753-3fbf8d3f-dac3-4ff0-a90d-56d517dce4f7/_temporary/0/task_202302080128297254348117363591048_0027_m_000000/' directory.
[2023-02-08T01:28:33.103+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:33 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675819675753-3fbf8d3f-dac3-4ff0-a90d-56d517dce4f7/' directory.
[2023-02-08T01:28:33.284+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:33 INFO BlockManagerInfo: Removed broadcast_35_piece0 on 4e853e3e6d16:39783 in memory (size: 74.3 KiB, free: 434.4 MiB)
[2023-02-08T01:28:33.956+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:33 INFO FileFormatWriter: Write Job 8fe00c9e-0ae4-4326-a638-970d4748b5c7 committed. Elapsed time: 1748 ms.
[2023-02-08T01:28:33.968+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:33 INFO FileFormatWriter: Finished processing stats for write job 8fe00c9e-0ae4-4326-a638-970d4748b5c7.
[2023-02-08T01:28:35.419+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:35 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=entsoe_analytics_1009, projectId=rafzul-analytics-1009, tableId=total_generation_staging}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=null, ignoreUnknownValue=null, sourceUris=[gs://entsoe_temp_1009/.spark-bigquery-local-1675819675753-3fbf8d3f-dac3-4ff0-a90d-56d517dce4f7/part-00000-47d6c31f-6e14-4a4e-b396-b624a753f965-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=true, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=rafzul-analytics-1009, job=02471a70-cbd0-496a-9e51-6cec62119757, location=US}
[2023-02-08T01:28:40.704+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:40 INFO BigQueryClient: Done loading to rafzul-analytics-1009.entsoe_analytics_1009.total_generation_staging. jobId: JobId{project=rafzul-analytics-1009, job=02471a70-cbd0-496a-9e51-6cec62119757, location=US}
[2023-02-08T01:28:41.745+0000] {spark_submit.py:495} INFO - MANTEP!!!!
[2023-02-08T01:28:41.884+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:41 INFO SparkContext: Invoking stop() from shutdown hook
[2023-02-08T01:28:41.901+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:41 INFO SparkUI: Stopped Spark web UI at http://4e853e3e6d16:4040
[2023-02-08T01:28:41.913+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:41 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-02-08T01:28:41.938+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:41 INFO MemoryStore: MemoryStore cleared
[2023-02-08T01:28:41.939+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:41 INFO BlockManager: BlockManager stopped
[2023-02-08T01:28:41.945+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:41 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-02-08T01:28:41.950+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:41 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-02-08T01:28:41.961+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:41 INFO SparkContext: Successfully stopped SparkContext
[2023-02-08T01:28:41.962+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:41 INFO ShutdownHookManager: Shutdown hook called
[2023-02-08T01:28:41.963+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-a643f247-bf04-4f23-a6a4-3be0279a13ee
[2023-02-08T01:28:41.970+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-a643f247-bf04-4f23-a6a4-3be0279a13ee/pyspark-fa958737-80de-4893-bc46-5fcdf819fa5d
[2023-02-08T01:28:41.977+0000] {spark_submit.py:495} INFO - 23/02/08 01:28:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-46f0a005-0619-4e80-a960-2b28ec4e7b9c
[2023-02-08T01:28:42.133+0000] {taskinstance.py:1322} INFO - Marking task as SUCCESS. dag_id=entsoe-energydata-backfill, task_id=stage_total_generation, execution_date=20210101T060000, start_date=20230208T012746, end_date=20230208T012842
[2023-02-08T01:28:42.216+0000] {local_task_job.py:159} INFO - Task exited with return code 0
[2023-02-08T01:28:42.239+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-02-08T01:32:22.951+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [queued]>
[2023-02-08T01:32:23.027+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [queued]>
[2023-02-08T01:32:23.027+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-02-08T01:32:23.027+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-02-08T01:32:23.028+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-02-08T01:32:23.103+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 06:00:00+00:00
[2023-02-08T01:32:23.144+0000] {standard_task_runner.py:55} INFO - Started process 4323 to run task
[2023-02-08T01:32:23.163+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T06:00:00+00:00', '--job-id', '1130', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpvjdem7bz']
[2023-02-08T01:32:23.171+0000] {standard_task_runner.py:83} INFO - Job 1130: Subtask stage_total_generation
[2023-02-08T01:32:23.627+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [running]> on host 4e853e3e6d16
[2023-02-08T01:32:24.271+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T06:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T06:00:00+00:00
[2023-02-08T01:32:24.308+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-02-08T01:32:24.312+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010600 202101010700 DE_TENNET 2021-01-01 00:00:00 2021-01-01 06:00:00
[2023-02-08T01:32:48.971+0000] {spark_submit.py:495} INFO - mulai periode: 2021-01-01 00:00:00
[2023-02-08T01:32:48.994+0000] {spark_submit.py:495} INFO - selesai periode: 2021-01-01 06:00:00
[2023-02-08T01:32:49.489+0000] {spark_submit.py:495} INFO - 23/02/08 01:32:49 INFO SparkContext: Running Spark version 3.3.1
[2023-02-08T01:32:50.690+0000] {spark_submit.py:495} INFO - 23/02/08 01:32:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-02-08T01:32:51.291+0000] {spark_submit.py:495} INFO - 23/02/08 01:32:51 INFO ResourceUtils: ==============================================================
[2023-02-08T01:32:51.292+0000] {spark_submit.py:495} INFO - 23/02/08 01:32:51 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-02-08T01:32:51.310+0000] {spark_submit.py:495} INFO - 23/02/08 01:32:51 INFO ResourceUtils: ==============================================================
[2023-02-08T01:32:51.312+0000] {spark_submit.py:495} INFO - 23/02/08 01:32:51 INFO SparkContext: Submitted application: gcp_playground
[2023-02-08T01:32:51.406+0000] {spark_submit.py:495} INFO - 23/02/08 01:32:51 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-02-08T01:32:51.417+0000] {spark_submit.py:495} INFO - 23/02/08 01:32:51 INFO ResourceProfile: Limiting resource is cpu
[2023-02-08T01:32:51.425+0000] {spark_submit.py:495} INFO - 23/02/08 01:32:51 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-02-08T01:32:51.601+0000] {spark_submit.py:495} INFO - 23/02/08 01:32:51 INFO SecurityManager: Changing view acls to: ***
[2023-02-08T01:32:51.604+0000] {spark_submit.py:495} INFO - 23/02/08 01:32:51 INFO SecurityManager: Changing modify acls to: ***
[2023-02-08T01:32:51.604+0000] {spark_submit.py:495} INFO - 23/02/08 01:32:51 INFO SecurityManager: Changing view acls groups to:
[2023-02-08T01:32:51.606+0000] {spark_submit.py:495} INFO - 23/02/08 01:32:51 INFO SecurityManager: Changing modify acls groups to:
[2023-02-08T01:32:51.606+0000] {spark_submit.py:495} INFO - 23/02/08 01:32:51 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-02-08T01:32:53.581+0000] {spark_submit.py:495} INFO - 23/02/08 01:32:53 INFO Utils: Successfully started service 'sparkDriver' on port 33969.
[2023-02-08T01:32:54.133+0000] {spark_submit.py:495} INFO - 23/02/08 01:32:54 INFO SparkEnv: Registering MapOutputTracker
[2023-02-08T01:32:54.360+0000] {spark_submit.py:495} INFO - 23/02/08 01:32:54 INFO SparkEnv: Registering BlockManagerMaster
[2023-02-08T01:32:54.434+0000] {spark_submit.py:495} INFO - 23/02/08 01:32:54 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-02-08T01:32:54.437+0000] {spark_submit.py:495} INFO - 23/02/08 01:32:54 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-02-08T01:32:54.487+0000] {spark_submit.py:495} INFO - 23/02/08 01:32:54 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-02-08T01:32:54.586+0000] {spark_submit.py:495} INFO - 23/02/08 01:32:54 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f5f2c41d-c37f-4af1-9f9b-76fddc48c895
[2023-02-08T01:32:54.691+0000] {spark_submit.py:495} INFO - 23/02/08 01:32:54 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-02-08T01:32:54.862+0000] {spark_submit.py:495} INFO - 23/02/08 01:32:54 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-02-08T01:32:56.228+0000] {spark_submit.py:495} INFO - 23/02/08 01:32:56 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-02-08T01:32:56.235+0000] {spark_submit.py:495} INFO - 23/02/08 01:32:56 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2023-02-08T01:32:56.263+0000] {spark_submit.py:495} INFO - 23/02/08 01:32:56 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[2023-02-08T01:32:56.273+0000] {spark_submit.py:495} INFO - 23/02/08 01:32:56 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[2023-02-08T01:32:56.462+0000] {spark_submit.py:495} INFO - 23/02/08 01:32:56 INFO Utils: Successfully started service 'SparkUI' on port 4044.
[2023-02-08T01:32:56.594+0000] {spark_submit.py:495} INFO - 23/02/08 01:32:56 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://4e853e3e6d16:33969/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675819969346
[2023-02-08T01:32:56.598+0000] {spark_submit.py:495} INFO - 23/02/08 01:32:56 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://4e853e3e6d16:33969/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675819969346
[2023-02-08T01:32:56.984+0000] {spark_submit.py:495} INFO - 23/02/08 01:32:56 INFO Executor: Starting executor ID driver on host 4e853e3e6d16
[2023-02-08T01:32:57.135+0000] {spark_submit.py:495} INFO - 23/02/08 01:32:57 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-02-08T01:32:57.219+0000] {spark_submit.py:495} INFO - 23/02/08 01:32:57 INFO Executor: Fetching spark://4e853e3e6d16:33969/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675819969346
[2023-02-08T01:32:57.650+0000] {spark_submit.py:495} INFO - 23/02/08 01:32:57 INFO TransportClientFactory: Successfully created connection to 4e853e3e6d16/172.20.0.4:33969 after 376 ms (0 ms spent in bootstraps)
[2023-02-08T01:32:57.672+0000] {spark_submit.py:495} INFO - 23/02/08 01:32:57 INFO Utils: Fetching spark://4e853e3e6d16:33969/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-4f47ea7f-778f-4946-adca-ceceb3748082/userFiles-69ef0588-4508-4bbc-a643-1d2db1d06d80/fetchFileTemp9845130683450349233.tmp
[2023-02-08T01:33:00.208+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:00 INFO Executor: Adding file:/tmp/spark-4f47ea7f-778f-4946-adca-ceceb3748082/userFiles-69ef0588-4508-4bbc-a643-1d2db1d06d80/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-02-08T01:33:00.211+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:00 INFO Executor: Fetching spark://4e853e3e6d16:33969/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675819969346
[2023-02-08T01:33:00.212+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:00 INFO Utils: Fetching spark://4e853e3e6d16:33969/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-4f47ea7f-778f-4946-adca-ceceb3748082/userFiles-69ef0588-4508-4bbc-a643-1d2db1d06d80/fetchFileTemp9527521255399881542.tmp
[2023-02-08T01:33:01.167+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:01 INFO Executor: Adding file:/tmp/spark-4f47ea7f-778f-4946-adca-ceceb3748082/userFiles-69ef0588-4508-4bbc-a643-1d2db1d06d80/gcs-connector-hadoop3-latest.jar to class loader
[2023-02-08T01:33:01.306+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:01 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33127.
[2023-02-08T01:33:01.321+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:01 INFO NettyBlockTransferService: Server created on 4e853e3e6d16:33127
[2023-02-08T01:33:01.323+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:01 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-02-08T01:33:01.325+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:01 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 4e853e3e6d16, 33127, None)
[2023-02-08T01:33:01.335+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:01 INFO BlockManagerMasterEndpoint: Registering block manager 4e853e3e6d16:33127 with 434.4 MiB RAM, BlockManagerId(driver, 4e853e3e6d16, 33127, None)
[2023-02-08T01:33:01.351+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:01 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 4e853e3e6d16, 33127, None)
[2023-02-08T01:33:01.352+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:01 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 4e853e3e6d16, 33127, None)
[2023-02-08T01:33:09.106+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:09 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-02-08T01:33:09.140+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:09 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-02-08T01:33:20.662+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:20 INFO InMemoryFileIndex: It took 271 ms to list leaf files for 1 paths.
[2023-02-08T01:33:21.811+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:21 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-02-08T01:33:22.054+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:22 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-02-08T01:33:22.071+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:22 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 4e853e3e6d16:33127 (size: 34.6 KiB, free: 434.4 MiB)
[2023-02-08T01:33:22.088+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:22 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-02-08T01:33:24.228+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:24 INFO FileInputFormat: Total input files to process : 1
[2023-02-08T01:33:24.300+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:24 INFO FileInputFormat: Total input files to process : 1
[2023-02-08T01:33:24.433+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:24 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-02-08T01:33:24.790+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:24 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-02-08T01:33:24.796+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:24 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-02-08T01:33:24.801+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:24 INFO DAGScheduler: Parents of final stage: List()
[2023-02-08T01:33:24.829+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:24 INFO DAGScheduler: Missing parents: List()
[2023-02-08T01:33:24.866+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:24 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-02-08T01:33:25.244+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:25 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-02-08T01:33:25.267+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:25 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-02-08T01:33:25.275+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:25 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 4e853e3e6d16:33127 (size: 4.6 KiB, free: 434.4 MiB)
[2023-02-08T01:33:25.287+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:25 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-02-08T01:33:25.349+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-02-08T01:33:25.350+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:25 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-02-08T01:33:25.517+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:25 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (4e853e3e6d16, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-02-08T01:33:25.611+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:25 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-02-08T01:33:26.117+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:26 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010600__202101010700.json:0+9371
[2023-02-08T01:33:27.205+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:27 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3437 bytes result sent to driver
[2023-02-08T01:33:27.251+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:27 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1763 ms on 4e853e3e6d16 (executor driver) (1/1)
[2023-02-08T01:33:27.267+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:27 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-02-08T01:33:27.278+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:27 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 2.295 s
[2023-02-08T01:33:27.302+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:27 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-02-08T01:33:27.304+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-02-08T01:33:27.319+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:27 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 2.858695 s
[2023-02-08T01:33:31.387+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:31 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 4e853e3e6d16:33127 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-02-08T01:33:31.488+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:31 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 4e853e3e6d16:33127 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-02-08T01:33:43.622+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:43 INFO FileSourceStrategy: Pushed Filters:
[2023-02-08T01:33:43.634+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:43 INFO FileSourceStrategy: Post-Scan Filters:
[2023-02-08T01:33:43.634+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:43 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-02-08T01:33:45.268+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:45 INFO CodeGenerator: Code generated in 743.257507 ms
[2023-02-08T01:33:45.281+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:45 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-02-08T01:33:45.323+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:45 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-02-08T01:33:45.327+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:45 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 4e853e3e6d16:33127 (size: 34.5 KiB, free: 434.4 MiB)
[2023-02-08T01:33:45.335+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:45 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-02-08T01:33:45.378+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:45 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-02-08T01:33:45.504+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:45 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-02-08T01:33:45.508+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:45 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-02-08T01:33:45.508+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:45 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-02-08T01:33:45.508+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:45 INFO DAGScheduler: Parents of final stage: List()
[2023-02-08T01:33:45.509+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:45 INFO DAGScheduler: Missing parents: List()
[2023-02-08T01:33:45.515+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:45 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-02-08T01:33:45.565+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:45 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-02-08T01:33:45.645+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:45 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-02-08T01:33:45.662+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:45 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 4e853e3e6d16:33127 (size: 7.7 KiB, free: 434.4 MiB)
[2023-02-08T01:33:45.663+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:45 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-02-08T01:33:45.667+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-02-08T01:33:45.668+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:45 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-02-08T01:33:45.676+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:45 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (4e853e3e6d16, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-02-08T01:33:45.724+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:45 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-02-08T01:33:46.730+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:46 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-02-08T01:33:47.985+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:47 INFO CodeGenerator: Code generated in 739.474306 ms
[2023-02-08T01:33:49.137+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:49 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1671 bytes result sent to driver
[2023-02-08T01:33:49.158+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:49 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 3484 ms on 4e853e3e6d16 (executor driver) (1/1)
[2023-02-08T01:33:49.159+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:49 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 3.609 s
[2023-02-08T01:33:49.159+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:49 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-02-08T01:33:49.162+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:49 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-02-08T01:33:49.163+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-02-08T01:33:49.163+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:49 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 3.658707 s
[2023-02-08T01:33:50.709+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:50 INFO FileSourceStrategy: Pushed Filters:
[2023-02-08T01:33:50.711+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:50 INFO FileSourceStrategy: Post-Scan Filters:
[2023-02-08T01:33:50.735+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:50 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-02-08T01:33:50.873+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:50 INFO CodeGenerator: Code generated in 47.724412 ms
[2023-02-08T01:33:50.890+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:50 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-02-08T01:33:50.934+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:50 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-02-08T01:33:50.935+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:50 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 4e853e3e6d16:33127 (size: 34.5 KiB, free: 434.3 MiB)
[2023-02-08T01:33:50.935+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:50 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-02-08T01:33:50.936+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:50 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-02-08T01:33:51.066+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:51 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-02-08T01:33:51.067+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:51 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-02-08T01:33:51.067+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:51 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-02-08T01:33:51.067+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:51 INFO DAGScheduler: Parents of final stage: List()
[2023-02-08T01:33:51.068+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:51 INFO DAGScheduler: Missing parents: List()
[2023-02-08T01:33:51.078+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:51 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-02-08T01:33:51.100+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:51 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-02-08T01:33:51.113+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:51 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-02-08T01:33:51.118+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:51 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 4e853e3e6d16:33127 (size: 7.7 KiB, free: 434.3 MiB)
[2023-02-08T01:33:51.128+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:51 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-02-08T01:33:51.134+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-02-08T01:33:51.135+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:51 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-02-08T01:33:51.148+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:51 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (4e853e3e6d16, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-02-08T01:33:51.156+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:51 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-02-08T01:33:51.325+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:51 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-02-08T01:33:51.777+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:51 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1585 bytes result sent to driver
[2023-02-08T01:33:51.882+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:51 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 729 ms on 4e853e3e6d16 (executor driver) (1/1)
[2023-02-08T01:33:51.897+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:51 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 0.819 s
[2023-02-08T01:33:51.900+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:51 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-02-08T01:33:51.912+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:51 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-02-08T01:33:51.919+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-02-08T01:33:51.974+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:51 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 0.895837 s
[2023-02-08T01:33:56.517+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:56 INFO FileSourceStrategy: Pushed Filters:
[2023-02-08T01:33:56.554+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:56 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-02-08T01:33:56.555+0000] {spark_submit.py:495} INFO - 23/02/08 01:33:56 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-02-08T01:34:02.805+0000] {spark_submit.py:495} INFO - 23/02/08 01:34:02 INFO CodeGenerator: Code generated in 5562.963067 ms
[2023-02-08T01:34:02.862+0000] {spark_submit.py:495} INFO - 23/02/08 01:34:02 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-02-08T01:34:04.294+0000] {spark_submit.py:495} INFO - 23/02/08 01:34:04 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-02-08T01:34:04.530+0000] {spark_submit.py:495} INFO - 23/02/08 01:34:04 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 4e853e3e6d16:33127 (size: 34.5 KiB, free: 434.3 MiB)
[2023-02-08T01:34:04.539+0000] {spark_submit.py:495} INFO - 23/02/08 01:34:04 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-02-08T01:34:04.551+0000] {spark_submit.py:495} INFO - 23/02/08 01:34:04 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-02-08T01:34:05.794+0000] {spark_submit.py:495} INFO - 23/02/08 01:34:05 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-02-08T01:34:05.824+0000] {spark_submit.py:495} INFO - 23/02/08 01:34:05 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-02-08T01:34:05.824+0000] {spark_submit.py:495} INFO - 23/02/08 01:34:05 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-02-08T01:34:05.825+0000] {spark_submit.py:495} INFO - 23/02/08 01:34:05 INFO DAGScheduler: Parents of final stage: List()
[2023-02-08T01:34:05.832+0000] {spark_submit.py:495} INFO - 23/02/08 01:34:05 INFO DAGScheduler: Missing parents: List()
[2023-02-08T01:34:05.843+0000] {spark_submit.py:495} INFO - 23/02/08 01:34:05 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-02-08T01:34:05.869+0000] {spark_submit.py:495} INFO - 23/02/08 01:34:05 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.6 MiB)
[2023-02-08T01:34:05.904+0000] {spark_submit.py:495} INFO - 23/02/08 01:34:05 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.6 MiB)
[2023-02-08T01:34:05.999+0000] {spark_submit.py:495} INFO - 23/02/08 01:34:05 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 4e853e3e6d16:33127 (size: 12.7 KiB, free: 434.3 MiB)
[2023-02-08T01:34:06.017+0000] {spark_submit.py:495} INFO - 23/02/08 01:34:05 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-02-08T01:34:06.018+0000] {spark_submit.py:495} INFO - 23/02/08 01:34:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-02-08T01:34:06.018+0000] {spark_submit.py:495} INFO - 23/02/08 01:34:05 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-02-08T01:34:06.055+0000] {spark_submit.py:495} INFO - 23/02/08 01:34:06 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (4e853e3e6d16, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-02-08T01:34:06.060+0000] {spark_submit.py:495} INFO - 23/02/08 01:34:06 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-02-08T01:34:06.808+0000] {spark_submit.py:495} INFO - 23/02/08 01:34:06 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-02-08T01:34:09.070+0000] {spark_submit.py:495} INFO - 23/02/08 01:34:09 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1755 bytes result sent to driver
[2023-02-08T01:34:09.090+0000] {spark_submit.py:495} INFO - 23/02/08 01:34:09 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 3047 ms on 4e853e3e6d16 (executor driver) (1/1)
[2023-02-08T01:34:09.091+0000] {spark_submit.py:495} INFO - 23/02/08 01:34:09 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 3.253 s
[2023-02-08T01:34:09.091+0000] {spark_submit.py:495} INFO - 23/02/08 01:34:09 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-02-08T01:34:09.091+0000] {spark_submit.py:495} INFO - 23/02/08 01:34:09 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-02-08T01:34:09.092+0000] {spark_submit.py:495} INFO - 23/02/08 01:34:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-02-08T01:34:09.092+0000] {spark_submit.py:495} INFO - 23/02/08 01:34:09 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 3.298669 s
[2023-02-08T01:34:09.469+0000] {spark_submit.py:495} INFO - 23/02/08 01:34:09 INFO FileSourceStrategy: Pushed Filters:
[2023-02-08T01:34:09.471+0000] {spark_submit.py:495} INFO - 23/02/08 01:34:09 INFO FileSourceStrategy: Post-Scan Filters:
[2023-02-08T01:34:09.474+0000] {spark_submit.py:495} INFO - 23/02/08 01:34:09 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-02-08T01:34:09.700+0000] {spark_submit.py:495} INFO - 23/02/08 01:34:09 INFO CodeGenerator: Code generated in 117.127791 ms
[2023-02-08T01:34:09.725+0000] {spark_submit.py:495} INFO - 23/02/08 01:34:09 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.4 MiB)
[2023-02-08T01:34:10.559+0000] {spark_submit.py:495} INFO - 23/02/08 01:34:10 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-02-08T01:34:10.586+0000] {spark_submit.py:495} INFO - 23/02/08 01:34:10 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 4e853e3e6d16:33127 (size: 34.5 KiB, free: 434.2 MiB)
[2023-02-08T01:34:10.598+0000] {spark_submit.py:495} INFO - 23/02/08 01:34:10 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-02-08T01:34:10.626+0000] {spark_submit.py:495} INFO - 23/02/08 01:34:10 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-02-08T01:34:11.638+0000] {spark_submit.py:495} INFO - 23/02/08 01:34:11 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-02-08T01:34:11.687+0000] {spark_submit.py:495} INFO - 23/02/08 01:34:11 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-02-08T01:34:11.688+0000] {spark_submit.py:495} INFO - 23/02/08 01:34:11 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-02-08T01:34:11.688+0000] {spark_submit.py:495} INFO - 23/02/08 01:34:11 INFO DAGScheduler: Parents of final stage: List()
[2023-02-08T01:34:11.689+0000] {spark_submit.py:495} INFO - 23/02/08 01:34:11 INFO DAGScheduler: Missing parents: List()
[2023-02-08T01:34:11.720+0000] {spark_submit.py:495} INFO - 23/02/08 01:34:11 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-02-08T01:34:11.856+0000] {spark_submit.py:495} INFO - 23/02/08 01:34:11 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-02-08T01:34:12.010+0000] {spark_submit.py:495} INFO - 23/02/08 01:34:12 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-02-08T01:34:12.017+0000] {spark_submit.py:495} INFO - 23/02/08 01:34:12 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 4e853e3e6d16:33127 (size: 7.6 KiB, free: 434.2 MiB)
[2023-02-08T01:34:12.152+0000] {spark_submit.py:495} INFO - 23/02/08 01:34:12 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-02-08T01:34:12.222+0000] {spark_submit.py:495} INFO - 23/02/08 01:34:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-02-08T01:34:12.236+0000] {spark_submit.py:495} INFO - 23/02/08 01:34:12 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-02-08T01:34:12.360+0000] {spark_submit.py:495} INFO - 23/02/08 01:34:12 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (4e853e3e6d16, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-02-08T01:34:12.365+0000] {spark_submit.py:495} INFO - 23/02/08 01:34:12 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-02-08T01:34:13.254+0000] {spark_submit.py:495} INFO - 23/02/08 01:34:13 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-02-08T01:35:09.306+0000] {spark_submit.py:495} INFO - 23/02/08 01:35:09 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,ArraySeq(),HashMap((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@5f01a8ae)) by listener AppStatusListener took 5.160269052s.
[2023-02-08T01:35:11.578+0000] {spark_submit.py:495} INFO - 23/02/08 01:35:11 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 4e853e3e6d16:33127 in memory (size: 7.7 KiB, free: 434.2 MiB)
[2023-02-08T01:35:12.097+0000] {spark_submit.py:495} INFO - 23/02/08 01:35:12 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 4e853e3e6d16:33127 in memory (size: 12.7 KiB, free: 434.3 MiB)
[2023-02-08T01:35:12.890+0000] {spark_submit.py:495} INFO - 23/02/08 01:35:12 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 4e853e3e6d16:33127 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-02-08T01:36:14.333+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:14 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1629 bytes result sent to driver
[2023-02-08T01:36:14.336+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:14 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 121979 ms on 4e853e3e6d16 (executor driver) (1/1)
[2023-02-08T01:36:14.336+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:14 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-02-08T01:36:14.336+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:14 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 122.597 s
[2023-02-08T01:36:14.337+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:14 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-02-08T01:36:14.337+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-02-08T01:36:14.337+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:14 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 122.182347 s
[2023-02-08T01:36:15.239+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:15 INFO FileSourceStrategy: Pushed Filters:
[2023-02-08T01:36:15.239+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:15 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-02-08T01:36:15.239+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:15 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-02-08T01:36:15.521+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:15 INFO CodeGenerator: Code generated in 162.3217 ms
[2023-02-08T01:36:15.526+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:15 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.3 MiB)
[2023-02-08T01:36:15.543+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:15 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-02-08T01:36:15.545+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:15 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 4e853e3e6d16:33127 (size: 34.5 KiB, free: 434.2 MiB)
[2023-02-08T01:36:15.555+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:15 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:204
[2023-02-08T01:36:15.555+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-02-08T01:36:15.603+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:15 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:204
[2023-02-08T01:36:15.603+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:15 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:204) with 1 output partitions
[2023-02-08T01:36:15.604+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:15 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:204)
[2023-02-08T01:36:15.604+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:15 INFO DAGScheduler: Parents of final stage: List()
[2023-02-08T01:36:15.604+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:15 INFO DAGScheduler: Missing parents: List()
[2023-02-08T01:36:15.605+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:15 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:204), which has no missing parents
[2023-02-08T01:36:15.615+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:15 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.2 MiB)
[2023-02-08T01:36:15.615+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:15 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.2 MiB)
[2023-02-08T01:36:15.615+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:15 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 4e853e3e6d16:33127 (size: 13.4 KiB, free: 434.2 MiB)
[2023-02-08T01:36:15.627+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:15 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-02-08T01:36:15.627+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:204) (first 15 tasks are for partitions Vector(0))
[2023-02-08T01:36:15.627+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:15 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-02-08T01:36:15.631+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:15 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (4e853e3e6d16, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-02-08T01:36:15.631+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:15 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-02-08T01:36:15.659+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:15 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-02-08T01:36:15.804+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:15 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2499 bytes result sent to driver
[2023-02-08T01:36:15.809+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:15 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 177 ms on 4e853e3e6d16 (executor driver) (1/1)
[2023-02-08T01:36:15.810+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:15 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:204) finished in 0.202 s
[2023-02-08T01:36:15.810+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:15 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-02-08T01:36:15.810+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:15 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-02-08T01:36:15.811+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-02-08T01:36:15.815+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:15 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:204, took 0.212861 s
[2023-02-08T01:36:16.046+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:16 INFO FileSourceStrategy: Pushed Filters:
[2023-02-08T01:36:16.049+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:16 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-02-08T01:36:16.051+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:16 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-02-08T01:36:16.430+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:16 INFO CodeGenerator: Code generated in 195.427472 ms
[2023-02-08T01:36:16.459+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:16 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 433.0 MiB)
[2023-02-08T01:36:16.525+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:16 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.9 MiB)
[2023-02-08T01:36:16.527+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:16 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 4e853e3e6d16:33127 (size: 34.5 KiB, free: 434.2 MiB)
[2023-02-08T01:36:16.529+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:16 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:205
[2023-02-08T01:36:16.532+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:16 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-02-08T01:36:16.573+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:16 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:205
[2023-02-08T01:36:16.576+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:16 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:205) with 1 output partitions
[2023-02-08T01:36:16.577+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:16 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:205)
[2023-02-08T01:36:16.577+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:16 INFO DAGScheduler: Parents of final stage: List()
[2023-02-08T01:36:16.578+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:16 INFO DAGScheduler: Missing parents: List()
[2023-02-08T01:36:16.583+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:16 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:205), which has no missing parents
[2023-02-08T01:36:16.584+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:16 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 432.9 MiB)
[2023-02-08T01:36:16.589+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:16 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.9 MiB)
[2023-02-08T01:36:16.590+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:16 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 4e853e3e6d16:33127 (size: 12.6 KiB, free: 434.2 MiB)
[2023-02-08T01:36:16.592+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:16 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-02-08T01:36:16.593+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:205) (first 15 tasks are for partitions Vector(0))
[2023-02-08T01:36:16.594+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:16 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-02-08T01:36:16.596+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:16 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (4e853e3e6d16, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-02-08T01:36:16.597+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:16 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-02-08T01:36:16.610+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:16 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-02-08T01:36:16.785+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:16 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-02-08T01:36:16.796+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:16 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 200 ms on 4e853e3e6d16 (executor driver) (1/1)
[2023-02-08T01:36:16.799+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:16 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-02-08T01:36:16.799+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:16 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:205) finished in 0.218 s
[2023-02-08T01:36:16.800+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:16 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-02-08T01:36:16.801+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-02-08T01:36:16.801+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:16 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:205, took 0.226170 s
[2023-02-08T01:36:17.059+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:17 INFO FileSourceStrategy: Pushed Filters:
[2023-02-08T01:36:17.064+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:17 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-02-08T01:36:17.065+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:17 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-02-08T01:36:17.477+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:17 INFO CodeGenerator: Code generated in 227.145091 ms
[2023-02-08T01:36:17.503+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:17 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.7 MiB)
[2023-02-08T01:36:17.533+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:17 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.7 MiB)
[2023-02-08T01:36:17.548+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:17 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 4e853e3e6d16:33127 (size: 34.5 KiB, free: 434.1 MiB)
[2023-02-08T01:36:17.549+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:17 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:206
[2023-02-08T01:36:17.551+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:17 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-02-08T01:36:17.690+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:17 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:206
[2023-02-08T01:36:17.694+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:17 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:206) with 1 output partitions
[2023-02-08T01:36:17.694+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:17 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:206)
[2023-02-08T01:36:17.695+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:17 INFO DAGScheduler: Parents of final stage: List()
[2023-02-08T01:36:17.695+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:17 INFO DAGScheduler: Missing parents: List()
[2023-02-08T01:36:17.696+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:17 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:206), which has no missing parents
[2023-02-08T01:36:17.708+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:17 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.6 MiB)
[2023-02-08T01:36:17.713+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:17 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.6 MiB)
[2023-02-08T01:36:17.718+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:17 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 4e853e3e6d16:33127 (size: 12.6 KiB, free: 434.1 MiB)
[2023-02-08T01:36:17.720+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:17 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-02-08T01:36:17.725+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:206) (first 15 tasks are for partitions Vector(0))
[2023-02-08T01:36:17.726+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:17 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-02-08T01:36:17.732+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:17 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (4e853e3e6d16, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-02-08T01:36:17.737+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:17 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-02-08T01:36:17.757+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:17 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-02-08T01:36:17.891+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:17 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1729 bytes result sent to driver
[2023-02-08T01:36:17.895+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:17 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 164 ms on 4e853e3e6d16 (executor driver) (1/1)
[2023-02-08T01:36:17.902+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:17 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-02-08T01:36:17.903+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:17 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:206) finished in 0.203 s
[2023-02-08T01:36:17.903+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:17 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-02-08T01:36:17.904+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-02-08T01:36:17.904+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:17 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:206, took 0.212812 s
[2023-02-08T01:36:18.941+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:18 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 4e853e3e6d16:33127 in memory (size: 12.6 KiB, free: 434.1 MiB)
[2023-02-08T01:36:18.994+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:18 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 4e853e3e6d16:33127 in memory (size: 12.6 KiB, free: 434.1 MiB)
[2023-02-08T01:36:19.036+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:19 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 4e853e3e6d16:33127 in memory (size: 13.4 KiB, free: 434.2 MiB)
[2023-02-08T01:36:19.120+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:19 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 4e853e3e6d16:33127 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-02-08T01:36:19.217+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:19 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 4e853e3e6d16:33127 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-02-08T01:36:24.149+0000] {spark_submit.py:495} INFO - 23/02/08 01:36:24 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 4e853e3e6d16:33127 in memory (size: 34.5 KiB, free: 434.3 MiB)
