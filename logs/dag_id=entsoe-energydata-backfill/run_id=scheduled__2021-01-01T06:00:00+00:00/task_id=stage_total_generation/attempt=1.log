[2023-01-31T05:11:09.107+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [queued]>
[2023-01-31T05:11:09.182+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [queued]>
[2023-01-31T05:11:09.182+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T05:11:09.183+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T05:11:09.183+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T05:11:09.321+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 06:00:00+00:00
[2023-01-31T05:11:09.348+0000] {standard_task_runner.py:55} INFO - Started process 12528 to run task
[2023-01-31T05:11:09.368+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T06:00:00+00:00', '--job-id', '885', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpn2j849r1']
[2023-01-31T05:11:09.376+0000] {standard_task_runner.py:83} INFO - Job 885: Subtask stage_total_generation
[2023-01-31T05:11:09.900+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [running]> on host 81d5fcd0285b
[2023-01-31T05:11:10.371+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T06:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T06:00:00+00:00
[2023-01-31T05:11:10.417+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T05:11:10.421+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010600 202101010700 DE_TENNET
[2023-01-31T05:11:42.661+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:42 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T05:11:43.704+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T05:11:45.161+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:45 INFO ResourceUtils: ==============================================================
[2023-01-31T05:11:45.170+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:45 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T05:11:45.177+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:45 INFO ResourceUtils: ==============================================================
[2023-01-31T05:11:45.179+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:45 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T05:11:45.379+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:45 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T05:11:45.442+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:45 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T05:11:45.461+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:45 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T05:11:45.987+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:45 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T05:11:45.995+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:45 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T05:11:46.001+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:46 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T05:11:46.012+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:46 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T05:11:46.018+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T05:11:48.404+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:48 INFO Utils: Successfully started service 'sparkDriver' on port 39581.
[2023-01-31T05:11:48.685+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:48 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T05:11:49.321+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:49 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T05:11:49.757+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:49 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T05:11:49.773+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:49 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T05:11:49.824+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:49 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T05:11:50.331+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:50 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-368630be-9f91-445a-b7a0-479a11f3eb1a
[2023-01-31T05:11:50.597+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:50 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T05:11:50.908+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:50 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T05:11:53.017+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:53 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T05:11:53.021+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:53 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2023-01-31T05:11:53.031+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:53 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[2023-01-31T05:11:53.042+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:53 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[2023-01-31T05:11:53.052+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:53 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[2023-01-31T05:11:53.062+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:53 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
[2023-01-31T05:11:53.125+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:53 INFO Utils: Successfully started service 'SparkUI' on port 4046.
[2023-01-31T05:11:53.527+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:53 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://81d5fcd0285b:39581/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675141902601
[2023-01-31T05:11:53.538+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:53 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://81d5fcd0285b:39581/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675141902601
[2023-01-31T05:11:54.500+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:54 INFO Executor: Starting executor ID driver on host 81d5fcd0285b
[2023-01-31T05:11:54.564+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:54 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T05:11:54.740+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:54 INFO Executor: Fetching spark://81d5fcd0285b:39581/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675141902601
[2023-01-31T05:11:55.677+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:55 INFO TransportClientFactory: Successfully created connection to 81d5fcd0285b/172.19.0.9:39581 after 660 ms (0 ms spent in bootstraps)
[2023-01-31T05:11:55.737+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:55 INFO Utils: Fetching spark://81d5fcd0285b:39581/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-d7a6afd5-72c7-4f7d-9113-17e132c0fd15/userFiles-c4bf4bdc-78a7-4e0d-812d-a4b414c5bfca/fetchFileTemp17605483383629990865.tmp
[2023-01-31T05:11:58.014+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:58 INFO Executor: Adding file:/tmp/spark-d7a6afd5-72c7-4f7d-9113-17e132c0fd15/userFiles-c4bf4bdc-78a7-4e0d-812d-a4b414c5bfca/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T05:11:58.018+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:58 INFO Executor: Fetching spark://81d5fcd0285b:39581/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675141902601
[2023-01-31T05:11:58.077+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:58 INFO Utils: Fetching spark://81d5fcd0285b:39581/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-d7a6afd5-72c7-4f7d-9113-17e132c0fd15/userFiles-c4bf4bdc-78a7-4e0d-812d-a4b414c5bfca/fetchFileTemp14632098204375159938.tmp
[2023-01-31T05:11:58.792+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:58 INFO Executor: Adding file:/tmp/spark-d7a6afd5-72c7-4f7d-9113-17e132c0fd15/userFiles-c4bf4bdc-78a7-4e0d-812d-a4b414c5bfca/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T05:11:58.882+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:58 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34067.
[2023-01-31T05:11:58.883+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:58 INFO NettyBlockTransferService: Server created on 81d5fcd0285b:34067
[2023-01-31T05:11:58.892+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:58 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T05:11:58.947+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:58 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 34067, None)
[2023-01-31T05:11:58.972+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:58 INFO BlockManagerMasterEndpoint: Registering block manager 81d5fcd0285b:34067 with 434.4 MiB RAM, BlockManagerId(driver, 81d5fcd0285b, 34067, None)
[2023-01-31T05:11:59.003+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:59 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 34067, None)
[2023-01-31T05:11:59.011+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:59 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 81d5fcd0285b, 34067, None)
[2023-01-31T05:12:05.919+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:05 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T05:12:06.067+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:06 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T05:12:29.427+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:29 INFO InMemoryFileIndex: It took 784 ms to list leaf files for 1 paths.
[2023-01-31T05:12:32.921+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:32 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T05:12:34.261+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:34 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T05:12:34.280+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:34 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 81d5fcd0285b:34067 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T05:12:34.344+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:34 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T05:12:39.898+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:39 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T05:12:40.216+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:40 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T05:12:40.481+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:40 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T05:12:40.639+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:40 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T05:12:40.643+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:40 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T05:12:40.647+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:40 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:12:40.662+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:40 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:12:40.700+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:40 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T05:12:41.402+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:41 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T05:12:41.462+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:41 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T05:12:41.464+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:41 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 81d5fcd0285b:34067 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T05:12:41.470+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:41 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:12:41.615+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:12:41.618+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:41 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T05:12:41.847+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:41 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T05:12:41.984+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:41 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T05:12:42.702+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:42 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010600__202101010700.json:0+9371
[2023-01-31T05:12:44.183+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:44 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3437 bytes result sent to driver
[2023-01-31T05:12:44.288+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:44 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2487 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:12:44.297+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:44 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T05:12:44.519+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:44 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 3.607 s
[2023-01-31T05:12:44.581+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:44 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:12:44.583+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T05:12:44.610+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:44 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 4.127886 s
[2023-01-31T05:12:49.114+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:49 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 81d5fcd0285b:34067 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T05:12:49.218+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:49 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 81d5fcd0285b:34067 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T05:13:11.399+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:11 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:11.410+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:11 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:13:11.411+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:11 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:13.834+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:13 INFO CodeGenerator: Code generated in 1233.737123 ms
[2023-01-31T05:13:13.847+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:13 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T05:13:13.896+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:13 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T05:13:13.903+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:13 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 81d5fcd0285b:34067 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T05:13:13.905+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:13 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T05:13:13.945+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:13 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:14.317+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T05:13:14.317+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T05:13:14.317+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T05:13:14.317+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:14.318+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:14.318+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T05:13:14.376+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T05:13:14.413+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T05:13:14.416+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 81d5fcd0285b:34067 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T05:13:14.420+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:14.427+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:14.434+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T05:13:14.458+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:14.466+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T05:13:15.055+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T05:13:15.476+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO CodeGenerator: Code generated in 361.444883 ms
[2023-01-31T05:13:15.905+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1628 bytes result sent to driver
[2023-01-31T05:13:15.931+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1481 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:15.937+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 1.602 s
[2023-01-31T05:13:15.939+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:15.940+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T05:13:15.944+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T05:13:15.950+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 1.651254 s
[2023-01-31T05:13:16.546+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:16 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:16.546+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:16 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:13:16.551+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:16 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:16.907+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:16 INFO CodeGenerator: Code generated in 216.423158 ms
[2023-01-31T05:13:17.000+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:16 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T05:13:17.485+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T05:13:17.490+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 81d5fcd0285b:34067 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:13:17.500+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T05:13:17.507+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:17.671+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T05:13:17.676+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T05:13:17.676+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T05:13:17.677+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:17.677+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:17.684+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T05:13:17.705+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T05:13:17.729+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T05:13:17.737+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 81d5fcd0285b:34067 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T05:13:17.743+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:17.748+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:17.751+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T05:13:17.759+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:17.765+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T05:13:17.871+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T05:13:18.268+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1628 bytes result sent to driver
[2023-01-31T05:13:18.303+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 546 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:18.320+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 0.624 s
[2023-01-31T05:13:18.321+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:18.324+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T05:13:18.332+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T05:13:18.333+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 0.655656 s
[2023-01-31T05:13:19.962+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:19.965+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T05:13:19.966+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:20.870+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO CodeGenerator: Code generated in 658.917522 ms
[2023-01-31T05:13:20.886+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T05:13:20.936+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T05:13:20.938+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 81d5fcd0285b:34067 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:13:20.946+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T05:13:20.949+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:21.103+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T05:13:21.104+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T05:13:21.104+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T05:13:21.105+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:21.116+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:21.118+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T05:13:21.128+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.6 MiB)
[2023-01-31T05:13:21.153+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.6 MiB)
[2023-01-31T05:13:21.157+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 81d5fcd0285b:34067 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T05:13:21.158+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:21.163+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:21.163+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T05:13:21.167+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:21.168+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T05:13:21.212+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T05:13:21.448+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-01-31T05:13:21.460+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 293 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:21.463+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T05:13:21.469+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 0.346 s
[2023-01-31T05:13:21.469+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:21.471+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T05:13:21.476+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 0.375060 s
[2023-01-31T05:13:21.642+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:21.642+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:13:21.644+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:21.799+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO CodeGenerator: Code generated in 94.697242 ms
[2023-01-31T05:13:21.836+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.4 MiB)
[2023-01-31T05:13:22.034+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T05:13:22.037+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 81d5fcd0285b:34067 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:13:22.046+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T05:13:22.062+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:22.503+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T05:13:22.517+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T05:13:22.518+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T05:13:22.519+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:22.519+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:22.526+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T05:13:22.577+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-31T05:13:22.636+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-01-31T05:13:22.643+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 81d5fcd0285b:34067 (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T05:13:22.657+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:22.659+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:22.660+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T05:13:22.675+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:22.677+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T05:13:22.809+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T05:13:23.064+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 81d5fcd0285b:34067 in memory (size: 7.7 KiB, free: 434.2 MiB)
[2023-01-31T05:13:23.143+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 81d5fcd0285b:34067 in memory (size: 7.7 KiB, free: 434.2 MiB)
[2023-01-31T05:13:23.267+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 81d5fcd0285b:34067 in memory (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T05:13:23.326+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1629 bytes result sent to driver
[2023-01-31T05:13:23.333+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 659 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:23.334+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T05:13:23.337+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 0.797 s
[2023-01-31T05:13:23.337+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:23.339+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T05:13:23.339+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 0.831941 s
[2023-01-31T05:13:25.181+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:25.201+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T05:13:25.202+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:25.564+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO CodeGenerator: Code generated in 300.422854 ms
[2023-01-31T05:13:25.580+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.3 MiB)
[2023-01-31T05:13:25.600+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T05:13:25.606+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 81d5fcd0285b:34067 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:13:25.610+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T05:13:25.618+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:25.769+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T05:13:25.772+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) with 1 output partitions
[2023-01-31T05:13:25.772+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201)
[2023-01-31T05:13:25.773+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:25.773+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:25.780+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201), which has no missing parents
[2023-01-31T05:13:25.811+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.2 MiB)
[2023-01-31T05:13:25.835+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.2 MiB)
[2023-01-31T05:13:25.855+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 81d5fcd0285b:34067 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T05:13:25.863+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:25.872+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:25.878+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T05:13:25.893+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:25.925+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T05:13:26.036+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T05:13:26.271+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2499 bytes result sent to driver
[2023-01-31T05:13:26.286+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 384 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:26.286+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T05:13:26.287+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) finished in 0.499 s
[2023-01-31T05:13:26.287+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:26.287+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T05:13:26.287+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201, took 0.510457 s
[2023-01-31T05:13:27.165+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:27 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:27.229+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:27 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T05:13:27.229+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:27 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:28.523+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO CodeGenerator: Code generated in 746.151762 ms
[2023-01-31T05:13:28.595+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 433.0 MiB)
[2023-01-31T05:13:28.776+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.9 MiB)
[2023-01-31T05:13:28.781+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 81d5fcd0285b:34067 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:13:28.790+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T05:13:28.804+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:28.986+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T05:13:28.990+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T05:13:28.991+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T05:13:28.997+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:28.998+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:29.010+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T05:13:29.054+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 432.9 MiB)
[2023-01-31T05:13:29.083+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.9 MiB)
[2023-01-31T05:13:29.089+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 81d5fcd0285b:34067 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T05:13:29.097+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:29.107+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:29.112+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T05:13:29.124+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:29.127+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T05:13:29.186+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T05:13:29.457+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-01-31T05:13:29.500+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 362 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:29.509+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T05:13:29.513+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 0.467 s
[2023-01-31T05:13:29.514+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:29.524+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T05:13:29.524+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 0.498966 s
[2023-01-31T05:13:29.808+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:29.812+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T05:13:29.816+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:30.172+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO CodeGenerator: Code generated in 212.318126 ms
[2023-01-31T05:13:30.188+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.7 MiB)
[2023-01-31T05:13:30.240+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.7 MiB)
[2023-01-31T05:13:30.240+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 81d5fcd0285b:34067 (size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T05:13:30.256+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T05:13:30.276+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:30.435+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T05:13:30.437+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T05:13:30.437+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T05:13:30.438+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:30.438+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:30.458+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T05:13:30.477+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.6 MiB)
[2023-01-31T05:13:30.486+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.6 MiB)
[2023-01-31T05:13:30.491+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 81d5fcd0285b:34067 (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T05:13:30.494+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:30.501+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:30.503+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T05:13:30.511+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:30.512+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T05:13:30.684+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T05:13:31.287+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:31 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1729 bytes result sent to driver
[2023-01-31T05:13:31.303+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:31 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 794 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:31.305+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:31 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T05:13:31.377+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:31 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 0.851 s
[2023-01-31T05:13:31.378+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:31 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:31.426+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T05:13:31.442+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:31 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 0.992042 s
[2023-01-31T05:13:38.220+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 81d5fcd0285b:34067 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:13:38.305+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 81d5fcd0285b:34067 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T05:13:38.388+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 81d5fcd0285b:34067 in memory (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T05:13:38.467+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 81d5fcd0285b:34067 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T05:13:38.555+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 81d5fcd0285b:34067 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:13:38.641+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 81d5fcd0285b:34067 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:13:38.708+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 81d5fcd0285b:34067 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:13:38.785+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 81d5fcd0285b:34067 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:13:38.918+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 81d5fcd0285b:34067 in memory (size: 13.4 KiB, free: 434.3 MiB)
[2023-01-31T05:13:39.005+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:39 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 81d5fcd0285b:34067 in memory (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T05:14:00.152+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:00 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:14:00.190+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:14:00.191+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:14:00.195+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:00 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:14:00.200+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:14:00.201+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:14:00.211+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:14:01.488+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-31T05:14:01.490+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO DAGScheduler: Got job 8 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T05:14:01.490+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO DAGScheduler: Final stage: ResultStage 8 (save at BigQueryWriteHelper.java:105)
[2023-01-31T05:14:01.491+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:14:01.499+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:14:01.500+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO DAGScheduler: Submitting ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T05:14:01.792+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 209.9 KiB, free 434.0 MiB)
[2023-01-31T05:14:01.823+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 74.3 KiB, free 433.9 MiB)
[2023-01-31T05:14:01.830+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 81d5fcd0285b:34067 (size: 74.3 KiB, free: 434.3 MiB)
[2023-01-31T05:14:01.836+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:14:01.841+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:14:01.843+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2023-01-31T05:14:01.865+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7484 bytes) taskResourceAssignments Map()
[2023-01-31T05:14:01.887+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2023-01-31T05:14:02.961+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:14:02.967+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:14:02.984+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:02 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:14:02.988+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:14:02.991+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:14:03.014+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:14:03.131+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:03 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T05:14:03.198+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:03 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T05:14:03.378+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:03 INFO ParquetOutputFormat: Parquet block size to 134217728
[2023-01-31T05:14:03.380+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:03 INFO ParquetOutputFormat: Validation is off
[2023-01-31T05:14:03.381+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:03 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-01-31T05:14:03.387+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:03 INFO ParquetOutputFormat: Parquet properties are:
[2023-01-31T05:14:03.387+0000] {spark_submit.py:495} INFO - Parquet page size to 1048576
[2023-01-31T05:14:03.388+0000] {spark_submit.py:495} INFO - Parquet dictionary page size to 1048576
[2023-01-31T05:14:03.388+0000] {spark_submit.py:495} INFO - Dictionary is true
[2023-01-31T05:14:03.388+0000] {spark_submit.py:495} INFO - Writer version is: PARQUET_1_0
[2023-01-31T05:14:03.391+0000] {spark_submit.py:495} INFO - Page size checking is: estimated
[2023-01-31T05:14:03.391+0000] {spark_submit.py:495} INFO - Min row count for page size check is: 100
[2023-01-31T05:14:03.394+0000] {spark_submit.py:495} INFO - Max row count for page size check is: 10000
[2023-01-31T05:14:03.395+0000] {spark_submit.py:495} INFO - Truncate length for column indexes is: 64
[2023-01-31T05:14:03.397+0000] {spark_submit.py:495} INFO - Truncate length for statistics min/max  is: 2147483647
[2023-01-31T05:14:03.398+0000] {spark_submit.py:495} INFO - Bloom filter enabled: false
[2023-01-31T05:14:03.401+0000] {spark_submit.py:495} INFO - Max Bloom filter size for a column is 1048576
[2023-01-31T05:14:03.401+0000] {spark_submit.py:495} INFO - Bloom filter expected number of distinct values are: null
[2023-01-31T05:14:03.404+0000] {spark_submit.py:495} INFO - Page row count limit to 20000
[2023-01-31T05:14:03.406+0000] {spark_submit.py:495} INFO - Writing page checksums is: on
[2023-01-31T05:14:03.580+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:03 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-01-31T05:14:03.580+0000] {spark_submit.py:495} INFO - {
[2023-01-31T05:14:03.581+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-31T05:14:03.581+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-31T05:14:03.582+0000] {spark_submit.py:495} INFO - "name" : "measured_at",
[2023-01-31T05:14:03.582+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T05:14:03.583+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T05:14:03.585+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:03.586+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:03.586+0000] {spark_submit.py:495} INFO - "name" : "created_at",
[2023-01-31T05:14:03.587+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T05:14:03.588+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T05:14:03.592+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:03.593+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:03.594+0000] {spark_submit.py:495} INFO - "name" : "Biomass - Actual Aggregated",
[2023-01-31T05:14:03.594+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:03.595+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:03.595+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:03.595+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:03.596+0000] {spark_submit.py:495} INFO - "name" : "Fossil Brown coal/Lignite - Actual Aggregated",
[2023-01-31T05:14:03.596+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:03.602+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:03.603+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:03.603+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:03.604+0000] {spark_submit.py:495} INFO - "name" : "Fossil Gas - Actual Aggregated",
[2023-01-31T05:14:03.604+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:03.605+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:03.606+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:03.606+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:03.612+0000] {spark_submit.py:495} INFO - "name" : "Fossil Hard coal - Actual Aggregated",
[2023-01-31T05:14:03.614+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:03.615+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:03.615+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:03.616+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:03.616+0000] {spark_submit.py:495} INFO - "name" : "Fossil Oil - Actual Aggregated",
[2023-01-31T05:14:03.617+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:03.617+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:03.618+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:03.618+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:03.618+0000] {spark_submit.py:495} INFO - "name" : "Geothermal - Actual Aggregated",
[2023-01-31T05:14:03.619+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:03.619+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:03.620+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:03.621+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:03.623+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Aggregated",
[2023-01-31T05:14:03.627+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:03.628+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:03.628+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:03.636+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:03.637+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Consumption",
[2023-01-31T05:14:03.638+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:03.638+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:03.638+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:03.641+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:03.641+0000] {spark_submit.py:495} INFO - "name" : "Hydro Run-of-river and poundage - Actual Aggregated",
[2023-01-31T05:14:03.642+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:03.644+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:03.645+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:03.645+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:03.646+0000] {spark_submit.py:495} INFO - "name" : "Hydro Water Reservoir - Actual Aggregated",
[2023-01-31T05:14:03.653+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:03.657+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:03.658+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:03.661+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:03.662+0000] {spark_submit.py:495} INFO - "name" : "Nuclear - Actual Aggregated",
[2023-01-31T05:14:03.663+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:03.664+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:03.664+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:03.665+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:03.665+0000] {spark_submit.py:495} INFO - "name" : "Other - Actual Aggregated",
[2023-01-31T05:14:03.666+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:03.666+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:03.667+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:03.668+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:03.668+0000] {spark_submit.py:495} INFO - "name" : "Other renewable - Actual Aggregated",
[2023-01-31T05:14:03.669+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:03.669+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:03.670+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:03.670+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:03.671+0000] {spark_submit.py:495} INFO - "name" : "Solar - Actual Aggregated",
[2023-01-31T05:14:03.671+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:03.672+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:03.672+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:03.673+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:03.673+0000] {spark_submit.py:495} INFO - "name" : "Waste - Actual Aggregated",
[2023-01-31T05:14:03.673+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:03.674+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:03.674+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:03.675+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:03.675+0000] {spark_submit.py:495} INFO - "name" : "Wind Offshore - Actual Aggregated",
[2023-01-31T05:14:03.676+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:03.676+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:03.677+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:03.677+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:03.678+0000] {spark_submit.py:495} INFO - "name" : "Wind Onshore - Actual Aggregated",
[2023-01-31T05:14:03.678+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:03.679+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:03.679+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:03.679+0000] {spark_submit.py:495} INFO - } ]
[2023-01-31T05:14:03.680+0000] {spark_submit.py:495} INFO - }
[2023-01-31T05:14:03.680+0000] {spark_submit.py:495} INFO - and corresponding Parquet message type:
[2023-01-31T05:14:03.681+0000] {spark_submit.py:495} INFO - message spark_schema {
[2023-01-31T05:14:03.681+0000] {spark_submit.py:495} INFO - required int96 measured_at;
[2023-01-31T05:14:03.682+0000] {spark_submit.py:495} INFO - required int96 created_at;
[2023-01-31T05:14:03.682+0000] {spark_submit.py:495} INFO - optional double Biomass - Actual Aggregated;
[2023-01-31T05:14:03.683+0000] {spark_submit.py:495} INFO - optional double Fossil Brown coal/Lignite - Actual Aggregated;
[2023-01-31T05:14:03.683+0000] {spark_submit.py:495} INFO - optional double Fossil Gas - Actual Aggregated;
[2023-01-31T05:14:03.683+0000] {spark_submit.py:495} INFO - optional double Fossil Hard coal - Actual Aggregated;
[2023-01-31T05:14:03.684+0000] {spark_submit.py:495} INFO - optional double Fossil Oil - Actual Aggregated;
[2023-01-31T05:14:03.684+0000] {spark_submit.py:495} INFO - optional double Geothermal - Actual Aggregated;
[2023-01-31T05:14:03.685+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Aggregated;
[2023-01-31T05:14:03.685+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Consumption;
[2023-01-31T05:14:03.685+0000] {spark_submit.py:495} INFO - optional double Hydro Run-of-river and poundage - Actual Aggregated;
[2023-01-31T05:14:03.686+0000] {spark_submit.py:495} INFO - optional double Hydro Water Reservoir - Actual Aggregated;
[2023-01-31T05:14:03.686+0000] {spark_submit.py:495} INFO - optional double Nuclear - Actual Aggregated;
[2023-01-31T05:14:03.687+0000] {spark_submit.py:495} INFO - optional double Other - Actual Aggregated;
[2023-01-31T05:14:03.687+0000] {spark_submit.py:495} INFO - optional double Other renewable - Actual Aggregated;
[2023-01-31T05:14:03.688+0000] {spark_submit.py:495} INFO - optional double Solar - Actual Aggregated;
[2023-01-31T05:14:03.688+0000] {spark_submit.py:495} INFO - optional double Waste - Actual Aggregated;
[2023-01-31T05:14:03.689+0000] {spark_submit.py:495} INFO - optional double Wind Offshore - Actual Aggregated;
[2023-01-31T05:14:03.689+0000] {spark_submit.py:495} INFO - optional double Wind Onshore - Actual Aggregated;
[2023-01-31T05:14:03.690+0000] {spark_submit.py:495} INFO - }
[2023-01-31T05:14:03.690+0000] {spark_submit.py:495} INFO - 
[2023-01-31T05:14:03.694+0000] {spark_submit.py:495} INFO - 
[2023-01-31T05:14:04.287+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:04 INFO CodecPool: Got brand-new compressor [.snappy]
[2023-01-31T05:14:10.885+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:10 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675141913985-1928f463-b79d-48eb-86ff-3b3f874c1a74/_temporary/0/_temporary/' directory.
[2023-01-31T05:14:10.886+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:10 INFO FileOutputCommitter: Saved output of task 'attempt_202301310514008531838664300346989_0008_m_000000_8' to gs://entsoe_temp_1009/.spark-bigquery-local-1675141913985-1928f463-b79d-48eb-86ff-3b3f874c1a74/_temporary/0/task_202301310514008531838664300346989_0008_m_000000
[2023-01-31T05:14:10.894+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:10 INFO SparkHadoopMapRedUtil: attempt_202301310514008531838664300346989_0008_m_000000_8: Committed. Elapsed time: 1672 ms.
[2023-01-31T05:14:10.990+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:10 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2588 bytes result sent to driver
[2023-01-31T05:14:10.996+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:10 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 9148 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:14:11.000+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:10 INFO DAGScheduler: ResultStage 8 (save at BigQueryWriteHelper.java:105) finished in 9.499 s
[2023-01-31T05:14:11.001+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:10 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:14:11.003+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:11 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2023-01-31T05:14:11.005+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2023-01-31T05:14:11.009+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:11 INFO DAGScheduler: Job 8 finished: save at BigQueryWriteHelper.java:105, took 9.519713 s
[2023-01-31T05:14:11.012+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:11 INFO FileFormatWriter: Start to commit write Job a3ea308e-f977-43b3-902a-b930b0f9efb9.
[2023-01-31T05:14:11.863+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:11 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675141913985-1928f463-b79d-48eb-86ff-3b3f874c1a74/_temporary/0/task_202301310514008531838664300346989_0008_m_000000/' directory.
[2023-01-31T05:14:12.295+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:12 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675141913985-1928f463-b79d-48eb-86ff-3b3f874c1a74/' directory.
[2023-01-31T05:14:12.577+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:12 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 81d5fcd0285b:34067 in memory (size: 74.3 KiB, free: 434.4 MiB)
[2023-01-31T05:14:13.653+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:13 INFO FileFormatWriter: Write Job a3ea308e-f977-43b3-902a-b930b0f9efb9 committed. Elapsed time: 2628 ms.
[2023-01-31T05:14:13.744+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:13 INFO FileFormatWriter: Finished processing stats for write job a3ea308e-f977-43b3-902a-b930b0f9efb9.
[2023-01-31T05:14:16.639+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:16 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=entsoe_analytics_1009, projectId=rafzul-analytics-1009, tableId=TEST_total_generation_staging}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=null, ignoreUnknownValue=null, sourceUris=[gs://entsoe_temp_1009/.spark-bigquery-local-1675141913985-1928f463-b79d-48eb-86ff-3b3f874c1a74/part-00000-282c7b2a-8cc6-456c-be32-8a14b9c49951-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=true, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=rafzul-analytics-1009, job=90c22869-f7fa-43d9-b743-6862b10f80cf, location=US}
[2023-01-31T05:14:23.313+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO BigQueryClient: Done loading to rafzul-analytics-1009.entsoe_analytics_1009.TEST_total_generation_staging. jobId: JobId{project=rafzul-analytics-1009, job=90c22869-f7fa-43d9-b743-6862b10f80cf, location=US}
[2023-01-31T05:14:23.750+0000] {spark_submit.py:495} INFO - MANTEP!!!!
[2023-01-31T05:14:23.875+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-31T05:14:23.899+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO SparkUI: Stopped Spark web UI at http://81d5fcd0285b:4046
[2023-01-31T05:14:23.915+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-31T05:14:23.934+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO MemoryStore: MemoryStore cleared
[2023-01-31T05:14:23.935+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO BlockManager: BlockManager stopped
[2023-01-31T05:14:23.939+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-31T05:14:23.945+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-31T05:14:23.953+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO SparkContext: Successfully stopped SparkContext
[2023-01-31T05:14:23.953+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO ShutdownHookManager: Shutdown hook called
[2023-01-31T05:14:23.954+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-d7a6afd5-72c7-4f7d-9113-17e132c0fd15/pyspark-2bc6f4c5-7013-4beb-b593-14d49a27d692
[2023-01-31T05:14:23.960+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-d7a6afd5-72c7-4f7d-9113-17e132c0fd15
[2023-01-31T05:14:23.967+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-344a3f2d-e38b-455c-ace4-8fb5dea4a208
[2023-01-31T05:14:24.111+0000] {taskinstance.py:1322} INFO - Marking task as SUCCESS. dag_id=entsoe-energydata-backfill, task_id=stage_total_generation, execution_date=20210101T060000, start_date=20230131T051109, end_date=20230131T051424
[2023-01-31T05:14:24.158+0000] {local_task_job.py:159} INFO - Task exited with return code 0
[2023-01-31T05:14:24.178+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T05:30:47.961+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [queued]>
[2023-01-31T05:30:48.103+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [queued]>
[2023-01-31T05:30:48.105+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T05:30:48.106+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T05:30:48.107+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T05:30:48.279+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 06:00:00+00:00
[2023-01-31T05:30:48.314+0000] {standard_task_runner.py:55} INFO - Started process 15825 to run task
[2023-01-31T05:30:48.343+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T06:00:00+00:00', '--job-id', '899', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpq4zgishu']
[2023-01-31T05:30:48.356+0000] {standard_task_runner.py:83} INFO - Job 899: Subtask stage_total_generation
[2023-01-31T05:30:49.845+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [running]> on host 81d5fcd0285b
[2023-01-31T05:30:50.560+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T06:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T06:00:00+00:00
[2023-01-31T05:30:50.671+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T05:30:50.697+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010600 202101010700 DE_TENNET
[2023-01-31T05:32:11.817+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:11 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T05:32:12.336+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T05:32:15.826+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:15 INFO ResourceUtils: ==============================================================
[2023-01-31T05:32:15.852+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:15 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T05:32:15.866+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:15 INFO ResourceUtils: ==============================================================
[2023-01-31T05:32:15.868+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:15 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T05:32:17.071+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:17 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T05:32:17.120+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:17 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T05:32:17.128+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:17 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T05:32:18.035+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:18 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T05:32:18.069+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:18 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T05:32:18.088+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:18 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T05:32:18.101+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:18 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T05:32:18.115+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:18 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T05:32:25.956+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:25 INFO Utils: Successfully started service 'sparkDriver' on port 38063.
[2023-01-31T05:32:26.781+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:26 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T05:32:27.025+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:27 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T05:32:27.512+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:27 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T05:32:27.523+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:27 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T05:32:27.551+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:27 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T05:32:27.913+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:27 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-4f9253cb-84a8-4075-b181-61ee87488360
[2023-01-31T05:32:29.321+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:29 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T05:32:29.639+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:29 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T05:32:36.029+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:36 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T05:32:36.032+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:36 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2023-01-31T05:32:36.060+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:36 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[2023-01-31T05:32:36.071+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:36 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[2023-01-31T05:32:36.088+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:36 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[2023-01-31T05:32:36.173+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:36 INFO Utils: Successfully started service 'SparkUI' on port 4045.
[2023-01-31T05:32:37.325+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:37 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://81d5fcd0285b:38063/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675143131701
[2023-01-31T05:32:37.326+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:37 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://81d5fcd0285b:38063/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675143131701
[2023-01-31T05:32:39.496+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:39 INFO Executor: Starting executor ID driver on host 81d5fcd0285b
[2023-01-31T05:32:39.550+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:39 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T05:32:40.063+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:40 INFO Executor: Fetching spark://81d5fcd0285b:38063/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675143131701
[2023-01-31T05:32:41.795+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:41 INFO TransportClientFactory: Successfully created connection to 81d5fcd0285b/172.19.0.9:38063 after 1055 ms (0 ms spent in bootstraps)
[2023-01-31T05:32:42.285+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:42 INFO Utils: Fetching spark://81d5fcd0285b:38063/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-e89d0ff7-1da0-42b5-b7bf-bad23b07d899/userFiles-278161f1-4b50-46b8-ae75-a9967acf7150/fetchFileTemp14362606257490450515.tmp
[2023-01-31T05:32:50.859+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:50 INFO Executor: Adding file:/tmp/spark-e89d0ff7-1da0-42b5-b7bf-bad23b07d899/userFiles-278161f1-4b50-46b8-ae75-a9967acf7150/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T05:32:50.882+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:50 INFO Executor: Fetching spark://81d5fcd0285b:38063/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675143131701
[2023-01-31T05:32:50.930+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:50 INFO Utils: Fetching spark://81d5fcd0285b:38063/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-e89d0ff7-1da0-42b5-b7bf-bad23b07d899/userFiles-278161f1-4b50-46b8-ae75-a9967acf7150/fetchFileTemp18027940009745349618.tmp
[2023-01-31T05:32:55.199+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:55 INFO Executor: Told to re-register on heartbeat
[2023-01-31T05:32:55.206+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:55 INFO BlockManager: BlockManager null re-registering with master
[2023-01-31T05:32:55.243+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:55 INFO BlockManagerMaster: Registering BlockManager null
[2023-01-31T05:32:55.435+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:55 ERROR Inbox: Ignoring error
[2023-01-31T05:32:55.436+0000] {spark_submit.py:495} INFO - java.lang.NullPointerException
[2023-01-31T05:32:55.436+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)
[2023-01-31T05:32:55.437+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)
[2023-01-31T05:32:55.438+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-01-31T05:32:55.438+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-01-31T05:32:55.439+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-01-31T05:32:55.439+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-01-31T05:32:55.440+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-01-31T05:32:55.465+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-01-31T05:32:55.465+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-01-31T05:32:55.466+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-31T05:32:55.467+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-31T05:32:55.467+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-31T05:32:55.511+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:55 INFO Executor: Adding file:/tmp/spark-e89d0ff7-1da0-42b5-b7bf-bad23b07d899/userFiles-278161f1-4b50-46b8-ae75-a9967acf7150/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T05:32:55.516+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:55 WARN Executor: Issue communicating with driver in heartbeater
[2023-01-31T05:32:55.517+0000] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-01-31T05:32:55.518+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-01-31T05:32:55.519+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-01-31T05:32:55.519+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2023-01-31T05:32:55.520+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2023-01-31T05:32:55.549+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:79)
[2023-01-31T05:32:55.550+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:643)
[2023-01-31T05:32:55.550+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2023-01-31T05:32:55.551+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2023-01-31T05:32:55.552+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
[2023-01-31T05:32:55.573+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2023-01-31T05:32:55.574+0000] {spark_submit.py:495} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2023-01-31T05:32:55.581+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-01-31T05:32:55.582+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2023-01-31T05:32:55.583+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2023-01-31T05:32:55.584+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-31T05:32:55.584+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-31T05:32:55.610+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-31T05:32:55.610+0000] {spark_submit.py:495} INFO - Caused by: java.lang.NullPointerException
[2023-01-31T05:32:55.611+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)
[2023-01-31T05:32:55.612+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)
[2023-01-31T05:32:55.612+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-01-31T05:32:55.673+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-01-31T05:32:55.674+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-01-31T05:32:55.701+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-01-31T05:32:55.702+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-01-31T05:32:55.702+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-01-31T05:32:55.703+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-01-31T05:32:55.703+0000] {spark_submit.py:495} INFO - ... 3 more
[2023-01-31T05:32:55.825+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:55 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36307.
[2023-01-31T05:32:55.826+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:55 INFO NettyBlockTransferService: Server created on 81d5fcd0285b:36307
[2023-01-31T05:32:55.938+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:55 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T05:32:56.006+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:56 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 36307, None)
[2023-01-31T05:32:56.065+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:56 INFO BlockManagerMasterEndpoint: Registering block manager 81d5fcd0285b:36307 with 434.4 MiB RAM, BlockManagerId(driver, 81d5fcd0285b, 36307, None)
[2023-01-31T05:32:56.111+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:56 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 36307, None)
[2023-01-31T05:32:56.128+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:56 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 81d5fcd0285b, 36307, None)
[2023-01-31T05:33:15.753+0000] {spark_submit.py:495} INFO - 23/01/31 05:33:15 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T05:33:15.829+0000] {spark_submit.py:495} INFO - 23/01/31 05:33:15 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T05:34:17.633+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:17 INFO InMemoryFileIndex: It took 2848 ms to list leaf files for 1 paths.
[2023-01-31T05:34:23.026+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:23 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T05:34:25.399+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:25 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T05:34:25.506+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:25 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 81d5fcd0285b:36307 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T05:34:25.609+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:25 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T05:34:37.097+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:37 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T05:34:37.773+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:37 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T05:34:38.281+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:38 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T05:34:38.597+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:38 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T05:34:38.620+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:38 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T05:34:38.637+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:38 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:34:38.717+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:38 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:34:38.857+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:38 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T05:34:42.948+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:42 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T05:34:43.038+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:43 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T05:34:43.050+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:43 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 81d5fcd0285b:36307 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T05:34:43.132+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:43 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:34:43.560+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:34:43.658+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:43 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T05:34:45.246+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:45 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T05:34:45.405+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:45 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T05:34:48.936+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:48 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010600__202101010700.json:0+9371
[2023-01-31T05:34:56.105+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:56 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3480 bytes result sent to driver
[2023-01-31T05:34:56.590+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:56 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 11387 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:34:56.683+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:56 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T05:34:56.981+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:56 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 17.480 s
[2023-01-31T05:34:57.127+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:57 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:34:57.137+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T05:34:57.184+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:57 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 18.915667 s
[2023-01-31T05:35:19.584+0000] {spark_submit.py:495} INFO - 23/01/31 05:35:19 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 81d5fcd0285b:36307 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T05:35:19.862+0000] {spark_submit.py:495} INFO - 23/01/31 05:35:19 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 81d5fcd0285b:36307 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T05:36:38.627+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:38 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:36:38.704+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:38 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:36:38.794+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:38 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:36:50.886+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:50 INFO CodeGenerator: Code generated in 4619.696551 ms
[2023-01-31T05:36:51.417+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:51 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T05:36:52.716+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:52 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T05:36:52.770+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:52 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 81d5fcd0285b:36307 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T05:36:52.933+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:52 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T05:36:53.609+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:53 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:36:55.732+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:55 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T05:36:55.760+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:55 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T05:36:55.765+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:55 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T05:36:55.773+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:55 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:36:55.774+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:55 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:36:55.775+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:55 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T05:36:55.924+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:55 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T05:36:55.961+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:55 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T05:36:55.966+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:55 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 81d5fcd0285b:36307 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T05:36:55.980+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:55 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:36:55.994+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:36:56.002+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:56 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T05:36:56.167+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:56 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:36:56.189+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:56 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T05:36:57.845+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:57 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T05:37:01.928+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:01 INFO CodeGenerator: Code generated in 3498.614469 ms
[2023-01-31T05:37:04.165+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:04 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1628 bytes result sent to driver
[2023-01-31T05:37:04.220+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:04 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 8193 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:37:04.245+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:04 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T05:37:04.261+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:04 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 8.430 s
[2023-01-31T05:37:04.269+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:04 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:37:04.276+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T05:37:04.297+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:04 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 8.563599 s
[2023-01-31T05:37:06.715+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:06 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:37:06.728+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:06 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:37:06.817+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:06 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:37:08.307+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:08 INFO CodeGenerator: Code generated in 879.718715 ms
[2023-01-31T05:37:08.821+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:08 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T05:37:10.368+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:10 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T05:37:10.520+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:10 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 81d5fcd0285b:36307 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:37:10.590+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:10 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T05:37:10.653+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:10 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:37:11.155+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 81d5fcd0285b:36307 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T05:37:11.452+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T05:37:11.462+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T05:37:11.467+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T05:37:11.553+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:37:11.578+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:37:11.612+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T05:37:11.672+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T05:37:11.681+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T05:37:11.727+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 81d5fcd0285b:36307 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T05:37:11.777+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:37:11.879+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:37:11.901+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T05:37:11.991+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:37:12.021+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:12 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T05:37:13.173+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:13 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T05:37:17.676+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:17 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1585 bytes result sent to driver
[2023-01-31T05:37:17.755+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:17 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 5761 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:37:17.763+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:17 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T05:37:17.801+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:17 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 6.156 s
[2023-01-31T05:37:17.861+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:17 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:37:17.870+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T05:37:17.882+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:17 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 6.417992 s
[2023-01-31T05:37:21.681+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:21 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:37:21.700+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:21 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T05:37:21.746+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:21 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:37:29.004+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO CodeGenerator: Code generated in 5327.690303 ms
[2023-01-31T05:37:29.065+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T05:37:29.197+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T05:37:29.202+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 81d5fcd0285b:36307 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:37:29.213+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T05:37:29.227+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:37:29.486+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T05:37:29.491+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T05:37:29.491+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T05:37:29.495+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:37:29.496+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:37:29.503+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T05:37:29.540+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.7 MiB)
[2023-01-31T05:37:29.605+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.6 MiB)
[2023-01-31T05:37:29.608+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 81d5fcd0285b:36307 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T05:37:29.611+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:37:29.616+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:37:29.617+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T05:37:29.622+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:37:29.625+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T05:37:29.709+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T05:37:29.937+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 81d5fcd0285b:36307 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T05:37:30.461+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:30 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1755 bytes result sent to driver
[2023-01-31T05:37:30.483+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:30 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 863 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:37:30.486+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:30 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T05:37:30.487+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:30 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 0.972 s
[2023-01-31T05:37:30.489+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:30 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:37:30.490+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T05:37:30.500+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:30 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 1.009733 s
[2023-01-31T05:37:31.332+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:31 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:37:31.333+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:31 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:37:31.333+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:31 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:37:32.361+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:32 INFO CodeGenerator: Code generated in 457.923908 ms
[2023-01-31T05:37:32.558+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:32 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.5 MiB)
[2023-01-31T05:37:33.256+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:33 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T05:37:33.263+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:33 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 81d5fcd0285b:36307 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:37:33.295+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:33 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T05:37:33.386+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:37:34.447+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:34 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T05:37:34.458+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:34 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T05:37:34.462+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:34 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T05:37:34.468+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:34 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:37:34.469+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:34 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:37:34.518+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:34 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T05:37:34.839+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:34 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-31T05:37:34.935+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:34 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-01-31T05:37:34.982+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:34 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 81d5fcd0285b:36307 (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T05:37:35.023+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:34 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:37:35.085+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:37:35.087+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:35 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T05:37:35.134+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:35 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:37:35.149+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:35 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T05:37:35.590+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:35 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T05:37:36.851+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:36 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1586 bytes result sent to driver
[2023-01-31T05:37:36.876+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:36 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 1744 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:37:36.885+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:36 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 2.212 s
[2023-01-31T05:37:36.887+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:36 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T05:37:36.891+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:36 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:37:36.895+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T05:37:36.901+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:36 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 2.446376 s
[2023-01-31T05:37:44.283+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:44 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:37:44.287+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:44 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T05:37:44.290+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:44 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:37:45.211+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:45 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 81d5fcd0285b:36307 in memory (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T05:37:45.414+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:45 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 81d5fcd0285b:36307 in memory (size: 7.6 KiB, free: 434.3 MiB)
[2023-01-31T05:37:48.750+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:48 INFO CodeGenerator: Code generated in 3706.47268 ms
[2023-01-31T05:37:48.798+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:48 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.3 MiB)
[2023-01-31T05:37:49.148+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:49 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T05:37:49.167+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:49 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 81d5fcd0285b:36307 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:37:49.180+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:49 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T05:37:49.215+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:49 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:37:50.171+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:50 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T05:37:50.186+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:50 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) with 1 output partitions
[2023-01-31T05:37:50.188+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:50 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201)
[2023-01-31T05:37:50.189+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:50 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:37:50.204+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:50 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:37:50.230+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:50 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201), which has no missing parents
[2023-01-31T05:37:50.392+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:50 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.2 MiB)
[2023-01-31T05:37:50.459+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:50 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.2 MiB)
[2023-01-31T05:37:50.465+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:50 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 81d5fcd0285b:36307 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T05:37:50.496+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:50 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:37:50.500+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:37:50.533+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:50 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T05:37:50.540+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:50 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:37:50.541+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:50 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T05:37:50.866+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:50 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T05:37:51.910+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:51 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2499 bytes result sent to driver
[2023-01-31T05:37:51.924+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:51 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 1387 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:37:51.925+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:51 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T05:37:51.930+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:51 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) finished in 1.677 s
[2023-01-31T05:37:51.939+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:51 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:37:51.949+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T05:37:51.961+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:51 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201, took 1.785086 s
[2023-01-31T05:37:53.868+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:53 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:37:53.912+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:53 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T05:37:53.952+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:53 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:37:56.533+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:56 INFO CodeGenerator: Code generated in 949.63418 ms
[2023-01-31T05:37:56.757+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:56 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 433.0 MiB)
[2023-01-31T05:37:57.624+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:57 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.0 MiB)
[2023-01-31T05:37:57.638+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:57 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 81d5fcd0285b:36307 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:37:57.704+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:57 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T05:37:57.782+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:57 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:37:58.826+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:58 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T05:37:58.861+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:58 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T05:37:58.862+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:58 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T05:37:58.862+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:58 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:37:58.863+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:58 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:37:58.918+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:58 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T05:37:59.033+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:59 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 432.9 MiB)
[2023-01-31T05:37:59.126+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:59 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.9 MiB)
[2023-01-31T05:37:59.143+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:59 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 81d5fcd0285b:36307 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T05:37:59.157+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:59 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:37:59.238+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:37:59.239+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:59 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T05:37:59.266+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:59 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:37:59.270+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:59 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T05:37:59.791+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:59 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T05:38:01.096+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:01 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-01-31T05:38:01.112+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:01 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 1835 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:38:01.115+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:01 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T05:38:01.116+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:01 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 2.129 s
[2023-01-31T05:38:01.134+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:01 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:38:01.138+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T05:38:01.139+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:01 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 2.281965 s
[2023-01-31T05:38:02.380+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:02 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:38:02.413+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:02 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T05:38:02.425+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:02 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:38:05.315+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:05 INFO CodeGenerator: Code generated in 2067.130993 ms
[2023-01-31T05:38:05.418+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:05 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.7 MiB)
[2023-01-31T05:38:06.049+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:06 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.7 MiB)
[2023-01-31T05:38:06.052+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:06 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 81d5fcd0285b:36307 (size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T05:38:06.097+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:06 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T05:38:06.242+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:06 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:38:07.710+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:07 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T05:38:07.721+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:07 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T05:38:07.724+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:07 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T05:38:07.734+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:07 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:38:07.755+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:07 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:38:07.765+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:07 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T05:38:07.874+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:07 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.7 MiB)
[2023-01-31T05:38:07.948+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:07 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.6 MiB)
[2023-01-31T05:38:07.955+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:07 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 81d5fcd0285b:36307 (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T05:38:07.967+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:07 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:38:07.986+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:38:07.993+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:07 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T05:38:08.057+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:08 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:38:08.065+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:08 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T05:38:08.426+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:08 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T05:38:09.336+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:09 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1772 bytes result sent to driver
[2023-01-31T05:38:09.348+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:09 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 1290 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:38:09.350+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:09 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T05:38:09.356+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:09 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 1.547 s
[2023-01-31T05:38:09.359+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:09 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:38:09.359+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T05:38:09.363+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:09 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 1.649337 s
[2023-01-31T05:38:37.675+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:37 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 81d5fcd0285b:36307 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:38:37.721+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:37 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 81d5fcd0285b:36307 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T05:38:37.837+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:37 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 81d5fcd0285b:36307 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:38:37.935+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:37 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 81d5fcd0285b:36307 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:38:38.053+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:38 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 81d5fcd0285b:36307 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:38:38.148+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:38 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 81d5fcd0285b:36307 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:38:38.222+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:38 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 81d5fcd0285b:36307 in memory (size: 13.4 KiB, free: 434.3 MiB)
[2023-01-31T05:38:38.320+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:38 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 81d5fcd0285b:36307 in memory (size: 12.6 KiB, free: 434.3 MiB)
[2023-01-31T05:38:38.407+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:38 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 81d5fcd0285b:36307 in memory (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T05:39:08.611+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:08 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:39:08.930+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:39:08.931+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:39:08.937+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:08 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:39:08.946+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:39:08.947+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:39:08.973+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:39:11.432+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:11 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-31T05:39:11.437+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:11 INFO DAGScheduler: Got job 8 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T05:39:11.438+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:11 INFO DAGScheduler: Final stage: ResultStage 8 (save at BigQueryWriteHelper.java:105)
[2023-01-31T05:39:11.441+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:11 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:39:11.443+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:11 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:39:11.454+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:11 INFO DAGScheduler: Submitting ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T05:39:11.648+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:11 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 209.9 KiB, free 434.0 MiB)
[2023-01-31T05:39:11.723+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:11 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 74.3 KiB, free 433.9 MiB)
[2023-01-31T05:39:11.729+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:11 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 81d5fcd0285b:36307 (size: 74.3 KiB, free: 434.3 MiB)
[2023-01-31T05:39:11.746+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:11 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:39:11.766+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:39:11.770+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:11 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2023-01-31T05:39:11.845+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:11 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7484 bytes) taskResourceAssignments Map()
[2023-01-31T05:39:11.871+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:11 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2023-01-31T05:39:12.686+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:39:12.689+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:39:12.718+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:12 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:39:12.726+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:39:12.733+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:39:12.748+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:39:12.930+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:12 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T05:39:12.947+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:12 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T05:39:13.309+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:13 INFO ParquetOutputFormat: Parquet block size to 134217728
[2023-01-31T05:39:13.313+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:13 INFO ParquetOutputFormat: Validation is off
[2023-01-31T05:39:13.316+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:13 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-01-31T05:39:13.321+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:13 INFO ParquetOutputFormat: Parquet properties are:
[2023-01-31T05:39:13.322+0000] {spark_submit.py:495} INFO - Parquet page size to 1048576
[2023-01-31T05:39:13.323+0000] {spark_submit.py:495} INFO - Parquet dictionary page size to 1048576
[2023-01-31T05:39:13.324+0000] {spark_submit.py:495} INFO - Dictionary is true
[2023-01-31T05:39:13.326+0000] {spark_submit.py:495} INFO - Writer version is: PARQUET_1_0
[2023-01-31T05:39:13.326+0000] {spark_submit.py:495} INFO - Page size checking is: estimated
[2023-01-31T05:39:13.327+0000] {spark_submit.py:495} INFO - Min row count for page size check is: 100
[2023-01-31T05:39:13.327+0000] {spark_submit.py:495} INFO - Max row count for page size check is: 10000
[2023-01-31T05:39:13.328+0000] {spark_submit.py:495} INFO - Truncate length for column indexes is: 64
[2023-01-31T05:39:13.328+0000] {spark_submit.py:495} INFO - Truncate length for statistics min/max  is: 2147483647
[2023-01-31T05:39:13.337+0000] {spark_submit.py:495} INFO - Bloom filter enabled: false
[2023-01-31T05:39:13.342+0000] {spark_submit.py:495} INFO - Max Bloom filter size for a column is 1048576
[2023-01-31T05:39:13.343+0000] {spark_submit.py:495} INFO - Bloom filter expected number of distinct values are: null
[2023-01-31T05:39:13.347+0000] {spark_submit.py:495} INFO - Page row count limit to 20000
[2023-01-31T05:39:13.352+0000] {spark_submit.py:495} INFO - Writing page checksums is: on
[2023-01-31T05:39:13.864+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:13 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-01-31T05:39:13.865+0000] {spark_submit.py:495} INFO - {
[2023-01-31T05:39:13.865+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-31T05:39:13.866+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-31T05:39:13.867+0000] {spark_submit.py:495} INFO - "name" : "measured_at",
[2023-01-31T05:39:13.867+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T05:39:13.868+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T05:39:13.868+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:39:13.870+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:39:13.871+0000] {spark_submit.py:495} INFO - "name" : "created_at",
[2023-01-31T05:39:13.896+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T05:39:13.900+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T05:39:13.911+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:39:13.933+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:39:13.950+0000] {spark_submit.py:495} INFO - "name" : "Biomass - Actual Aggregated",
[2023-01-31T05:39:13.980+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:39:13.985+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:39:13.987+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:39:13.993+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:39:13.994+0000] {spark_submit.py:495} INFO - "name" : "Fossil Brown coal/Lignite - Actual Aggregated",
[2023-01-31T05:39:14.006+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:39:14.007+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:39:14.008+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:39:14.018+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:39:14.021+0000] {spark_submit.py:495} INFO - "name" : "Fossil Gas - Actual Aggregated",
[2023-01-31T05:39:14.027+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:39:14.033+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:39:14.039+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:39:14.044+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:39:14.050+0000] {spark_submit.py:495} INFO - "name" : "Fossil Hard coal - Actual Aggregated",
[2023-01-31T05:39:14.061+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:39:14.062+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:39:14.063+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:39:14.063+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:39:14.064+0000] {spark_submit.py:495} INFO - "name" : "Fossil Oil - Actual Aggregated",
[2023-01-31T05:39:14.079+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:39:14.081+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:39:14.085+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:39:14.104+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:39:14.110+0000] {spark_submit.py:495} INFO - "name" : "Geothermal - Actual Aggregated",
[2023-01-31T05:39:14.116+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:39:14.125+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:39:14.131+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:39:14.137+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:39:14.138+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Aggregated",
[2023-01-31T05:39:14.141+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:39:14.142+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:39:14.143+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:39:14.143+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:39:14.144+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Consumption",
[2023-01-31T05:39:14.144+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:39:14.149+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:39:14.155+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:39:14.160+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:39:14.165+0000] {spark_submit.py:495} INFO - "name" : "Hydro Run-of-river and poundage - Actual Aggregated",
[2023-01-31T05:39:14.168+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:39:14.172+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:39:14.176+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:39:14.180+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:39:14.184+0000] {spark_submit.py:495} INFO - "name" : "Hydro Water Reservoir - Actual Aggregated",
[2023-01-31T05:39:14.186+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:39:14.188+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:39:14.193+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:39:14.195+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:39:14.198+0000] {spark_submit.py:495} INFO - "name" : "Nuclear - Actual Aggregated",
[2023-01-31T05:39:14.206+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:39:14.215+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:39:14.216+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:39:14.224+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:39:14.225+0000] {spark_submit.py:495} INFO - "name" : "Other - Actual Aggregated",
[2023-01-31T05:39:14.226+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:39:14.227+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:39:14.227+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:39:14.230+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:39:14.235+0000] {spark_submit.py:495} INFO - "name" : "Other renewable - Actual Aggregated",
[2023-01-31T05:39:14.244+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:39:14.246+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:39:14.249+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:39:14.250+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:39:14.253+0000] {spark_submit.py:495} INFO - "name" : "Solar - Actual Aggregated",
[2023-01-31T05:39:14.261+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:39:14.266+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:39:14.268+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:39:14.271+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:39:14.274+0000] {spark_submit.py:495} INFO - "name" : "Waste - Actual Aggregated",
[2023-01-31T05:39:14.280+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:39:14.290+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:39:14.294+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:39:14.301+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:39:14.307+0000] {spark_submit.py:495} INFO - "name" : "Wind Offshore - Actual Aggregated",
[2023-01-31T05:39:14.311+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:39:14.323+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:39:14.327+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:39:14.329+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:39:14.331+0000] {spark_submit.py:495} INFO - "name" : "Wind Onshore - Actual Aggregated",
[2023-01-31T05:39:14.333+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:39:14.334+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:39:14.336+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:39:14.341+0000] {spark_submit.py:495} INFO - } ]
[2023-01-31T05:39:14.343+0000] {spark_submit.py:495} INFO - }
[2023-01-31T05:39:14.344+0000] {spark_submit.py:495} INFO - and corresponding Parquet message type:
[2023-01-31T05:39:14.347+0000] {spark_submit.py:495} INFO - message spark_schema {
[2023-01-31T05:39:14.349+0000] {spark_submit.py:495} INFO - required int96 measured_at;
[2023-01-31T05:39:14.350+0000] {spark_submit.py:495} INFO - required int96 created_at;
[2023-01-31T05:39:14.353+0000] {spark_submit.py:495} INFO - optional double Biomass - Actual Aggregated;
[2023-01-31T05:39:14.354+0000] {spark_submit.py:495} INFO - optional double Fossil Brown coal/Lignite - Actual Aggregated;
[2023-01-31T05:39:14.356+0000] {spark_submit.py:495} INFO - optional double Fossil Gas - Actual Aggregated;
[2023-01-31T05:39:14.357+0000] {spark_submit.py:495} INFO - optional double Fossil Hard coal - Actual Aggregated;
[2023-01-31T05:39:14.361+0000] {spark_submit.py:495} INFO - optional double Fossil Oil - Actual Aggregated;
[2023-01-31T05:39:14.364+0000] {spark_submit.py:495} INFO - optional double Geothermal - Actual Aggregated;
[2023-01-31T05:39:14.366+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Aggregated;
[2023-01-31T05:39:14.368+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Consumption;
[2023-01-31T05:39:14.369+0000] {spark_submit.py:495} INFO - optional double Hydro Run-of-river and poundage - Actual Aggregated;
[2023-01-31T05:39:14.371+0000] {spark_submit.py:495} INFO - optional double Hydro Water Reservoir - Actual Aggregated;
[2023-01-31T05:39:14.375+0000] {spark_submit.py:495} INFO - optional double Nuclear - Actual Aggregated;
[2023-01-31T05:39:14.375+0000] {spark_submit.py:495} INFO - optional double Other - Actual Aggregated;
[2023-01-31T05:39:14.393+0000] {spark_submit.py:495} INFO - optional double Other renewable - Actual Aggregated;
[2023-01-31T05:39:14.396+0000] {spark_submit.py:495} INFO - optional double Solar - Actual Aggregated;
[2023-01-31T05:39:14.405+0000] {spark_submit.py:495} INFO - optional double Waste - Actual Aggregated;
[2023-01-31T05:39:14.411+0000] {spark_submit.py:495} INFO - optional double Wind Offshore - Actual Aggregated;
[2023-01-31T05:39:14.416+0000] {spark_submit.py:495} INFO - optional double Wind Onshore - Actual Aggregated;
[2023-01-31T05:39:14.434+0000] {spark_submit.py:495} INFO - }
[2023-01-31T05:39:14.437+0000] {spark_submit.py:495} INFO - 
[2023-01-31T05:39:14.440+0000] {spark_submit.py:495} INFO - 
[2023-01-31T05:39:16.025+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:16 INFO CodecPool: Got brand-new compressor [.snappy]
[2023-01-31T05:39:29.329+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:29 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675143158616-81ec2dd0-39fd-4bd5-9bc9-398227d4ae7e/_temporary/0/_temporary/' directory.
[2023-01-31T05:39:29.338+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:29 INFO FileOutputCommitter: Saved output of task 'attempt_202301310539105163398628901132425_0008_m_000000_8' to gs://entsoe_temp_1009/.spark-bigquery-local-1675143158616-81ec2dd0-39fd-4bd5-9bc9-398227d4ae7e/_temporary/0/task_202301310539105163398628901132425_0008_m_000000
[2023-01-31T05:39:29.339+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:29 INFO SparkHadoopMapRedUtil: attempt_202301310539105163398628901132425_0008_m_000000_8: Committed. Elapsed time: 2856 ms.
[2023-01-31T05:39:29.687+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:29 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2588 bytes result sent to driver
[2023-01-31T05:39:29.710+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:29 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 17921 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:39:29.716+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:29 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2023-01-31T05:39:29.831+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:29 INFO DAGScheduler: ResultStage 8 (save at BigQueryWriteHelper.java:105) finished in 18.257 s
[2023-01-31T05:39:29.841+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:29 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:39:29.842+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2023-01-31T05:39:29.843+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:29 INFO DAGScheduler: Job 8 finished: save at BigQueryWriteHelper.java:105, took 18.404606 s
[2023-01-31T05:39:29.857+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:29 INFO FileFormatWriter: Start to commit write Job 274d7189-95e4-49ac-a01f-41287bcbd33a.
[2023-01-31T05:39:31.185+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:31 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675143158616-81ec2dd0-39fd-4bd5-9bc9-398227d4ae7e/_temporary/0/task_202301310539105163398628901132425_0008_m_000000/' directory.
[2023-01-31T05:39:31.584+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:31 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675143158616-81ec2dd0-39fd-4bd5-9bc9-398227d4ae7e/' directory.
[2023-01-31T05:39:32.301+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:32 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 81d5fcd0285b:36307 in memory (size: 74.3 KiB, free: 434.4 MiB)
[2023-01-31T05:39:32.486+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:32 INFO FileFormatWriter: Write Job 274d7189-95e4-49ac-a01f-41287bcbd33a committed. Elapsed time: 2615 ms.
[2023-01-31T05:39:32.516+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:32 INFO FileFormatWriter: Finished processing stats for write job 274d7189-95e4-49ac-a01f-41287bcbd33a.
[2023-01-31T05:39:34.853+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:34 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=entsoe_analytics_1009, projectId=rafzul-analytics-1009, tableId=TEST_total_generation_staging}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=null, ignoreUnknownValue=null, sourceUris=[gs://entsoe_temp_1009/.spark-bigquery-local-1675143158616-81ec2dd0-39fd-4bd5-9bc9-398227d4ae7e/part-00000-7204c14a-f28c-4ae1-8bd5-ab747fd58b87-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=true, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=rafzul-analytics-1009, job=a4b0c663-2ef9-45dc-a0fd-08e1041b5fc6, location=US}
[2023-01-31T05:39:38.695+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:38 INFO BigQueryClient: Done loading to rafzul-analytics-1009.entsoe_analytics_1009.TEST_total_generation_staging. jobId: JobId{project=rafzul-analytics-1009, job=a4b0c663-2ef9-45dc-a0fd-08e1041b5fc6, location=US}
[2023-01-31T05:39:39.750+0000] {spark_submit.py:495} INFO - MANTEP!!!!
[2023-01-31T05:39:40.020+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:40 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-31T05:39:40.044+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:40 INFO SparkUI: Stopped Spark web UI at http://81d5fcd0285b:4045
[2023-01-31T05:39:40.073+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:40 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-31T05:39:40.106+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:40 INFO MemoryStore: MemoryStore cleared
[2023-01-31T05:39:40.107+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:40 INFO BlockManager: BlockManager stopped
[2023-01-31T05:39:40.113+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:40 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-31T05:39:40.119+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:40 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-31T05:39:40.130+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:40 INFO SparkContext: Successfully stopped SparkContext
[2023-01-31T05:39:40.131+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:40 INFO ShutdownHookManager: Shutdown hook called
[2023-01-31T05:39:40.132+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:40 INFO ShutdownHookManager: Deleting directory /tmp/spark-e89d0ff7-1da0-42b5-b7bf-bad23b07d899/pyspark-67cc98ab-1eb0-44f1-acdb-cbdb7fe018f1
[2023-01-31T05:39:40.140+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:40 INFO ShutdownHookManager: Deleting directory /tmp/spark-e89d0ff7-1da0-42b5-b7bf-bad23b07d899
[2023-01-31T05:39:40.148+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:40 INFO ShutdownHookManager: Deleting directory /tmp/spark-064b2e91-e689-4562-a56e-cf4552b2fc68
[2023-01-31T05:39:40.360+0000] {taskinstance.py:1322} INFO - Marking task as SUCCESS. dag_id=entsoe-energydata-backfill, task_id=stage_total_generation, execution_date=20210101T060000, start_date=20230131T053047, end_date=20230131T053940
[2023-01-31T05:39:40.442+0000] {local_task_job.py:159} INFO - Task exited with return code 0
[2023-01-31T05:39:40.495+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T07:39:04.192+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [queued]>
[2023-01-31T07:39:04.324+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [queued]>
[2023-01-31T07:39:04.325+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T07:39:04.326+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T07:39:04.327+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T07:39:04.418+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 06:00:00+00:00
[2023-01-31T07:39:04.547+0000] {standard_task_runner.py:55} INFO - Started process 25834 to run task
[2023-01-31T07:39:04.624+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T06:00:00+00:00', '--job-id', '913', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp9vk387_y']
[2023-01-31T07:39:04.672+0000] {standard_task_runner.py:83} INFO - Job 913: Subtask stage_total_generation
[2023-01-31T07:39:05.915+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [running]> on host 81d5fcd0285b
[2023-01-31T07:39:06.862+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T06:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T06:00:00+00:00
[2023-01-31T07:39:07.091+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T07:39:07.096+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010600 202101010700 DE_TENNET
[2023-01-31T07:40:24.664+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:24 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T07:40:26.811+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T07:40:31.887+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:31 INFO ResourceUtils: ==============================================================
[2023-01-31T07:40:31.999+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:31 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T07:40:32.034+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:31 INFO ResourceUtils: ==============================================================
[2023-01-31T07:40:32.142+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:32 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T07:40:33.517+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:33 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T07:40:33.855+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:33 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T07:40:33.930+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:33 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T07:40:35.951+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:35 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T07:40:36.057+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:36 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T07:40:36.160+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:36 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T07:40:36.268+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:36 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T07:40:36.316+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:36 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T07:40:49.226+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:49 INFO Utils: Successfully started service 'sparkDriver' on port 37871.
[2023-01-31T07:40:50.524+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:50 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T07:40:52.675+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:52 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T07:40:53.625+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:53 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T07:40:53.633+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:53 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T07:40:53.658+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:53 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T07:40:54.385+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:54 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b9aea595-5404-4d69-81ca-3545d4530195
[2023-01-31T07:40:54.517+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:54 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T07:40:55.865+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:55 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T07:41:03.137+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:03 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T07:41:03.157+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:03 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2023-01-31T07:41:03.183+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:03 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[2023-01-31T07:41:03.218+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:03 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[2023-01-31T07:41:03.257+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:03 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[2023-01-31T07:41:03.258+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:03 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
[2023-01-31T07:41:03.472+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:03 INFO Utils: Successfully started service 'SparkUI' on port 4046.
[2023-01-31T07:41:04.985+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:04 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://81d5fcd0285b:37871/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675150824563
[2023-01-31T07:41:05.005+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:04 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://81d5fcd0285b:37871/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675150824563
[2023-01-31T07:41:06.414+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:06 INFO Executor: Starting executor ID driver on host 81d5fcd0285b
[2023-01-31T07:41:06.684+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:06 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T07:41:07.034+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:07 INFO Executor: Fetching spark://81d5fcd0285b:37871/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675150824563
[2023-01-31T07:41:08.374+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:08 INFO TransportClientFactory: Successfully created connection to 81d5fcd0285b/172.19.0.9:37871 after 1135 ms (0 ms spent in bootstraps)
[2023-01-31T07:41:08.580+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:08 INFO Utils: Fetching spark://81d5fcd0285b:37871/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-36349aee-2534-4650-b9b1-43890d392200/userFiles-7b500f39-df78-488f-b7ec-30402b2c2342/fetchFileTemp11253485400958233692.tmp
[2023-01-31T07:41:15.523+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:15 INFO Executor: Adding file:/tmp/spark-36349aee-2534-4650-b9b1-43890d392200/userFiles-7b500f39-df78-488f-b7ec-30402b2c2342/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T07:41:15.525+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:15 INFO Executor: Fetching spark://81d5fcd0285b:37871/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675150824563
[2023-01-31T07:41:15.540+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:15 INFO Utils: Fetching spark://81d5fcd0285b:37871/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-36349aee-2534-4650-b9b1-43890d392200/userFiles-7b500f39-df78-488f-b7ec-30402b2c2342/fetchFileTemp17250526110526649466.tmp
[2023-01-31T07:41:20.958+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:20 INFO Executor: Adding file:/tmp/spark-36349aee-2534-4650-b9b1-43890d392200/userFiles-7b500f39-df78-488f-b7ec-30402b2c2342/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T07:41:21.069+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42647.
[2023-01-31T07:41:21.069+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:21 INFO NettyBlockTransferService: Server created on 81d5fcd0285b:42647
[2023-01-31T07:41:21.093+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:21 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T07:41:21.286+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:21 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 42647, None)
[2023-01-31T07:41:21.331+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:21 INFO BlockManagerMasterEndpoint: Registering block manager 81d5fcd0285b:42647 with 434.4 MiB RAM, BlockManagerId(driver, 81d5fcd0285b, 42647, None)
[2023-01-31T07:41:21.382+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:21 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 42647, None)
[2023-01-31T07:41:21.406+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:21 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 81d5fcd0285b, 42647, None)
[2023-01-31T07:41:25.466+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:25 INFO Executor: Told to re-register on heartbeat
[2023-01-31T07:41:25.543+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:25 INFO BlockManager: BlockManager BlockManagerId(driver, 81d5fcd0285b, 42647, None) re-registering with master
[2023-01-31T07:41:25.546+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:25 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 42647, None)
[2023-01-31T07:41:25.592+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:25 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 42647, None)
[2023-01-31T07:41:25.593+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:25 INFO BlockManager: Reporting 0 blocks to the master.
[2023-01-31T07:41:39.859+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:39 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T07:41:40.032+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:40 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T07:42:22.685+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:22 INFO InMemoryFileIndex: It took 1609 ms to list leaf files for 1 paths.
[2023-01-31T07:42:28.737+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:28 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T07:42:31.266+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:31 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T07:42:31.287+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:31 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 81d5fcd0285b:42647 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T07:42:31.385+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:31 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T07:42:45.272+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:45 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T07:42:45.822+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:45 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T07:42:46.442+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:46 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T07:42:46.856+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:46 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T07:42:46.863+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:46 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T07:42:46.865+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:46 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:42:46.886+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:46 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:42:46.938+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:46 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T07:42:49.661+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:49 INFO AsyncEventQueue: Process of event SparkListenerJobStart(0,1675150966888,ArraySeq(org.apache.spark.scheduler.StageInfo@271762d9),{spark.master=local, spark.driver.port=37871, spark.submit.pyFiles=, spark.app.startTime=1675150824563, spark.rdd.compress=True, spark.executor.id=driver, spark.app.name=gcp_playground, spark.submit.deployMode=client, spark.hadoop.google.cloud.auth.service.account.json.keyfile=/.google/credentials/google_credentials.json, spark.driver.host=81d5fcd0285b, spark.app.id=local-1675150865556, spark.app.submitTime=1675150804798, spark.hadoop.fs.AbstractFileSystem.gs.impl=google.cloud.hadoop.fs.gcs.GoogleHadoopFS, spark.sql.session.timeZone=UTC, spark.hadoop.google.cloud.auth.service.account.enable=true, spark.app.initial.jar.urls=spark://81d5fcd0285b:37871/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar,spark://81d5fcd0285b:37871/jars/gcs-connector-hadoop3-latest.jar, spark.executor.extraJavaOptions=-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED, spark.jars=/opt/spark/jars/gcs-connector-hadoop3-latest.jar, /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar, spark.sql.warehouse.dir=file:/opt/***/spark-warehouse, spark.serializer.objectStreamReset=100, spark.driver.extraJavaOptions=-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED}) by listener AppStatusListener took 2.609701602s.
[2023-01-31T07:42:50.764+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:50 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T07:42:50.850+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:50 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T07:42:50.878+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:50 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 81d5fcd0285b:42647 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T07:42:50.961+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:50 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:42:51.918+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:42:51.998+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:51 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T07:42:54.774+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:54 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T07:42:55.346+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:55 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T07:42:58.061+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:58 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010600__202101010700.json:0+9371
[2023-01-31T07:43:00.405+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:00 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3480 bytes result sent to driver
[2023-01-31T07:43:00.558+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:00 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 6492 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:43:00.598+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:00 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T07:43:00.722+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:00 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 12.262 s
[2023-01-31T07:43:00.822+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:00 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:43:00.843+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T07:43:00.929+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:00 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 14.483592 s
[2023-01-31T07:43:25.304+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:25 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 81d5fcd0285b:42647 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T07:43:25.412+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:25 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 81d5fcd0285b:42647 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T07:43:27.433+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:43:27.486+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T07:43:27.592+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:43:31.776+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:31 INFO CodeGenerator: Code generated in 1697.888888 ms
[2023-01-31T07:43:31.929+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:31 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T07:43:31.965+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:31 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T07:43:31.966+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:31 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 81d5fcd0285b:42647 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T07:43:31.968+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:31 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T07:43:32.010+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:43:32.821+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T07:43:32.822+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T07:43:32.823+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T07:43:32.823+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:43:32.825+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:43:32.855+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T07:43:33.009+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T07:43:33.122+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T07:43:33.129+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 81d5fcd0285b:42647 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T07:43:33.141+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:43:33.170+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:43:33.171+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T07:43:33.280+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:43:33.294+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T07:43:34.365+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T07:43:36.012+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:36 INFO CodeGenerator: Code generated in 1296.691482 ms
[2023-01-31T07:43:37.363+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:37 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1628 bytes result sent to driver
[2023-01-31T07:43:37.407+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:37 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 4214 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:43:37.420+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:37 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 4.491 s
[2023-01-31T07:43:37.421+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:37 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T07:43:37.430+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:37 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:43:37.456+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T07:43:37.457+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:37 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 4.634064 s
[2023-01-31T07:43:38.558+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:38 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:43:38.564+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:38 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T07:43:38.572+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:38 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:43:39.073+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:39 INFO CodeGenerator: Code generated in 283.730482 ms
[2023-01-31T07:43:39.153+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:39 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T07:43:39.705+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:39 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T07:43:39.719+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:39 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 81d5fcd0285b:42647 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:43:39.744+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:39 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T07:43:39.822+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:39 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:43:40.240+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T07:43:40.245+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T07:43:40.246+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T07:43:40.248+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:43:40.249+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:43:40.273+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T07:43:40.346+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T07:43:40.391+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T07:43:40.396+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 81d5fcd0285b:42647 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T07:43:40.414+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:43:40.431+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:43:40.431+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T07:43:40.444+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:43:40.463+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T07:43:40.764+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T07:43:42.508+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:42 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1585 bytes result sent to driver
[2023-01-31T07:43:42.624+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:42 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 2181 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:43:42.648+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:42 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 2.335 s
[2023-01-31T07:43:42.650+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:42 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:43:42.664+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:42 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T07:43:42.669+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T07:43:42.677+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:42 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 2.433340 s
[2023-01-31T07:43:46.138+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:46 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:43:46.161+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:46 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T07:43:46.183+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:46 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:43:46.878+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:46 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 81d5fcd0285b:42647 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T07:43:47.113+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:47 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 81d5fcd0285b:42647 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T07:43:48.789+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:48 INFO CodeGenerator: Code generated in 464.498202 ms
[2023-01-31T07:43:48.809+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:48 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T07:43:48.856+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:48 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T07:43:48.901+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:48 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 81d5fcd0285b:42647 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:43:48.902+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:48 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T07:43:48.921+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:43:49.996+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:49 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T07:43:50.005+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T07:43:50.006+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T07:43:50.007+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:43:50.013+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:43:50.029+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T07:43:50.169+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.7 MiB)
[2023-01-31T07:43:50.247+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.7 MiB)
[2023-01-31T07:43:50.263+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 81d5fcd0285b:42647 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T07:43:50.267+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:43:50.279+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:43:50.281+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T07:43:50.302+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:43:50.307+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T07:43:50.733+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T07:43:51.841+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:51 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-01-31T07:43:51.862+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:51 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 1562 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:43:51.868+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:51 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T07:43:51.885+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:51 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 1.800 s
[2023-01-31T07:43:51.891+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:51 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:43:51.892+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T07:43:51.906+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:51 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 1.901835 s
[2023-01-31T07:43:53.395+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:53 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:43:53.418+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:53 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T07:43:53.421+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:53 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:43:54.027+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:54 INFO CodeGenerator: Code generated in 379.117003 ms
[2023-01-31T07:43:54.163+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:54 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.5 MiB)
[2023-01-31T07:43:54.733+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:54 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T07:43:54.777+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:54 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 81d5fcd0285b:42647 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:43:54.778+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:54 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T07:43:54.793+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:54 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:43:55.805+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:55 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T07:43:55.853+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:55 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T07:43:55.854+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:55 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T07:43:55.854+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:55 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:43:55.879+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:55 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:43:55.880+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:55 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T07:43:56.059+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:56 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-31T07:43:56.120+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:56 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-01-31T07:43:56.152+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:56 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 81d5fcd0285b:42647 (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T07:43:56.190+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:56 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:43:56.204+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:43:56.211+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:56 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T07:43:56.226+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:56 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:43:56.227+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:56 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T07:43:56.576+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:56 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T07:43:57.620+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:57 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1629 bytes result sent to driver
[2023-01-31T07:43:57.633+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:57 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 1416 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:43:57.644+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:57 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 1.664 s
[2023-01-31T07:43:57.645+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:57 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:43:57.649+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:57 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T07:43:57.654+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T07:43:57.663+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:57 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 1.853308 s
[2023-01-31T07:44:06.133+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:06 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:44:06.223+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:06 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T07:44:06.290+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:06 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:44:10.400+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:10 INFO CodeGenerator: Code generated in 1938.734174 ms
[2023-01-31T07:44:10.417+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:10 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.2 MiB)
[2023-01-31T07:44:10.485+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:10 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T07:44:10.492+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:10 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 81d5fcd0285b:42647 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T07:44:10.506+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:10 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T07:44:10.583+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:10 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:44:11.547+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:11 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 81d5fcd0285b:42647 in memory (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T07:44:11.893+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:11 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 81d5fcd0285b:42647 in memory (size: 12.7 KiB, free: 434.2 MiB)
[2023-01-31T07:44:11.909+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:11 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T07:44:11.920+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:11 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) with 1 output partitions
[2023-01-31T07:44:11.922+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:11 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201)
[2023-01-31T07:44:11.923+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:11 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:44:11.936+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:11 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:44:11.962+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:11 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201), which has no missing parents
[2023-01-31T07:44:12.272+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:12 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.2 MiB)
[2023-01-31T07:44:12.347+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:12 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.2 MiB)
[2023-01-31T07:44:12.369+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:12 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 81d5fcd0285b:42647 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T07:44:12.392+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:12 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:44:12.395+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:44:12.464+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:12 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T07:44:12.521+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:12 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:44:12.535+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:12 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T07:44:13.036+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:13 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T07:44:14.963+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:14 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2499 bytes result sent to driver
[2023-01-31T07:44:14.974+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:14 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 2450 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:44:14.975+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:14 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T07:44:14.986+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:14 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) finished in 2.990 s
[2023-01-31T07:44:14.995+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:14 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:44:14.996+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T07:44:14.997+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:14 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201, took 3.079531 s
[2023-01-31T07:44:17.109+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:17 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:44:17.110+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:17 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T07:44:17.111+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:17 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:44:19.262+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:19 INFO CodeGenerator: Code generated in 1034.154904 ms
[2023-01-31T07:44:19.874+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:19 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 433.0 MiB)
[2023-01-31T07:44:19.975+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:19 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.0 MiB)
[2023-01-31T07:44:19.976+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:19 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 81d5fcd0285b:42647 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T07:44:19.980+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:19 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T07:44:20.025+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:20 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:44:20.728+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:20 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T07:44:20.789+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:20 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T07:44:20.801+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:20 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T07:44:20.802+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:20 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:44:20.833+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:20 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:44:20.859+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:20 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T07:44:21.293+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:21 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 432.9 MiB)
[2023-01-31T07:44:21.357+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:21 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.9 MiB)
[2023-01-31T07:44:21.377+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:21 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 81d5fcd0285b:42647 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T07:44:21.451+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:21 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:44:21.458+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:44:21.460+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:21 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T07:44:21.549+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:21 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:44:21.561+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:21 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T07:44:21.826+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:21 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T07:44:23.017+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:22 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-01-31T07:44:23.040+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:23 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 1484 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:44:23.041+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:23 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T07:44:23.069+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:23 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 2.034 s
[2023-01-31T07:44:23.070+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:23 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:44:23.070+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T07:44:23.071+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:23 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 2.319453 s
[2023-01-31T07:44:25.389+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:25 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:44:25.390+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:25 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T07:44:25.424+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:25 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:44:29.984+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:29 INFO CodeGenerator: Code generated in 2944.339645 ms
[2023-01-31T07:44:30.262+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:30 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.7 MiB)
[2023-01-31T07:44:31.197+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:31 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.7 MiB)
[2023-01-31T07:44:31.197+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:31 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 81d5fcd0285b:42647 (size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T07:44:31.225+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:31 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T07:44:31.349+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:31 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:44:32.345+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:32 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T07:44:32.346+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:32 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T07:44:32.365+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:32 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T07:44:32.366+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:32 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:44:32.366+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:32 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:44:32.367+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:32 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T07:44:32.437+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:32 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.7 MiB)
[2023-01-31T07:44:32.478+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:32 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.6 MiB)
[2023-01-31T07:44:32.483+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:32 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 81d5fcd0285b:42647 (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T07:44:32.495+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:32 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:44:32.521+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:44:32.525+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:32 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T07:44:32.550+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:32 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:44:32.554+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:32 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T07:44:32.721+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:32 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T07:44:33.205+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:33 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1729 bytes result sent to driver
[2023-01-31T07:44:33.225+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:33 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 662 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:44:33.247+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:33 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T07:44:33.248+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:33 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 0.855 s
[2023-01-31T07:44:33.249+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:33 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:44:33.249+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T07:44:33.250+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:33 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 0.908616 s
[2023-01-31T07:44:38.613+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:38 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 81d5fcd0285b:42647 in memory (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T07:44:38.836+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:38 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 81d5fcd0285b:42647 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T07:44:39.044+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:39 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 81d5fcd0285b:42647 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T07:44:39.469+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:39 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 81d5fcd0285b:42647 in memory (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T07:44:40.012+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:40 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 81d5fcd0285b:42647 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T07:44:40.220+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:40 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 81d5fcd0285b:42647 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:44:40.297+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:40 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 81d5fcd0285b:42647 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:44:40.471+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:40 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 81d5fcd0285b:42647 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:45:17.315+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:17 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T07:45:17.420+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T07:45:17.421+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T07:45:17.428+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:17 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T07:45:17.429+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T07:45:17.429+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T07:45:17.432+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T07:45:18.151+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-31T07:45:18.154+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO DAGScheduler: Got job 8 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T07:45:18.155+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO DAGScheduler: Final stage: ResultStage 8 (save at BigQueryWriteHelper.java:105)
[2023-01-31T07:45:18.158+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:45:18.158+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:45:18.163+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO DAGScheduler: Submitting ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T07:45:18.383+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 209.9 KiB, free 433.7 MiB)
[2023-01-31T07:45:18.428+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 74.3 KiB, free 433.7 MiB)
[2023-01-31T07:45:18.431+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 81d5fcd0285b:42647 (size: 74.3 KiB, free: 434.3 MiB)
[2023-01-31T07:45:18.435+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:45:18.449+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:45:18.449+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2023-01-31T07:45:18.468+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7484 bytes) taskResourceAssignments Map()
[2023-01-31T07:45:18.476+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2023-01-31T07:45:18.616+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T07:45:18.617+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T07:45:18.618+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T07:45:18.618+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T07:45:18.619+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T07:45:18.624+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T07:45:18.640+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T07:45:18.644+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T07:45:18.736+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO ParquetOutputFormat: Parquet block size to 134217728
[2023-01-31T07:45:18.737+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO ParquetOutputFormat: Validation is off
[2023-01-31T07:45:18.737+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-01-31T07:45:18.738+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO ParquetOutputFormat: Parquet properties are:
[2023-01-31T07:45:18.738+0000] {spark_submit.py:495} INFO - Parquet page size to 1048576
[2023-01-31T07:45:18.738+0000] {spark_submit.py:495} INFO - Parquet dictionary page size to 1048576
[2023-01-31T07:45:18.738+0000] {spark_submit.py:495} INFO - Dictionary is true
[2023-01-31T07:45:18.738+0000] {spark_submit.py:495} INFO - Writer version is: PARQUET_1_0
[2023-01-31T07:45:18.739+0000] {spark_submit.py:495} INFO - Page size checking is: estimated
[2023-01-31T07:45:18.739+0000] {spark_submit.py:495} INFO - Min row count for page size check is: 100
[2023-01-31T07:45:18.739+0000] {spark_submit.py:495} INFO - Max row count for page size check is: 10000
[2023-01-31T07:45:18.739+0000] {spark_submit.py:495} INFO - Truncate length for column indexes is: 64
[2023-01-31T07:45:18.739+0000] {spark_submit.py:495} INFO - Truncate length for statistics min/max  is: 2147483647
[2023-01-31T07:45:18.739+0000] {spark_submit.py:495} INFO - Bloom filter enabled: false
[2023-01-31T07:45:18.740+0000] {spark_submit.py:495} INFO - Max Bloom filter size for a column is 1048576
[2023-01-31T07:45:18.740+0000] {spark_submit.py:495} INFO - Bloom filter expected number of distinct values are: null
[2023-01-31T07:45:18.740+0000] {spark_submit.py:495} INFO - Page row count limit to 20000
[2023-01-31T07:45:18.740+0000] {spark_submit.py:495} INFO - Writing page checksums is: on
[2023-01-31T07:45:18.926+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-01-31T07:45:18.927+0000] {spark_submit.py:495} INFO - {
[2023-01-31T07:45:18.927+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-31T07:45:18.928+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-31T07:45:18.928+0000] {spark_submit.py:495} INFO - "name" : "measured_at",
[2023-01-31T07:45:18.928+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T07:45:18.928+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T07:45:18.929+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.929+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.929+0000] {spark_submit.py:495} INFO - "name" : "created_at",
[2023-01-31T07:45:18.929+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T07:45:18.929+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T07:45:18.930+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.930+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.930+0000] {spark_submit.py:495} INFO - "name" : "Biomass - Actual Aggregated",
[2023-01-31T07:45:18.930+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.931+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.931+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.931+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.931+0000] {spark_submit.py:495} INFO - "name" : "Fossil Brown coal/Lignite - Actual Aggregated",
[2023-01-31T07:45:18.931+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.931+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.932+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.932+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.932+0000] {spark_submit.py:495} INFO - "name" : "Fossil Gas - Actual Aggregated",
[2023-01-31T07:45:18.932+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.932+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.933+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.934+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.935+0000] {spark_submit.py:495} INFO - "name" : "Fossil Hard coal - Actual Aggregated",
[2023-01-31T07:45:18.935+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.936+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.937+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.939+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.939+0000] {spark_submit.py:495} INFO - "name" : "Fossil Oil - Actual Aggregated",
[2023-01-31T07:45:18.939+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.939+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.939+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.941+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.941+0000] {spark_submit.py:495} INFO - "name" : "Geothermal - Actual Aggregated",
[2023-01-31T07:45:18.941+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.943+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.943+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.944+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.949+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Aggregated",
[2023-01-31T07:45:18.949+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.949+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.950+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.950+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.950+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Consumption",
[2023-01-31T07:45:18.950+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.950+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.950+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.951+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.951+0000] {spark_submit.py:495} INFO - "name" : "Hydro Run-of-river and poundage - Actual Aggregated",
[2023-01-31T07:45:18.951+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.951+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.951+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.951+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.952+0000] {spark_submit.py:495} INFO - "name" : "Hydro Water Reservoir - Actual Aggregated",
[2023-01-31T07:45:18.952+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.952+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.952+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.952+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.963+0000] {spark_submit.py:495} INFO - "name" : "Nuclear - Actual Aggregated",
[2023-01-31T07:45:18.963+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.963+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.963+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.964+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.964+0000] {spark_submit.py:495} INFO - "name" : "Other - Actual Aggregated",
[2023-01-31T07:45:18.964+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.964+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.964+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.965+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.965+0000] {spark_submit.py:495} INFO - "name" : "Other renewable - Actual Aggregated",
[2023-01-31T07:45:18.965+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.965+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.965+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.966+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.966+0000] {spark_submit.py:495} INFO - "name" : "Solar - Actual Aggregated",
[2023-01-31T07:45:18.966+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.966+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.966+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.966+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.967+0000] {spark_submit.py:495} INFO - "name" : "Waste - Actual Aggregated",
[2023-01-31T07:45:18.967+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.967+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.967+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.967+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.967+0000] {spark_submit.py:495} INFO - "name" : "Wind Offshore - Actual Aggregated",
[2023-01-31T07:45:18.968+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.968+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.968+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.977+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.977+0000] {spark_submit.py:495} INFO - "name" : "Wind Onshore - Actual Aggregated",
[2023-01-31T07:45:18.977+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.977+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.979+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.980+0000] {spark_submit.py:495} INFO - } ]
[2023-01-31T07:45:18.980+0000] {spark_submit.py:495} INFO - }
[2023-01-31T07:45:18.980+0000] {spark_submit.py:495} INFO - and corresponding Parquet message type:
[2023-01-31T07:45:18.980+0000] {spark_submit.py:495} INFO - message spark_schema {
[2023-01-31T07:45:18.980+0000] {spark_submit.py:495} INFO - required int96 measured_at;
[2023-01-31T07:45:18.981+0000] {spark_submit.py:495} INFO - required int96 created_at;
[2023-01-31T07:45:18.981+0000] {spark_submit.py:495} INFO - optional double Biomass - Actual Aggregated;
[2023-01-31T07:45:18.981+0000] {spark_submit.py:495} INFO - optional double Fossil Brown coal/Lignite - Actual Aggregated;
[2023-01-31T07:45:18.981+0000] {spark_submit.py:495} INFO - optional double Fossil Gas - Actual Aggregated;
[2023-01-31T07:45:18.981+0000] {spark_submit.py:495} INFO - optional double Fossil Hard coal - Actual Aggregated;
[2023-01-31T07:45:18.981+0000] {spark_submit.py:495} INFO - optional double Fossil Oil - Actual Aggregated;
[2023-01-31T07:45:18.981+0000] {spark_submit.py:495} INFO - optional double Geothermal - Actual Aggregated;
[2023-01-31T07:45:18.982+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Aggregated;
[2023-01-31T07:45:18.982+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Consumption;
[2023-01-31T07:45:18.982+0000] {spark_submit.py:495} INFO - optional double Hydro Run-of-river and poundage - Actual Aggregated;
[2023-01-31T07:45:18.983+0000] {spark_submit.py:495} INFO - optional double Hydro Water Reservoir - Actual Aggregated;
[2023-01-31T07:45:18.983+0000] {spark_submit.py:495} INFO - optional double Nuclear - Actual Aggregated;
[2023-01-31T07:45:18.983+0000] {spark_submit.py:495} INFO - optional double Other - Actual Aggregated;
[2023-01-31T07:45:18.983+0000] {spark_submit.py:495} INFO - optional double Other renewable - Actual Aggregated;
[2023-01-31T07:45:18.984+0000] {spark_submit.py:495} INFO - optional double Solar - Actual Aggregated;
[2023-01-31T07:45:18.984+0000] {spark_submit.py:495} INFO - optional double Waste - Actual Aggregated;
[2023-01-31T07:45:18.984+0000] {spark_submit.py:495} INFO - optional double Wind Offshore - Actual Aggregated;
[2023-01-31T07:45:18.984+0000] {spark_submit.py:495} INFO - optional double Wind Onshore - Actual Aggregated;
[2023-01-31T07:45:18.984+0000] {spark_submit.py:495} INFO - }
[2023-01-31T07:45:18.985+0000] {spark_submit.py:495} INFO - 
[2023-01-31T07:45:18.985+0000] {spark_submit.py:495} INFO - 
[2023-01-31T07:45:19.677+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:19 INFO CodecPool: Got brand-new compressor [.snappy]
[2023-01-31T07:45:20.765+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:20 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 81d5fcd0285b:42647 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:45:29.633+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:29 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675150865556-37aece3d-4e7d-46cd-9fdc-798c246d4773/_temporary/0/_temporary/' directory.
[2023-01-31T07:45:29.635+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:29 INFO FileOutputCommitter: Saved output of task 'attempt_202301310745181247869227893411133_0008_m_000000_8' to gs://entsoe_temp_1009/.spark-bigquery-local-1675150865556-37aece3d-4e7d-46cd-9fdc-798c246d4773/_temporary/0/task_202301310745181247869227893411133_0008_m_000000
[2023-01-31T07:45:29.636+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:29 INFO SparkHadoopMapRedUtil: attempt_202301310745181247869227893411133_0008_m_000000_8: Committed. Elapsed time: 1822 ms.
[2023-01-31T07:45:29.698+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:29 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2588 bytes result sent to driver
[2023-01-31T07:45:29.699+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:29 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 11238 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:45:29.699+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:29 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2023-01-31T07:45:29.719+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:29 INFO DAGScheduler: ResultStage 8 (save at BigQueryWriteHelper.java:105) finished in 11.535 s
[2023-01-31T07:45:29.719+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:29 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:45:29.720+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2023-01-31T07:45:29.721+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:29 INFO DAGScheduler: Job 8 finished: save at BigQueryWriteHelper.java:105, took 11.553484 s
[2023-01-31T07:45:29.733+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:29 INFO FileFormatWriter: Start to commit write Job 4bd6c209-fad9-4e01-8c96-ef4d4b6f48fc.
[2023-01-31T07:45:30.922+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:30 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675150865556-37aece3d-4e7d-46cd-9fdc-798c246d4773/_temporary/0/task_202301310745181247869227893411133_0008_m_000000/' directory.
[2023-01-31T07:45:32.198+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:32 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675150865556-37aece3d-4e7d-46cd-9fdc-798c246d4773/' directory.
[2023-01-31T07:45:33.365+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:33 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 81d5fcd0285b:42647 in memory (size: 74.3 KiB, free: 434.4 MiB)
[2023-01-31T07:45:33.768+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:33 INFO FileFormatWriter: Write Job 4bd6c209-fad9-4e01-8c96-ef4d4b6f48fc committed. Elapsed time: 4017 ms.
[2023-01-31T07:45:33.816+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:33 INFO FileFormatWriter: Finished processing stats for write job 4bd6c209-fad9-4e01-8c96-ef4d4b6f48fc.
[2023-01-31T07:45:41.101+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:41 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=entsoe_analytics_1009, projectId=rafzul-analytics-1009, tableId=TEST_total_generation_staging}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=null, ignoreUnknownValue=null, sourceUris=[gs://entsoe_temp_1009/.spark-bigquery-local-1675150865556-37aece3d-4e7d-46cd-9fdc-798c246d4773/part-00000-1ea7f152-5492-4276-af7f-ddce3159e24a-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=true, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=rafzul-analytics-1009, job=d6d9dff3-30b7-4f47-b2f0-902edbb27742, location=US}
[2023-01-31T07:45:46.105+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:46 INFO BigQueryClient: Done loading to rafzul-analytics-1009.entsoe_analytics_1009.TEST_total_generation_staging. jobId: JobId{project=rafzul-analytics-1009, job=d6d9dff3-30b7-4f47-b2f0-902edbb27742, location=US}
[2023-01-31T07:45:47.413+0000] {spark_submit.py:495} INFO - MANTEP!!!!
[2023-01-31T07:45:47.776+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:47 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-31T07:45:47.808+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:47 INFO SparkUI: Stopped Spark web UI at http://81d5fcd0285b:4046
[2023-01-31T07:45:47.845+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:47 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-31T07:45:47.894+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:47 INFO MemoryStore: MemoryStore cleared
[2023-01-31T07:45:47.895+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:47 INFO BlockManager: BlockManager stopped
[2023-01-31T07:45:47.904+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:47 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-31T07:45:47.915+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:47 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-31T07:45:47.936+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:47 INFO SparkContext: Successfully stopped SparkContext
[2023-01-31T07:45:47.937+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:47 INFO ShutdownHookManager: Shutdown hook called
[2023-01-31T07:45:47.939+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:47 INFO ShutdownHookManager: Deleting directory /tmp/spark-36349aee-2534-4650-b9b1-43890d392200/pyspark-fd3950a5-4ad6-4071-95e2-af672c105892
[2023-01-31T07:45:47.957+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:47 INFO ShutdownHookManager: Deleting directory /tmp/spark-36349aee-2534-4650-b9b1-43890d392200
[2023-01-31T07:45:47.990+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:47 INFO ShutdownHookManager: Deleting directory /tmp/spark-08c10094-4ff3-4f41-96cd-480ec1eb7c88
[2023-01-31T07:45:48.300+0000] {taskinstance.py:1322} INFO - Marking task as SUCCESS. dag_id=entsoe-energydata-backfill, task_id=stage_total_generation, execution_date=20210101T060000, start_date=20230131T073904, end_date=20230131T074548
[2023-01-31T07:45:48.369+0000] {local_task_job.py:159} INFO - Task exited with return code 0
[2023-01-31T07:45:48.427+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T08:21:37.225+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [queued]>
[2023-01-31T08:21:37.253+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [queued]>
[2023-01-31T08:21:37.254+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T08:21:37.254+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T08:21:37.254+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T08:21:37.290+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 06:00:00+00:00
[2023-01-31T08:21:37.304+0000] {standard_task_runner.py:55} INFO - Started process 2288 to run task
[2023-01-31T08:21:37.314+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T06:00:00+00:00', '--job-id', '929', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpg83wwcpz']
[2023-01-31T08:21:37.320+0000] {standard_task_runner.py:83} INFO - Job 929: Subtask stage_total_generation
[2023-01-31T08:21:37.498+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [running]> on host 81d5fcd0285b
[2023-01-31T08:21:37.717+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T06:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T06:00:00+00:00
[2023-01-31T08:21:37.839+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T08:21:37.842+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010600 202101010700 DE_TENNET
[2023-01-31T08:22:00.986+0000] {spark_submit.py:495} INFO - {{ start_date }}
[2023-01-31T08:22:00.986+0000] {spark_submit.py:495} INFO - mulai periode: {{ start_date }}
[2023-01-31T08:22:00.987+0000] {spark_submit.py:495} INFO - selesai periode: {{ end_date }}
[2023-01-31T08:22:02.018+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:02 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T08:22:02.722+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T08:22:04.237+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:04 INFO ResourceUtils: ==============================================================
[2023-01-31T08:22:04.275+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:04 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T08:22:04.284+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:04 INFO ResourceUtils: ==============================================================
[2023-01-31T08:22:04.303+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:04 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T08:22:04.707+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:04 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T08:22:04.795+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:04 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T08:22:04.809+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:04 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T08:22:05.496+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:05 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T08:22:05.519+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:05 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T08:22:05.522+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:05 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T08:22:05.557+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:05 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T08:22:05.564+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:05 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T08:22:08.744+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:08 INFO Utils: Successfully started service 'sparkDriver' on port 41523.
[2023-01-31T08:22:09.076+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:09 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T08:22:09.258+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:09 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T08:22:09.464+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:09 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T08:22:09.467+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:09 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T08:22:09.530+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:09 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T08:22:09.836+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:09 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-feb0f9b1-c8ed-4a3f-97a6-e7b03b4fa0ac
[2023-01-31T08:22:09.916+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:09 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T08:22:10.042+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:10 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T08:22:12.497+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:12 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T08:22:12.509+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:12 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2023-01-31T08:22:12.509+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:12 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[2023-01-31T08:22:12.520+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:12 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[2023-01-31T08:22:12.529+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:12 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[2023-01-31T08:22:12.567+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:12 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
[2023-01-31T08:22:12.637+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:12 INFO Utils: Successfully started service 'SparkUI' on port 4046.
[2023-01-31T08:22:13.020+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:13 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://81d5fcd0285b:41523/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675153321818
[2023-01-31T08:22:13.022+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:13 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://81d5fcd0285b:41523/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675153321818
[2023-01-31T08:22:13.660+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:13 INFO Executor: Starting executor ID driver on host 81d5fcd0285b
[2023-01-31T08:22:13.681+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:13 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T08:22:13.769+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:13 INFO Executor: Fetching spark://81d5fcd0285b:41523/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675153321818
[2023-01-31T08:22:14.031+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:14 INFO TransportClientFactory: Successfully created connection to 81d5fcd0285b/172.19.0.5:41523 after 158 ms (0 ms spent in bootstraps)
[2023-01-31T08:22:14.051+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:14 INFO Utils: Fetching spark://81d5fcd0285b:41523/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-987f480c-d346-4f2b-b445-93d153c34b87/userFiles-5daa858d-2b06-4b6d-a0cf-2a89fc9d4d08/fetchFileTemp14085226129108755483.tmp
[2023-01-31T08:22:15.493+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:15 INFO Executor: Adding file:/tmp/spark-987f480c-d346-4f2b-b445-93d153c34b87/userFiles-5daa858d-2b06-4b6d-a0cf-2a89fc9d4d08/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T08:22:15.493+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:15 INFO Executor: Fetching spark://81d5fcd0285b:41523/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675153321818
[2023-01-31T08:22:15.498+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:15 INFO Utils: Fetching spark://81d5fcd0285b:41523/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-987f480c-d346-4f2b-b445-93d153c34b87/userFiles-5daa858d-2b06-4b6d-a0cf-2a89fc9d4d08/fetchFileTemp531585759194433136.tmp
[2023-01-31T08:22:16.295+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:16 INFO Executor: Adding file:/tmp/spark-987f480c-d346-4f2b-b445-93d153c34b87/userFiles-5daa858d-2b06-4b6d-a0cf-2a89fc9d4d08/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T08:22:16.321+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:16 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44169.
[2023-01-31T08:22:16.322+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:16 INFO NettyBlockTransferService: Server created on 81d5fcd0285b:44169
[2023-01-31T08:22:16.326+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:16 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T08:22:16.418+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:16 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 44169, None)
[2023-01-31T08:22:16.424+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:16 INFO BlockManagerMasterEndpoint: Registering block manager 81d5fcd0285b:44169 with 434.4 MiB RAM, BlockManagerId(driver, 81d5fcd0285b, 44169, None)
[2023-01-31T08:22:16.458+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:16 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 44169, None)
[2023-01-31T08:22:16.461+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:16 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 81d5fcd0285b, 44169, None)
[2023-01-31T08:22:21.050+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:21 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T08:22:21.078+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:21 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T08:22:36.539+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:36 INFO InMemoryFileIndex: It took 719 ms to list leaf files for 1 paths.
[2023-01-31T08:22:38.799+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:38 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T08:22:39.381+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:39 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T08:22:39.417+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:39 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 81d5fcd0285b:44169 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T08:22:39.428+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:39 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T08:22:44.136+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:44 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T08:22:44.208+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:44 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T08:22:44.285+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:44 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T08:22:44.350+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:44 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T08:22:44.352+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:44 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T08:22:44.360+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:44 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:22:44.373+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:44 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:22:44.435+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:44 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T08:22:45.057+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:45 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T08:22:45.129+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:45 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T08:22:45.129+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:45 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 81d5fcd0285b:44169 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T08:22:45.138+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:45 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:22:45.370+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:22:45.400+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:45 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T08:22:45.818+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:45 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T08:22:45.928+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:45 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T08:22:46.728+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:46 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010600__202101010700.json:0+9371
[2023-01-31T08:22:48.406+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:48 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3437 bytes result sent to driver
[2023-01-31T08:22:48.503+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:48 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2750 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:22:48.533+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:48 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T08:22:48.634+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:48 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 4.096 s
[2023-01-31T08:22:48.718+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:48 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:22:48.725+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T08:22:48.754+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:48 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 4.465358 s
[2023-01-31T08:22:51.664+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:51 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 81d5fcd0285b:44169 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T08:22:51.731+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:51 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 81d5fcd0285b:44169 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T08:23:14.141+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:14 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:14.292+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:14 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T08:23:14.388+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:14 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:20.922+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO CodeGenerator: Code generated in 4018.634399 ms
[2023-01-31T08:23:20.942+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T08:23:20.977+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T08:23:20.978+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 81d5fcd0285b:44169 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T08:23:20.982+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T08:23:21.118+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:21.318+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T08:23:21.332+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T08:23:21.333+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T08:23:21.334+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:21.343+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:21.355+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T08:23:21.399+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T08:23:21.399+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T08:23:21.399+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 81d5fcd0285b:44169 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T08:23:21.400+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:21.428+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:21.429+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T08:23:21.449+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:21.485+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T08:23:22.659+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:22 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T08:23:24.569+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:24 INFO CodeGenerator: Code generated in 1587.683288 ms
[2023-01-31T08:23:25.299+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1628 bytes result sent to driver
[2023-01-31T08:23:25.325+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 3876 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:25.331+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 3.965 s
[2023-01-31T08:23:25.331+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:25.337+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T08:23:25.338+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T08:23:25.340+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 4.021056 s
[2023-01-31T08:23:25.822+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:25.826+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T08:23:25.830+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:26.230+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO CodeGenerator: Code generated in 272.40346 ms
[2023-01-31T08:23:26.337+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T08:23:26.571+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T08:23:26.586+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 81d5fcd0285b:44169 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T08:23:26.591+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T08:23:26.614+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:26.878+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T08:23:26.901+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T08:23:26.901+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T08:23:26.902+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:26.909+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:26.910+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T08:23:26.927+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T08:23:26.976+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T08:23:26.994+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 81d5fcd0285b:44169 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T08:23:26.997+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:26.999+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:27.000+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T08:23:27.005+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:27.006+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T08:23:27.142+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T08:23:27.508+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 81d5fcd0285b:44169 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T08:23:27.826+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1628 bytes result sent to driver
[2023-01-31T08:23:27.865+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 859 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:27.865+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T08:23:27.874+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 0.963 s
[2023-01-31T08:23:27.875+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:27.875+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T08:23:27.875+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 0.978485 s
[2023-01-31T08:23:29.442+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:29.448+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T08:23:29.458+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:30.874+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:30 INFO CodeGenerator: Code generated in 887.894502 ms
[2023-01-31T08:23:30.924+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:30 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T08:23:31.100+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T08:23:31.103+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 81d5fcd0285b:44169 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T08:23:31.118+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T08:23:31.133+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:31.515+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T08:23:31.521+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T08:23:31.521+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T08:23:31.522+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:31.523+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:31.536+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T08:23:31.556+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.7 MiB)
[2023-01-31T08:23:31.578+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.6 MiB)
[2023-01-31T08:23:31.619+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 81d5fcd0285b:44169 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T08:23:31.639+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:31.641+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:31.655+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T08:23:31.659+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:31.660+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T08:23:31.886+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T08:23:32.662+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-01-31T08:23:32.693+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 1036 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:32.695+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T08:23:32.712+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 1.179 s
[2023-01-31T08:23:32.715+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:32.715+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T08:23:32.718+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 1.197252 s
[2023-01-31T08:23:33.015+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:33.017+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T08:23:33.021+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:33.215+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO CodeGenerator: Code generated in 104.162924 ms
[2023-01-31T08:23:33.253+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.4 MiB)
[2023-01-31T08:23:33.380+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T08:23:33.386+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 81d5fcd0285b:44169 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T08:23:33.403+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T08:23:33.407+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:33.713+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T08:23:33.720+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T08:23:33.721+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T08:23:33.721+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:33.721+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:33.725+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T08:23:33.738+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-31T08:23:33.743+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-01-31T08:23:33.756+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 81d5fcd0285b:44169 (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T08:23:33.764+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:33.777+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:33.778+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T08:23:33.782+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:33.783+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T08:23:33.945+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T08:23:34.261+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1586 bytes result sent to driver
[2023-01-31T08:23:34.267+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 486 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:34.267+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T08:23:34.273+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 0.535 s
[2023-01-31T08:23:34.274+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:34.274+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T08:23:34.274+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 0.555615 s
[2023-01-31T08:23:37.887+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:37 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:37.889+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:37 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T08:23:37.951+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:37 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:38.890+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO CodeGenerator: Code generated in 521.721489 ms
[2023-01-31T08:23:38.909+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.2 MiB)
[2023-01-31T08:23:38.969+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T08:23:38.972+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 81d5fcd0285b:44169 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T08:23:38.974+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T08:23:38.977+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:39.493+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T08:23:39.495+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) with 1 output partitions
[2023-01-31T08:23:39.496+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201)
[2023-01-31T08:23:39.496+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:39.496+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:39.511+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201), which has no missing parents
[2023-01-31T08:23:39.536+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.1 MiB)
[2023-01-31T08:23:39.567+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.1 MiB)
[2023-01-31T08:23:39.570+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 81d5fcd0285b:44169 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T08:23:39.572+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:39.577+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:39.581+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T08:23:39.586+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:39.587+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T08:23:39.653+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T08:23:40.420+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:40 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2542 bytes result sent to driver
[2023-01-31T08:23:40.426+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:40 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 843 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:40.427+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:40 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T08:23:40.433+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:40 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) finished in 0.924 s
[2023-01-31T08:23:40.434+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:40 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:40.434+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T08:23:40.435+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:40 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201, took 0.935212 s
[2023-01-31T08:23:43.014+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:43 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:43.077+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:43 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T08:23:43.077+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:43 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:45.233+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:45 INFO CodeGenerator: Code generated in 1139.804635 ms
[2023-01-31T08:23:45.393+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:45 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 432.9 MiB)
[2023-01-31T08:23:45.818+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:45 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.9 MiB)
[2023-01-31T08:23:45.819+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:45 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 81d5fcd0285b:44169 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T08:23:45.831+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:45 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T08:23:45.834+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:45 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:46.174+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:46 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T08:23:46.174+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:46 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T08:23:46.175+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:46 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T08:23:46.175+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:46 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:46.176+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:46 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:46.182+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:46 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T08:23:46.224+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:46 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 432.8 MiB)
[2023-01-31T08:23:46.292+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:46 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.8 MiB)
[2023-01-31T08:23:46.298+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:46 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 81d5fcd0285b:44169 (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T08:23:46.319+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:46 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:46.327+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:46.328+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:46 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T08:23:46.357+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:46 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:46.358+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:46 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T08:23:46.444+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:46 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T08:23:47.094+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:47 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-01-31T08:23:47.104+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:47 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 741 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:47.105+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:47 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T08:23:47.105+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:47 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 0.921 s
[2023-01-31T08:23:47.106+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:47 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:47.106+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T08:23:47.106+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:47 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 0.930278 s
[2023-01-31T08:23:47.685+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:47 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:47.702+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:47 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T08:23:47.715+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:47 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:48.843+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:48 INFO CodeGenerator: Code generated in 724.11262 ms
[2023-01-31T08:23:48.872+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:48 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.6 MiB)
[2023-01-31T08:23:49.000+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.6 MiB)
[2023-01-31T08:23:49.003+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 81d5fcd0285b:44169 (size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T08:23:49.004+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T08:23:49.006+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:49.190+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T08:23:49.191+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T08:23:49.191+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T08:23:49.191+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:49.192+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:49.202+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T08:23:49.222+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.6 MiB)
[2023-01-31T08:23:49.232+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.5 MiB)
[2023-01-31T08:23:49.235+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 81d5fcd0285b:44169 (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T08:23:49.236+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:49.236+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:49.237+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T08:23:49.240+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:49.243+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T08:23:49.326+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T08:23:49.558+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1729 bytes result sent to driver
[2023-01-31T08:23:49.569+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 329 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:49.569+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T08:23:49.572+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 0.364 s
[2023-01-31T08:23:49.573+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:49.573+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T08:23:49.573+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 0.383636 s
