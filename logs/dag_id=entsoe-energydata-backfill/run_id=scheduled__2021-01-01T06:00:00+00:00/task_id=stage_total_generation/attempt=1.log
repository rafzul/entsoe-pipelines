[2023-01-31T05:11:09.107+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [queued]>
[2023-01-31T05:11:09.182+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [queued]>
[2023-01-31T05:11:09.182+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T05:11:09.183+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T05:11:09.183+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T05:11:09.321+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 06:00:00+00:00
[2023-01-31T05:11:09.348+0000] {standard_task_runner.py:55} INFO - Started process 12528 to run task
[2023-01-31T05:11:09.368+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T06:00:00+00:00', '--job-id', '885', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpn2j849r1']
[2023-01-31T05:11:09.376+0000] {standard_task_runner.py:83} INFO - Job 885: Subtask stage_total_generation
[2023-01-31T05:11:09.900+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [running]> on host 81d5fcd0285b
[2023-01-31T05:11:10.371+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T06:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T06:00:00+00:00
[2023-01-31T05:11:10.417+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T05:11:10.421+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010600 202101010700 DE_TENNET
[2023-01-31T05:11:42.661+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:42 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T05:11:43.704+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T05:11:45.161+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:45 INFO ResourceUtils: ==============================================================
[2023-01-31T05:11:45.170+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:45 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T05:11:45.177+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:45 INFO ResourceUtils: ==============================================================
[2023-01-31T05:11:45.179+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:45 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T05:11:45.379+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:45 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T05:11:45.442+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:45 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T05:11:45.461+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:45 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T05:11:45.987+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:45 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T05:11:45.995+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:45 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T05:11:46.001+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:46 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T05:11:46.012+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:46 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T05:11:46.018+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T05:11:48.404+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:48 INFO Utils: Successfully started service 'sparkDriver' on port 39581.
[2023-01-31T05:11:48.685+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:48 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T05:11:49.321+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:49 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T05:11:49.757+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:49 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T05:11:49.773+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:49 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T05:11:49.824+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:49 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T05:11:50.331+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:50 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-368630be-9f91-445a-b7a0-479a11f3eb1a
[2023-01-31T05:11:50.597+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:50 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T05:11:50.908+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:50 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T05:11:53.017+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:53 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T05:11:53.021+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:53 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2023-01-31T05:11:53.031+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:53 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[2023-01-31T05:11:53.042+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:53 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[2023-01-31T05:11:53.052+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:53 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[2023-01-31T05:11:53.062+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:53 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
[2023-01-31T05:11:53.125+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:53 INFO Utils: Successfully started service 'SparkUI' on port 4046.
[2023-01-31T05:11:53.527+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:53 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://81d5fcd0285b:39581/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675141902601
[2023-01-31T05:11:53.538+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:53 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://81d5fcd0285b:39581/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675141902601
[2023-01-31T05:11:54.500+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:54 INFO Executor: Starting executor ID driver on host 81d5fcd0285b
[2023-01-31T05:11:54.564+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:54 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T05:11:54.740+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:54 INFO Executor: Fetching spark://81d5fcd0285b:39581/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675141902601
[2023-01-31T05:11:55.677+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:55 INFO TransportClientFactory: Successfully created connection to 81d5fcd0285b/172.19.0.9:39581 after 660 ms (0 ms spent in bootstraps)
[2023-01-31T05:11:55.737+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:55 INFO Utils: Fetching spark://81d5fcd0285b:39581/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-d7a6afd5-72c7-4f7d-9113-17e132c0fd15/userFiles-c4bf4bdc-78a7-4e0d-812d-a4b414c5bfca/fetchFileTemp17605483383629990865.tmp
[2023-01-31T05:11:58.014+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:58 INFO Executor: Adding file:/tmp/spark-d7a6afd5-72c7-4f7d-9113-17e132c0fd15/userFiles-c4bf4bdc-78a7-4e0d-812d-a4b414c5bfca/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T05:11:58.018+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:58 INFO Executor: Fetching spark://81d5fcd0285b:39581/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675141902601
[2023-01-31T05:11:58.077+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:58 INFO Utils: Fetching spark://81d5fcd0285b:39581/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-d7a6afd5-72c7-4f7d-9113-17e132c0fd15/userFiles-c4bf4bdc-78a7-4e0d-812d-a4b414c5bfca/fetchFileTemp14632098204375159938.tmp
[2023-01-31T05:11:58.792+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:58 INFO Executor: Adding file:/tmp/spark-d7a6afd5-72c7-4f7d-9113-17e132c0fd15/userFiles-c4bf4bdc-78a7-4e0d-812d-a4b414c5bfca/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T05:11:58.882+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:58 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34067.
[2023-01-31T05:11:58.883+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:58 INFO NettyBlockTransferService: Server created on 81d5fcd0285b:34067
[2023-01-31T05:11:58.892+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:58 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T05:11:58.947+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:58 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 34067, None)
[2023-01-31T05:11:58.972+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:58 INFO BlockManagerMasterEndpoint: Registering block manager 81d5fcd0285b:34067 with 434.4 MiB RAM, BlockManagerId(driver, 81d5fcd0285b, 34067, None)
[2023-01-31T05:11:59.003+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:59 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 34067, None)
[2023-01-31T05:11:59.011+0000] {spark_submit.py:495} INFO - 23/01/31 05:11:59 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 81d5fcd0285b, 34067, None)
[2023-01-31T05:12:05.919+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:05 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T05:12:06.067+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:06 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T05:12:29.427+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:29 INFO InMemoryFileIndex: It took 784 ms to list leaf files for 1 paths.
[2023-01-31T05:12:32.921+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:32 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T05:12:34.261+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:34 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T05:12:34.280+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:34 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 81d5fcd0285b:34067 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T05:12:34.344+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:34 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T05:12:39.898+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:39 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T05:12:40.216+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:40 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T05:12:40.481+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:40 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T05:12:40.639+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:40 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T05:12:40.643+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:40 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T05:12:40.647+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:40 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:12:40.662+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:40 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:12:40.700+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:40 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T05:12:41.402+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:41 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T05:12:41.462+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:41 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T05:12:41.464+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:41 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 81d5fcd0285b:34067 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T05:12:41.470+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:41 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:12:41.615+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:12:41.618+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:41 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T05:12:41.847+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:41 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T05:12:41.984+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:41 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T05:12:42.702+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:42 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010600__202101010700.json:0+9371
[2023-01-31T05:12:44.183+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:44 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3437 bytes result sent to driver
[2023-01-31T05:12:44.288+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:44 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2487 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:12:44.297+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:44 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T05:12:44.519+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:44 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 3.607 s
[2023-01-31T05:12:44.581+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:44 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:12:44.583+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T05:12:44.610+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:44 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 4.127886 s
[2023-01-31T05:12:49.114+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:49 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 81d5fcd0285b:34067 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T05:12:49.218+0000] {spark_submit.py:495} INFO - 23/01/31 05:12:49 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 81d5fcd0285b:34067 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T05:13:11.399+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:11 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:11.410+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:11 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:13:11.411+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:11 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:13.834+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:13 INFO CodeGenerator: Code generated in 1233.737123 ms
[2023-01-31T05:13:13.847+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:13 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T05:13:13.896+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:13 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T05:13:13.903+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:13 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 81d5fcd0285b:34067 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T05:13:13.905+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:13 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T05:13:13.945+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:13 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:14.317+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T05:13:14.317+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T05:13:14.317+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T05:13:14.317+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:14.318+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:14.318+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T05:13:14.376+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T05:13:14.413+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T05:13:14.416+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 81d5fcd0285b:34067 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T05:13:14.420+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:14.427+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:14.434+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T05:13:14.458+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:14.466+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:14 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T05:13:15.055+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T05:13:15.476+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO CodeGenerator: Code generated in 361.444883 ms
[2023-01-31T05:13:15.905+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1628 bytes result sent to driver
[2023-01-31T05:13:15.931+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1481 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:15.937+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 1.602 s
[2023-01-31T05:13:15.939+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:15.940+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T05:13:15.944+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T05:13:15.950+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:15 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 1.651254 s
[2023-01-31T05:13:16.546+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:16 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:16.546+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:16 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:13:16.551+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:16 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:16.907+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:16 INFO CodeGenerator: Code generated in 216.423158 ms
[2023-01-31T05:13:17.000+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:16 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T05:13:17.485+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T05:13:17.490+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 81d5fcd0285b:34067 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:13:17.500+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T05:13:17.507+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:17.671+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T05:13:17.676+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T05:13:17.676+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T05:13:17.677+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:17.677+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:17.684+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T05:13:17.705+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T05:13:17.729+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T05:13:17.737+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 81d5fcd0285b:34067 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T05:13:17.743+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:17.748+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:17.751+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T05:13:17.759+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:17.765+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T05:13:17.871+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:17 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T05:13:18.268+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1628 bytes result sent to driver
[2023-01-31T05:13:18.303+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 546 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:18.320+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 0.624 s
[2023-01-31T05:13:18.321+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:18.324+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T05:13:18.332+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T05:13:18.333+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:18 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 0.655656 s
[2023-01-31T05:13:19.962+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:19.965+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T05:13:19.966+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:19 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:20.870+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO CodeGenerator: Code generated in 658.917522 ms
[2023-01-31T05:13:20.886+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T05:13:20.936+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T05:13:20.938+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 81d5fcd0285b:34067 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:13:20.946+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T05:13:20.949+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:20 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:21.103+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T05:13:21.104+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T05:13:21.104+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T05:13:21.105+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:21.116+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:21.118+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T05:13:21.128+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.6 MiB)
[2023-01-31T05:13:21.153+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.6 MiB)
[2023-01-31T05:13:21.157+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 81d5fcd0285b:34067 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T05:13:21.158+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:21.163+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:21.163+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T05:13:21.167+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:21.168+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T05:13:21.212+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T05:13:21.448+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-01-31T05:13:21.460+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 293 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:21.463+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T05:13:21.469+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 0.346 s
[2023-01-31T05:13:21.469+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:21.471+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T05:13:21.476+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 0.375060 s
[2023-01-31T05:13:21.642+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:21.642+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:13:21.644+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:21.799+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO CodeGenerator: Code generated in 94.697242 ms
[2023-01-31T05:13:21.836+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:21 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.4 MiB)
[2023-01-31T05:13:22.034+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T05:13:22.037+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 81d5fcd0285b:34067 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:13:22.046+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T05:13:22.062+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:22.503+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T05:13:22.517+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T05:13:22.518+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T05:13:22.519+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:22.519+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:22.526+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T05:13:22.577+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-31T05:13:22.636+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-01-31T05:13:22.643+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 81d5fcd0285b:34067 (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T05:13:22.657+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:22.659+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:22.660+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T05:13:22.675+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:22.677+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T05:13:22.809+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:22 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T05:13:23.064+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 81d5fcd0285b:34067 in memory (size: 7.7 KiB, free: 434.2 MiB)
[2023-01-31T05:13:23.143+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 81d5fcd0285b:34067 in memory (size: 7.7 KiB, free: 434.2 MiB)
[2023-01-31T05:13:23.267+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 81d5fcd0285b:34067 in memory (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T05:13:23.326+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1629 bytes result sent to driver
[2023-01-31T05:13:23.333+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 659 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:23.334+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T05:13:23.337+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 0.797 s
[2023-01-31T05:13:23.337+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:23.339+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T05:13:23.339+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:23 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 0.831941 s
[2023-01-31T05:13:25.181+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:25.201+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T05:13:25.202+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:25.564+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO CodeGenerator: Code generated in 300.422854 ms
[2023-01-31T05:13:25.580+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.3 MiB)
[2023-01-31T05:13:25.600+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T05:13:25.606+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 81d5fcd0285b:34067 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:13:25.610+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T05:13:25.618+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:25.769+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T05:13:25.772+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) with 1 output partitions
[2023-01-31T05:13:25.772+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201)
[2023-01-31T05:13:25.773+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:25.773+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:25.780+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201), which has no missing parents
[2023-01-31T05:13:25.811+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.2 MiB)
[2023-01-31T05:13:25.835+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.2 MiB)
[2023-01-31T05:13:25.855+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 81d5fcd0285b:34067 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T05:13:25.863+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:25.872+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:25.878+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T05:13:25.893+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:25.925+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:25 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T05:13:26.036+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T05:13:26.271+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2499 bytes result sent to driver
[2023-01-31T05:13:26.286+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 384 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:26.286+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T05:13:26.287+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) finished in 0.499 s
[2023-01-31T05:13:26.287+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:26.287+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T05:13:26.287+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:26 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201, took 0.510457 s
[2023-01-31T05:13:27.165+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:27 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:27.229+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:27 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T05:13:27.229+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:27 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:28.523+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO CodeGenerator: Code generated in 746.151762 ms
[2023-01-31T05:13:28.595+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 433.0 MiB)
[2023-01-31T05:13:28.776+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.9 MiB)
[2023-01-31T05:13:28.781+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 81d5fcd0285b:34067 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:13:28.790+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T05:13:28.804+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:28.986+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T05:13:28.990+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T05:13:28.991+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T05:13:28.997+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:28.998+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:28 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:29.010+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T05:13:29.054+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 432.9 MiB)
[2023-01-31T05:13:29.083+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.9 MiB)
[2023-01-31T05:13:29.089+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 81d5fcd0285b:34067 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T05:13:29.097+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:29.107+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:29.112+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T05:13:29.124+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:29.127+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T05:13:29.186+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T05:13:29.457+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-01-31T05:13:29.500+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 362 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:29.509+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T05:13:29.513+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 0.467 s
[2023-01-31T05:13:29.514+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:29.524+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T05:13:29.524+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 0.498966 s
[2023-01-31T05:13:29.808+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:13:29.812+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T05:13:29.816+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:29 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:13:30.172+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO CodeGenerator: Code generated in 212.318126 ms
[2023-01-31T05:13:30.188+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.7 MiB)
[2023-01-31T05:13:30.240+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.7 MiB)
[2023-01-31T05:13:30.240+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 81d5fcd0285b:34067 (size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T05:13:30.256+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T05:13:30.276+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:13:30.435+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T05:13:30.437+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T05:13:30.437+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T05:13:30.438+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:13:30.438+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:13:30.458+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T05:13:30.477+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.6 MiB)
[2023-01-31T05:13:30.486+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.6 MiB)
[2023-01-31T05:13:30.491+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 81d5fcd0285b:34067 (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T05:13:30.494+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:13:30.501+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:13:30.503+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T05:13:30.511+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:13:30.512+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T05:13:30.684+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:30 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T05:13:31.287+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:31 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1729 bytes result sent to driver
[2023-01-31T05:13:31.303+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:31 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 794 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:13:31.305+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:31 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T05:13:31.377+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:31 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 0.851 s
[2023-01-31T05:13:31.378+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:31 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:13:31.426+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T05:13:31.442+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:31 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 0.992042 s
[2023-01-31T05:13:38.220+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 81d5fcd0285b:34067 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:13:38.305+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 81d5fcd0285b:34067 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T05:13:38.388+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 81d5fcd0285b:34067 in memory (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T05:13:38.467+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 81d5fcd0285b:34067 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T05:13:38.555+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 81d5fcd0285b:34067 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:13:38.641+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 81d5fcd0285b:34067 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:13:38.708+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 81d5fcd0285b:34067 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:13:38.785+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 81d5fcd0285b:34067 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:13:38.918+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:38 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 81d5fcd0285b:34067 in memory (size: 13.4 KiB, free: 434.3 MiB)
[2023-01-31T05:13:39.005+0000] {spark_submit.py:495} INFO - 23/01/31 05:13:39 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 81d5fcd0285b:34067 in memory (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T05:14:00.152+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:00 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:14:00.190+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:14:00.191+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:14:00.195+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:00 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:14:00.200+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:14:00.201+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:14:00.211+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:14:01.488+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-31T05:14:01.490+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO DAGScheduler: Got job 8 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T05:14:01.490+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO DAGScheduler: Final stage: ResultStage 8 (save at BigQueryWriteHelper.java:105)
[2023-01-31T05:14:01.491+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:14:01.499+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:14:01.500+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO DAGScheduler: Submitting ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T05:14:01.792+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 209.9 KiB, free 434.0 MiB)
[2023-01-31T05:14:01.823+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 74.3 KiB, free 433.9 MiB)
[2023-01-31T05:14:01.830+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 81d5fcd0285b:34067 (size: 74.3 KiB, free: 434.3 MiB)
[2023-01-31T05:14:01.836+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:14:01.841+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:14:01.843+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2023-01-31T05:14:01.865+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7484 bytes) taskResourceAssignments Map()
[2023-01-31T05:14:01.887+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:01 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2023-01-31T05:14:02.961+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:14:02.967+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:14:02.984+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:02 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:14:02.988+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:14:02.991+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:14:03.014+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:14:03.131+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:03 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T05:14:03.198+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:03 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T05:14:03.378+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:03 INFO ParquetOutputFormat: Parquet block size to 134217728
[2023-01-31T05:14:03.380+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:03 INFO ParquetOutputFormat: Validation is off
[2023-01-31T05:14:03.381+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:03 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-01-31T05:14:03.387+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:03 INFO ParquetOutputFormat: Parquet properties are:
[2023-01-31T05:14:03.387+0000] {spark_submit.py:495} INFO - Parquet page size to 1048576
[2023-01-31T05:14:03.388+0000] {spark_submit.py:495} INFO - Parquet dictionary page size to 1048576
[2023-01-31T05:14:03.388+0000] {spark_submit.py:495} INFO - Dictionary is true
[2023-01-31T05:14:03.388+0000] {spark_submit.py:495} INFO - Writer version is: PARQUET_1_0
[2023-01-31T05:14:03.391+0000] {spark_submit.py:495} INFO - Page size checking is: estimated
[2023-01-31T05:14:03.391+0000] {spark_submit.py:495} INFO - Min row count for page size check is: 100
[2023-01-31T05:14:03.394+0000] {spark_submit.py:495} INFO - Max row count for page size check is: 10000
[2023-01-31T05:14:03.395+0000] {spark_submit.py:495} INFO - Truncate length for column indexes is: 64
[2023-01-31T05:14:03.397+0000] {spark_submit.py:495} INFO - Truncate length for statistics min/max  is: 2147483647
[2023-01-31T05:14:03.398+0000] {spark_submit.py:495} INFO - Bloom filter enabled: false
[2023-01-31T05:14:03.401+0000] {spark_submit.py:495} INFO - Max Bloom filter size for a column is 1048576
[2023-01-31T05:14:03.401+0000] {spark_submit.py:495} INFO - Bloom filter expected number of distinct values are: null
[2023-01-31T05:14:03.404+0000] {spark_submit.py:495} INFO - Page row count limit to 20000
[2023-01-31T05:14:03.406+0000] {spark_submit.py:495} INFO - Writing page checksums is: on
[2023-01-31T05:14:03.580+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:03 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-01-31T05:14:03.580+0000] {spark_submit.py:495} INFO - {
[2023-01-31T05:14:03.581+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-31T05:14:03.581+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-31T05:14:03.582+0000] {spark_submit.py:495} INFO - "name" : "measured_at",
[2023-01-31T05:14:03.582+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T05:14:03.583+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T05:14:03.585+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:03.586+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:03.586+0000] {spark_submit.py:495} INFO - "name" : "created_at",
[2023-01-31T05:14:03.587+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T05:14:03.588+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T05:14:03.592+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:03.593+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:03.594+0000] {spark_submit.py:495} INFO - "name" : "Biomass - Actual Aggregated",
[2023-01-31T05:14:03.594+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:03.595+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:03.595+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:03.595+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:03.596+0000] {spark_submit.py:495} INFO - "name" : "Fossil Brown coal/Lignite - Actual Aggregated",
[2023-01-31T05:14:03.596+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:03.602+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:03.603+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:03.603+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:03.604+0000] {spark_submit.py:495} INFO - "name" : "Fossil Gas - Actual Aggregated",
[2023-01-31T05:14:03.604+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:03.605+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:03.606+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:03.606+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:03.612+0000] {spark_submit.py:495} INFO - "name" : "Fossil Hard coal - Actual Aggregated",
[2023-01-31T05:14:03.614+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:03.615+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:03.615+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:03.616+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:03.616+0000] {spark_submit.py:495} INFO - "name" : "Fossil Oil - Actual Aggregated",
[2023-01-31T05:14:03.617+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:03.617+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:03.618+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:03.618+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:03.618+0000] {spark_submit.py:495} INFO - "name" : "Geothermal - Actual Aggregated",
[2023-01-31T05:14:03.619+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:03.619+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:03.620+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:03.621+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:03.623+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Aggregated",
[2023-01-31T05:14:03.627+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:03.628+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:03.628+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:03.636+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:03.637+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Consumption",
[2023-01-31T05:14:03.638+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:03.638+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:03.638+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:03.641+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:03.641+0000] {spark_submit.py:495} INFO - "name" : "Hydro Run-of-river and poundage - Actual Aggregated",
[2023-01-31T05:14:03.642+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:03.644+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:03.645+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:03.645+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:03.646+0000] {spark_submit.py:495} INFO - "name" : "Hydro Water Reservoir - Actual Aggregated",
[2023-01-31T05:14:03.653+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:03.657+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:03.658+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:03.661+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:03.662+0000] {spark_submit.py:495} INFO - "name" : "Nuclear - Actual Aggregated",
[2023-01-31T05:14:03.663+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:03.664+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:03.664+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:03.665+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:03.665+0000] {spark_submit.py:495} INFO - "name" : "Other - Actual Aggregated",
[2023-01-31T05:14:03.666+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:03.666+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:03.667+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:03.668+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:03.668+0000] {spark_submit.py:495} INFO - "name" : "Other renewable - Actual Aggregated",
[2023-01-31T05:14:03.669+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:03.669+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:03.670+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:03.670+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:03.671+0000] {spark_submit.py:495} INFO - "name" : "Solar - Actual Aggregated",
[2023-01-31T05:14:03.671+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:03.672+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:03.672+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:03.673+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:03.673+0000] {spark_submit.py:495} INFO - "name" : "Waste - Actual Aggregated",
[2023-01-31T05:14:03.673+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:03.674+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:03.674+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:03.675+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:03.675+0000] {spark_submit.py:495} INFO - "name" : "Wind Offshore - Actual Aggregated",
[2023-01-31T05:14:03.676+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:03.676+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:03.677+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:03.677+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:14:03.678+0000] {spark_submit.py:495} INFO - "name" : "Wind Onshore - Actual Aggregated",
[2023-01-31T05:14:03.678+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:14:03.679+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:14:03.679+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:14:03.679+0000] {spark_submit.py:495} INFO - } ]
[2023-01-31T05:14:03.680+0000] {spark_submit.py:495} INFO - }
[2023-01-31T05:14:03.680+0000] {spark_submit.py:495} INFO - and corresponding Parquet message type:
[2023-01-31T05:14:03.681+0000] {spark_submit.py:495} INFO - message spark_schema {
[2023-01-31T05:14:03.681+0000] {spark_submit.py:495} INFO - required int96 measured_at;
[2023-01-31T05:14:03.682+0000] {spark_submit.py:495} INFO - required int96 created_at;
[2023-01-31T05:14:03.682+0000] {spark_submit.py:495} INFO - optional double Biomass - Actual Aggregated;
[2023-01-31T05:14:03.683+0000] {spark_submit.py:495} INFO - optional double Fossil Brown coal/Lignite - Actual Aggregated;
[2023-01-31T05:14:03.683+0000] {spark_submit.py:495} INFO - optional double Fossil Gas - Actual Aggregated;
[2023-01-31T05:14:03.683+0000] {spark_submit.py:495} INFO - optional double Fossil Hard coal - Actual Aggregated;
[2023-01-31T05:14:03.684+0000] {spark_submit.py:495} INFO - optional double Fossil Oil - Actual Aggregated;
[2023-01-31T05:14:03.684+0000] {spark_submit.py:495} INFO - optional double Geothermal - Actual Aggregated;
[2023-01-31T05:14:03.685+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Aggregated;
[2023-01-31T05:14:03.685+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Consumption;
[2023-01-31T05:14:03.685+0000] {spark_submit.py:495} INFO - optional double Hydro Run-of-river and poundage - Actual Aggregated;
[2023-01-31T05:14:03.686+0000] {spark_submit.py:495} INFO - optional double Hydro Water Reservoir - Actual Aggregated;
[2023-01-31T05:14:03.686+0000] {spark_submit.py:495} INFO - optional double Nuclear - Actual Aggregated;
[2023-01-31T05:14:03.687+0000] {spark_submit.py:495} INFO - optional double Other - Actual Aggregated;
[2023-01-31T05:14:03.687+0000] {spark_submit.py:495} INFO - optional double Other renewable - Actual Aggregated;
[2023-01-31T05:14:03.688+0000] {spark_submit.py:495} INFO - optional double Solar - Actual Aggregated;
[2023-01-31T05:14:03.688+0000] {spark_submit.py:495} INFO - optional double Waste - Actual Aggregated;
[2023-01-31T05:14:03.689+0000] {spark_submit.py:495} INFO - optional double Wind Offshore - Actual Aggregated;
[2023-01-31T05:14:03.689+0000] {spark_submit.py:495} INFO - optional double Wind Onshore - Actual Aggregated;
[2023-01-31T05:14:03.690+0000] {spark_submit.py:495} INFO - }
[2023-01-31T05:14:03.690+0000] {spark_submit.py:495} INFO - 
[2023-01-31T05:14:03.694+0000] {spark_submit.py:495} INFO - 
[2023-01-31T05:14:04.287+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:04 INFO CodecPool: Got brand-new compressor [.snappy]
[2023-01-31T05:14:10.885+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:10 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675141913985-1928f463-b79d-48eb-86ff-3b3f874c1a74/_temporary/0/_temporary/' directory.
[2023-01-31T05:14:10.886+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:10 INFO FileOutputCommitter: Saved output of task 'attempt_202301310514008531838664300346989_0008_m_000000_8' to gs://entsoe_temp_1009/.spark-bigquery-local-1675141913985-1928f463-b79d-48eb-86ff-3b3f874c1a74/_temporary/0/task_202301310514008531838664300346989_0008_m_000000
[2023-01-31T05:14:10.894+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:10 INFO SparkHadoopMapRedUtil: attempt_202301310514008531838664300346989_0008_m_000000_8: Committed. Elapsed time: 1672 ms.
[2023-01-31T05:14:10.990+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:10 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2588 bytes result sent to driver
[2023-01-31T05:14:10.996+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:10 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 9148 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:14:11.000+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:10 INFO DAGScheduler: ResultStage 8 (save at BigQueryWriteHelper.java:105) finished in 9.499 s
[2023-01-31T05:14:11.001+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:10 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:14:11.003+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:11 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2023-01-31T05:14:11.005+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2023-01-31T05:14:11.009+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:11 INFO DAGScheduler: Job 8 finished: save at BigQueryWriteHelper.java:105, took 9.519713 s
[2023-01-31T05:14:11.012+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:11 INFO FileFormatWriter: Start to commit write Job a3ea308e-f977-43b3-902a-b930b0f9efb9.
[2023-01-31T05:14:11.863+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:11 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675141913985-1928f463-b79d-48eb-86ff-3b3f874c1a74/_temporary/0/task_202301310514008531838664300346989_0008_m_000000/' directory.
[2023-01-31T05:14:12.295+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:12 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675141913985-1928f463-b79d-48eb-86ff-3b3f874c1a74/' directory.
[2023-01-31T05:14:12.577+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:12 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 81d5fcd0285b:34067 in memory (size: 74.3 KiB, free: 434.4 MiB)
[2023-01-31T05:14:13.653+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:13 INFO FileFormatWriter: Write Job a3ea308e-f977-43b3-902a-b930b0f9efb9 committed. Elapsed time: 2628 ms.
[2023-01-31T05:14:13.744+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:13 INFO FileFormatWriter: Finished processing stats for write job a3ea308e-f977-43b3-902a-b930b0f9efb9.
[2023-01-31T05:14:16.639+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:16 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=entsoe_analytics_1009, projectId=rafzul-analytics-1009, tableId=TEST_total_generation_staging}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=null, ignoreUnknownValue=null, sourceUris=[gs://entsoe_temp_1009/.spark-bigquery-local-1675141913985-1928f463-b79d-48eb-86ff-3b3f874c1a74/part-00000-282c7b2a-8cc6-456c-be32-8a14b9c49951-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=true, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=rafzul-analytics-1009, job=90c22869-f7fa-43d9-b743-6862b10f80cf, location=US}
[2023-01-31T05:14:23.313+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO BigQueryClient: Done loading to rafzul-analytics-1009.entsoe_analytics_1009.TEST_total_generation_staging. jobId: JobId{project=rafzul-analytics-1009, job=90c22869-f7fa-43d9-b743-6862b10f80cf, location=US}
[2023-01-31T05:14:23.750+0000] {spark_submit.py:495} INFO - MANTEP!!!!
[2023-01-31T05:14:23.875+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-31T05:14:23.899+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO SparkUI: Stopped Spark web UI at http://81d5fcd0285b:4046
[2023-01-31T05:14:23.915+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-31T05:14:23.934+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO MemoryStore: MemoryStore cleared
[2023-01-31T05:14:23.935+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO BlockManager: BlockManager stopped
[2023-01-31T05:14:23.939+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-31T05:14:23.945+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-31T05:14:23.953+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO SparkContext: Successfully stopped SparkContext
[2023-01-31T05:14:23.953+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO ShutdownHookManager: Shutdown hook called
[2023-01-31T05:14:23.954+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-d7a6afd5-72c7-4f7d-9113-17e132c0fd15/pyspark-2bc6f4c5-7013-4beb-b593-14d49a27d692
[2023-01-31T05:14:23.960+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-d7a6afd5-72c7-4f7d-9113-17e132c0fd15
[2023-01-31T05:14:23.967+0000] {spark_submit.py:495} INFO - 23/01/31 05:14:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-344a3f2d-e38b-455c-ace4-8fb5dea4a208
[2023-01-31T05:14:24.111+0000] {taskinstance.py:1322} INFO - Marking task as SUCCESS. dag_id=entsoe-energydata-backfill, task_id=stage_total_generation, execution_date=20210101T060000, start_date=20230131T051109, end_date=20230131T051424
[2023-01-31T05:14:24.158+0000] {local_task_job.py:159} INFO - Task exited with return code 0
[2023-01-31T05:14:24.178+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T05:30:47.961+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [queued]>
[2023-01-31T05:30:48.103+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [queued]>
[2023-01-31T05:30:48.105+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T05:30:48.106+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T05:30:48.107+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T05:30:48.279+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 06:00:00+00:00
[2023-01-31T05:30:48.314+0000] {standard_task_runner.py:55} INFO - Started process 15825 to run task
[2023-01-31T05:30:48.343+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T06:00:00+00:00', '--job-id', '899', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpq4zgishu']
[2023-01-31T05:30:48.356+0000] {standard_task_runner.py:83} INFO - Job 899: Subtask stage_total_generation
[2023-01-31T05:30:49.845+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [running]> on host 81d5fcd0285b
[2023-01-31T05:30:50.560+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T06:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T06:00:00+00:00
[2023-01-31T05:30:50.671+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T05:30:50.697+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010600 202101010700 DE_TENNET
[2023-01-31T05:32:11.817+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:11 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T05:32:12.336+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T05:32:15.826+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:15 INFO ResourceUtils: ==============================================================
[2023-01-31T05:32:15.852+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:15 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T05:32:15.866+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:15 INFO ResourceUtils: ==============================================================
[2023-01-31T05:32:15.868+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:15 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T05:32:17.071+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:17 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T05:32:17.120+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:17 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T05:32:17.128+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:17 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T05:32:18.035+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:18 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T05:32:18.069+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:18 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T05:32:18.088+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:18 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T05:32:18.101+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:18 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T05:32:18.115+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:18 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T05:32:25.956+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:25 INFO Utils: Successfully started service 'sparkDriver' on port 38063.
[2023-01-31T05:32:26.781+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:26 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T05:32:27.025+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:27 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T05:32:27.512+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:27 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T05:32:27.523+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:27 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T05:32:27.551+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:27 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T05:32:27.913+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:27 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-4f9253cb-84a8-4075-b181-61ee87488360
[2023-01-31T05:32:29.321+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:29 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T05:32:29.639+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:29 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T05:32:36.029+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:36 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T05:32:36.032+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:36 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2023-01-31T05:32:36.060+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:36 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[2023-01-31T05:32:36.071+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:36 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[2023-01-31T05:32:36.088+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:36 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[2023-01-31T05:32:36.173+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:36 INFO Utils: Successfully started service 'SparkUI' on port 4045.
[2023-01-31T05:32:37.325+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:37 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://81d5fcd0285b:38063/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675143131701
[2023-01-31T05:32:37.326+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:37 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://81d5fcd0285b:38063/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675143131701
[2023-01-31T05:32:39.496+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:39 INFO Executor: Starting executor ID driver on host 81d5fcd0285b
[2023-01-31T05:32:39.550+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:39 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T05:32:40.063+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:40 INFO Executor: Fetching spark://81d5fcd0285b:38063/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675143131701
[2023-01-31T05:32:41.795+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:41 INFO TransportClientFactory: Successfully created connection to 81d5fcd0285b/172.19.0.9:38063 after 1055 ms (0 ms spent in bootstraps)
[2023-01-31T05:32:42.285+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:42 INFO Utils: Fetching spark://81d5fcd0285b:38063/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-e89d0ff7-1da0-42b5-b7bf-bad23b07d899/userFiles-278161f1-4b50-46b8-ae75-a9967acf7150/fetchFileTemp14362606257490450515.tmp
[2023-01-31T05:32:50.859+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:50 INFO Executor: Adding file:/tmp/spark-e89d0ff7-1da0-42b5-b7bf-bad23b07d899/userFiles-278161f1-4b50-46b8-ae75-a9967acf7150/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T05:32:50.882+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:50 INFO Executor: Fetching spark://81d5fcd0285b:38063/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675143131701
[2023-01-31T05:32:50.930+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:50 INFO Utils: Fetching spark://81d5fcd0285b:38063/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-e89d0ff7-1da0-42b5-b7bf-bad23b07d899/userFiles-278161f1-4b50-46b8-ae75-a9967acf7150/fetchFileTemp18027940009745349618.tmp
[2023-01-31T05:32:55.199+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:55 INFO Executor: Told to re-register on heartbeat
[2023-01-31T05:32:55.206+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:55 INFO BlockManager: BlockManager null re-registering with master
[2023-01-31T05:32:55.243+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:55 INFO BlockManagerMaster: Registering BlockManager null
[2023-01-31T05:32:55.435+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:55 ERROR Inbox: Ignoring error
[2023-01-31T05:32:55.436+0000] {spark_submit.py:495} INFO - java.lang.NullPointerException
[2023-01-31T05:32:55.436+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)
[2023-01-31T05:32:55.437+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)
[2023-01-31T05:32:55.438+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-01-31T05:32:55.438+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-01-31T05:32:55.439+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-01-31T05:32:55.439+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-01-31T05:32:55.440+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-01-31T05:32:55.465+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-01-31T05:32:55.465+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-01-31T05:32:55.466+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-31T05:32:55.467+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-31T05:32:55.467+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-31T05:32:55.511+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:55 INFO Executor: Adding file:/tmp/spark-e89d0ff7-1da0-42b5-b7bf-bad23b07d899/userFiles-278161f1-4b50-46b8-ae75-a9967acf7150/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T05:32:55.516+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:55 WARN Executor: Issue communicating with driver in heartbeater
[2023-01-31T05:32:55.517+0000] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-01-31T05:32:55.518+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-01-31T05:32:55.519+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-01-31T05:32:55.519+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2023-01-31T05:32:55.520+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2023-01-31T05:32:55.549+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:79)
[2023-01-31T05:32:55.550+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:643)
[2023-01-31T05:32:55.550+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2023-01-31T05:32:55.551+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2023-01-31T05:32:55.552+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
[2023-01-31T05:32:55.573+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2023-01-31T05:32:55.574+0000] {spark_submit.py:495} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2023-01-31T05:32:55.581+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-01-31T05:32:55.582+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2023-01-31T05:32:55.583+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2023-01-31T05:32:55.584+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-31T05:32:55.584+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-31T05:32:55.610+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-31T05:32:55.610+0000] {spark_submit.py:495} INFO - Caused by: java.lang.NullPointerException
[2023-01-31T05:32:55.611+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)
[2023-01-31T05:32:55.612+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)
[2023-01-31T05:32:55.612+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-01-31T05:32:55.673+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-01-31T05:32:55.674+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-01-31T05:32:55.701+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-01-31T05:32:55.702+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-01-31T05:32:55.702+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-01-31T05:32:55.703+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-01-31T05:32:55.703+0000] {spark_submit.py:495} INFO - ... 3 more
[2023-01-31T05:32:55.825+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:55 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36307.
[2023-01-31T05:32:55.826+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:55 INFO NettyBlockTransferService: Server created on 81d5fcd0285b:36307
[2023-01-31T05:32:55.938+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:55 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T05:32:56.006+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:56 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 36307, None)
[2023-01-31T05:32:56.065+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:56 INFO BlockManagerMasterEndpoint: Registering block manager 81d5fcd0285b:36307 with 434.4 MiB RAM, BlockManagerId(driver, 81d5fcd0285b, 36307, None)
[2023-01-31T05:32:56.111+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:56 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 36307, None)
[2023-01-31T05:32:56.128+0000] {spark_submit.py:495} INFO - 23/01/31 05:32:56 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 81d5fcd0285b, 36307, None)
[2023-01-31T05:33:15.753+0000] {spark_submit.py:495} INFO - 23/01/31 05:33:15 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T05:33:15.829+0000] {spark_submit.py:495} INFO - 23/01/31 05:33:15 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T05:34:17.633+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:17 INFO InMemoryFileIndex: It took 2848 ms to list leaf files for 1 paths.
[2023-01-31T05:34:23.026+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:23 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T05:34:25.399+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:25 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T05:34:25.506+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:25 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 81d5fcd0285b:36307 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T05:34:25.609+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:25 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T05:34:37.097+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:37 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T05:34:37.773+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:37 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T05:34:38.281+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:38 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T05:34:38.597+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:38 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T05:34:38.620+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:38 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T05:34:38.637+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:38 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:34:38.717+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:38 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:34:38.857+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:38 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T05:34:42.948+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:42 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T05:34:43.038+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:43 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T05:34:43.050+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:43 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 81d5fcd0285b:36307 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T05:34:43.132+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:43 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:34:43.560+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:34:43.658+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:43 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T05:34:45.246+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:45 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T05:34:45.405+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:45 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T05:34:48.936+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:48 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010600__202101010700.json:0+9371
[2023-01-31T05:34:56.105+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:56 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3480 bytes result sent to driver
[2023-01-31T05:34:56.590+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:56 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 11387 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:34:56.683+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:56 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T05:34:56.981+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:56 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 17.480 s
[2023-01-31T05:34:57.127+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:57 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:34:57.137+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T05:34:57.184+0000] {spark_submit.py:495} INFO - 23/01/31 05:34:57 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 18.915667 s
[2023-01-31T05:35:19.584+0000] {spark_submit.py:495} INFO - 23/01/31 05:35:19 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 81d5fcd0285b:36307 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T05:35:19.862+0000] {spark_submit.py:495} INFO - 23/01/31 05:35:19 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 81d5fcd0285b:36307 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T05:36:38.627+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:38 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:36:38.704+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:38 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:36:38.794+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:38 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:36:50.886+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:50 INFO CodeGenerator: Code generated in 4619.696551 ms
[2023-01-31T05:36:51.417+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:51 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T05:36:52.716+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:52 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T05:36:52.770+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:52 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 81d5fcd0285b:36307 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T05:36:52.933+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:52 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T05:36:53.609+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:53 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:36:55.732+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:55 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T05:36:55.760+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:55 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T05:36:55.765+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:55 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T05:36:55.773+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:55 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:36:55.774+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:55 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:36:55.775+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:55 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T05:36:55.924+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:55 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T05:36:55.961+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:55 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T05:36:55.966+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:55 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 81d5fcd0285b:36307 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T05:36:55.980+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:55 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:36:55.994+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:36:56.002+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:56 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T05:36:56.167+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:56 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:36:56.189+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:56 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T05:36:57.845+0000] {spark_submit.py:495} INFO - 23/01/31 05:36:57 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T05:37:01.928+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:01 INFO CodeGenerator: Code generated in 3498.614469 ms
[2023-01-31T05:37:04.165+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:04 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1628 bytes result sent to driver
[2023-01-31T05:37:04.220+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:04 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 8193 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:37:04.245+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:04 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T05:37:04.261+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:04 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 8.430 s
[2023-01-31T05:37:04.269+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:04 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:37:04.276+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T05:37:04.297+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:04 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 8.563599 s
[2023-01-31T05:37:06.715+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:06 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:37:06.728+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:06 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:37:06.817+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:06 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:37:08.307+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:08 INFO CodeGenerator: Code generated in 879.718715 ms
[2023-01-31T05:37:08.821+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:08 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T05:37:10.368+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:10 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T05:37:10.520+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:10 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 81d5fcd0285b:36307 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:37:10.590+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:10 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T05:37:10.653+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:10 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:37:11.155+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 81d5fcd0285b:36307 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T05:37:11.452+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T05:37:11.462+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T05:37:11.467+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T05:37:11.553+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:37:11.578+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:37:11.612+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T05:37:11.672+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T05:37:11.681+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T05:37:11.727+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 81d5fcd0285b:36307 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T05:37:11.777+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:37:11.879+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:37:11.901+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T05:37:11.991+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:11 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:37:12.021+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:12 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T05:37:13.173+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:13 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T05:37:17.676+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:17 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1585 bytes result sent to driver
[2023-01-31T05:37:17.755+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:17 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 5761 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:37:17.763+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:17 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T05:37:17.801+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:17 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 6.156 s
[2023-01-31T05:37:17.861+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:17 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:37:17.870+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T05:37:17.882+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:17 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 6.417992 s
[2023-01-31T05:37:21.681+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:21 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:37:21.700+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:21 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T05:37:21.746+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:21 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:37:29.004+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO CodeGenerator: Code generated in 5327.690303 ms
[2023-01-31T05:37:29.065+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T05:37:29.197+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T05:37:29.202+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 81d5fcd0285b:36307 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:37:29.213+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T05:37:29.227+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:37:29.486+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T05:37:29.491+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T05:37:29.491+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T05:37:29.495+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:37:29.496+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:37:29.503+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T05:37:29.540+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.7 MiB)
[2023-01-31T05:37:29.605+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.6 MiB)
[2023-01-31T05:37:29.608+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 81d5fcd0285b:36307 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T05:37:29.611+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:37:29.616+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:37:29.617+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T05:37:29.622+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:37:29.625+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T05:37:29.709+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T05:37:29.937+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:29 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 81d5fcd0285b:36307 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T05:37:30.461+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:30 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1755 bytes result sent to driver
[2023-01-31T05:37:30.483+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:30 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 863 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:37:30.486+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:30 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T05:37:30.487+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:30 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 0.972 s
[2023-01-31T05:37:30.489+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:30 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:37:30.490+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T05:37:30.500+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:30 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 1.009733 s
[2023-01-31T05:37:31.332+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:31 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:37:31.333+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:31 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T05:37:31.333+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:31 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:37:32.361+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:32 INFO CodeGenerator: Code generated in 457.923908 ms
[2023-01-31T05:37:32.558+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:32 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.5 MiB)
[2023-01-31T05:37:33.256+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:33 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T05:37:33.263+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:33 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 81d5fcd0285b:36307 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:37:33.295+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:33 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T05:37:33.386+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:37:34.447+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:34 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T05:37:34.458+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:34 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T05:37:34.462+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:34 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T05:37:34.468+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:34 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:37:34.469+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:34 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:37:34.518+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:34 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T05:37:34.839+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:34 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-31T05:37:34.935+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:34 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-01-31T05:37:34.982+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:34 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 81d5fcd0285b:36307 (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T05:37:35.023+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:34 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:37:35.085+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:37:35.087+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:35 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T05:37:35.134+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:35 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:37:35.149+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:35 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T05:37:35.590+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:35 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T05:37:36.851+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:36 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1586 bytes result sent to driver
[2023-01-31T05:37:36.876+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:36 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 1744 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:37:36.885+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:36 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 2.212 s
[2023-01-31T05:37:36.887+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:36 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T05:37:36.891+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:36 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:37:36.895+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T05:37:36.901+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:36 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 2.446376 s
[2023-01-31T05:37:44.283+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:44 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:37:44.287+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:44 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T05:37:44.290+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:44 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:37:45.211+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:45 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 81d5fcd0285b:36307 in memory (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T05:37:45.414+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:45 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 81d5fcd0285b:36307 in memory (size: 7.6 KiB, free: 434.3 MiB)
[2023-01-31T05:37:48.750+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:48 INFO CodeGenerator: Code generated in 3706.47268 ms
[2023-01-31T05:37:48.798+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:48 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.3 MiB)
[2023-01-31T05:37:49.148+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:49 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T05:37:49.167+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:49 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 81d5fcd0285b:36307 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:37:49.180+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:49 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T05:37:49.215+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:49 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:37:50.171+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:50 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T05:37:50.186+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:50 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) with 1 output partitions
[2023-01-31T05:37:50.188+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:50 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201)
[2023-01-31T05:37:50.189+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:50 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:37:50.204+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:50 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:37:50.230+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:50 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201), which has no missing parents
[2023-01-31T05:37:50.392+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:50 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.2 MiB)
[2023-01-31T05:37:50.459+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:50 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.2 MiB)
[2023-01-31T05:37:50.465+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:50 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 81d5fcd0285b:36307 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T05:37:50.496+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:50 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:37:50.500+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:37:50.533+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:50 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T05:37:50.540+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:50 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:37:50.541+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:50 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T05:37:50.866+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:50 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T05:37:51.910+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:51 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2499 bytes result sent to driver
[2023-01-31T05:37:51.924+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:51 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 1387 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:37:51.925+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:51 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T05:37:51.930+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:51 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) finished in 1.677 s
[2023-01-31T05:37:51.939+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:51 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:37:51.949+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T05:37:51.961+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:51 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201, took 1.785086 s
[2023-01-31T05:37:53.868+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:53 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:37:53.912+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:53 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T05:37:53.952+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:53 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:37:56.533+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:56 INFO CodeGenerator: Code generated in 949.63418 ms
[2023-01-31T05:37:56.757+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:56 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 433.0 MiB)
[2023-01-31T05:37:57.624+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:57 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.0 MiB)
[2023-01-31T05:37:57.638+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:57 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 81d5fcd0285b:36307 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:37:57.704+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:57 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T05:37:57.782+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:57 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:37:58.826+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:58 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T05:37:58.861+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:58 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T05:37:58.862+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:58 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T05:37:58.862+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:58 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:37:58.863+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:58 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:37:58.918+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:58 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T05:37:59.033+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:59 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 432.9 MiB)
[2023-01-31T05:37:59.126+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:59 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.9 MiB)
[2023-01-31T05:37:59.143+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:59 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 81d5fcd0285b:36307 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T05:37:59.157+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:59 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:37:59.238+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:37:59.239+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:59 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T05:37:59.266+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:59 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:37:59.270+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:59 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T05:37:59.791+0000] {spark_submit.py:495} INFO - 23/01/31 05:37:59 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T05:38:01.096+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:01 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-01-31T05:38:01.112+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:01 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 1835 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:38:01.115+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:01 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T05:38:01.116+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:01 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 2.129 s
[2023-01-31T05:38:01.134+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:01 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:38:01.138+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T05:38:01.139+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:01 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 2.281965 s
[2023-01-31T05:38:02.380+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:02 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T05:38:02.413+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:02 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T05:38:02.425+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:02 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T05:38:05.315+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:05 INFO CodeGenerator: Code generated in 2067.130993 ms
[2023-01-31T05:38:05.418+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:05 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.7 MiB)
[2023-01-31T05:38:06.049+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:06 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.7 MiB)
[2023-01-31T05:38:06.052+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:06 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 81d5fcd0285b:36307 (size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T05:38:06.097+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:06 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T05:38:06.242+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:06 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T05:38:07.710+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:07 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T05:38:07.721+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:07 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T05:38:07.724+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:07 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T05:38:07.734+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:07 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:38:07.755+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:07 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:38:07.765+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:07 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T05:38:07.874+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:07 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.7 MiB)
[2023-01-31T05:38:07.948+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:07 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.6 MiB)
[2023-01-31T05:38:07.955+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:07 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 81d5fcd0285b:36307 (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T05:38:07.967+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:07 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:38:07.986+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:38:07.993+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:07 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T05:38:08.057+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:08 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T05:38:08.065+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:08 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T05:38:08.426+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:08 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T05:38:09.336+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:09 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1772 bytes result sent to driver
[2023-01-31T05:38:09.348+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:09 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 1290 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:38:09.350+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:09 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T05:38:09.356+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:09 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 1.547 s
[2023-01-31T05:38:09.359+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:09 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:38:09.359+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T05:38:09.363+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:09 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 1.649337 s
[2023-01-31T05:38:37.675+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:37 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 81d5fcd0285b:36307 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:38:37.721+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:37 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 81d5fcd0285b:36307 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T05:38:37.837+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:37 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 81d5fcd0285b:36307 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:38:37.935+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:37 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 81d5fcd0285b:36307 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T05:38:38.053+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:38 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 81d5fcd0285b:36307 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:38:38.148+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:38 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 81d5fcd0285b:36307 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T05:38:38.222+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:38 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 81d5fcd0285b:36307 in memory (size: 13.4 KiB, free: 434.3 MiB)
[2023-01-31T05:38:38.320+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:38 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 81d5fcd0285b:36307 in memory (size: 12.6 KiB, free: 434.3 MiB)
[2023-01-31T05:38:38.407+0000] {spark_submit.py:495} INFO - 23/01/31 05:38:38 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 81d5fcd0285b:36307 in memory (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T05:39:08.611+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:08 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:39:08.930+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:39:08.931+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:39:08.937+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:08 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:39:08.946+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:39:08.947+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:39:08.973+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:39:11.432+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:11 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-31T05:39:11.437+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:11 INFO DAGScheduler: Got job 8 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T05:39:11.438+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:11 INFO DAGScheduler: Final stage: ResultStage 8 (save at BigQueryWriteHelper.java:105)
[2023-01-31T05:39:11.441+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:11 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T05:39:11.443+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:11 INFO DAGScheduler: Missing parents: List()
[2023-01-31T05:39:11.454+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:11 INFO DAGScheduler: Submitting ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T05:39:11.648+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:11 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 209.9 KiB, free 434.0 MiB)
[2023-01-31T05:39:11.723+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:11 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 74.3 KiB, free 433.9 MiB)
[2023-01-31T05:39:11.729+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:11 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 81d5fcd0285b:36307 (size: 74.3 KiB, free: 434.3 MiB)
[2023-01-31T05:39:11.746+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:11 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513
[2023-01-31T05:39:11.766+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T05:39:11.770+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:11 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2023-01-31T05:39:11.845+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:11 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7484 bytes) taskResourceAssignments Map()
[2023-01-31T05:39:11.871+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:11 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2023-01-31T05:39:12.686+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:39:12.689+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:39:12.718+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:12 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:39:12.726+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T05:39:12.733+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T05:39:12.748+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T05:39:12.930+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:12 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T05:39:12.947+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:12 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T05:39:13.309+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:13 INFO ParquetOutputFormat: Parquet block size to 134217728
[2023-01-31T05:39:13.313+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:13 INFO ParquetOutputFormat: Validation is off
[2023-01-31T05:39:13.316+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:13 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-01-31T05:39:13.321+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:13 INFO ParquetOutputFormat: Parquet properties are:
[2023-01-31T05:39:13.322+0000] {spark_submit.py:495} INFO - Parquet page size to 1048576
[2023-01-31T05:39:13.323+0000] {spark_submit.py:495} INFO - Parquet dictionary page size to 1048576
[2023-01-31T05:39:13.324+0000] {spark_submit.py:495} INFO - Dictionary is true
[2023-01-31T05:39:13.326+0000] {spark_submit.py:495} INFO - Writer version is: PARQUET_1_0
[2023-01-31T05:39:13.326+0000] {spark_submit.py:495} INFO - Page size checking is: estimated
[2023-01-31T05:39:13.327+0000] {spark_submit.py:495} INFO - Min row count for page size check is: 100
[2023-01-31T05:39:13.327+0000] {spark_submit.py:495} INFO - Max row count for page size check is: 10000
[2023-01-31T05:39:13.328+0000] {spark_submit.py:495} INFO - Truncate length for column indexes is: 64
[2023-01-31T05:39:13.328+0000] {spark_submit.py:495} INFO - Truncate length for statistics min/max  is: 2147483647
[2023-01-31T05:39:13.337+0000] {spark_submit.py:495} INFO - Bloom filter enabled: false
[2023-01-31T05:39:13.342+0000] {spark_submit.py:495} INFO - Max Bloom filter size for a column is 1048576
[2023-01-31T05:39:13.343+0000] {spark_submit.py:495} INFO - Bloom filter expected number of distinct values are: null
[2023-01-31T05:39:13.347+0000] {spark_submit.py:495} INFO - Page row count limit to 20000
[2023-01-31T05:39:13.352+0000] {spark_submit.py:495} INFO - Writing page checksums is: on
[2023-01-31T05:39:13.864+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:13 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-01-31T05:39:13.865+0000] {spark_submit.py:495} INFO - {
[2023-01-31T05:39:13.865+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-31T05:39:13.866+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-31T05:39:13.867+0000] {spark_submit.py:495} INFO - "name" : "measured_at",
[2023-01-31T05:39:13.867+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T05:39:13.868+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T05:39:13.868+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:39:13.870+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:39:13.871+0000] {spark_submit.py:495} INFO - "name" : "created_at",
[2023-01-31T05:39:13.896+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T05:39:13.900+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T05:39:13.911+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:39:13.933+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:39:13.950+0000] {spark_submit.py:495} INFO - "name" : "Biomass - Actual Aggregated",
[2023-01-31T05:39:13.980+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:39:13.985+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:39:13.987+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:39:13.993+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:39:13.994+0000] {spark_submit.py:495} INFO - "name" : "Fossil Brown coal/Lignite - Actual Aggregated",
[2023-01-31T05:39:14.006+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:39:14.007+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:39:14.008+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:39:14.018+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:39:14.021+0000] {spark_submit.py:495} INFO - "name" : "Fossil Gas - Actual Aggregated",
[2023-01-31T05:39:14.027+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:39:14.033+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:39:14.039+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:39:14.044+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:39:14.050+0000] {spark_submit.py:495} INFO - "name" : "Fossil Hard coal - Actual Aggregated",
[2023-01-31T05:39:14.061+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:39:14.062+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:39:14.063+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:39:14.063+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:39:14.064+0000] {spark_submit.py:495} INFO - "name" : "Fossil Oil - Actual Aggregated",
[2023-01-31T05:39:14.079+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:39:14.081+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:39:14.085+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:39:14.104+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:39:14.110+0000] {spark_submit.py:495} INFO - "name" : "Geothermal - Actual Aggregated",
[2023-01-31T05:39:14.116+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:39:14.125+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:39:14.131+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:39:14.137+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:39:14.138+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Aggregated",
[2023-01-31T05:39:14.141+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:39:14.142+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:39:14.143+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:39:14.143+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:39:14.144+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Consumption",
[2023-01-31T05:39:14.144+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:39:14.149+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:39:14.155+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:39:14.160+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:39:14.165+0000] {spark_submit.py:495} INFO - "name" : "Hydro Run-of-river and poundage - Actual Aggregated",
[2023-01-31T05:39:14.168+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:39:14.172+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:39:14.176+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:39:14.180+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:39:14.184+0000] {spark_submit.py:495} INFO - "name" : "Hydro Water Reservoir - Actual Aggregated",
[2023-01-31T05:39:14.186+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:39:14.188+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:39:14.193+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:39:14.195+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:39:14.198+0000] {spark_submit.py:495} INFO - "name" : "Nuclear - Actual Aggregated",
[2023-01-31T05:39:14.206+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:39:14.215+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:39:14.216+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:39:14.224+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:39:14.225+0000] {spark_submit.py:495} INFO - "name" : "Other - Actual Aggregated",
[2023-01-31T05:39:14.226+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:39:14.227+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:39:14.227+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:39:14.230+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:39:14.235+0000] {spark_submit.py:495} INFO - "name" : "Other renewable - Actual Aggregated",
[2023-01-31T05:39:14.244+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:39:14.246+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:39:14.249+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:39:14.250+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:39:14.253+0000] {spark_submit.py:495} INFO - "name" : "Solar - Actual Aggregated",
[2023-01-31T05:39:14.261+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:39:14.266+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:39:14.268+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:39:14.271+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:39:14.274+0000] {spark_submit.py:495} INFO - "name" : "Waste - Actual Aggregated",
[2023-01-31T05:39:14.280+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:39:14.290+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:39:14.294+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:39:14.301+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:39:14.307+0000] {spark_submit.py:495} INFO - "name" : "Wind Offshore - Actual Aggregated",
[2023-01-31T05:39:14.311+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:39:14.323+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:39:14.327+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:39:14.329+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T05:39:14.331+0000] {spark_submit.py:495} INFO - "name" : "Wind Onshore - Actual Aggregated",
[2023-01-31T05:39:14.333+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T05:39:14.334+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T05:39:14.336+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T05:39:14.341+0000] {spark_submit.py:495} INFO - } ]
[2023-01-31T05:39:14.343+0000] {spark_submit.py:495} INFO - }
[2023-01-31T05:39:14.344+0000] {spark_submit.py:495} INFO - and corresponding Parquet message type:
[2023-01-31T05:39:14.347+0000] {spark_submit.py:495} INFO - message spark_schema {
[2023-01-31T05:39:14.349+0000] {spark_submit.py:495} INFO - required int96 measured_at;
[2023-01-31T05:39:14.350+0000] {spark_submit.py:495} INFO - required int96 created_at;
[2023-01-31T05:39:14.353+0000] {spark_submit.py:495} INFO - optional double Biomass - Actual Aggregated;
[2023-01-31T05:39:14.354+0000] {spark_submit.py:495} INFO - optional double Fossil Brown coal/Lignite - Actual Aggregated;
[2023-01-31T05:39:14.356+0000] {spark_submit.py:495} INFO - optional double Fossil Gas - Actual Aggregated;
[2023-01-31T05:39:14.357+0000] {spark_submit.py:495} INFO - optional double Fossil Hard coal - Actual Aggregated;
[2023-01-31T05:39:14.361+0000] {spark_submit.py:495} INFO - optional double Fossil Oil - Actual Aggregated;
[2023-01-31T05:39:14.364+0000] {spark_submit.py:495} INFO - optional double Geothermal - Actual Aggregated;
[2023-01-31T05:39:14.366+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Aggregated;
[2023-01-31T05:39:14.368+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Consumption;
[2023-01-31T05:39:14.369+0000] {spark_submit.py:495} INFO - optional double Hydro Run-of-river and poundage - Actual Aggregated;
[2023-01-31T05:39:14.371+0000] {spark_submit.py:495} INFO - optional double Hydro Water Reservoir - Actual Aggregated;
[2023-01-31T05:39:14.375+0000] {spark_submit.py:495} INFO - optional double Nuclear - Actual Aggregated;
[2023-01-31T05:39:14.375+0000] {spark_submit.py:495} INFO - optional double Other - Actual Aggregated;
[2023-01-31T05:39:14.393+0000] {spark_submit.py:495} INFO - optional double Other renewable - Actual Aggregated;
[2023-01-31T05:39:14.396+0000] {spark_submit.py:495} INFO - optional double Solar - Actual Aggregated;
[2023-01-31T05:39:14.405+0000] {spark_submit.py:495} INFO - optional double Waste - Actual Aggregated;
[2023-01-31T05:39:14.411+0000] {spark_submit.py:495} INFO - optional double Wind Offshore - Actual Aggregated;
[2023-01-31T05:39:14.416+0000] {spark_submit.py:495} INFO - optional double Wind Onshore - Actual Aggregated;
[2023-01-31T05:39:14.434+0000] {spark_submit.py:495} INFO - }
[2023-01-31T05:39:14.437+0000] {spark_submit.py:495} INFO - 
[2023-01-31T05:39:14.440+0000] {spark_submit.py:495} INFO - 
[2023-01-31T05:39:16.025+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:16 INFO CodecPool: Got brand-new compressor [.snappy]
[2023-01-31T05:39:29.329+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:29 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675143158616-81ec2dd0-39fd-4bd5-9bc9-398227d4ae7e/_temporary/0/_temporary/' directory.
[2023-01-31T05:39:29.338+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:29 INFO FileOutputCommitter: Saved output of task 'attempt_202301310539105163398628901132425_0008_m_000000_8' to gs://entsoe_temp_1009/.spark-bigquery-local-1675143158616-81ec2dd0-39fd-4bd5-9bc9-398227d4ae7e/_temporary/0/task_202301310539105163398628901132425_0008_m_000000
[2023-01-31T05:39:29.339+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:29 INFO SparkHadoopMapRedUtil: attempt_202301310539105163398628901132425_0008_m_000000_8: Committed. Elapsed time: 2856 ms.
[2023-01-31T05:39:29.687+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:29 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2588 bytes result sent to driver
[2023-01-31T05:39:29.710+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:29 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 17921 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T05:39:29.716+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:29 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2023-01-31T05:39:29.831+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:29 INFO DAGScheduler: ResultStage 8 (save at BigQueryWriteHelper.java:105) finished in 18.257 s
[2023-01-31T05:39:29.841+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:29 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T05:39:29.842+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2023-01-31T05:39:29.843+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:29 INFO DAGScheduler: Job 8 finished: save at BigQueryWriteHelper.java:105, took 18.404606 s
[2023-01-31T05:39:29.857+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:29 INFO FileFormatWriter: Start to commit write Job 274d7189-95e4-49ac-a01f-41287bcbd33a.
[2023-01-31T05:39:31.185+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:31 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675143158616-81ec2dd0-39fd-4bd5-9bc9-398227d4ae7e/_temporary/0/task_202301310539105163398628901132425_0008_m_000000/' directory.
[2023-01-31T05:39:31.584+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:31 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675143158616-81ec2dd0-39fd-4bd5-9bc9-398227d4ae7e/' directory.
[2023-01-31T05:39:32.301+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:32 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 81d5fcd0285b:36307 in memory (size: 74.3 KiB, free: 434.4 MiB)
[2023-01-31T05:39:32.486+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:32 INFO FileFormatWriter: Write Job 274d7189-95e4-49ac-a01f-41287bcbd33a committed. Elapsed time: 2615 ms.
[2023-01-31T05:39:32.516+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:32 INFO FileFormatWriter: Finished processing stats for write job 274d7189-95e4-49ac-a01f-41287bcbd33a.
[2023-01-31T05:39:34.853+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:34 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=entsoe_analytics_1009, projectId=rafzul-analytics-1009, tableId=TEST_total_generation_staging}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=null, ignoreUnknownValue=null, sourceUris=[gs://entsoe_temp_1009/.spark-bigquery-local-1675143158616-81ec2dd0-39fd-4bd5-9bc9-398227d4ae7e/part-00000-7204c14a-f28c-4ae1-8bd5-ab747fd58b87-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=true, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=rafzul-analytics-1009, job=a4b0c663-2ef9-45dc-a0fd-08e1041b5fc6, location=US}
[2023-01-31T05:39:38.695+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:38 INFO BigQueryClient: Done loading to rafzul-analytics-1009.entsoe_analytics_1009.TEST_total_generation_staging. jobId: JobId{project=rafzul-analytics-1009, job=a4b0c663-2ef9-45dc-a0fd-08e1041b5fc6, location=US}
[2023-01-31T05:39:39.750+0000] {spark_submit.py:495} INFO - MANTEP!!!!
[2023-01-31T05:39:40.020+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:40 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-31T05:39:40.044+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:40 INFO SparkUI: Stopped Spark web UI at http://81d5fcd0285b:4045
[2023-01-31T05:39:40.073+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:40 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-31T05:39:40.106+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:40 INFO MemoryStore: MemoryStore cleared
[2023-01-31T05:39:40.107+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:40 INFO BlockManager: BlockManager stopped
[2023-01-31T05:39:40.113+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:40 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-31T05:39:40.119+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:40 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-31T05:39:40.130+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:40 INFO SparkContext: Successfully stopped SparkContext
[2023-01-31T05:39:40.131+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:40 INFO ShutdownHookManager: Shutdown hook called
[2023-01-31T05:39:40.132+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:40 INFO ShutdownHookManager: Deleting directory /tmp/spark-e89d0ff7-1da0-42b5-b7bf-bad23b07d899/pyspark-67cc98ab-1eb0-44f1-acdb-cbdb7fe018f1
[2023-01-31T05:39:40.140+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:40 INFO ShutdownHookManager: Deleting directory /tmp/spark-e89d0ff7-1da0-42b5-b7bf-bad23b07d899
[2023-01-31T05:39:40.148+0000] {spark_submit.py:495} INFO - 23/01/31 05:39:40 INFO ShutdownHookManager: Deleting directory /tmp/spark-064b2e91-e689-4562-a56e-cf4552b2fc68
[2023-01-31T05:39:40.360+0000] {taskinstance.py:1322} INFO - Marking task as SUCCESS. dag_id=entsoe-energydata-backfill, task_id=stage_total_generation, execution_date=20210101T060000, start_date=20230131T053047, end_date=20230131T053940
[2023-01-31T05:39:40.442+0000] {local_task_job.py:159} INFO - Task exited with return code 0
[2023-01-31T05:39:40.495+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T07:39:04.192+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [queued]>
[2023-01-31T07:39:04.324+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [queued]>
[2023-01-31T07:39:04.325+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T07:39:04.326+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T07:39:04.327+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T07:39:04.418+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 06:00:00+00:00
[2023-01-31T07:39:04.547+0000] {standard_task_runner.py:55} INFO - Started process 25834 to run task
[2023-01-31T07:39:04.624+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T06:00:00+00:00', '--job-id', '913', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp9vk387_y']
[2023-01-31T07:39:04.672+0000] {standard_task_runner.py:83} INFO - Job 913: Subtask stage_total_generation
[2023-01-31T07:39:05.915+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [running]> on host 81d5fcd0285b
[2023-01-31T07:39:06.862+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T06:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T06:00:00+00:00
[2023-01-31T07:39:07.091+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T07:39:07.096+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010600 202101010700 DE_TENNET
[2023-01-31T07:40:24.664+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:24 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T07:40:26.811+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T07:40:31.887+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:31 INFO ResourceUtils: ==============================================================
[2023-01-31T07:40:31.999+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:31 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T07:40:32.034+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:31 INFO ResourceUtils: ==============================================================
[2023-01-31T07:40:32.142+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:32 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T07:40:33.517+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:33 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T07:40:33.855+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:33 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T07:40:33.930+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:33 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T07:40:35.951+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:35 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T07:40:36.057+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:36 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T07:40:36.160+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:36 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T07:40:36.268+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:36 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T07:40:36.316+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:36 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T07:40:49.226+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:49 INFO Utils: Successfully started service 'sparkDriver' on port 37871.
[2023-01-31T07:40:50.524+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:50 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T07:40:52.675+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:52 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T07:40:53.625+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:53 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T07:40:53.633+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:53 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T07:40:53.658+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:53 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T07:40:54.385+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:54 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b9aea595-5404-4d69-81ca-3545d4530195
[2023-01-31T07:40:54.517+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:54 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T07:40:55.865+0000] {spark_submit.py:495} INFO - 23/01/31 07:40:55 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T07:41:03.137+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:03 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T07:41:03.157+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:03 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2023-01-31T07:41:03.183+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:03 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[2023-01-31T07:41:03.218+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:03 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[2023-01-31T07:41:03.257+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:03 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[2023-01-31T07:41:03.258+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:03 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
[2023-01-31T07:41:03.472+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:03 INFO Utils: Successfully started service 'SparkUI' on port 4046.
[2023-01-31T07:41:04.985+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:04 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://81d5fcd0285b:37871/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675150824563
[2023-01-31T07:41:05.005+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:04 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://81d5fcd0285b:37871/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675150824563
[2023-01-31T07:41:06.414+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:06 INFO Executor: Starting executor ID driver on host 81d5fcd0285b
[2023-01-31T07:41:06.684+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:06 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T07:41:07.034+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:07 INFO Executor: Fetching spark://81d5fcd0285b:37871/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675150824563
[2023-01-31T07:41:08.374+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:08 INFO TransportClientFactory: Successfully created connection to 81d5fcd0285b/172.19.0.9:37871 after 1135 ms (0 ms spent in bootstraps)
[2023-01-31T07:41:08.580+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:08 INFO Utils: Fetching spark://81d5fcd0285b:37871/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-36349aee-2534-4650-b9b1-43890d392200/userFiles-7b500f39-df78-488f-b7ec-30402b2c2342/fetchFileTemp11253485400958233692.tmp
[2023-01-31T07:41:15.523+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:15 INFO Executor: Adding file:/tmp/spark-36349aee-2534-4650-b9b1-43890d392200/userFiles-7b500f39-df78-488f-b7ec-30402b2c2342/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T07:41:15.525+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:15 INFO Executor: Fetching spark://81d5fcd0285b:37871/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675150824563
[2023-01-31T07:41:15.540+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:15 INFO Utils: Fetching spark://81d5fcd0285b:37871/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-36349aee-2534-4650-b9b1-43890d392200/userFiles-7b500f39-df78-488f-b7ec-30402b2c2342/fetchFileTemp17250526110526649466.tmp
[2023-01-31T07:41:20.958+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:20 INFO Executor: Adding file:/tmp/spark-36349aee-2534-4650-b9b1-43890d392200/userFiles-7b500f39-df78-488f-b7ec-30402b2c2342/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T07:41:21.069+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42647.
[2023-01-31T07:41:21.069+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:21 INFO NettyBlockTransferService: Server created on 81d5fcd0285b:42647
[2023-01-31T07:41:21.093+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:21 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T07:41:21.286+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:21 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 42647, None)
[2023-01-31T07:41:21.331+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:21 INFO BlockManagerMasterEndpoint: Registering block manager 81d5fcd0285b:42647 with 434.4 MiB RAM, BlockManagerId(driver, 81d5fcd0285b, 42647, None)
[2023-01-31T07:41:21.382+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:21 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 42647, None)
[2023-01-31T07:41:21.406+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:21 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 81d5fcd0285b, 42647, None)
[2023-01-31T07:41:25.466+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:25 INFO Executor: Told to re-register on heartbeat
[2023-01-31T07:41:25.543+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:25 INFO BlockManager: BlockManager BlockManagerId(driver, 81d5fcd0285b, 42647, None) re-registering with master
[2023-01-31T07:41:25.546+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:25 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 42647, None)
[2023-01-31T07:41:25.592+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:25 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 42647, None)
[2023-01-31T07:41:25.593+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:25 INFO BlockManager: Reporting 0 blocks to the master.
[2023-01-31T07:41:39.859+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:39 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T07:41:40.032+0000] {spark_submit.py:495} INFO - 23/01/31 07:41:40 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T07:42:22.685+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:22 INFO InMemoryFileIndex: It took 1609 ms to list leaf files for 1 paths.
[2023-01-31T07:42:28.737+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:28 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T07:42:31.266+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:31 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T07:42:31.287+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:31 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 81d5fcd0285b:42647 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T07:42:31.385+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:31 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T07:42:45.272+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:45 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T07:42:45.822+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:45 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T07:42:46.442+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:46 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T07:42:46.856+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:46 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T07:42:46.863+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:46 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T07:42:46.865+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:46 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:42:46.886+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:46 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:42:46.938+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:46 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T07:42:49.661+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:49 INFO AsyncEventQueue: Process of event SparkListenerJobStart(0,1675150966888,ArraySeq(org.apache.spark.scheduler.StageInfo@271762d9),{spark.master=local, spark.driver.port=37871, spark.submit.pyFiles=, spark.app.startTime=1675150824563, spark.rdd.compress=True, spark.executor.id=driver, spark.app.name=gcp_playground, spark.submit.deployMode=client, spark.hadoop.google.cloud.auth.service.account.json.keyfile=/.google/credentials/google_credentials.json, spark.driver.host=81d5fcd0285b, spark.app.id=local-1675150865556, spark.app.submitTime=1675150804798, spark.hadoop.fs.AbstractFileSystem.gs.impl=google.cloud.hadoop.fs.gcs.GoogleHadoopFS, spark.sql.session.timeZone=UTC, spark.hadoop.google.cloud.auth.service.account.enable=true, spark.app.initial.jar.urls=spark://81d5fcd0285b:37871/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar,spark://81d5fcd0285b:37871/jars/gcs-connector-hadoop3-latest.jar, spark.executor.extraJavaOptions=-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED, spark.jars=/opt/spark/jars/gcs-connector-hadoop3-latest.jar, /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar, spark.sql.warehouse.dir=file:/opt/***/spark-warehouse, spark.serializer.objectStreamReset=100, spark.driver.extraJavaOptions=-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED}) by listener AppStatusListener took 2.609701602s.
[2023-01-31T07:42:50.764+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:50 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T07:42:50.850+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:50 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T07:42:50.878+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:50 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 81d5fcd0285b:42647 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T07:42:50.961+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:50 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:42:51.918+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:42:51.998+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:51 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T07:42:54.774+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:54 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T07:42:55.346+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:55 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T07:42:58.061+0000] {spark_submit.py:495} INFO - 23/01/31 07:42:58 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010600__202101010700.json:0+9371
[2023-01-31T07:43:00.405+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:00 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3480 bytes result sent to driver
[2023-01-31T07:43:00.558+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:00 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 6492 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:43:00.598+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:00 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T07:43:00.722+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:00 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 12.262 s
[2023-01-31T07:43:00.822+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:00 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:43:00.843+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T07:43:00.929+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:00 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 14.483592 s
[2023-01-31T07:43:25.304+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:25 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 81d5fcd0285b:42647 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T07:43:25.412+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:25 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 81d5fcd0285b:42647 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T07:43:27.433+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:43:27.486+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T07:43:27.592+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:27 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:43:31.776+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:31 INFO CodeGenerator: Code generated in 1697.888888 ms
[2023-01-31T07:43:31.929+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:31 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T07:43:31.965+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:31 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T07:43:31.966+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:31 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 81d5fcd0285b:42647 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T07:43:31.968+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:31 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T07:43:32.010+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:43:32.821+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T07:43:32.822+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T07:43:32.823+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T07:43:32.823+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:43:32.825+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:43:32.855+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:32 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T07:43:33.009+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T07:43:33.122+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T07:43:33.129+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 81d5fcd0285b:42647 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T07:43:33.141+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:43:33.170+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:43:33.171+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T07:43:33.280+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:43:33.294+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:33 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T07:43:34.365+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:34 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T07:43:36.012+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:36 INFO CodeGenerator: Code generated in 1296.691482 ms
[2023-01-31T07:43:37.363+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:37 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1628 bytes result sent to driver
[2023-01-31T07:43:37.407+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:37 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 4214 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:43:37.420+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:37 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 4.491 s
[2023-01-31T07:43:37.421+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:37 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T07:43:37.430+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:37 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:43:37.456+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T07:43:37.457+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:37 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 4.634064 s
[2023-01-31T07:43:38.558+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:38 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:43:38.564+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:38 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T07:43:38.572+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:38 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:43:39.073+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:39 INFO CodeGenerator: Code generated in 283.730482 ms
[2023-01-31T07:43:39.153+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:39 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T07:43:39.705+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:39 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T07:43:39.719+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:39 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 81d5fcd0285b:42647 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:43:39.744+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:39 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T07:43:39.822+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:39 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:43:40.240+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T07:43:40.245+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T07:43:40.246+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T07:43:40.248+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:43:40.249+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:43:40.273+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T07:43:40.346+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T07:43:40.391+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T07:43:40.396+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 81d5fcd0285b:42647 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T07:43:40.414+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:43:40.431+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:43:40.431+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T07:43:40.444+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:43:40.463+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T07:43:40.764+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:40 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T07:43:42.508+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:42 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1585 bytes result sent to driver
[2023-01-31T07:43:42.624+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:42 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 2181 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:43:42.648+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:42 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 2.335 s
[2023-01-31T07:43:42.650+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:42 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:43:42.664+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:42 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T07:43:42.669+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T07:43:42.677+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:42 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 2.433340 s
[2023-01-31T07:43:46.138+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:46 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:43:46.161+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:46 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T07:43:46.183+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:46 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:43:46.878+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:46 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 81d5fcd0285b:42647 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T07:43:47.113+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:47 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 81d5fcd0285b:42647 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T07:43:48.789+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:48 INFO CodeGenerator: Code generated in 464.498202 ms
[2023-01-31T07:43:48.809+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:48 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T07:43:48.856+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:48 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T07:43:48.901+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:48 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 81d5fcd0285b:42647 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:43:48.902+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:48 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T07:43:48.921+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:43:49.996+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:49 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T07:43:50.005+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T07:43:50.006+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T07:43:50.007+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:43:50.013+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:43:50.029+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T07:43:50.169+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.7 MiB)
[2023-01-31T07:43:50.247+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.7 MiB)
[2023-01-31T07:43:50.263+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 81d5fcd0285b:42647 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T07:43:50.267+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:43:50.279+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:43:50.281+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T07:43:50.302+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:43:50.307+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T07:43:50.733+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:50 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T07:43:51.841+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:51 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-01-31T07:43:51.862+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:51 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 1562 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:43:51.868+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:51 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T07:43:51.885+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:51 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 1.800 s
[2023-01-31T07:43:51.891+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:51 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:43:51.892+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T07:43:51.906+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:51 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 1.901835 s
[2023-01-31T07:43:53.395+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:53 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:43:53.418+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:53 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T07:43:53.421+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:53 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:43:54.027+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:54 INFO CodeGenerator: Code generated in 379.117003 ms
[2023-01-31T07:43:54.163+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:54 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.5 MiB)
[2023-01-31T07:43:54.733+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:54 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T07:43:54.777+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:54 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 81d5fcd0285b:42647 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:43:54.778+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:54 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T07:43:54.793+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:54 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:43:55.805+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:55 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T07:43:55.853+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:55 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T07:43:55.854+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:55 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T07:43:55.854+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:55 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:43:55.879+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:55 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:43:55.880+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:55 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T07:43:56.059+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:56 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-31T07:43:56.120+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:56 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-01-31T07:43:56.152+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:56 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 81d5fcd0285b:42647 (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T07:43:56.190+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:56 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:43:56.204+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:43:56.211+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:56 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T07:43:56.226+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:56 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:43:56.227+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:56 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T07:43:56.576+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:56 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T07:43:57.620+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:57 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1629 bytes result sent to driver
[2023-01-31T07:43:57.633+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:57 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 1416 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:43:57.644+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:57 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 1.664 s
[2023-01-31T07:43:57.645+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:57 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:43:57.649+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:57 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T07:43:57.654+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T07:43:57.663+0000] {spark_submit.py:495} INFO - 23/01/31 07:43:57 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 1.853308 s
[2023-01-31T07:44:06.133+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:06 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:44:06.223+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:06 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T07:44:06.290+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:06 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:44:10.400+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:10 INFO CodeGenerator: Code generated in 1938.734174 ms
[2023-01-31T07:44:10.417+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:10 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.2 MiB)
[2023-01-31T07:44:10.485+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:10 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T07:44:10.492+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:10 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 81d5fcd0285b:42647 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T07:44:10.506+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:10 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T07:44:10.583+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:10 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:44:11.547+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:11 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 81d5fcd0285b:42647 in memory (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T07:44:11.893+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:11 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 81d5fcd0285b:42647 in memory (size: 12.7 KiB, free: 434.2 MiB)
[2023-01-31T07:44:11.909+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:11 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T07:44:11.920+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:11 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) with 1 output partitions
[2023-01-31T07:44:11.922+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:11 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201)
[2023-01-31T07:44:11.923+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:11 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:44:11.936+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:11 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:44:11.962+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:11 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201), which has no missing parents
[2023-01-31T07:44:12.272+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:12 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.2 MiB)
[2023-01-31T07:44:12.347+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:12 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.2 MiB)
[2023-01-31T07:44:12.369+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:12 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 81d5fcd0285b:42647 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T07:44:12.392+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:12 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:44:12.395+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:44:12.464+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:12 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T07:44:12.521+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:12 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:44:12.535+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:12 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T07:44:13.036+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:13 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T07:44:14.963+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:14 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2499 bytes result sent to driver
[2023-01-31T07:44:14.974+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:14 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 2450 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:44:14.975+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:14 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T07:44:14.986+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:14 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) finished in 2.990 s
[2023-01-31T07:44:14.995+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:14 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:44:14.996+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T07:44:14.997+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:14 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201, took 3.079531 s
[2023-01-31T07:44:17.109+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:17 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:44:17.110+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:17 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T07:44:17.111+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:17 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:44:19.262+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:19 INFO CodeGenerator: Code generated in 1034.154904 ms
[2023-01-31T07:44:19.874+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:19 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 433.0 MiB)
[2023-01-31T07:44:19.975+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:19 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.0 MiB)
[2023-01-31T07:44:19.976+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:19 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 81d5fcd0285b:42647 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T07:44:19.980+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:19 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T07:44:20.025+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:20 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:44:20.728+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:20 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T07:44:20.789+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:20 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T07:44:20.801+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:20 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T07:44:20.802+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:20 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:44:20.833+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:20 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:44:20.859+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:20 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T07:44:21.293+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:21 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 432.9 MiB)
[2023-01-31T07:44:21.357+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:21 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.9 MiB)
[2023-01-31T07:44:21.377+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:21 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 81d5fcd0285b:42647 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T07:44:21.451+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:21 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:44:21.458+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:44:21.460+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:21 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T07:44:21.549+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:21 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:44:21.561+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:21 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T07:44:21.826+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:21 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T07:44:23.017+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:22 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-01-31T07:44:23.040+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:23 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 1484 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:44:23.041+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:23 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T07:44:23.069+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:23 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 2.034 s
[2023-01-31T07:44:23.070+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:23 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:44:23.070+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T07:44:23.071+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:23 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 2.319453 s
[2023-01-31T07:44:25.389+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:25 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T07:44:25.390+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:25 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T07:44:25.424+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:25 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T07:44:29.984+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:29 INFO CodeGenerator: Code generated in 2944.339645 ms
[2023-01-31T07:44:30.262+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:30 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.7 MiB)
[2023-01-31T07:44:31.197+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:31 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.7 MiB)
[2023-01-31T07:44:31.197+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:31 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 81d5fcd0285b:42647 (size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T07:44:31.225+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:31 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T07:44:31.349+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:31 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T07:44:32.345+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:32 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T07:44:32.346+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:32 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T07:44:32.365+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:32 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T07:44:32.366+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:32 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:44:32.366+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:32 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:44:32.367+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:32 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T07:44:32.437+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:32 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.7 MiB)
[2023-01-31T07:44:32.478+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:32 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.6 MiB)
[2023-01-31T07:44:32.483+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:32 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 81d5fcd0285b:42647 (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T07:44:32.495+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:32 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:44:32.521+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:44:32.525+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:32 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T07:44:32.550+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:32 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T07:44:32.554+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:32 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T07:44:32.721+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:32 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T07:44:33.205+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:33 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1729 bytes result sent to driver
[2023-01-31T07:44:33.225+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:33 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 662 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:44:33.247+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:33 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T07:44:33.248+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:33 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 0.855 s
[2023-01-31T07:44:33.249+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:33 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:44:33.249+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T07:44:33.250+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:33 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 0.908616 s
[2023-01-31T07:44:38.613+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:38 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 81d5fcd0285b:42647 in memory (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T07:44:38.836+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:38 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 81d5fcd0285b:42647 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T07:44:39.044+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:39 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 81d5fcd0285b:42647 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T07:44:39.469+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:39 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 81d5fcd0285b:42647 in memory (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T07:44:40.012+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:40 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 81d5fcd0285b:42647 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T07:44:40.220+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:40 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 81d5fcd0285b:42647 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:44:40.297+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:40 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 81d5fcd0285b:42647 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:44:40.471+0000] {spark_submit.py:495} INFO - 23/01/31 07:44:40 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 81d5fcd0285b:42647 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:45:17.315+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:17 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T07:45:17.420+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T07:45:17.421+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T07:45:17.428+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:17 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T07:45:17.429+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T07:45:17.429+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T07:45:17.432+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T07:45:18.151+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-31T07:45:18.154+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO DAGScheduler: Got job 8 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T07:45:18.155+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO DAGScheduler: Final stage: ResultStage 8 (save at BigQueryWriteHelper.java:105)
[2023-01-31T07:45:18.158+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T07:45:18.158+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO DAGScheduler: Missing parents: List()
[2023-01-31T07:45:18.163+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO DAGScheduler: Submitting ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T07:45:18.383+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 209.9 KiB, free 433.7 MiB)
[2023-01-31T07:45:18.428+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 74.3 KiB, free 433.7 MiB)
[2023-01-31T07:45:18.431+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 81d5fcd0285b:42647 (size: 74.3 KiB, free: 434.3 MiB)
[2023-01-31T07:45:18.435+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513
[2023-01-31T07:45:18.449+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T07:45:18.449+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2023-01-31T07:45:18.468+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7484 bytes) taskResourceAssignments Map()
[2023-01-31T07:45:18.476+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2023-01-31T07:45:18.616+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T07:45:18.617+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T07:45:18.618+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T07:45:18.618+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T07:45:18.619+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T07:45:18.624+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T07:45:18.640+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T07:45:18.644+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T07:45:18.736+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO ParquetOutputFormat: Parquet block size to 134217728
[2023-01-31T07:45:18.737+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO ParquetOutputFormat: Validation is off
[2023-01-31T07:45:18.737+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-01-31T07:45:18.738+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO ParquetOutputFormat: Parquet properties are:
[2023-01-31T07:45:18.738+0000] {spark_submit.py:495} INFO - Parquet page size to 1048576
[2023-01-31T07:45:18.738+0000] {spark_submit.py:495} INFO - Parquet dictionary page size to 1048576
[2023-01-31T07:45:18.738+0000] {spark_submit.py:495} INFO - Dictionary is true
[2023-01-31T07:45:18.738+0000] {spark_submit.py:495} INFO - Writer version is: PARQUET_1_0
[2023-01-31T07:45:18.739+0000] {spark_submit.py:495} INFO - Page size checking is: estimated
[2023-01-31T07:45:18.739+0000] {spark_submit.py:495} INFO - Min row count for page size check is: 100
[2023-01-31T07:45:18.739+0000] {spark_submit.py:495} INFO - Max row count for page size check is: 10000
[2023-01-31T07:45:18.739+0000] {spark_submit.py:495} INFO - Truncate length for column indexes is: 64
[2023-01-31T07:45:18.739+0000] {spark_submit.py:495} INFO - Truncate length for statistics min/max  is: 2147483647
[2023-01-31T07:45:18.739+0000] {spark_submit.py:495} INFO - Bloom filter enabled: false
[2023-01-31T07:45:18.740+0000] {spark_submit.py:495} INFO - Max Bloom filter size for a column is 1048576
[2023-01-31T07:45:18.740+0000] {spark_submit.py:495} INFO - Bloom filter expected number of distinct values are: null
[2023-01-31T07:45:18.740+0000] {spark_submit.py:495} INFO - Page row count limit to 20000
[2023-01-31T07:45:18.740+0000] {spark_submit.py:495} INFO - Writing page checksums is: on
[2023-01-31T07:45:18.926+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:18 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-01-31T07:45:18.927+0000] {spark_submit.py:495} INFO - {
[2023-01-31T07:45:18.927+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-31T07:45:18.928+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-31T07:45:18.928+0000] {spark_submit.py:495} INFO - "name" : "measured_at",
[2023-01-31T07:45:18.928+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T07:45:18.928+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T07:45:18.929+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.929+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.929+0000] {spark_submit.py:495} INFO - "name" : "created_at",
[2023-01-31T07:45:18.929+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T07:45:18.929+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T07:45:18.930+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.930+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.930+0000] {spark_submit.py:495} INFO - "name" : "Biomass - Actual Aggregated",
[2023-01-31T07:45:18.930+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.931+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.931+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.931+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.931+0000] {spark_submit.py:495} INFO - "name" : "Fossil Brown coal/Lignite - Actual Aggregated",
[2023-01-31T07:45:18.931+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.931+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.932+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.932+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.932+0000] {spark_submit.py:495} INFO - "name" : "Fossil Gas - Actual Aggregated",
[2023-01-31T07:45:18.932+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.932+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.933+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.934+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.935+0000] {spark_submit.py:495} INFO - "name" : "Fossil Hard coal - Actual Aggregated",
[2023-01-31T07:45:18.935+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.936+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.937+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.939+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.939+0000] {spark_submit.py:495} INFO - "name" : "Fossil Oil - Actual Aggregated",
[2023-01-31T07:45:18.939+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.939+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.939+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.941+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.941+0000] {spark_submit.py:495} INFO - "name" : "Geothermal - Actual Aggregated",
[2023-01-31T07:45:18.941+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.943+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.943+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.944+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.949+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Aggregated",
[2023-01-31T07:45:18.949+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.949+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.950+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.950+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.950+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Consumption",
[2023-01-31T07:45:18.950+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.950+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.950+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.951+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.951+0000] {spark_submit.py:495} INFO - "name" : "Hydro Run-of-river and poundage - Actual Aggregated",
[2023-01-31T07:45:18.951+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.951+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.951+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.951+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.952+0000] {spark_submit.py:495} INFO - "name" : "Hydro Water Reservoir - Actual Aggregated",
[2023-01-31T07:45:18.952+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.952+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.952+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.952+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.963+0000] {spark_submit.py:495} INFO - "name" : "Nuclear - Actual Aggregated",
[2023-01-31T07:45:18.963+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.963+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.963+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.964+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.964+0000] {spark_submit.py:495} INFO - "name" : "Other - Actual Aggregated",
[2023-01-31T07:45:18.964+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.964+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.964+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.965+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.965+0000] {spark_submit.py:495} INFO - "name" : "Other renewable - Actual Aggregated",
[2023-01-31T07:45:18.965+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.965+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.965+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.966+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.966+0000] {spark_submit.py:495} INFO - "name" : "Solar - Actual Aggregated",
[2023-01-31T07:45:18.966+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.966+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.966+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.966+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.967+0000] {spark_submit.py:495} INFO - "name" : "Waste - Actual Aggregated",
[2023-01-31T07:45:18.967+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.967+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.967+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.967+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.967+0000] {spark_submit.py:495} INFO - "name" : "Wind Offshore - Actual Aggregated",
[2023-01-31T07:45:18.968+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.968+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.968+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.977+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T07:45:18.977+0000] {spark_submit.py:495} INFO - "name" : "Wind Onshore - Actual Aggregated",
[2023-01-31T07:45:18.977+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T07:45:18.977+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T07:45:18.979+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T07:45:18.980+0000] {spark_submit.py:495} INFO - } ]
[2023-01-31T07:45:18.980+0000] {spark_submit.py:495} INFO - }
[2023-01-31T07:45:18.980+0000] {spark_submit.py:495} INFO - and corresponding Parquet message type:
[2023-01-31T07:45:18.980+0000] {spark_submit.py:495} INFO - message spark_schema {
[2023-01-31T07:45:18.980+0000] {spark_submit.py:495} INFO - required int96 measured_at;
[2023-01-31T07:45:18.981+0000] {spark_submit.py:495} INFO - required int96 created_at;
[2023-01-31T07:45:18.981+0000] {spark_submit.py:495} INFO - optional double Biomass - Actual Aggregated;
[2023-01-31T07:45:18.981+0000] {spark_submit.py:495} INFO - optional double Fossil Brown coal/Lignite - Actual Aggregated;
[2023-01-31T07:45:18.981+0000] {spark_submit.py:495} INFO - optional double Fossil Gas - Actual Aggregated;
[2023-01-31T07:45:18.981+0000] {spark_submit.py:495} INFO - optional double Fossil Hard coal - Actual Aggregated;
[2023-01-31T07:45:18.981+0000] {spark_submit.py:495} INFO - optional double Fossil Oil - Actual Aggregated;
[2023-01-31T07:45:18.981+0000] {spark_submit.py:495} INFO - optional double Geothermal - Actual Aggregated;
[2023-01-31T07:45:18.982+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Aggregated;
[2023-01-31T07:45:18.982+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Consumption;
[2023-01-31T07:45:18.982+0000] {spark_submit.py:495} INFO - optional double Hydro Run-of-river and poundage - Actual Aggregated;
[2023-01-31T07:45:18.983+0000] {spark_submit.py:495} INFO - optional double Hydro Water Reservoir - Actual Aggregated;
[2023-01-31T07:45:18.983+0000] {spark_submit.py:495} INFO - optional double Nuclear - Actual Aggregated;
[2023-01-31T07:45:18.983+0000] {spark_submit.py:495} INFO - optional double Other - Actual Aggregated;
[2023-01-31T07:45:18.983+0000] {spark_submit.py:495} INFO - optional double Other renewable - Actual Aggregated;
[2023-01-31T07:45:18.984+0000] {spark_submit.py:495} INFO - optional double Solar - Actual Aggregated;
[2023-01-31T07:45:18.984+0000] {spark_submit.py:495} INFO - optional double Waste - Actual Aggregated;
[2023-01-31T07:45:18.984+0000] {spark_submit.py:495} INFO - optional double Wind Offshore - Actual Aggregated;
[2023-01-31T07:45:18.984+0000] {spark_submit.py:495} INFO - optional double Wind Onshore - Actual Aggregated;
[2023-01-31T07:45:18.984+0000] {spark_submit.py:495} INFO - }
[2023-01-31T07:45:18.985+0000] {spark_submit.py:495} INFO - 
[2023-01-31T07:45:18.985+0000] {spark_submit.py:495} INFO - 
[2023-01-31T07:45:19.677+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:19 INFO CodecPool: Got brand-new compressor [.snappy]
[2023-01-31T07:45:20.765+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:20 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 81d5fcd0285b:42647 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T07:45:29.633+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:29 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675150865556-37aece3d-4e7d-46cd-9fdc-798c246d4773/_temporary/0/_temporary/' directory.
[2023-01-31T07:45:29.635+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:29 INFO FileOutputCommitter: Saved output of task 'attempt_202301310745181247869227893411133_0008_m_000000_8' to gs://entsoe_temp_1009/.spark-bigquery-local-1675150865556-37aece3d-4e7d-46cd-9fdc-798c246d4773/_temporary/0/task_202301310745181247869227893411133_0008_m_000000
[2023-01-31T07:45:29.636+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:29 INFO SparkHadoopMapRedUtil: attempt_202301310745181247869227893411133_0008_m_000000_8: Committed. Elapsed time: 1822 ms.
[2023-01-31T07:45:29.698+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:29 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2588 bytes result sent to driver
[2023-01-31T07:45:29.699+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:29 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 11238 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T07:45:29.699+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:29 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2023-01-31T07:45:29.719+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:29 INFO DAGScheduler: ResultStage 8 (save at BigQueryWriteHelper.java:105) finished in 11.535 s
[2023-01-31T07:45:29.719+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:29 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T07:45:29.720+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2023-01-31T07:45:29.721+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:29 INFO DAGScheduler: Job 8 finished: save at BigQueryWriteHelper.java:105, took 11.553484 s
[2023-01-31T07:45:29.733+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:29 INFO FileFormatWriter: Start to commit write Job 4bd6c209-fad9-4e01-8c96-ef4d4b6f48fc.
[2023-01-31T07:45:30.922+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:30 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675150865556-37aece3d-4e7d-46cd-9fdc-798c246d4773/_temporary/0/task_202301310745181247869227893411133_0008_m_000000/' directory.
[2023-01-31T07:45:32.198+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:32 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675150865556-37aece3d-4e7d-46cd-9fdc-798c246d4773/' directory.
[2023-01-31T07:45:33.365+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:33 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 81d5fcd0285b:42647 in memory (size: 74.3 KiB, free: 434.4 MiB)
[2023-01-31T07:45:33.768+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:33 INFO FileFormatWriter: Write Job 4bd6c209-fad9-4e01-8c96-ef4d4b6f48fc committed. Elapsed time: 4017 ms.
[2023-01-31T07:45:33.816+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:33 INFO FileFormatWriter: Finished processing stats for write job 4bd6c209-fad9-4e01-8c96-ef4d4b6f48fc.
[2023-01-31T07:45:41.101+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:41 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=entsoe_analytics_1009, projectId=rafzul-analytics-1009, tableId=TEST_total_generation_staging}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=null, ignoreUnknownValue=null, sourceUris=[gs://entsoe_temp_1009/.spark-bigquery-local-1675150865556-37aece3d-4e7d-46cd-9fdc-798c246d4773/part-00000-1ea7f152-5492-4276-af7f-ddce3159e24a-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=true, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=rafzul-analytics-1009, job=d6d9dff3-30b7-4f47-b2f0-902edbb27742, location=US}
[2023-01-31T07:45:46.105+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:46 INFO BigQueryClient: Done loading to rafzul-analytics-1009.entsoe_analytics_1009.TEST_total_generation_staging. jobId: JobId{project=rafzul-analytics-1009, job=d6d9dff3-30b7-4f47-b2f0-902edbb27742, location=US}
[2023-01-31T07:45:47.413+0000] {spark_submit.py:495} INFO - MANTEP!!!!
[2023-01-31T07:45:47.776+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:47 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-31T07:45:47.808+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:47 INFO SparkUI: Stopped Spark web UI at http://81d5fcd0285b:4046
[2023-01-31T07:45:47.845+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:47 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-31T07:45:47.894+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:47 INFO MemoryStore: MemoryStore cleared
[2023-01-31T07:45:47.895+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:47 INFO BlockManager: BlockManager stopped
[2023-01-31T07:45:47.904+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:47 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-31T07:45:47.915+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:47 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-31T07:45:47.936+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:47 INFO SparkContext: Successfully stopped SparkContext
[2023-01-31T07:45:47.937+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:47 INFO ShutdownHookManager: Shutdown hook called
[2023-01-31T07:45:47.939+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:47 INFO ShutdownHookManager: Deleting directory /tmp/spark-36349aee-2534-4650-b9b1-43890d392200/pyspark-fd3950a5-4ad6-4071-95e2-af672c105892
[2023-01-31T07:45:47.957+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:47 INFO ShutdownHookManager: Deleting directory /tmp/spark-36349aee-2534-4650-b9b1-43890d392200
[2023-01-31T07:45:47.990+0000] {spark_submit.py:495} INFO - 23/01/31 07:45:47 INFO ShutdownHookManager: Deleting directory /tmp/spark-08c10094-4ff3-4f41-96cd-480ec1eb7c88
[2023-01-31T07:45:48.300+0000] {taskinstance.py:1322} INFO - Marking task as SUCCESS. dag_id=entsoe-energydata-backfill, task_id=stage_total_generation, execution_date=20210101T060000, start_date=20230131T073904, end_date=20230131T074548
[2023-01-31T07:45:48.369+0000] {local_task_job.py:159} INFO - Task exited with return code 0
[2023-01-31T07:45:48.427+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T08:21:37.225+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [queued]>
[2023-01-31T08:21:37.253+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [queued]>
[2023-01-31T08:21:37.254+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T08:21:37.254+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T08:21:37.254+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T08:21:37.290+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 06:00:00+00:00
[2023-01-31T08:21:37.304+0000] {standard_task_runner.py:55} INFO - Started process 2288 to run task
[2023-01-31T08:21:37.314+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T06:00:00+00:00', '--job-id', '929', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpg83wwcpz']
[2023-01-31T08:21:37.320+0000] {standard_task_runner.py:83} INFO - Job 929: Subtask stage_total_generation
[2023-01-31T08:21:37.498+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [running]> on host 81d5fcd0285b
[2023-01-31T08:21:37.717+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T06:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T06:00:00+00:00
[2023-01-31T08:21:37.839+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T08:21:37.842+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010600 202101010700 DE_TENNET
[2023-01-31T08:22:00.986+0000] {spark_submit.py:495} INFO - {{ start_date }}
[2023-01-31T08:22:00.986+0000] {spark_submit.py:495} INFO - mulai periode: {{ start_date }}
[2023-01-31T08:22:00.987+0000] {spark_submit.py:495} INFO - selesai periode: {{ end_date }}
[2023-01-31T08:22:02.018+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:02 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T08:22:02.722+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T08:22:04.237+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:04 INFO ResourceUtils: ==============================================================
[2023-01-31T08:22:04.275+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:04 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T08:22:04.284+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:04 INFO ResourceUtils: ==============================================================
[2023-01-31T08:22:04.303+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:04 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T08:22:04.707+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:04 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T08:22:04.795+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:04 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T08:22:04.809+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:04 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T08:22:05.496+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:05 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T08:22:05.519+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:05 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T08:22:05.522+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:05 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T08:22:05.557+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:05 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T08:22:05.564+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:05 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T08:22:08.744+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:08 INFO Utils: Successfully started service 'sparkDriver' on port 41523.
[2023-01-31T08:22:09.076+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:09 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T08:22:09.258+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:09 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T08:22:09.464+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:09 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T08:22:09.467+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:09 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T08:22:09.530+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:09 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T08:22:09.836+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:09 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-feb0f9b1-c8ed-4a3f-97a6-e7b03b4fa0ac
[2023-01-31T08:22:09.916+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:09 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T08:22:10.042+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:10 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T08:22:12.497+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:12 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T08:22:12.509+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:12 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2023-01-31T08:22:12.509+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:12 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[2023-01-31T08:22:12.520+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:12 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[2023-01-31T08:22:12.529+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:12 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[2023-01-31T08:22:12.567+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:12 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
[2023-01-31T08:22:12.637+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:12 INFO Utils: Successfully started service 'SparkUI' on port 4046.
[2023-01-31T08:22:13.020+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:13 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://81d5fcd0285b:41523/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675153321818
[2023-01-31T08:22:13.022+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:13 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://81d5fcd0285b:41523/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675153321818
[2023-01-31T08:22:13.660+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:13 INFO Executor: Starting executor ID driver on host 81d5fcd0285b
[2023-01-31T08:22:13.681+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:13 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T08:22:13.769+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:13 INFO Executor: Fetching spark://81d5fcd0285b:41523/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675153321818
[2023-01-31T08:22:14.031+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:14 INFO TransportClientFactory: Successfully created connection to 81d5fcd0285b/172.19.0.5:41523 after 158 ms (0 ms spent in bootstraps)
[2023-01-31T08:22:14.051+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:14 INFO Utils: Fetching spark://81d5fcd0285b:41523/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-987f480c-d346-4f2b-b445-93d153c34b87/userFiles-5daa858d-2b06-4b6d-a0cf-2a89fc9d4d08/fetchFileTemp14085226129108755483.tmp
[2023-01-31T08:22:15.493+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:15 INFO Executor: Adding file:/tmp/spark-987f480c-d346-4f2b-b445-93d153c34b87/userFiles-5daa858d-2b06-4b6d-a0cf-2a89fc9d4d08/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T08:22:15.493+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:15 INFO Executor: Fetching spark://81d5fcd0285b:41523/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675153321818
[2023-01-31T08:22:15.498+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:15 INFO Utils: Fetching spark://81d5fcd0285b:41523/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-987f480c-d346-4f2b-b445-93d153c34b87/userFiles-5daa858d-2b06-4b6d-a0cf-2a89fc9d4d08/fetchFileTemp531585759194433136.tmp
[2023-01-31T08:22:16.295+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:16 INFO Executor: Adding file:/tmp/spark-987f480c-d346-4f2b-b445-93d153c34b87/userFiles-5daa858d-2b06-4b6d-a0cf-2a89fc9d4d08/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T08:22:16.321+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:16 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44169.
[2023-01-31T08:22:16.322+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:16 INFO NettyBlockTransferService: Server created on 81d5fcd0285b:44169
[2023-01-31T08:22:16.326+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:16 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T08:22:16.418+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:16 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 44169, None)
[2023-01-31T08:22:16.424+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:16 INFO BlockManagerMasterEndpoint: Registering block manager 81d5fcd0285b:44169 with 434.4 MiB RAM, BlockManagerId(driver, 81d5fcd0285b, 44169, None)
[2023-01-31T08:22:16.458+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:16 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 44169, None)
[2023-01-31T08:22:16.461+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:16 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 81d5fcd0285b, 44169, None)
[2023-01-31T08:22:21.050+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:21 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T08:22:21.078+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:21 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T08:22:36.539+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:36 INFO InMemoryFileIndex: It took 719 ms to list leaf files for 1 paths.
[2023-01-31T08:22:38.799+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:38 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T08:22:39.381+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:39 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T08:22:39.417+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:39 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 81d5fcd0285b:44169 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T08:22:39.428+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:39 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T08:22:44.136+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:44 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T08:22:44.208+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:44 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T08:22:44.285+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:44 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T08:22:44.350+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:44 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T08:22:44.352+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:44 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T08:22:44.360+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:44 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:22:44.373+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:44 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:22:44.435+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:44 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T08:22:45.057+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:45 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T08:22:45.129+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:45 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T08:22:45.129+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:45 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 81d5fcd0285b:44169 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T08:22:45.138+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:45 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:22:45.370+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:22:45.400+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:45 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T08:22:45.818+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:45 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T08:22:45.928+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:45 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T08:22:46.728+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:46 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010600__202101010700.json:0+9371
[2023-01-31T08:22:48.406+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:48 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3437 bytes result sent to driver
[2023-01-31T08:22:48.503+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:48 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2750 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:22:48.533+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:48 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T08:22:48.634+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:48 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 4.096 s
[2023-01-31T08:22:48.718+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:48 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:22:48.725+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T08:22:48.754+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:48 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 4.465358 s
[2023-01-31T08:22:51.664+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:51 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 81d5fcd0285b:44169 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T08:22:51.731+0000] {spark_submit.py:495} INFO - 23/01/31 08:22:51 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 81d5fcd0285b:44169 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T08:23:14.141+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:14 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:14.292+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:14 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T08:23:14.388+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:14 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:20.922+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO CodeGenerator: Code generated in 4018.634399 ms
[2023-01-31T08:23:20.942+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T08:23:20.977+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T08:23:20.978+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 81d5fcd0285b:44169 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T08:23:20.982+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:20 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T08:23:21.118+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:21.318+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T08:23:21.332+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T08:23:21.333+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T08:23:21.334+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:21.343+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:21.355+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T08:23:21.399+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T08:23:21.399+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T08:23:21.399+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 81d5fcd0285b:44169 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T08:23:21.400+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:21.428+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:21.429+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T08:23:21.449+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:21.485+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:21 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T08:23:22.659+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:22 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T08:23:24.569+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:24 INFO CodeGenerator: Code generated in 1587.683288 ms
[2023-01-31T08:23:25.299+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1628 bytes result sent to driver
[2023-01-31T08:23:25.325+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 3876 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:25.331+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 3.965 s
[2023-01-31T08:23:25.331+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:25.337+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T08:23:25.338+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T08:23:25.340+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 4.021056 s
[2023-01-31T08:23:25.822+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:25.826+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T08:23:25.830+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:25 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:26.230+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO CodeGenerator: Code generated in 272.40346 ms
[2023-01-31T08:23:26.337+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T08:23:26.571+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T08:23:26.586+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 81d5fcd0285b:44169 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T08:23:26.591+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T08:23:26.614+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:26.878+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T08:23:26.901+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T08:23:26.901+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T08:23:26.902+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:26.909+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:26.910+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T08:23:26.927+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T08:23:26.976+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T08:23:26.994+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 81d5fcd0285b:44169 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T08:23:26.997+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:26.999+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:27.000+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T08:23:27.005+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:27.006+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T08:23:27.142+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T08:23:27.508+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 81d5fcd0285b:44169 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T08:23:27.826+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1628 bytes result sent to driver
[2023-01-31T08:23:27.865+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 859 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:27.865+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T08:23:27.874+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 0.963 s
[2023-01-31T08:23:27.875+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:27.875+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T08:23:27.875+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:27 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 0.978485 s
[2023-01-31T08:23:29.442+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:29.448+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T08:23:29.458+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:29 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:30.874+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:30 INFO CodeGenerator: Code generated in 887.894502 ms
[2023-01-31T08:23:30.924+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:30 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T08:23:31.100+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T08:23:31.103+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 81d5fcd0285b:44169 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T08:23:31.118+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T08:23:31.133+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:31.515+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T08:23:31.521+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T08:23:31.521+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T08:23:31.522+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:31.523+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:31.536+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T08:23:31.556+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.7 MiB)
[2023-01-31T08:23:31.578+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.6 MiB)
[2023-01-31T08:23:31.619+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 81d5fcd0285b:44169 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T08:23:31.639+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:31.641+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:31.655+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T08:23:31.659+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:31.660+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T08:23:31.886+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:31 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T08:23:32.662+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-01-31T08:23:32.693+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 1036 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:32.695+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T08:23:32.712+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 1.179 s
[2023-01-31T08:23:32.715+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:32.715+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T08:23:32.718+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:32 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 1.197252 s
[2023-01-31T08:23:33.015+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:33.017+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T08:23:33.021+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:33.215+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO CodeGenerator: Code generated in 104.162924 ms
[2023-01-31T08:23:33.253+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.4 MiB)
[2023-01-31T08:23:33.380+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T08:23:33.386+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 81d5fcd0285b:44169 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T08:23:33.403+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T08:23:33.407+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:33.713+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T08:23:33.720+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T08:23:33.721+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T08:23:33.721+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:33.721+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:33.725+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T08:23:33.738+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-31T08:23:33.743+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-01-31T08:23:33.756+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 81d5fcd0285b:44169 (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T08:23:33.764+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:33.777+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:33.778+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T08:23:33.782+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:33.783+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T08:23:33.945+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:33 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T08:23:34.261+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1586 bytes result sent to driver
[2023-01-31T08:23:34.267+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 486 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:34.267+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T08:23:34.273+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 0.535 s
[2023-01-31T08:23:34.274+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:34.274+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T08:23:34.274+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:34 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 0.555615 s
[2023-01-31T08:23:37.887+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:37 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:37.889+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:37 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T08:23:37.951+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:37 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:38.890+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO CodeGenerator: Code generated in 521.721489 ms
[2023-01-31T08:23:38.909+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.2 MiB)
[2023-01-31T08:23:38.969+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T08:23:38.972+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 81d5fcd0285b:44169 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T08:23:38.974+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T08:23:38.977+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:39.493+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T08:23:39.495+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) with 1 output partitions
[2023-01-31T08:23:39.496+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201)
[2023-01-31T08:23:39.496+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:39.496+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:39.511+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201), which has no missing parents
[2023-01-31T08:23:39.536+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.1 MiB)
[2023-01-31T08:23:39.567+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.1 MiB)
[2023-01-31T08:23:39.570+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 81d5fcd0285b:44169 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T08:23:39.572+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:39.577+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:39.581+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T08:23:39.586+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:39.587+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T08:23:39.653+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:39 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T08:23:40.420+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:40 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2542 bytes result sent to driver
[2023-01-31T08:23:40.426+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:40 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 843 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:40.427+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:40 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T08:23:40.433+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:40 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) finished in 0.924 s
[2023-01-31T08:23:40.434+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:40 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:40.434+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T08:23:40.435+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:40 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201, took 0.935212 s
[2023-01-31T08:23:43.014+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:43 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:43.077+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:43 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T08:23:43.077+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:43 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:45.233+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:45 INFO CodeGenerator: Code generated in 1139.804635 ms
[2023-01-31T08:23:45.393+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:45 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 432.9 MiB)
[2023-01-31T08:23:45.818+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:45 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.9 MiB)
[2023-01-31T08:23:45.819+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:45 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 81d5fcd0285b:44169 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T08:23:45.831+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:45 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T08:23:45.834+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:45 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:46.174+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:46 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T08:23:46.174+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:46 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T08:23:46.175+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:46 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T08:23:46.175+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:46 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:46.176+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:46 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:46.182+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:46 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T08:23:46.224+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:46 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 432.8 MiB)
[2023-01-31T08:23:46.292+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:46 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.8 MiB)
[2023-01-31T08:23:46.298+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:46 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 81d5fcd0285b:44169 (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T08:23:46.319+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:46 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:46.327+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:46.328+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:46 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T08:23:46.357+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:46 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:46.358+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:46 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T08:23:46.444+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:46 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T08:23:47.094+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:47 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-01-31T08:23:47.104+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:47 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 741 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:47.105+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:47 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T08:23:47.105+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:47 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 0.921 s
[2023-01-31T08:23:47.106+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:47 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:47.106+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T08:23:47.106+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:47 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 0.930278 s
[2023-01-31T08:23:47.685+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:47 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T08:23:47.702+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:47 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T08:23:47.715+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:47 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T08:23:48.843+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:48 INFO CodeGenerator: Code generated in 724.11262 ms
[2023-01-31T08:23:48.872+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:48 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.6 MiB)
[2023-01-31T08:23:49.000+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.6 MiB)
[2023-01-31T08:23:49.003+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 81d5fcd0285b:44169 (size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T08:23:49.004+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T08:23:49.006+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T08:23:49.190+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T08:23:49.191+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T08:23:49.191+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T08:23:49.191+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T08:23:49.192+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO DAGScheduler: Missing parents: List()
[2023-01-31T08:23:49.202+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T08:23:49.222+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.6 MiB)
[2023-01-31T08:23:49.232+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.5 MiB)
[2023-01-31T08:23:49.235+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 81d5fcd0285b:44169 (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T08:23:49.236+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T08:23:49.236+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T08:23:49.237+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T08:23:49.240+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T08:23:49.243+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T08:23:49.326+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T08:23:49.558+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1729 bytes result sent to driver
[2023-01-31T08:23:49.569+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 329 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T08:23:49.569+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T08:23:49.572+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 0.364 s
[2023-01-31T08:23:49.573+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T08:23:49.573+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T08:23:49.573+0000] {spark_submit.py:495} INFO - 23/01/31 08:23:49 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 0.383636 s
[2023-01-31T12:01:45.512+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [queued]>
[2023-01-31T12:01:45.529+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [queued]>
[2023-01-31T12:01:45.529+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T12:01:45.529+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T12:01:45.530+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T12:01:45.549+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 06:00:00+00:00
[2023-01-31T12:01:45.561+0000] {standard_task_runner.py:55} INFO - Started process 158 to run task
[2023-01-31T12:01:45.568+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T06:00:00+00:00', '--job-id', '981', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp70t9b3ke']
[2023-01-31T12:01:45.572+0000] {standard_task_runner.py:83} INFO - Job 981: Subtask stage_total_generation
[2023-01-31T12:01:45.679+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [running]> on host 8124f810dec3
[2023-01-31T12:01:45.917+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T06:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T06:00:00+00:00
[2023-01-31T12:01:45.935+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T12:01:45.937+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 398, in submit
    spark_submit_cmd = self._build_spark_submit_command(application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 344, in _build_spark_submit_command
    self.log.info("Spark-Submit cmd: %s", self._mask_cmd(connection_cmd))
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 256, in _mask_cmd
    " ".join(connection_cmd),
TypeError: sequence item 10: expected str instance, DateTime found
[2023-01-31T12:01:45.954+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=entsoe-energydata-backfill, task_id=stage_total_generation, execution_date=20210101T060000, start_date=20230131T120145, end_date=20230131T120145
[2023-01-31T12:01:45.982+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 981 for task stage_total_generation (sequence item 10: expected str instance, DateTime found; 158)
[2023-01-31T12:01:46.035+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-31T12:01:46.084+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T12:08:53.342+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [queued]>
[2023-01-31T12:08:53.444+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [queued]>
[2023-01-31T12:08:53.450+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T12:08:53.451+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T12:08:53.452+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T12:08:53.705+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 06:00:00+00:00
[2023-01-31T12:08:53.786+0000] {standard_task_runner.py:55} INFO - Started process 668 to run task
[2023-01-31T12:08:53.840+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T06:00:00+00:00', '--job-id', '1000', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp1ftjjpho']
[2023-01-31T12:08:53.851+0000] {standard_task_runner.py:83} INFO - Job 1000: Subtask stage_total_generation
[2023-01-31T12:08:54.514+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [running]> on host 8124f810dec3
[2023-01-31T12:08:55.119+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T06:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T06:00:00+00:00
[2023-01-31T12:08:55.215+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T12:08:55.229+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010600 202101010700 DE_TENNET 202101010000 202101010600
[2023-01-31T12:09:23.922+0000] {spark_submit.py:495} INFO - mulai periode: 202101010000
[2023-01-31T12:09:23.927+0000] {spark_submit.py:495} INFO - selesai periode: 202101010600
[2023-01-31T12:09:24.412+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:24 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T12:09:25.088+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T12:09:25.819+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:25 INFO ResourceUtils: ==============================================================
[2023-01-31T12:09:25.830+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:25 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T12:09:25.831+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:25 INFO ResourceUtils: ==============================================================
[2023-01-31T12:09:25.831+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:25 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T12:09:25.921+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:25 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T12:09:25.954+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:25 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T12:09:25.961+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:25 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T12:09:26.359+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:26 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T12:09:26.366+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:26 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T12:09:26.369+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:26 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T12:09:26.371+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:26 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T12:09:26.376+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:26 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T12:09:30.398+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:30 INFO Utils: Successfully started service 'sparkDriver' on port 38201.
[2023-01-31T12:09:31.034+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:31 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T12:09:31.406+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:31 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T12:09:31.728+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:31 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T12:09:31.728+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:31 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T12:09:31.820+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:31 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T12:09:32.302+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:32 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c361dfb5-61a7-467e-9513-335476c83f2c
[2023-01-31T12:09:32.458+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:32 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T12:09:32.614+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:32 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T12:09:34.781+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:34 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T12:09:34.781+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:34 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2023-01-31T12:09:34.821+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:34 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[2023-01-31T12:09:34.871+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:34 INFO Utils: Successfully started service 'SparkUI' on port 4043.
[2023-01-31T12:09:35.137+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:35 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://8124f810dec3:38201/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675166964358
[2023-01-31T12:09:35.139+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:35 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://8124f810dec3:38201/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675166964358
[2023-01-31T12:09:36.206+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:36 INFO Executor: Starting executor ID driver on host 8124f810dec3
[2023-01-31T12:09:36.225+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:36 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T12:09:36.390+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:36 INFO Executor: Fetching spark://8124f810dec3:38201/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675166964358
[2023-01-31T12:09:37.150+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:37 INFO TransportClientFactory: Successfully created connection to 8124f810dec3/172.20.0.9:38201 after 326 ms (0 ms spent in bootstraps)
[2023-01-31T12:09:37.174+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:37 INFO Utils: Fetching spark://8124f810dec3:38201/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-2db5be7b-ef5a-4120-8c74-ccf15da16021/userFiles-a3c2d450-6ac9-421c-8ed8-2258353ac2dc/fetchFileTemp12840900895484472944.tmp
[2023-01-31T12:09:40.082+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:40 INFO Executor: Adding file:/tmp/spark-2db5be7b-ef5a-4120-8c74-ccf15da16021/userFiles-a3c2d450-6ac9-421c-8ed8-2258353ac2dc/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T12:09:40.084+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:40 INFO Executor: Fetching spark://8124f810dec3:38201/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675166964358
[2023-01-31T12:09:40.127+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:40 INFO Utils: Fetching spark://8124f810dec3:38201/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-2db5be7b-ef5a-4120-8c74-ccf15da16021/userFiles-a3c2d450-6ac9-421c-8ed8-2258353ac2dc/fetchFileTemp17206456634475009939.tmp
[2023-01-31T12:09:41.186+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:41 INFO Executor: Adding file:/tmp/spark-2db5be7b-ef5a-4120-8c74-ccf15da16021/userFiles-a3c2d450-6ac9-421c-8ed8-2258353ac2dc/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T12:09:41.249+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:41 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36903.
[2023-01-31T12:09:41.249+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:41 INFO NettyBlockTransferService: Server created on 8124f810dec3:36903
[2023-01-31T12:09:41.259+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:41 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T12:09:41.358+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:41 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:09:41.397+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:41 INFO BlockManagerMasterEndpoint: Registering block manager 8124f810dec3:36903 with 434.4 MiB RAM, BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:09:41.426+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:41 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:09:41.444+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:41 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:09:50.898+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:50 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T12:09:50.959+0000] {spark_submit.py:495} INFO - 23/01/31 12:09:50 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T12:10:13.347+0000] {spark_submit.py:495} INFO - 23/01/31 12:10:13 INFO InMemoryFileIndex: It took 2683 ms to list leaf files for 1 paths.
[2023-01-31T12:10:30.741+0000] {spark_submit.py:495} INFO - 23/01/31 12:10:30 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T12:11:26.684+0000] {base_job.py:232} ERROR - LocalTaskJob heartbeat got an exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3361, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 325, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 888, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 491, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 271, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 386, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 684, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 210, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 680, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/jobs/base_job.py", line 228, in heartbeat
    self.heartbeat_callback(session=session)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/jobs/local_task_job.py", line 178, in heartbeat_callback
    self.task_instance.refresh_from_db()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 795, in refresh_from_db
    ti = qry.one_or_none()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2849, in one_or_none
    return self._iter().one_or_none()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2915, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1552, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3315, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3394, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3364, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2198, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 210, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3361, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 325, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 888, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 491, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 271, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 386, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 684, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 210, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 680, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-01-31T12:13:17.203+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:17 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.TimeoutException: Cannot receive any reply from 8124f810dec3:38201 in 10000 milliseconds
[2023-01-31T12:13:17.961+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:17 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T12:13:18.034+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 8124f810dec3:36903 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T12:13:18.305+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:18 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T12:13:35.326+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:35 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T12:13:35.397+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:35 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T12:13:35.785+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:35 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T12:13:36.094+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:36 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T12:13:36.095+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:36 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T12:13:36.103+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:36 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T12:13:36.227+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:36 INFO DAGScheduler: Missing parents: List()
[2023-01-31T12:13:36.380+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:36 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T12:13:40.910+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:40 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T12:13:41.333+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:41 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T12:13:41.334+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:41 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 8124f810dec3:36903 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T12:13:41.343+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:41 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T12:13:41.343+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T12:13:41.343+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:41 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T12:13:42.764+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:42 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (8124f810dec3, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T12:13:46.018+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:45 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T12:13:46.486+0000] {spark_submit.py:495} INFO - 23/01/31 12:13:46 INFO AsyncEventQueue: Process of event SparkListenerTaskStart(0,0,org.apache.spark.scheduler.TaskInfo@175fd10a) by listener AppStatusListener took 1.142717793s.
[2023-01-31T12:14:15.847+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:15 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010600__202101010700.json:0+9371
[2023-01-31T12:14:18.430+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:18 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3437 bytes result sent to driver
[2023-01-31T12:14:18.632+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:18 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 36077 ms on 8124f810dec3 (executor driver) (1/1)
[2023-01-31T12:14:18.671+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:18 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T12:14:18.717+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:18 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 42.070 s
[2023-01-31T12:14:18.758+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:18 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T12:14:18.763+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T12:14:18.779+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:18 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 42.974771 s
[2023-01-31T12:14:19.563+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:19 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 8124f810dec3:36903 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T12:14:19.645+0000] {spark_submit.py:495} INFO - 23/01/31 12:14:19 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 8124f810dec3:36903 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T12:15:20.981+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:20 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T12:15:21.000+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:20 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T12:15:21.021+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:21 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T12:15:24.151+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:24 INFO CodeGenerator: Code generated in 1975.532912 ms
[2023-01-31T12:15:24.345+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:24 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T12:15:24.687+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:24 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T12:15:24.700+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:24 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 8124f810dec3:36903 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T12:15:24.721+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:24 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T12:15:24.925+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:24 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T12:15:25.825+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:25 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T12:15:25.832+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:25 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T12:15:25.838+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:25 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T12:15:25.839+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:25 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T12:15:25.839+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:25 INFO DAGScheduler: Missing parents: List()
[2023-01-31T12:15:25.850+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:25 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T12:15:26.032+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:26 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T12:15:26.076+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:26 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T12:15:26.078+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:26 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 8124f810dec3:36903 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T12:15:26.085+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:26 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T12:15:26.086+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T12:15:26.086+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:26 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T12:15:26.168+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:26 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (8124f810dec3, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T12:15:26.185+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:26 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T12:15:26.917+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:26 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T12:15:28.772+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:28 INFO CodeGenerator: Code generated in 1314.424471 ms
[2023-01-31T12:15:32.671+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:32 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1671 bytes result sent to driver
[2023-01-31T12:15:37.794+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:37 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,ArraySeq(),HashMap((1,0) -> org.apache.spark.executor.ExecutorMetrics@2f8a3848)) by listener AppStatusListener took 1.283246532s.
[2023-01-31T12:15:37.986+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:37 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 11323 ms on 8124f810dec3 (executor driver) (1/1)
[2023-01-31T12:15:38.078+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:37 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T12:15:38.084+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:37 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 11.780 s
[2023-01-31T12:15:38.084+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:37 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T12:15:38.085+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T12:15:38.085+0000] {spark_submit.py:495} INFO - 23/01/31 12:15:37 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 11.984813 s
[2023-01-31T12:16:31.970+0000] {base_job.py:232} ERROR - LocalTaskJob heartbeat got an exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3361, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 325, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 888, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 491, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 271, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 386, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 684, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 210, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 680, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/jobs/base_job.py", line 204, in heartbeat
    session.merge(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in merge
    return self._merge(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3131, in _merge
    merged = self.get(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2848, in get
    return self._get_impl(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2970, in _get_impl
    return db_load_fn(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/loading.py", line 530, in load_on_pk_identity
    session.execute(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1552, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3315, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3394, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3364, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2198, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 210, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3361, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 325, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 888, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 491, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 271, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 386, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 684, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 210, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 680, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 578, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-01-31T12:20:31.153+0000] {spark_submit.py:495} INFO - 23/01/31 12:20:31 WARN NettyRpcEnv: Ignored message: HeartbeatResponse(false)
[2023-01-31T12:20:31.319+0000] {spark_submit.py:495} INFO - 23/01/31 12:20:31 WARN Executor: Issue communicating with driver in heartbeater
[2023-01-31T12:20:31.587+0000] {spark_submit.py:495} INFO - org.apache.spark.rpc.RpcTimeoutException: Future timed out after [10000 milliseconds]. This timeout is controlled by spark.executor.heartbeatInterval
[2023-01-31T12:20:31.694+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
[2023-01-31T12:20:31.695+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
[2023-01-31T12:20:31.695+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
[2023-01-31T12:20:31.695+0000] {spark_submit.py:495} INFO - at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)
[2023-01-31T12:20:31.695+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)
[2023-01-31T12:20:31.696+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2023-01-31T12:20:31.696+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1053)
[2023-01-31T12:20:31.696+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2023-01-31T12:20:31.696+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
[2023-01-31T12:20:31.696+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2023-01-31T12:20:31.697+0000] {spark_submit.py:495} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2023-01-31T12:20:31.697+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-01-31T12:20:31.700+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2023-01-31T12:20:31.700+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2023-01-31T12:20:31.700+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-31T12:20:31.701+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-31T12:20:31.701+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-31T12:20:31.701+0000] {spark_submit.py:495} INFO - Caused by: java.util.concurrent.TimeoutException: Future timed out after [10000 milliseconds]
[2023-01-31T12:20:31.701+0000] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryAwait0(Promise.scala:248)
[2023-01-31T12:20:31.701+0000] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:261)
[2023-01-31T12:20:31.702+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:293)
[2023-01-31T12:20:31.702+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-01-31T12:20:31.702+0000] {spark_submit.py:495} INFO - ... 12 more
[2023-01-31T12:20:31.705+0000] {spark_submit.py:495} INFO - 23/01/31 12:20:31 INFO AsyncEventQueue: Process of event SparkListenerTaskEnd(1,0,ResultTask,Success,org.apache.spark.scheduler.TaskInfo@e43d4b9,org.apache.spark.executor.ExecutorMetrics@96d8513,org.apache.spark.executor.TaskMetrics@13ad88f6) by listener SQLAppStatusListener took 293.083764754s.
[2023-01-31T12:21:59.791+0000] {spark_submit.py:495} INFO - 23/01/31 12:21:59 WARN Executor: Issue communicating with driver in heartbeater
[2023-01-31T12:21:59.798+0000] {spark_submit.py:495} INFO - org.apache.spark.rpc.RpcTimeoutException: Future timed out after [10000 milliseconds]. This timeout is controlled by spark.executor.heartbeatInterval
[2023-01-31T12:21:59.798+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
[2023-01-31T12:21:59.799+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
[2023-01-31T12:21:59.799+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
[2023-01-31T12:21:59.799+0000] {spark_submit.py:495} INFO - at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)
[2023-01-31T12:21:59.809+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)
[2023-01-31T12:21:59.810+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2023-01-31T12:21:59.810+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1053)
[2023-01-31T12:21:59.810+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2023-01-31T12:21:59.811+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
[2023-01-31T12:21:59.811+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2023-01-31T12:21:59.811+0000] {spark_submit.py:495} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2023-01-31T12:21:59.811+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-01-31T12:21:59.812+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2023-01-31T12:21:59.812+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2023-01-31T12:21:59.812+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-31T12:21:59.813+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-31T12:21:59.813+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-31T12:21:59.813+0000] {spark_submit.py:495} INFO - Caused by: java.util.concurrent.TimeoutException: Future timed out after [10000 milliseconds]
[2023-01-31T12:21:59.814+0000] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryAwait0(Promise.scala:248)
[2023-01-31T12:21:59.815+0000] {spark_submit.py:495} INFO - at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:261)
[2023-01-31T12:21:59.815+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:293)
[2023-01-31T12:21:59.815+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-01-31T12:21:59.816+0000] {spark_submit.py:495} INFO - ... 12 more
[2023-01-31T12:21:59.902+0000] {spark_submit.py:495} INFO - 23/01/31 12:21:59 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 271218 ms exceeds timeout 120000 ms
[2023-01-31T12:22:00.159+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 WARN NettyRpcEnv: Ignored message: HeartbeatResponse(true)
[2023-01-31T12:22:00.160+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:00.164+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:00.165+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:00.338+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:00 WARN SparkContext: Killing executors is not supported by current scheduler.
[2023-01-31T12:22:01.059+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:01.062+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManager: Reporting 4 blocks to the master.
[2023-01-31T12:22:01.271+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T12:22:01.346+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T12:22:01.407+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:01.408+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:01.412+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:01.440+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:01.450+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManager: Reporting 4 blocks to the master.
[2023-01-31T12:22:01.470+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T12:22:01.475+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T12:22:01.478+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:01.492+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:01.495+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:01.496+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:01.515+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManager: Reporting 4 blocks to the master.
[2023-01-31T12:22:01.515+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T12:22:01.582+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T12:22:01.583+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:01.641+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:01.647+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:01.664+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:01.664+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManager: Reporting 4 blocks to the master.
[2023-01-31T12:22:01.783+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T12:22:01.783+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T12:22:01.845+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:01.937+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:01.938+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:01.939+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:01.939+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManager: Reporting 4 blocks to the master.
[2023-01-31T12:22:02.044+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:01 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T12:22:02.076+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T12:22:02.174+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:02.175+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:02.203+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:02.213+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:02.213+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManager: Reporting 4 blocks to the master.
[2023-01-31T12:22:02.214+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T12:22:02.216+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T12:22:02.262+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:02.275+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:02.281+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:02.310+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:02.317+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManager: Reporting 4 blocks to the master.
[2023-01-31T12:22:02.318+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T12:22:02.318+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T12:22:02.319+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:02.319+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:02.319+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:02.320+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:02.321+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManager: Reporting 4 blocks to the master.
[2023-01-31T12:22:02.322+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T12:22:02.352+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T12:22:02.391+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T12:22:02.392+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T12:22:02.393+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T12:22:02.422+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:02.424+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:02.429+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:02.430+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:02.442+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManager: Reporting 4 blocks to the master.
[2023-01-31T12:22:02.451+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T12:22:02.464+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T12:22:02.536+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:02.538+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:02.551+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:02.566+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:02.568+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManager: Reporting 4 blocks to the master.
[2023-01-31T12:22:02.599+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T12:22:02.621+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T12:22:02.669+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:02.688+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:02.692+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:02.714+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO CodeGenerator: Code generated in 47.979612 ms
[2023-01-31T12:22:02.722+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:02.725+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T12:22:02.740+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManager: Reporting 5 blocks to the master.
[2023-01-31T12:22:02.744+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T12:22:02.785+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T12:22:02.857+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:02.858+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:02.858+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:02.860+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:02.888+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManager: Reporting 5 blocks to the master.
[2023-01-31T12:22:02.912+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T12:22:02.919+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T12:22:02.960+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:02.962+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:02.969+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:02.977+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:02.983+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManager: Reporting 5 blocks to the master.
[2023-01-31T12:22:02.987+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:02 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T12:22:03.022+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T12:22:03.092+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:03.104+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:03.127+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:03.136+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T12:22:03.141+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:03.141+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManager: Reporting 6 blocks to the master.
[2023-01-31T12:22:03.149+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 8124f810dec3:36903 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:03.178+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T12:22:03.184+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:03.202+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:03.206+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T12:22:03.240+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:03.272+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:03.287+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:03.296+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:03.300+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:03.326+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T12:22:03.330+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T12:22:03.331+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T12:22:03.331+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T12:22:03.333+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO DAGScheduler: Missing parents: List()
[2023-01-31T12:22:03.435+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T12:22:03.439+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManager: Reporting 6 blocks to the master.
[2023-01-31T12:22:03.510+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:03.514+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:03.560+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T12:22:03.564+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:03.578+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:03.579+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:03.579+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:03.603+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T12:22:03.622+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:03.626+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManager: Reporting 8 blocks to the master.
[2023-01-31T12:22:03.656+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 8124f810dec3:36903 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:03.662+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T12:22:03.698+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_5_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:03.720+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:03.728+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:03.830+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:03.847+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T12:22:03.849+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T12:22:03.850+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:03.851+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:03.851+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:03.858+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (8124f810dec3, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T12:22:03.861+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:03.862+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManager: Reporting 8 blocks to the master.
[2023-01-31T12:22:03.946+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_5_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:03.950+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T12:22:03.954+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:03.957+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:03 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:04.039+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:04.078+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:04.078+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:04.079+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:04.083+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:04.083+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManager: Reporting 8 blocks to the master.
[2023-01-31T12:22:04.084+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_5_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:04.092+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:04.135+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:04.140+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:04.150+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:04.152+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:04.152+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:04.205+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:04.210+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManager: Reporting 8 blocks to the master.
[2023-01-31T12:22:04.210+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_5_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:04.214+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:04.381+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:04.410+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:04.416+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:04.418+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:04.419+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:04.508+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:04.510+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManager: Reporting 8 blocks to the master.
[2023-01-31T12:22:04.516+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:04 INFO BlockManagerInfo: Updated broadcast_5_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:05.176+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:05 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:05.279+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:05 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:05.459+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:05 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:05.474+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:05 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:05.744+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:05 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:05.745+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:05 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:05.769+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:05 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:05.769+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:05 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T12:22:05.859+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:05 INFO BlockManager: Reporting 8 blocks to the master.
[2023-01-31T12:22:05.862+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:05 INFO BlockManagerInfo: Updated broadcast_5_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:05.934+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:05 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:05.971+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:05 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:06.152+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:06.167+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:06.264+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:06.279+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:06.514+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:06.515+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO BlockManager: Reporting 8 blocks to the master.
[2023-01-31T12:22:06.617+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO BlockManagerInfo: Updated broadcast_5_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:06.630+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:06.683+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:06.807+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:06.811+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:06.817+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:06.829+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:06.919+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:06.920+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO BlockManager: Reporting 8 blocks to the master.
[2023-01-31T12:22:06.957+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:06 INFO BlockManagerInfo: Updated broadcast_5_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:07.083+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:07.174+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:07.325+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:07.332+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:07.333+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:07.334+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:07.426+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:07.426+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManager: Reporting 8 blocks to the master.
[2023-01-31T12:22:07.458+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerInfo: Updated broadcast_5_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:07.624+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:07.663+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:07.667+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:07.767+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:07.771+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:07.803+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:07.806+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:07.807+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManager: Reporting 8 blocks to the master.
[2023-01-31T12:22:07.810+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerInfo: Updated broadcast_5_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:07.934+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:07.941+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:07.951+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:07.956+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:07.958+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:07.959+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:07.961+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:07.962+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManager: Reporting 8 blocks to the master.
[2023-01-31T12:22:07.965+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerInfo: Updated broadcast_5_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:07.968+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:07.988+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:07.999+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:07 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.007+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:08.009+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:08.013+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:08.019+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:08.022+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManager: Reporting 8 blocks to the master.
[2023-01-31T12:22:08.027+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_5_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.042+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.081+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.109+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.170+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:08.182+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:08.262+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:08.276+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:08.284+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManager: Reporting 8 blocks to the master.
[2023-01-31T12:22:08.285+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_5_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.295+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.417+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.526+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.530+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:08.532+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:08.532+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:08.535+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:08.535+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManager: Reporting 8 blocks to the master.
[2023-01-31T12:22:08.539+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_5_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.542+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.545+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.549+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.555+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:08.558+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:08.559+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:08.561+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:08.565+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManager: Reporting 8 blocks to the master.
[2023-01-31T12:22:08.568+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_5_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.572+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.578+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.586+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.605+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:08.606+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:08.606+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:08.635+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:08.642+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManager: Reporting 8 blocks to the master.
[2023-01-31T12:22:08.677+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_5_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.709+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.776+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.863+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:08.874+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:08.876+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:08.892+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:08.975+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:08.976+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManager: Reporting 8 blocks to the master.
[2023-01-31T12:22:08.978+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:08 INFO BlockManagerInfo: Updated broadcast_5_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:09.033+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:09.057+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1628 bytes result sent to driver
[2023-01-31T12:22:09.067+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:09.070+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:09.093+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 5236 ms on 8124f810dec3 (executor driver) (1/1)
[2023-01-31T12:22:09.093+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T12:22:09.096+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:09.097+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:09.097+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 5.593 s
[2023-01-31T12:22:09.097+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:09.098+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T12:22:09.113+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T12:22:09.114+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 5.770560 s
[2023-01-31T12:22:09.153+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:09.154+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManager: Reporting 8 blocks to the master.
[2023-01-31T12:22:09.157+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerInfo: Updated broadcast_5_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:09.177+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:09.177+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:09.215+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:09.221+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:09.222+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:09.223+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:09.228+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:09.232+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManager: Reporting 8 blocks to the master.
[2023-01-31T12:22:09.236+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerInfo: Updated broadcast_5_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:09.245+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:09.252+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:09.262+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:09.269+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:09.270+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:09.270+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:09.303+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:09.326+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManager: Reporting 8 blocks to the master.
[2023-01-31T12:22:09.368+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerInfo: Updated broadcast_5_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:09.449+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:09.477+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:09.554+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:09.564+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:09.565+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:09.566+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:09.600+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:09.600+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManager: Reporting 8 blocks to the master.
[2023-01-31T12:22:09.625+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerInfo: Updated broadcast_5_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:09.644+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:09.700+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:09.741+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:09.745+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:09.747+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:09.747+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:09.779+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:09.780+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManager: Reporting 8 blocks to the master.
[2023-01-31T12:22:09.795+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerInfo: Updated broadcast_5_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:09.802+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:09.831+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:10.010+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:09 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:12.263+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:12 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T12:22:12.270+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:12 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T12:22:12.272+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:12 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T12:22:13.280+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:13 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:13.281+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:13 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:13.283+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:13 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:13.300+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:13 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:13.308+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:13 INFO BlockManager: Reporting 8 blocks to the master.
[2023-01-31T12:22:13.322+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:13 INFO BlockManagerInfo: Updated broadcast_5_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:13.350+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:13 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:13.416+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:13 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:13.499+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:13 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:15.236+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:15 INFO CodeGenerator: Code generated in 2496.636191 ms
[2023-01-31T12:22:15.292+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:15 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T12:22:15.555+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:15 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T12:22:15.561+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:15 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 8124f810dec3:36903 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:15.616+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:15 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T12:22:15.764+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T12:22:16.197+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:16 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 8124f810dec3:36903 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:16.524+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:16 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T12:22:16.539+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:16 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T12:22:16.540+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:16 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T12:22:16.541+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:16 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T12:22:16.541+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:16 INFO DAGScheduler: Missing parents: List()
[2023-01-31T12:22:16.550+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:16 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T12:22:16.619+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:16 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.7 MiB)
[2023-01-31T12:22:16.637+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:16 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.6 MiB)
[2023-01-31T12:22:16.678+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:16 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 8124f810dec3:36903 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:16.706+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:16 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T12:22:16.716+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T12:22:16.722+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:16 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T12:22:16.745+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:16 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (8124f810dec3, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T12:22:16.748+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:16 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T12:22:16.887+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:16 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T12:22:17.465+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:17 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-01-31T12:22:17.481+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:17 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 741 ms on 8124f810dec3 (executor driver) (1/1)
[2023-01-31T12:22:17.491+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:17 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 0.921 s
[2023-01-31T12:22:17.493+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:17 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T12:22:17.494+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:17 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T12:22:17.495+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T12:22:17.502+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:17 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 0.972237 s
[2023-01-31T12:22:17.858+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:17 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T12:22:17.863+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:17 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T12:22:17.870+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:17 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T12:22:18.088+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:18 INFO CodeGenerator: Code generated in 167.285984 ms
[2023-01-31T12:22:18.157+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:18 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.4 MiB)
[2023-01-31T12:22:18.409+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:18 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T12:22:18.428+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:18 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 8124f810dec3:36903 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:18.454+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:18 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T12:22:18.487+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:18 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T12:22:18.991+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:18 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T12:22:18.996+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:18 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T12:22:18.996+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:18 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T12:22:18.997+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:18 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T12:22:18.997+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:18 INFO DAGScheduler: Missing parents: List()
[2023-01-31T12:22:19.033+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:19 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T12:22:19.116+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:19 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-31T12:22:19.202+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:19 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-01-31T12:22:19.210+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:19 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 8124f810dec3:36903 (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T12:22:19.212+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:19 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T12:22:19.232+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T12:22:19.233+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:19 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T12:22:19.258+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:19 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (8124f810dec3, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T12:22:19.259+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:19 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T12:22:19.440+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:19 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T12:22:19.839+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:19 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1586 bytes result sent to driver
[2023-01-31T12:22:19.844+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:19 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 587 ms on 8124f810dec3 (executor driver) (1/1)
[2023-01-31T12:22:19.850+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:19 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T12:22:19.851+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:19 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 0.816 s
[2023-01-31T12:22:19.852+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:19 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T12:22:19.852+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T12:22:19.854+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:19 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 0.854744 s
[2023-01-31T12:22:22.111+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:22 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 8124f810dec3:36903 in memory (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T12:22:22.126+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:22 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 8124f810dec3:36903 in memory (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:22.483+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:22 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T12:22:22.485+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:22 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T12:22:22.486+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:22 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T12:22:23.212+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO CodeGenerator: Code generated in 420.040685 ms
[2023-01-31T12:22:23.222+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.3 MiB)
[2023-01-31T12:22:23.263+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T12:22:23.265+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:23.266+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:23.267+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 8124f810dec3:36903 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:23.268+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:23.270+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T12:22:23.271+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:23.273+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO BlockManager: Reporting 12 blocks to the master.
[2023-01-31T12:22:23.300+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO BlockManagerInfo: Updated broadcast_10_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:23.308+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T12:22:23.333+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.2 MiB)
[2023-01-31T12:22:23.356+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO BlockManagerInfo: Updated broadcast_4_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:23.364+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO BlockManagerInfo: Updated broadcast_8_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:23.403+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO BlockManagerInfo: Updated broadcast_6_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:23.424+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:23.514+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T12:22:23.515+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T12:22:23.519+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T12:22:23.524+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T12:22:23.525+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO DAGScheduler: Missing parents: List()
[2023-01-31T12:22:23.526+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T12:22:23.542+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.2 MiB)
[2023-01-31T12:22:23.550+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.2 MiB)
[2023-01-31T12:22:23.555+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 8124f810dec3:36903 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T12:22:23.557+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T12:22:23.560+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T12:22:23.562+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T12:22:23.575+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (8124f810dec3, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T12:22:23.578+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T12:22:23.624+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T12:22:23.899+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2499 bytes result sent to driver
[2023-01-31T12:22:23.904+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 335 ms on 8124f810dec3 (executor driver) (1/1)
[2023-01-31T12:22:23.905+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T12:22:23.908+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 0.382 s
[2023-01-31T12:22:23.909+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T12:22:23.909+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T12:22:23.910+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:23 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 0.393507 s
[2023-01-31T12:22:24.185+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:24 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T12:22:24.190+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:24 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T12:22:24.192+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:24 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T12:22:24.696+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:24 INFO CodeGenerator: Code generated in 353.552339 ms
[2023-01-31T12:22:24.722+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:24 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 433.0 MiB)
[2023-01-31T12:22:24.820+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:24 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.9 MiB)
[2023-01-31T12:22:24.824+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:24 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 8124f810dec3:36903 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:24.825+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:24 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T12:22:24.833+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:24 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T12:22:24.936+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:24 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 8124f810dec3:36903 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:24.997+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:24 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 8124f810dec3:36903 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:25.023+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T12:22:25.025+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T12:22:25.025+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T12:22:25.026+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T12:22:25.026+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO DAGScheduler: Missing parents: List()
[2023-01-31T12:22:25.029+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T12:22:25.038+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 433.4 MiB)
[2023-01-31T12:22:25.050+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 433.4 MiB)
[2023-01-31T12:22:25.062+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 8124f810dec3:36903 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:25.065+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 8124f810dec3:36903 (size: 12.6 KiB, free: 434.3 MiB)
[2023-01-31T12:22:25.067+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T12:22:25.069+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T12:22:25.070+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T12:22:25.074+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (8124f810dec3, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T12:22:25.075+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T12:22:25.113+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T12:22:25.124+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 8124f810dec3:36903 in memory (size: 13.4 KiB, free: 434.3 MiB)
[2023-01-31T12:22:25.257+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-01-31T12:22:25.265+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 192 ms on 8124f810dec3 (executor driver) (1/1)
[2023-01-31T12:22:25.265+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T12:22:25.268+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 0.233 s
[2023-01-31T12:22:25.269+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T12:22:25.269+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T12:22:25.269+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 0.243503 s
[2023-01-31T12:22:25.442+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T12:22:25.444+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T12:22:25.446+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T12:22:25.781+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO CodeGenerator: Code generated in 238.4051 ms
[2023-01-31T12:22:25.794+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 433.4 MiB)
[2023-01-31T12:22:25.832+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T12:22:25.833+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 8124f810dec3:36903 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T12:22:25.835+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:204
[2023-01-31T12:22:25.841+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T12:22:25.937+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:204
[2023-01-31T12:22:25.939+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:204) with 1 output partitions
[2023-01-31T12:22:25.940+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:204)
[2023-01-31T12:22:25.940+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T12:22:25.940+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO DAGScheduler: Missing parents: List()
[2023-01-31T12:22:25.950+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:204), which has no missing parents
[2023-01-31T12:22:25.954+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 433.4 MiB)
[2023-01-31T12:22:25.957+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 433.4 MiB)
[2023-01-31T12:22:25.963+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 8124f810dec3:36903 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T12:22:25.967+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T12:22:25.968+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:204) (first 15 tasks are for partitions Vector(0))
[2023-01-31T12:22:25.969+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T12:22:25.972+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (8124f810dec3, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T12:22:25.974+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T12:22:25.997+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:25 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T12:22:26.132+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:26 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1729 bytes result sent to driver
[2023-01-31T12:22:26.160+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:26 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 189 ms on 8124f810dec3 (executor driver) (1/1)
[2023-01-31T12:22:26.164+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:26 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T12:22:26.164+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:26 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:204) finished in 0.213 s
[2023-01-31T12:22:26.164+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:26 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T12:22:26.164+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T12:22:26.165+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:26 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:204, took 0.224836 s
[2023-01-31T12:22:26.850+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:26 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 8124f810dec3:36903 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T12:22:26.855+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:26 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 8124f810dec3:36903 in memory (size: 12.6 KiB, free: 434.3 MiB)
[2023-01-31T12:22:28.774+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:28 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 8124f810dec3:36903 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:28.804+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:28 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 8124f810dec3:36903 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:33.264+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:33 INFO Executor: Told to re-register on heartbeat
[2023-01-31T12:22:33.264+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:33 INFO BlockManager: BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None) re-registering with master
[2023-01-31T12:22:33.265+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:33 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:33.266+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:33 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8124f810dec3, 36903, None)
[2023-01-31T12:22:33.266+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:33 INFO BlockManager: Reporting 6 blocks to the master.
[2023-01-31T12:22:33.268+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:33 INFO BlockManagerInfo: Updated broadcast_10_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:33.271+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:33 INFO BlockManagerInfo: Updated broadcast_3_piece0 in memory on 8124f810dec3:36903 (current size: 7.7 KiB, original size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:33.274+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:33 INFO BlockManagerInfo: Updated broadcast_2_piece0 in memory on 8124f810dec3:36903 (current size: 34.5 KiB, original size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T12:22:36.499+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:36 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T12:22:36.562+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T12:22:36.564+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T12:22:36.567+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:36 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T12:22:36.568+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T12:22:36.568+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T12:22:36.578+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T12:22:36.914+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:36 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-31T12:22:36.915+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:36 INFO DAGScheduler: Got job 8 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T12:22:36.915+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:36 INFO DAGScheduler: Final stage: ResultStage 8 (save at BigQueryWriteHelper.java:105)
[2023-01-31T12:22:36.915+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:36 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T12:22:36.915+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:36 INFO DAGScheduler: Missing parents: List()
[2023-01-31T12:22:36.916+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:36 INFO DAGScheduler: Submitting ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T12:22:36.946+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:36 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 209.9 KiB, free 433.7 MiB)
[2023-01-31T12:22:36.948+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:36 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 74.3 KiB, free 433.6 MiB)
[2023-01-31T12:22:36.948+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:36 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 8124f810dec3:36903 (size: 74.3 KiB, free: 434.3 MiB)
[2023-01-31T12:22:36.949+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:36 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513
[2023-01-31T12:22:36.949+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T12:22:36.950+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:36 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2023-01-31T12:22:36.953+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:36 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (8124f810dec3, executor driver, partition 0, PROCESS_LOCAL, 7484 bytes) taskResourceAssignments Map()
[2023-01-31T12:22:36.954+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:36 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2023-01-31T12:22:37.025+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T12:22:37.025+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T12:22:37.026+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:37 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T12:22:37.026+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T12:22:37.027+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T12:22:37.027+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T12:22:37.032+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:37 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T12:22:37.035+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:37 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T12:22:37.064+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:37 INFO ParquetOutputFormat: Parquet block size to 134217728
[2023-01-31T12:22:37.064+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:37 INFO ParquetOutputFormat: Validation is off
[2023-01-31T12:22:37.065+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:37 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-01-31T12:22:37.065+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:37 INFO ParquetOutputFormat: Parquet properties are:
[2023-01-31T12:22:37.065+0000] {spark_submit.py:495} INFO - Parquet page size to 1048576
[2023-01-31T12:22:37.065+0000] {spark_submit.py:495} INFO - Parquet dictionary page size to 1048576
[2023-01-31T12:22:37.065+0000] {spark_submit.py:495} INFO - Dictionary is true
[2023-01-31T12:22:37.065+0000] {spark_submit.py:495} INFO - Writer version is: PARQUET_1_0
[2023-01-31T12:22:37.066+0000] {spark_submit.py:495} INFO - Page size checking is: estimated
[2023-01-31T12:22:37.066+0000] {spark_submit.py:495} INFO - Min row count for page size check is: 100
[2023-01-31T12:22:37.066+0000] {spark_submit.py:495} INFO - Max row count for page size check is: 10000
[2023-01-31T12:22:37.066+0000] {spark_submit.py:495} INFO - Truncate length for column indexes is: 64
[2023-01-31T12:22:37.067+0000] {spark_submit.py:495} INFO - Truncate length for statistics min/max  is: 2147483647
[2023-01-31T12:22:37.067+0000] {spark_submit.py:495} INFO - Bloom filter enabled: false
[2023-01-31T12:22:37.067+0000] {spark_submit.py:495} INFO - Max Bloom filter size for a column is 1048576
[2023-01-31T12:22:37.067+0000] {spark_submit.py:495} INFO - Bloom filter expected number of distinct values are: null
[2023-01-31T12:22:37.067+0000] {spark_submit.py:495} INFO - Page row count limit to 20000
[2023-01-31T12:22:37.068+0000] {spark_submit.py:495} INFO - Writing page checksums is: on
[2023-01-31T12:22:37.099+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:37 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-01-31T12:22:37.099+0000] {spark_submit.py:495} INFO - {
[2023-01-31T12:22:37.099+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-31T12:22:37.100+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-31T12:22:37.100+0000] {spark_submit.py:495} INFO - "name" : "measured_at",
[2023-01-31T12:22:37.100+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T12:22:37.100+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T12:22:37.101+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:37.101+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:37.101+0000] {spark_submit.py:495} INFO - "name" : "created_at",
[2023-01-31T12:22:37.102+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T12:22:37.102+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T12:22:37.102+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:37.102+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:37.103+0000] {spark_submit.py:495} INFO - "name" : "Biomass - Actual Aggregated",
[2023-01-31T12:22:37.103+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:37.103+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:37.103+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:37.103+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:37.103+0000] {spark_submit.py:495} INFO - "name" : "Fossil Brown coal/Lignite - Actual Aggregated",
[2023-01-31T12:22:37.103+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:37.104+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:37.104+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:37.104+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:37.104+0000] {spark_submit.py:495} INFO - "name" : "Fossil Gas - Actual Aggregated",
[2023-01-31T12:22:37.104+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:37.104+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:37.105+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:37.105+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:37.105+0000] {spark_submit.py:495} INFO - "name" : "Fossil Hard coal - Actual Aggregated",
[2023-01-31T12:22:37.105+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:37.105+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:37.105+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:37.105+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:37.106+0000] {spark_submit.py:495} INFO - "name" : "Fossil Oil - Actual Aggregated",
[2023-01-31T12:22:37.107+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:37.108+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:37.108+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:37.108+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:37.108+0000] {spark_submit.py:495} INFO - "name" : "Geothermal - Actual Aggregated",
[2023-01-31T12:22:37.108+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:37.108+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:37.109+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:37.109+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:37.109+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Aggregated",
[2023-01-31T12:22:37.109+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:37.109+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:37.109+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:37.109+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:37.109+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Consumption",
[2023-01-31T12:22:37.110+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:37.110+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:37.110+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:37.110+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:37.110+0000] {spark_submit.py:495} INFO - "name" : "Hydro Run-of-river and poundage - Actual Aggregated",
[2023-01-31T12:22:37.110+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:37.110+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:37.111+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:37.111+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:37.111+0000] {spark_submit.py:495} INFO - "name" : "Hydro Water Reservoir - Actual Aggregated",
[2023-01-31T12:22:37.111+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:37.111+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:37.111+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:37.112+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:37.112+0000] {spark_submit.py:495} INFO - "name" : "Nuclear - Actual Aggregated",
[2023-01-31T12:22:37.112+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:37.112+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:37.112+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:37.112+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:37.112+0000] {spark_submit.py:495} INFO - "name" : "Other - Actual Aggregated",
[2023-01-31T12:22:37.113+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:37.113+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:37.113+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:37.113+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:37.113+0000] {spark_submit.py:495} INFO - "name" : "Other renewable - Actual Aggregated",
[2023-01-31T12:22:37.113+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:37.113+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:37.114+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:37.114+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:37.114+0000] {spark_submit.py:495} INFO - "name" : "Solar - Actual Aggregated",
[2023-01-31T12:22:37.114+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:37.114+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:37.114+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:37.114+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:37.115+0000] {spark_submit.py:495} INFO - "name" : "Waste - Actual Aggregated",
[2023-01-31T12:22:37.115+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:37.115+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:37.115+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:37.115+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:37.115+0000] {spark_submit.py:495} INFO - "name" : "Wind Offshore - Actual Aggregated",
[2023-01-31T12:22:37.115+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:37.116+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:37.116+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:37.116+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T12:22:37.116+0000] {spark_submit.py:495} INFO - "name" : "Wind Onshore - Actual Aggregated",
[2023-01-31T12:22:37.116+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T12:22:37.116+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T12:22:37.116+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T12:22:37.117+0000] {spark_submit.py:495} INFO - } ]
[2023-01-31T12:22:37.117+0000] {spark_submit.py:495} INFO - }
[2023-01-31T12:22:37.117+0000] {spark_submit.py:495} INFO - and corresponding Parquet message type:
[2023-01-31T12:22:37.117+0000] {spark_submit.py:495} INFO - message spark_schema {
[2023-01-31T12:22:37.117+0000] {spark_submit.py:495} INFO - required int96 measured_at;
[2023-01-31T12:22:37.117+0000] {spark_submit.py:495} INFO - required int96 created_at;
[2023-01-31T12:22:37.117+0000] {spark_submit.py:495} INFO - optional double Biomass - Actual Aggregated;
[2023-01-31T12:22:37.117+0000] {spark_submit.py:495} INFO - optional double Fossil Brown coal/Lignite - Actual Aggregated;
[2023-01-31T12:22:37.118+0000] {spark_submit.py:495} INFO - optional double Fossil Gas - Actual Aggregated;
[2023-01-31T12:22:37.118+0000] {spark_submit.py:495} INFO - optional double Fossil Hard coal - Actual Aggregated;
[2023-01-31T12:22:37.118+0000] {spark_submit.py:495} INFO - optional double Fossil Oil - Actual Aggregated;
[2023-01-31T12:22:37.118+0000] {spark_submit.py:495} INFO - optional double Geothermal - Actual Aggregated;
[2023-01-31T12:22:37.118+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Aggregated;
[2023-01-31T12:22:37.118+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Consumption;
[2023-01-31T12:22:37.119+0000] {spark_submit.py:495} INFO - optional double Hydro Run-of-river and poundage - Actual Aggregated;
[2023-01-31T12:22:37.119+0000] {spark_submit.py:495} INFO - optional double Hydro Water Reservoir - Actual Aggregated;
[2023-01-31T12:22:37.119+0000] {spark_submit.py:495} INFO - optional double Nuclear - Actual Aggregated;
[2023-01-31T12:22:37.119+0000] {spark_submit.py:495} INFO - optional double Other - Actual Aggregated;
[2023-01-31T12:22:37.119+0000] {spark_submit.py:495} INFO - optional double Other renewable - Actual Aggregated;
[2023-01-31T12:22:37.119+0000] {spark_submit.py:495} INFO - optional double Solar - Actual Aggregated;
[2023-01-31T12:22:37.120+0000] {spark_submit.py:495} INFO - optional double Waste - Actual Aggregated;
[2023-01-31T12:22:37.120+0000] {spark_submit.py:495} INFO - optional double Wind Offshore - Actual Aggregated;
[2023-01-31T12:22:37.120+0000] {spark_submit.py:495} INFO - optional double Wind Onshore - Actual Aggregated;
[2023-01-31T12:22:37.120+0000] {spark_submit.py:495} INFO - }
[2023-01-31T12:22:37.120+0000] {spark_submit.py:495} INFO - 
[2023-01-31T12:22:37.120+0000] {spark_submit.py:495} INFO - 
[2023-01-31T12:22:37.373+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:37 INFO CodecPool: Got brand-new compressor [.snappy]
[2023-01-31T12:22:37.629+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:37 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 8124f810dec3:36903 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T12:22:39.236+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:39 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675166976081-c3a76610-54ae-4c28-bf0c-abcfa660f1e1/_temporary/0/_temporary/' directory.
[2023-01-31T12:22:39.236+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:39 INFO FileOutputCommitter: Saved output of task 'attempt_20230131122236328934217227920603_0008_m_000000_8' to gs://entsoe_temp_1009/.spark-bigquery-local-1675166976081-c3a76610-54ae-4c28-bf0c-abcfa660f1e1/_temporary/0/task_20230131122236328934217227920603_0008_m_000000
[2023-01-31T12:22:39.237+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:39 INFO SparkHadoopMapRedUtil: attempt_20230131122236328934217227920603_0008_m_000000_8: Committed. Elapsed time: 858 ms.
[2023-01-31T12:22:39.243+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:39 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2588 bytes result sent to driver
[2023-01-31T12:22:39.244+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:39 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 2294 ms on 8124f810dec3 (executor driver) (1/1)
[2023-01-31T12:22:39.244+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:39 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2023-01-31T12:22:39.244+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:39 INFO DAGScheduler: ResultStage 8 (save at BigQueryWriteHelper.java:105) finished in 2.327 s
[2023-01-31T12:22:39.245+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:39 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T12:22:39.245+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2023-01-31T12:22:39.245+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:39 INFO DAGScheduler: Job 8 finished: save at BigQueryWriteHelper.java:105, took 2.331003 s
[2023-01-31T12:22:39.247+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:39 INFO FileFormatWriter: Start to commit write Job 3566e665-a387-4f46-b631-57b5b748baf9.
[2023-01-31T12:22:39.910+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:39 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675166976081-c3a76610-54ae-4c28-bf0c-abcfa660f1e1/_temporary/0/task_20230131122236328934217227920603_0008_m_000000/' directory.
[2023-01-31T12:22:41.258+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:41 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675166976081-c3a76610-54ae-4c28-bf0c-abcfa660f1e1/' directory.
[2023-01-31T12:22:41.409+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:41 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 8124f810dec3:36903 in memory (size: 74.3 KiB, free: 434.3 MiB)
[2023-01-31T12:22:41.454+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:41 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 8124f810dec3:36903 in memory (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T12:22:41.851+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:41 INFO FileFormatWriter: Write Job 3566e665-a387-4f46-b631-57b5b748baf9 committed. Elapsed time: 2603 ms.
[2023-01-31T12:22:41.856+0000] {spark_submit.py:495} INFO - 23/01/31 12:22:41 INFO FileFormatWriter: Finished processing stats for write job 3566e665-a387-4f46-b631-57b5b748baf9.
[2023-01-31T12:22:41.909+0000] {local_task_job.py:223} WARNING - State of this instance has been externally set to up_for_retry. Terminating instance.
[2023-01-31T12:22:41.912+0000] {process_utils.py:129} INFO - Sending Signals.SIGTERM to group 668. PIDs of all processes in the group: [673, 960, 668]
[2023-01-31T12:22:41.912+0000] {process_utils.py:84} INFO - Sending the signal Signals.SIGTERM to group 668
[2023-01-31T12:22:41.912+0000] {taskinstance.py:1483} ERROR - Received SIGTERM. Terminating subprocesses.
[2023-01-31T12:22:41.913+0000] {spark_submit.py:620} INFO - Sending kill signal to spark-submit
[2023-01-31T12:22:41.926+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 414, in submit
    self._process_spark_submit_log(iter(self._submit_sp.stdout))  # type: ignore
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 463, in _process_spark_submit_log
    for line in itr:
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 1485, in signal_handler
    raise AirflowException("Task received SIGTERM signal")
airflow.exceptions.AirflowException: Task received SIGTERM signal
[2023-01-31T12:22:41.932+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=entsoe-energydata-backfill, task_id=stage_total_generation, execution_date=20210101T060000, start_date=20230131T120853, end_date=20230131T122241
[2023-01-31T12:22:41.947+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 1000 for task stage_total_generation (Task received SIGTERM signal; 668)
[2023-01-31T12:22:42.005+0000] {process_utils.py:79} INFO - Process psutil.Process(pid=673, status='terminated', started='12:08:54') (673) terminated with exit code None
[2023-01-31T12:22:42.005+0000] {process_utils.py:79} INFO - Process psutil.Process(pid=668, status='terminated', exitcode=1, started='12:08:53') (668) terminated with exit code 1
[2023-01-31T12:22:42.006+0000] {process_utils.py:79} INFO - Process psutil.Process(pid=960, status='terminated', started='12:09:17') (960) terminated with exit code None
[2023-01-31T13:03:43.057+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [queued]>
[2023-01-31T13:03:43.121+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [queued]>
[2023-01-31T13:03:43.122+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T13:03:43.123+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T13:03:43.124+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T13:03:43.179+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 06:00:00+00:00
[2023-01-31T13:03:43.194+0000] {standard_task_runner.py:55} INFO - Started process 227 to run task
[2023-01-31T13:03:43.200+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T06:00:00+00:00', '--job-id', '1029', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpis74emdt']
[2023-01-31T13:03:43.213+0000] {standard_task_runner.py:83} INFO - Job 1029: Subtask stage_total_generation
[2023-01-31T13:03:43.392+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [running]> on host b37fe3cbf330
[2023-01-31T13:03:43.733+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T06:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T06:00:00+00:00
[2023-01-31T13:03:43.781+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T13:03:43.782+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010600 202101010700 DE_TENNET 202101010000 202101010600
[2023-01-31T13:04:16.962+0000] {spark_submit.py:495} INFO - mulai periode: 202101010000
[2023-01-31T13:04:16.964+0000] {spark_submit.py:495} INFO - selesai periode: 202101010600
[2023-01-31T13:04:18.218+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:18 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T13:04:20.608+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T13:04:22.227+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:22 INFO ResourceUtils: ==============================================================
[2023-01-31T13:04:22.229+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:22 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T13:04:22.232+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:22 INFO ResourceUtils: ==============================================================
[2023-01-31T13:04:22.234+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:22 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T13:04:22.404+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:22 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T13:04:22.435+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:22 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T13:04:22.443+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:22 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T13:04:22.864+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:22 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T13:04:22.869+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:22 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T13:04:22.873+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:22 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T13:04:22.875+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:22 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T13:04:22.883+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:22 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T13:04:28.836+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:28 INFO Utils: Successfully started service 'sparkDriver' on port 34829.
[2023-01-31T13:04:30.298+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:30 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T13:04:31.085+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:31 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T13:04:31.957+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:31 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T13:04:31.982+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:31 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T13:04:32.056+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:32 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T13:04:32.297+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:32 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f00e830c-6644-4013-ac6a-a26b369ec280
[2023-01-31T13:04:32.493+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:32 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T13:04:32.765+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:32 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T13:04:35.075+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:35 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-01-31T13:04:35.410+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:35 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://b37fe3cbf330:34829/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675170258000
[2023-01-31T13:04:35.420+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:35 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://b37fe3cbf330:34829/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675170258000
[2023-01-31T13:04:36.710+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:36 INFO Executor: Starting executor ID driver on host b37fe3cbf330
[2023-01-31T13:04:37.070+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:37 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T13:04:37.575+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:37 INFO Executor: Fetching spark://b37fe3cbf330:34829/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675170258000
[2023-01-31T13:04:39.132+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:39 INFO TransportClientFactory: Successfully created connection to b37fe3cbf330/172.19.0.8:34829 after 902 ms (0 ms spent in bootstraps)
[2023-01-31T13:04:39.226+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:39 INFO Utils: Fetching spark://b37fe3cbf330:34829/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-e1437171-d55c-4105-9c19-ebea9dd1a587/userFiles-cfa397b1-ee31-4748-89fe-dabd80386d2b/fetchFileTemp4307517289469770544.tmp
[2023-01-31T13:04:45.223+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:45 INFO Executor: Adding file:/tmp/spark-e1437171-d55c-4105-9c19-ebea9dd1a587/userFiles-cfa397b1-ee31-4748-89fe-dabd80386d2b/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T13:04:45.246+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:45 INFO Executor: Fetching spark://b37fe3cbf330:34829/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675170258000
[2023-01-31T13:04:45.272+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:45 INFO Utils: Fetching spark://b37fe3cbf330:34829/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-e1437171-d55c-4105-9c19-ebea9dd1a587/userFiles-cfa397b1-ee31-4748-89fe-dabd80386d2b/fetchFileTemp5204228240187704032.tmp
[2023-01-31T13:04:48.454+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:48 INFO Executor: Adding file:/tmp/spark-e1437171-d55c-4105-9c19-ebea9dd1a587/userFiles-cfa397b1-ee31-4748-89fe-dabd80386d2b/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T13:04:48.851+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:48 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41543.
[2023-01-31T13:04:48.852+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:48 INFO NettyBlockTransferService: Server created on b37fe3cbf330:41543
[2023-01-31T13:04:48.894+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:48 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T13:04:48.959+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:48 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, b37fe3cbf330, 41543, None)
[2023-01-31T13:04:49.026+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:48 INFO Executor: Told to re-register on heartbeat
[2023-01-31T13:04:49.028+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:49 INFO BlockManagerMasterEndpoint: Registering block manager b37fe3cbf330:41543 with 434.4 MiB RAM, BlockManagerId(driver, b37fe3cbf330, 41543, None)
[2023-01-31T13:04:49.044+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:49 INFO BlockManager: BlockManager null re-registering with master
[2023-01-31T13:04:49.046+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:49 INFO BlockManagerMaster: Registering BlockManager null
[2023-01-31T13:04:49.118+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:49 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, b37fe3cbf330, 41543, None)
[2023-01-31T13:04:49.126+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:49 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, b37fe3cbf330, 41543, None)
[2023-01-31T13:04:49.207+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:49 WARN Executor: Issue communicating with driver in heartbeater
[2023-01-31T13:04:49.208+0000] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-01-31T13:04:49.208+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-01-31T13:04:49.210+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-01-31T13:04:49.210+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2023-01-31T13:04:49.210+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2023-01-31T13:04:49.211+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:79)
[2023-01-31T13:04:49.211+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:643)
[2023-01-31T13:04:49.212+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2023-01-31T13:04:49.212+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2023-01-31T13:04:49.212+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
[2023-01-31T13:04:49.212+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2023-01-31T13:04:49.213+0000] {spark_submit.py:495} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2023-01-31T13:04:49.213+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-01-31T13:04:49.213+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2023-01-31T13:04:49.214+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2023-01-31T13:04:49.270+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-31T13:04:49.271+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-31T13:04:49.271+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-31T13:04:49.272+0000] {spark_submit.py:495} INFO - Caused by: java.lang.NullPointerException
[2023-01-31T13:04:49.272+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)
[2023-01-31T13:04:49.273+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)
[2023-01-31T13:04:49.273+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-01-31T13:04:49.273+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-01-31T13:04:49.274+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-01-31T13:04:49.274+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-01-31T13:04:49.329+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-01-31T13:04:49.330+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-01-31T13:04:49.331+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-01-31T13:04:49.331+0000] {spark_submit.py:495} INFO - ... 3 more
[2023-01-31T13:04:49.331+0000] {spark_submit.py:495} INFO - 23/01/31 13:04:49 ERROR Inbox: Ignoring error
[2023-01-31T13:04:49.332+0000] {spark_submit.py:495} INFO - java.lang.NullPointerException
[2023-01-31T13:04:49.332+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)
[2023-01-31T13:04:49.333+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)
[2023-01-31T13:04:49.333+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-01-31T13:04:49.333+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-01-31T13:04:49.334+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-01-31T13:04:49.334+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-01-31T13:04:49.335+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-01-31T13:04:49.335+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-01-31T13:04:49.335+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-01-31T13:04:49.336+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-31T13:04:49.336+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-31T13:04:49.337+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-31T13:05:03.062+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:03 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T13:05:03.358+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:03 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T13:05:30.559+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:30 INFO InMemoryFileIndex: It took 292 ms to list leaf files for 1 paths.
[2023-01-31T13:05:30.936+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:30 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T13:05:31.382+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:31 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T13:05:31.390+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:31 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on b37fe3cbf330:41543 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T13:05:31.458+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:31 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T13:05:35.465+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:35 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T13:05:35.934+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:35 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T13:05:36.766+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:36 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T13:05:37.049+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:37 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T13:05:37.051+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:37 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T13:05:37.053+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:37 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T13:05:37.060+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:37 INFO DAGScheduler: Missing parents: List()
[2023-01-31T13:05:37.098+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:37 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T13:05:38.068+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:38 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T13:05:38.126+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:38 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T13:05:38.142+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:38 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on b37fe3cbf330:41543 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T13:05:38.155+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:38 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T13:05:38.585+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T13:05:38.586+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:38 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T13:05:39.896+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:39 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T13:05:40.060+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:40 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T13:05:42.178+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:42 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010600__202101010700.json:0+9371
[2023-01-31T13:05:45.616+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:45 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3480 bytes result sent to driver
[2023-01-31T13:05:45.881+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:45 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 6467 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T13:05:45.933+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:45 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T13:05:45.970+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:45 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 8.508 s
[2023-01-31T13:05:46.027+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:46 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T13:05:46.034+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T13:05:46.060+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:46 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 9.291058 s
[2023-01-31T13:05:57.132+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:57 INFO BlockManagerInfo: Removed broadcast_1_piece0 on b37fe3cbf330:41543 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T13:05:57.226+0000] {spark_submit.py:495} INFO - 23/01/31 13:05:57 INFO BlockManagerInfo: Removed broadcast_0_piece0 on b37fe3cbf330:41543 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T13:06:21.054+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:21 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T13:06:21.089+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:21 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T13:06:21.135+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:21 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T13:06:24.019+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO CodeGenerator: Code generated in 1498.545649 ms
[2023-01-31T13:06:24.049+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T13:06:24.128+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T13:06:24.130+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on b37fe3cbf330:41543 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T13:06:24.133+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T13:06:24.182+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T13:06:24.673+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T13:06:24.680+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T13:06:24.680+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T13:06:24.681+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T13:06:24.681+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO DAGScheduler: Missing parents: List()
[2023-01-31T13:06:24.695+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T13:06:24.761+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T13:06:24.797+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T13:06:24.802+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on b37fe3cbf330:41543 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T13:06:24.806+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T13:06:24.810+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T13:06:24.811+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T13:06:24.836+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T13:06:24.840+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:24 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T13:06:25.456+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:25 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T13:06:26.077+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO CodeGenerator: Code generated in 488.472135 ms
[2023-01-31T13:06:26.541+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1628 bytes result sent to driver
[2023-01-31T13:06:26.555+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1734 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T13:06:26.559+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 1.825 s
[2023-01-31T13:06:26.560+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T13:06:26.561+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T13:06:26.564+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T13:06:26.571+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 1.893884 s
[2023-01-31T13:06:26.825+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T13:06:26.826+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T13:06:26.828+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T13:06:26.940+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO CodeGenerator: Code generated in 71.294155 ms
[2023-01-31T13:06:26.980+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:26 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T13:06:27.027+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T13:06:27.035+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on b37fe3cbf330:41543 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T13:06:27.041+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T13:06:27.050+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T13:06:27.191+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T13:06:27.197+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T13:06:27.197+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T13:06:27.198+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T13:06:27.198+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO DAGScheduler: Missing parents: List()
[2023-01-31T13:06:27.204+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T13:06:27.256+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T13:06:27.312+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T13:06:27.318+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on b37fe3cbf330:41543 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T13:06:27.320+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T13:06:27.338+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T13:06:27.340+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T13:06:27.359+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T13:06:27.374+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T13:06:27.494+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:27 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T13:06:28.016+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1628 bytes result sent to driver
[2023-01-31T13:06:28.037+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 677 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T13:06:28.039+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T13:06:28.046+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 0.817 s
[2023-01-31T13:06:28.047+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T13:06:28.047+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T13:06:28.047+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 0.854658 s
[2023-01-31T13:06:28.902+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T13:06:28.908+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T13:06:28.913+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:28 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T13:06:29.532+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO CodeGenerator: Code generated in 393.710611 ms
[2023-01-31T13:06:29.574+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T13:06:29.626+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T13:06:29.626+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on b37fe3cbf330:41543 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T13:06:29.627+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T13:06:29.627+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T13:06:29.778+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T13:06:29.781+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T13:06:29.781+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T13:06:29.781+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T13:06:29.782+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO DAGScheduler: Missing parents: List()
[2023-01-31T13:06:29.794+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T13:06:29.805+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.6 MiB)
[2023-01-31T13:06:29.818+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.6 MiB)
[2023-01-31T13:06:29.820+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on b37fe3cbf330:41543 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T13:06:29.823+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T13:06:29.835+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T13:06:29.836+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T13:06:29.841+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T13:06:29.842+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T13:06:29.906+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:29 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T13:06:30.050+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-01-31T13:06:30.064+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 224 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T13:06:30.066+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 0.279 s
[2023-01-31T13:06:30.066+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T13:06:30.067+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T13:06:30.067+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T13:06:30.078+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 0.291997 s
[2023-01-31T13:06:30.228+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T13:06:30.228+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T13:06:30.229+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T13:06:30.558+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO CodeGenerator: Code generated in 225.380948 ms
[2023-01-31T13:06:30.578+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.4 MiB)
[2023-01-31T13:06:30.629+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T13:06:30.633+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on b37fe3cbf330:41543 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T13:06:30.634+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T13:06:30.636+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T13:06:30.777+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T13:06:30.779+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T13:06:30.780+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T13:06:30.780+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T13:06:30.780+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO DAGScheduler: Missing parents: List()
[2023-01-31T13:06:30.784+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T13:06:30.808+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-31T13:06:30.826+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-01-31T13:06:30.828+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on b37fe3cbf330:41543 (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T13:06:30.832+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T13:06:30.841+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T13:06:30.843+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T13:06:30.846+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T13:06:30.853+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T13:06:30.920+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:30 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T13:06:31.095+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:31 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1585 bytes result sent to driver
[2023-01-31T13:06:31.261+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:31 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 415 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T13:06:31.261+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:31 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T13:06:31.267+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:31 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 0.474 s
[2023-01-31T13:06:31.268+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:31 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T13:06:31.268+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T13:06:31.268+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:31 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 0.489725 s
[2023-01-31T13:06:31.290+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:31 INFO BlockManagerInfo: Removed broadcast_5_piece0 on b37fe3cbf330:41543 in memory (size: 7.7 KiB, free: 434.2 MiB)
[2023-01-31T13:06:31.344+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:31 INFO BlockManagerInfo: Removed broadcast_7_piece0 on b37fe3cbf330:41543 in memory (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T13:06:31.436+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:31 INFO BlockManagerInfo: Removed broadcast_3_piece0 on b37fe3cbf330:41543 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T13:06:32.676+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:32 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T13:06:32.695+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:32 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T13:06:32.697+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:32 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T13:06:33.422+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:33 INFO CodeGenerator: Code generated in 464.927155 ms
[2023-01-31T13:06:33.456+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:33 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.3 MiB)
[2023-01-31T13:06:33.659+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:33 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T13:06:33.660+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:33 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on b37fe3cbf330:41543 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T13:06:33.671+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:33 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T13:06:33.681+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T13:06:33.926+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:33 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T13:06:33.926+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:33 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T13:06:33.927+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:33 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T13:06:33.927+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:33 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T13:06:33.927+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:33 INFO DAGScheduler: Missing parents: List()
[2023-01-31T13:06:33.934+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:33 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T13:06:33.981+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:33 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.2 MiB)
[2023-01-31T13:06:34.001+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:33 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.2 MiB)
[2023-01-31T13:06:34.004+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on b37fe3cbf330:41543 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T13:06:34.011+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T13:06:34.020+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T13:06:34.024+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T13:06:34.031+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T13:06:34.034+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T13:06:34.082+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T13:06:34.334+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2499 bytes result sent to driver
[2023-01-31T13:06:34.338+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 304 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T13:06:34.339+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T13:06:34.339+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 0.406 s
[2023-01-31T13:06:34.342+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T13:06:34.343+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T13:06:34.343+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 0.416040 s
[2023-01-31T13:06:34.664+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T13:06:34.665+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T13:06:34.666+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:34 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T13:06:35.311+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO CodeGenerator: Code generated in 362.193857 ms
[2023-01-31T13:06:35.345+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 433.0 MiB)
[2023-01-31T13:06:35.439+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.9 MiB)
[2023-01-31T13:06:35.443+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on b37fe3cbf330:41543 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T13:06:35.454+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T13:06:35.464+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T13:06:35.652+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T13:06:35.653+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T13:06:35.653+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T13:06:35.653+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T13:06:35.653+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO DAGScheduler: Missing parents: List()
[2023-01-31T13:06:35.664+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T13:06:35.683+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 432.9 MiB)
[2023-01-31T13:06:35.728+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.9 MiB)
[2023-01-31T13:06:35.729+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on b37fe3cbf330:41543 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T13:06:35.730+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T13:06:35.751+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T13:06:35.751+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T13:06:35.754+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T13:06:35.758+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T13:06:35.795+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:35 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T13:06:36.012+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:36 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-01-31T13:06:36.020+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:36 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 267 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T13:06:36.021+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:36 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T13:06:36.024+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:36 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 0.363 s
[2023-01-31T13:06:36.029+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:36 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T13:06:36.029+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T13:06:36.029+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:36 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 0.375015 s
[2023-01-31T13:06:36.354+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:36 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T13:06:36.355+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:36 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T13:06:36.363+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:36 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T13:06:37.029+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO CodeGenerator: Code generated in 292.461027 ms
[2023-01-31T13:06:37.061+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.7 MiB)
[2023-01-31T13:06:37.161+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.7 MiB)
[2023-01-31T13:06:37.162+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on b37fe3cbf330:41543 (size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T13:06:37.169+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:204
[2023-01-31T13:06:37.177+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203675 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T13:06:37.306+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:204
[2023-01-31T13:06:37.309+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:204) with 1 output partitions
[2023-01-31T13:06:37.309+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:204)
[2023-01-31T13:06:37.309+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T13:06:37.310+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO DAGScheduler: Missing parents: List()
[2023-01-31T13:06:37.313+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:204), which has no missing parents
[2023-01-31T13:06:37.319+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.6 MiB)
[2023-01-31T13:06:37.325+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.6 MiB)
[2023-01-31T13:06:37.331+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on b37fe3cbf330:41543 (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T13:06:37.333+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T13:06:37.335+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:204) (first 15 tasks are for partitions Vector(0))
[2023-01-31T13:06:37.336+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T13:06:37.338+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T13:06:37.340+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T13:06:37.363+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010600__202101010700.json, range: 0-9371, partition values: [empty row]
[2023-01-31T13:06:37.500+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1729 bytes result sent to driver
[2023-01-31T13:06:37.505+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 165 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T13:06:37.505+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T13:06:37.506+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:204) finished in 0.192 s
[2023-01-31T13:06:37.506+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T13:06:37.506+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T13:06:37.506+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:37 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:204, took 0.198604 s
[2023-01-31T13:06:41.238+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:41 INFO BlockManagerInfo: Removed broadcast_4_piece0 on b37fe3cbf330:41543 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T13:06:41.301+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:41 INFO BlockManagerInfo: Removed broadcast_13_piece0 on b37fe3cbf330:41543 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T13:06:41.409+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:41 INFO BlockManagerInfo: Removed broadcast_12_piece0 on b37fe3cbf330:41543 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T13:06:41.464+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:41 INFO BlockManagerInfo: Removed broadcast_2_piece0 on b37fe3cbf330:41543 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T13:06:41.510+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:41 INFO BlockManagerInfo: Removed broadcast_9_piece0 on b37fe3cbf330:41543 in memory (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T13:06:41.574+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:41 INFO BlockManagerInfo: Removed broadcast_15_piece0 on b37fe3cbf330:41543 in memory (size: 12.6 KiB, free: 434.3 MiB)
[2023-01-31T13:06:41.647+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:41 INFO BlockManagerInfo: Removed broadcast_11_piece0 on b37fe3cbf330:41543 in memory (size: 13.4 KiB, free: 434.3 MiB)
[2023-01-31T13:06:41.684+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:41 INFO BlockManagerInfo: Removed broadcast_14_piece0 on b37fe3cbf330:41543 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T13:06:41.740+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:41 INFO BlockManagerInfo: Removed broadcast_6_piece0 on b37fe3cbf330:41543 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T13:06:49.622+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:49 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T13:06:49.719+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T13:06:49.725+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T13:06:49.726+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:49 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T13:06:49.726+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T13:06:49.726+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T13:06:49.743+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T13:06:50.530+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-31T13:06:50.530+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO DAGScheduler: Got job 8 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T13:06:50.531+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO DAGScheduler: Final stage: ResultStage 8 (save at BigQueryWriteHelper.java:105)
[2023-01-31T13:06:50.531+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T13:06:50.531+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO DAGScheduler: Missing parents: List()
[2023-01-31T13:06:50.536+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO DAGScheduler: Submitting ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T13:06:50.754+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 209.9 KiB, free 433.7 MiB)
[2023-01-31T13:06:50.759+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 74.3 KiB, free 433.7 MiB)
[2023-01-31T13:06:50.759+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on b37fe3cbf330:41543 (size: 74.3 KiB, free: 434.3 MiB)
[2023-01-31T13:06:50.762+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513
[2023-01-31T13:06:50.762+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T13:06:50.762+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2023-01-31T13:06:50.782+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (b37fe3cbf330, executor driver, partition 0, PROCESS_LOCAL, 7484 bytes) taskResourceAssignments Map()
[2023-01-31T13:06:50.788+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:50 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2023-01-31T13:06:51.199+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T13:06:51.199+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T13:06:51.201+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T13:06:51.202+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T13:06:51.202+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T13:06:51.205+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T13:06:51.219+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T13:06:51.228+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T13:06:51.331+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO ParquetOutputFormat: Parquet block size to 134217728
[2023-01-31T13:06:51.331+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO ParquetOutputFormat: Validation is off
[2023-01-31T13:06:51.342+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-01-31T13:06:51.343+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO ParquetOutputFormat: Parquet properties are:
[2023-01-31T13:06:51.343+0000] {spark_submit.py:495} INFO - Parquet page size to 1048576
[2023-01-31T13:06:51.343+0000] {spark_submit.py:495} INFO - Parquet dictionary page size to 1048576
[2023-01-31T13:06:51.343+0000] {spark_submit.py:495} INFO - Dictionary is true
[2023-01-31T13:06:51.345+0000] {spark_submit.py:495} INFO - Writer version is: PARQUET_1_0
[2023-01-31T13:06:51.345+0000] {spark_submit.py:495} INFO - Page size checking is: estimated
[2023-01-31T13:06:51.346+0000] {spark_submit.py:495} INFO - Min row count for page size check is: 100
[2023-01-31T13:06:51.346+0000] {spark_submit.py:495} INFO - Max row count for page size check is: 10000
[2023-01-31T13:06:51.346+0000] {spark_submit.py:495} INFO - Truncate length for column indexes is: 64
[2023-01-31T13:06:51.347+0000] {spark_submit.py:495} INFO - Truncate length for statistics min/max  is: 2147483647
[2023-01-31T13:06:51.347+0000] {spark_submit.py:495} INFO - Bloom filter enabled: false
[2023-01-31T13:06:51.348+0000] {spark_submit.py:495} INFO - Max Bloom filter size for a column is 1048576
[2023-01-31T13:06:51.350+0000] {spark_submit.py:495} INFO - Bloom filter expected number of distinct values are: null
[2023-01-31T13:06:51.353+0000] {spark_submit.py:495} INFO - Page row count limit to 20000
[2023-01-31T13:06:51.353+0000] {spark_submit.py:495} INFO - Writing page checksums is: on
[2023-01-31T13:06:51.427+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:51 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-01-31T13:06:51.428+0000] {spark_submit.py:495} INFO - {
[2023-01-31T13:06:51.428+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-31T13:06:51.428+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-31T13:06:51.428+0000] {spark_submit.py:495} INFO - "name" : "measured_at",
[2023-01-31T13:06:51.428+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T13:06:51.429+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T13:06:51.429+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.429+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.429+0000] {spark_submit.py:495} INFO - "name" : "created_at",
[2023-01-31T13:06:51.429+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T13:06:51.436+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T13:06:51.436+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.449+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.472+0000] {spark_submit.py:495} INFO - "name" : "Biomass - Actual Aggregated",
[2023-01-31T13:06:51.475+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.475+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.475+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.476+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.476+0000] {spark_submit.py:495} INFO - "name" : "Fossil Brown coal/Lignite - Actual Aggregated",
[2023-01-31T13:06:51.476+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.476+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.476+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.476+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.476+0000] {spark_submit.py:495} INFO - "name" : "Fossil Gas - Actual Aggregated",
[2023-01-31T13:06:51.477+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.477+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.477+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.477+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.477+0000] {spark_submit.py:495} INFO - "name" : "Fossil Hard coal - Actual Aggregated",
[2023-01-31T13:06:51.477+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.477+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.478+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.478+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.478+0000] {spark_submit.py:495} INFO - "name" : "Fossil Oil - Actual Aggregated",
[2023-01-31T13:06:51.484+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.484+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.484+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.485+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.485+0000] {spark_submit.py:495} INFO - "name" : "Geothermal - Actual Aggregated",
[2023-01-31T13:06:51.488+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.490+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.491+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.491+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.491+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Aggregated",
[2023-01-31T13:06:51.492+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.494+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.494+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.496+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.498+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Consumption",
[2023-01-31T13:06:51.499+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.502+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.503+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.504+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.504+0000] {spark_submit.py:495} INFO - "name" : "Hydro Run-of-river and poundage - Actual Aggregated",
[2023-01-31T13:06:51.505+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.506+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.507+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.508+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.508+0000] {spark_submit.py:495} INFO - "name" : "Hydro Water Reservoir - Actual Aggregated",
[2023-01-31T13:06:51.508+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.509+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.509+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.510+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.510+0000] {spark_submit.py:495} INFO - "name" : "Nuclear - Actual Aggregated",
[2023-01-31T13:06:51.511+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.511+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.514+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.515+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.515+0000] {spark_submit.py:495} INFO - "name" : "Other - Actual Aggregated",
[2023-01-31T13:06:51.520+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.521+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.522+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.523+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.523+0000] {spark_submit.py:495} INFO - "name" : "Other renewable - Actual Aggregated",
[2023-01-31T13:06:51.524+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.524+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.525+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.526+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.527+0000] {spark_submit.py:495} INFO - "name" : "Solar - Actual Aggregated",
[2023-01-31T13:06:51.527+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.528+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.528+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.529+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.529+0000] {spark_submit.py:495} INFO - "name" : "Waste - Actual Aggregated",
[2023-01-31T13:06:51.530+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.530+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.531+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.531+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.531+0000] {spark_submit.py:495} INFO - "name" : "Wind Offshore - Actual Aggregated",
[2023-01-31T13:06:51.533+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.533+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.533+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.534+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T13:06:51.534+0000] {spark_submit.py:495} INFO - "name" : "Wind Onshore - Actual Aggregated",
[2023-01-31T13:06:51.535+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T13:06:51.537+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T13:06:51.537+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T13:06:51.537+0000] {spark_submit.py:495} INFO - } ]
[2023-01-31T13:06:51.538+0000] {spark_submit.py:495} INFO - }
[2023-01-31T13:06:51.538+0000] {spark_submit.py:495} INFO - and corresponding Parquet message type:
[2023-01-31T13:06:51.539+0000] {spark_submit.py:495} INFO - message spark_schema {
[2023-01-31T13:06:51.539+0000] {spark_submit.py:495} INFO - required int96 measured_at;
[2023-01-31T13:06:51.540+0000] {spark_submit.py:495} INFO - required int96 created_at;
[2023-01-31T13:06:51.540+0000] {spark_submit.py:495} INFO - optional double Biomass - Actual Aggregated;
[2023-01-31T13:06:51.541+0000] {spark_submit.py:495} INFO - optional double Fossil Brown coal/Lignite - Actual Aggregated;
[2023-01-31T13:06:51.541+0000] {spark_submit.py:495} INFO - optional double Fossil Gas - Actual Aggregated;
[2023-01-31T13:06:51.544+0000] {spark_submit.py:495} INFO - optional double Fossil Hard coal - Actual Aggregated;
[2023-01-31T13:06:51.545+0000] {spark_submit.py:495} INFO - optional double Fossil Oil - Actual Aggregated;
[2023-01-31T13:06:51.545+0000] {spark_submit.py:495} INFO - optional double Geothermal - Actual Aggregated;
[2023-01-31T13:06:51.547+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Aggregated;
[2023-01-31T13:06:51.548+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Consumption;
[2023-01-31T13:06:51.548+0000] {spark_submit.py:495} INFO - optional double Hydro Run-of-river and poundage - Actual Aggregated;
[2023-01-31T13:06:51.548+0000] {spark_submit.py:495} INFO - optional double Hydro Water Reservoir - Actual Aggregated;
[2023-01-31T13:06:51.549+0000] {spark_submit.py:495} INFO - optional double Nuclear - Actual Aggregated;
[2023-01-31T13:06:51.549+0000] {spark_submit.py:495} INFO - optional double Other - Actual Aggregated;
[2023-01-31T13:06:51.549+0000] {spark_submit.py:495} INFO - optional double Other renewable - Actual Aggregated;
[2023-01-31T13:06:51.549+0000] {spark_submit.py:495} INFO - optional double Solar - Actual Aggregated;
[2023-01-31T13:06:51.550+0000] {spark_submit.py:495} INFO - optional double Waste - Actual Aggregated;
[2023-01-31T13:06:51.550+0000] {spark_submit.py:495} INFO - optional double Wind Offshore - Actual Aggregated;
[2023-01-31T13:06:51.553+0000] {spark_submit.py:495} INFO - optional double Wind Onshore - Actual Aggregated;
[2023-01-31T13:06:51.554+0000] {spark_submit.py:495} INFO - }
[2023-01-31T13:06:51.554+0000] {spark_submit.py:495} INFO - 
[2023-01-31T13:06:51.554+0000] {spark_submit.py:495} INFO - 
[2023-01-31T13:06:52.080+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:52 INFO CodecPool: Got brand-new compressor [.snappy]
[2023-01-31T13:06:56.631+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:56 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675170276333-b447705c-2eab-43e4-b856-492fc35b0eeb/_temporary/0/_temporary/' directory.
[2023-01-31T13:06:56.631+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:56 INFO FileOutputCommitter: Saved output of task 'attempt_202301311306507982672693499282250_0008_m_000000_8' to gs://entsoe_temp_1009/.spark-bigquery-local-1675170276333-b447705c-2eab-43e4-b856-492fc35b0eeb/_temporary/0/task_202301311306507982672693499282250_0008_m_000000
[2023-01-31T13:06:56.632+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:56 INFO SparkHadoopMapRedUtil: attempt_202301311306507982672693499282250_0008_m_000000_8: Committed. Elapsed time: 1611 ms.
[2023-01-31T13:06:56.648+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:56 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2588 bytes result sent to driver
[2023-01-31T13:06:56.650+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:56 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 5887 ms on b37fe3cbf330 (executor driver) (1/1)
[2023-01-31T13:06:56.651+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:56 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2023-01-31T13:06:56.654+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:56 INFO DAGScheduler: ResultStage 8 (save at BigQueryWriteHelper.java:105) finished in 6.114 s
[2023-01-31T13:06:56.654+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:56 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T13:06:56.654+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2023-01-31T13:06:56.654+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:56 INFO DAGScheduler: Job 8 finished: save at BigQueryWriteHelper.java:105, took 6.124829 s
[2023-01-31T13:06:56.655+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:56 INFO FileFormatWriter: Start to commit write Job 14c06971-6c49-445b-9ace-fce2abe2c2fe.
[2023-01-31T13:06:57.798+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:57 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675170276333-b447705c-2eab-43e4-b856-492fc35b0eeb/_temporary/0/task_202301311306507982672693499282250_0008_m_000000/' directory.
[2023-01-31T13:06:58.096+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:58 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675170276333-b447705c-2eab-43e4-b856-492fc35b0eeb/' directory.
[2023-01-31T13:06:58.249+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:58 INFO BlockManagerInfo: Removed broadcast_16_piece0 on b37fe3cbf330:41543 in memory (size: 74.3 KiB, free: 434.3 MiB)
[2023-01-31T13:06:58.328+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:58 INFO BlockManagerInfo: Removed broadcast_8_piece0 on b37fe3cbf330:41543 in memory (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T13:06:58.860+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:58 INFO FileFormatWriter: Write Job 14c06971-6c49-445b-9ace-fce2abe2c2fe committed. Elapsed time: 2203 ms.
[2023-01-31T13:06:58.863+0000] {spark_submit.py:495} INFO - 23/01/31 13:06:58 INFO FileFormatWriter: Finished processing stats for write job 14c06971-6c49-445b-9ace-fce2abe2c2fe.
[2023-01-31T13:07:00.866+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:00 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=entsoe_analytics_1009, projectId=rafzul-analytics-1009, tableId=TEST_total_generation_staging}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=null, ignoreUnknownValue=null, sourceUris=[gs://entsoe_temp_1009/.spark-bigquery-local-1675170276333-b447705c-2eab-43e4-b856-492fc35b0eeb/part-00000-d658df12-be8f-4c4d-a940-1d3f0a576451-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=true, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=rafzul-analytics-1009, job=922eaaa0-dc35-4a57-aa05-a208751a0cc1, location=US}
[2023-01-31T13:07:06.025+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:06 INFO BigQueryClient: Done loading to rafzul-analytics-1009.entsoe_analytics_1009.TEST_total_generation_staging. jobId: JobId{project=rafzul-analytics-1009, job=922eaaa0-dc35-4a57-aa05-a208751a0cc1, location=US}
[2023-01-31T13:07:06.995+0000] {spark_submit.py:495} INFO - MANTEP!!!!
[2023-01-31T13:07:07.510+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:07 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-31T13:07:07.587+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:07 INFO SparkUI: Stopped Spark web UI at http://b37fe3cbf330:4040
[2023-01-31T13:07:07.680+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:07 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-31T13:07:07.916+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:07 INFO MemoryStore: MemoryStore cleared
[2023-01-31T13:07:07.930+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:07 INFO BlockManager: BlockManager stopped
[2023-01-31T13:07:07.954+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:07 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-31T13:07:07.980+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:07 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-31T13:07:08.126+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:08 INFO SparkContext: Successfully stopped SparkContext
[2023-01-31T13:07:08.134+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:08 INFO ShutdownHookManager: Shutdown hook called
[2023-01-31T13:07:08.157+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:08 INFO ShutdownHookManager: Deleting directory /tmp/spark-abcabe63-9517-496b-a92c-1a4af2224a91
[2023-01-31T13:07:08.204+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:08 INFO ShutdownHookManager: Deleting directory /tmp/spark-e1437171-d55c-4105-9c19-ebea9dd1a587
[2023-01-31T13:07:08.228+0000] {spark_submit.py:495} INFO - 23/01/31 13:07:08 INFO ShutdownHookManager: Deleting directory /tmp/spark-e1437171-d55c-4105-9c19-ebea9dd1a587/pyspark-0b0b4dc1-de45-4065-bb8b-062f61b6598f
[2023-01-31T13:07:08.587+0000] {taskinstance.py:1322} INFO - Marking task as SUCCESS. dag_id=entsoe-energydata-backfill, task_id=stage_total_generation, execution_date=20210101T060000, start_date=20230131T130343, end_date=20230131T130708
[2023-01-31T13:07:08.669+0000] {local_task_job.py:159} INFO - Task exited with return code 0
[2023-01-31T13:07:08.725+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T14:09:44.043+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [queued]>
[2023-01-31T14:09:44.072+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [queued]>
[2023-01-31T14:09:44.074+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T14:09:44.075+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T14:09:44.076+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T14:09:44.125+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 06:00:00+00:00
[2023-01-31T14:09:44.141+0000] {standard_task_runner.py:55} INFO - Started process 6107 to run task
[2023-01-31T14:09:44.172+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T06:00:00+00:00', '--job-id', '1043', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpdd_xhif8']
[2023-01-31T14:09:44.186+0000] {standard_task_runner.py:83} INFO - Job 1043: Subtask stage_total_generation
[2023-01-31T14:09:44.429+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [running]> on host b37fe3cbf330
[2023-01-31T14:09:44.832+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T06:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T06:00:00+00:00
[2023-01-31T14:09:44.887+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T14:09:44.893+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 398, in submit
    spark_submit_cmd = self._build_spark_submit_command(application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 344, in _build_spark_submit_command
    self.log.info("Spark-Submit cmd: %s", self._mask_cmd(connection_cmd))
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 256, in _mask_cmd
    " ".join(connection_cmd),
TypeError: sequence item 10: expected str instance, DateTime found
[2023-01-31T14:09:44.913+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=entsoe-energydata-backfill, task_id=stage_total_generation, execution_date=20210101T060000, start_date=20230131T140944, end_date=20230131T140944
[2023-01-31T14:09:44.978+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 1043 for task stage_total_generation (sequence item 10: expected str instance, DateTime found; 6107)
[2023-01-31T14:09:45.031+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-31T14:09:45.176+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T14:10:53.494+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [queued]>
[2023-01-31T14:10:53.553+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [queued]>
[2023-01-31T14:10:53.554+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T14:10:53.554+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T14:10:53.555+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T14:10:53.624+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 06:00:00+00:00
[2023-01-31T14:10:53.654+0000] {standard_task_runner.py:55} INFO - Started process 6210 to run task
[2023-01-31T14:10:53.672+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata-backfill', 'stage_total_generation', 'scheduled__2021-01-01T06:00:00+00:00', '--job-id', '1056', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp_rxnxc4b']
[2023-01-31T14:10:53.684+0000] {standard_task_runner.py:83} INFO - Job 1056: Subtask stage_total_generation
[2023-01-31T14:10:54.040+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata-backfill.stage_total_generation scheduled__2021-01-01T06:00:00+00:00 [running]> on host b37fe3cbf330
[2023-01-31T14:10:54.429+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata-backfill
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T06:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T06:00:00+00:00
[2023-01-31T14:10:54.479+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T14:10:54.487+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 398, in submit
    spark_submit_cmd = self._build_spark_submit_command(application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 344, in _build_spark_submit_command
    self.log.info("Spark-Submit cmd: %s", self._mask_cmd(connection_cmd))
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 256, in _mask_cmd
    " ".join(connection_cmd),
TypeError: sequence item 10: expected str instance, DateTime found
[2023-01-31T14:10:54.529+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=entsoe-energydata-backfill, task_id=stage_total_generation, execution_date=20210101T060000, start_date=20230131T141053, end_date=20230131T141054
[2023-01-31T14:10:54.615+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 1056 for task stage_total_generation (sequence item 10: expected str instance, DateTime found; 6210)
[2023-01-31T14:10:54.680+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-31T14:10:54.771+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
