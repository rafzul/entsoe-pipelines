[2023-01-20T16:16:53.917+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T16:16:53.930+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T16:16:53.930+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T16:16:53.930+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-20T16:16:53.930+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T16:16:53.941+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-20T16:16:53.951+0000] {standard_task_runner.py:55} INFO - Started process 197 to run task
[2023-01-20T16:16:53.956+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '61', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpueq872vm']
[2023-01-20T16:16:53.959+0000] {standard_task_runner.py:83} INFO - Job 61: Subtask transform_stage_generation
[2023-01-20T16:16:54.025+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T16:16:54.112+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-20T16:16:54.120+0000] {spark_submit.py:222} INFO - Could not load connection string spark_default, defaulting to yarn
[2023-01-20T16:16:54.121+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master yarn --name arrow-spark /opt/***/plugins/scripts/transform_stage.py total_generation 202101010000 202101010100 Europe/Berlin DE_TENNET
[2023-01-20T16:16:54.127+0000] {spark_submit.py:495} INFO - /usr/local/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit: line 27: /opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-class: No such file or directory
[2023-01-20T16:16:54.138+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master yarn --name arrow-spark /opt/***/plugins/scripts/transform_stage.py total_generation 202101010000 202101010100 Europe/Berlin DE_TENNET. Error code is: 127.
[2023-01-20T16:16:54.142+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230120T161653, end_date=20230120T161654
[2023-01-20T16:16:54.155+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 61 for task transform_stage_generation (Cannot execute: spark-submit --master yarn --name arrow-spark /opt/***/plugins/scripts/transform_stage.py total_generation 202101010000 202101010100 Europe/Berlin DE_TENNET. Error code is: 127.; 197)
[2023-01-20T16:16:54.207+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T16:16:54.228+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T16:19:06.401+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T16:19:06.413+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T16:19:06.414+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T16:19:06.414+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-20T16:19:06.414+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T16:19:06.428+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-20T16:19:06.437+0000] {standard_task_runner.py:55} INFO - Started process 337 to run task
[2023-01-20T16:19:06.442+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '65', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp15kc1xg3']
[2023-01-20T16:19:06.445+0000] {standard_task_runner.py:83} INFO - Job 65: Subtask transform_stage_generation
[2023-01-20T16:19:06.518+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T16:19:06.594+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-20T16:19:06.605+0000] {spark_submit.py:222} INFO - Could not load connection string spark_default, defaulting to yarn
[2023-01-20T16:19:06.606+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master yarn --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 Europe/Berlin DE_TENNET
[2023-01-20T16:19:06.612+0000] {spark_submit.py:495} INFO - /usr/local/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit: line 27: /opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-class: No such file or directory
[2023-01-20T16:19:06.624+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master yarn --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 Europe/Berlin DE_TENNET. Error code is: 127.
[2023-01-20T16:19:06.628+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230120T161906, end_date=20230120T161906
[2023-01-20T16:19:06.643+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 65 for task transform_stage_generation (Cannot execute: spark-submit --master yarn --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 Europe/Berlin DE_TENNET. Error code is: 127.; 337)
[2023-01-20T16:19:06.694+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T16:19:06.717+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T16:22:27.122+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T16:22:27.156+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T16:22:27.157+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T16:22:27.158+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-20T16:22:27.159+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T16:22:27.197+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-20T16:22:27.228+0000] {standard_task_runner.py:55} INFO - Started process 552 to run task
[2023-01-20T16:22:27.246+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '68', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp0horvmzi']
[2023-01-20T16:22:27.255+0000] {standard_task_runner.py:83} INFO - Job 68: Subtask transform_stage_generation
[2023-01-20T16:22:27.472+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T16:22:27.669+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-20T16:22:27.695+0000] {spark_submit.py:222} INFO - Could not load connection string spark_default, defaulting to yarn
[2023-01-20T16:22:27.698+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master yarn --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-20T16:22:27.715+0000] {spark_submit.py:495} INFO - /usr/local/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit: line 27: /opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-class: No such file or directory
[2023-01-20T16:22:27.743+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master yarn --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 127.
[2023-01-20T16:22:27.754+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230120T162227, end_date=20230120T162227
[2023-01-20T16:22:27.799+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 68 for task transform_stage_generation (Cannot execute: spark-submit --master yarn --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 127.; 552)
[2023-01-20T16:22:27.860+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T16:22:27.952+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T16:31:17.792+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T16:31:17.809+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T16:31:17.809+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T16:31:17.809+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-20T16:31:17.809+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T16:31:17.823+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-20T16:31:17.847+0000] {standard_task_runner.py:55} INFO - Started process 1114 to run task
[2023-01-20T16:31:17.852+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '75', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpsk_yetc7']
[2023-01-20T16:31:17.855+0000] {standard_task_runner.py:83} INFO - Job 75: Subtask transform_stage_generation
[2023-01-20T16:31:17.938+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T16:31:18.018+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-20T16:31:18.027+0000] {spark_submit.py:222} INFO - Could not load connection string spark_default, defaulting to yarn
[2023-01-20T16:31:18.030+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master yarn --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-20T16:31:18.036+0000] {spark_submit.py:495} INFO - /usr/local/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit: line 27: /opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-class: No such file or directory
[2023-01-20T16:31:18.046+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master yarn --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 127.
[2023-01-20T16:31:18.050+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230120T163117, end_date=20230120T163118
[2023-01-20T16:31:18.063+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 75 for task transform_stage_generation (Cannot execute: spark-submit --master yarn --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 127.; 1114)
[2023-01-20T16:31:18.103+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T16:31:18.124+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T17:55:57.464+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T17:55:57.476+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T17:55:57.476+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T17:55:57.476+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-20T17:55:57.476+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T17:55:57.491+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-20T17:55:57.500+0000] {standard_task_runner.py:55} INFO - Started process 6213 to run task
[2023-01-20T17:55:57.505+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '84', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpnytjgm5k']
[2023-01-20T17:55:57.508+0000] {standard_task_runner.py:83} INFO - Job 84: Subtask transform_stage_generation
[2023-01-20T17:55:57.575+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T17:55:57.662+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-20T17:55:57.664+0000] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-01-20T17:55:57.666+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master  --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-20T17:55:57.679+0000] {spark_submit.py:495} INFO - /usr/local/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit: line 27: /opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-class: No such file or directory
[2023-01-20T17:55:57.688+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master  --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 127.
[2023-01-20T17:55:57.693+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230120T175557, end_date=20230120T175557
[2023-01-20T17:55:57.706+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 84 for task transform_stage_generation (Cannot execute: spark-submit --master  --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 127.; 6213)
[2023-01-20T17:55:57.756+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T17:55:57.776+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T17:59:00.638+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T17:59:00.651+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T17:59:00.651+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T17:59:00.652+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-20T17:59:00.652+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T17:59:00.669+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-20T17:59:00.681+0000] {standard_task_runner.py:55} INFO - Started process 6426 to run task
[2023-01-20T17:59:00.687+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '89', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmplxuv_u9q']
[2023-01-20T17:59:00.692+0000] {standard_task_runner.py:83} INFO - Job 89: Subtask transform_stage_generation
[2023-01-20T17:59:00.809+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T17:59:00.900+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-20T17:59:00.902+0000] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-01-20T17:59:00.905+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master  --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-20T17:59:00.912+0000] {spark_submit.py:495} INFO - /usr/local/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit: line 27: /opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-class: No such file or directory
[2023-01-20T17:59:00.927+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master  --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 127.
[2023-01-20T17:59:00.933+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230120T175900, end_date=20230120T175900
[2023-01-20T17:59:00.949+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 89 for task transform_stage_generation (Cannot execute: spark-submit --master  --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 127.; 6426)
[2023-01-20T17:59:00.979+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T17:59:01.001+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T18:05:02.149+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T18:05:02.164+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T18:05:02.164+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T18:05:02.164+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-20T18:05:02.164+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T18:05:02.184+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-20T18:05:02.201+0000] {standard_task_runner.py:55} INFO - Started process 6786 to run task
[2023-01-20T18:05:02.209+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '94', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpplc320p7']
[2023-01-20T18:05:02.214+0000] {standard_task_runner.py:83} INFO - Job 94: Subtask transform_stage_generation
[2023-01-20T18:05:02.324+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T18:05:02.449+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-20T18:05:02.451+0000] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-01-20T18:05:02.454+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master  --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-20T18:05:02.464+0000] {spark_submit.py:495} INFO - /usr/local/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit: line 27: /opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-class: No such file or directory
[2023-01-20T18:05:02.479+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master  --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 127.
[2023-01-20T18:05:02.484+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230120T180502, end_date=20230120T180502
[2023-01-20T18:05:02.505+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 94 for task transform_stage_generation (Cannot execute: spark-submit --master  --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 127.; 6786)
[2023-01-20T18:05:02.555+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T18:05:02.578+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T18:11:15.403+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T18:11:15.414+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T18:11:15.414+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T18:11:15.414+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-20T18:11:15.414+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T18:11:15.426+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-20T18:11:15.435+0000] {standard_task_runner.py:55} INFO - Started process 7212 to run task
[2023-01-20T18:11:15.439+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '100', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpkg9tefe2']
[2023-01-20T18:11:15.442+0000] {standard_task_runner.py:83} INFO - Job 100: Subtask transform_stage_generation
[2023-01-20T18:11:15.514+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T18:11:15.581+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-20T18:11:15.581+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 156, in execute
    self._hook = self._get_hook()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 165, in _get_hook
    return SparkSubmitHook(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 155, in __init__
    raise RuntimeError(
RuntimeError: The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/opt/spark-3.3.1-bin-hadoop3-scala2.13bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH
[2023-01-20T18:11:15.590+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230120T181115, end_date=20230120T181115
[2023-01-20T18:11:15.599+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 100 for task transform_stage_generation (The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/opt/spark-3.3.1-bin-hadoop3-scala2.13bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH; 7212)
[2023-01-20T18:11:15.651+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T18:11:15.665+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T18:12:43.110+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T18:12:43.146+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T18:12:43.147+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T18:12:43.147+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-20T18:12:43.147+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T18:12:43.187+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-20T18:12:43.204+0000] {standard_task_runner.py:55} INFO - Started process 7307 to run task
[2023-01-20T18:12:43.214+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '104', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpwfwt1dxc']
[2023-01-20T18:12:43.222+0000] {standard_task_runner.py:83} INFO - Job 104: Subtask transform_stage_generation
[2023-01-20T18:12:43.392+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T18:12:43.537+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-20T18:12:43.538+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 156, in execute
    self._hook = self._get_hook()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 165, in _get_hook
    return SparkSubmitHook(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 155, in __init__
    raise RuntimeError(
RuntimeError: The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH
[2023-01-20T18:12:43.558+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230120T181243, end_date=20230120T181243
[2023-01-20T18:12:43.580+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 104 for task transform_stage_generation (The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH; 7307)
[2023-01-20T18:12:43.625+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T18:12:43.659+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T18:16:15.968+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T18:16:15.996+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T18:16:15.996+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T18:16:15.997+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-20T18:16:15.998+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T18:16:16.028+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-20T18:16:16.061+0000] {standard_task_runner.py:55} INFO - Started process 7543 to run task
[2023-01-20T18:16:16.068+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '108', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpc1whcyu5']
[2023-01-20T18:16:16.076+0000] {standard_task_runner.py:83} INFO - Job 108: Subtask transform_stage_generation
[2023-01-20T18:16:16.231+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T18:16:16.410+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-20T18:16:16.411+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 156, in execute
    self._hook = self._get_hook()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 165, in _get_hook
    return SparkSubmitHook(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 155, in __init__
    raise RuntimeError(
RuntimeError: The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH
[2023-01-20T18:16:16.437+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230120T181615, end_date=20230120T181616
[2023-01-20T18:16:16.472+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 108 for task transform_stage_generation (The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH; 7543)
[2023-01-20T18:16:16.518+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T18:16:16.558+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T18:18:55.636+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T18:18:55.654+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T18:18:55.654+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T18:18:55.654+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-20T18:18:55.654+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T18:18:55.675+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-20T18:18:55.690+0000] {standard_task_runner.py:55} INFO - Started process 7714 to run task
[2023-01-20T18:18:55.698+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '113', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpm9vipqnn']
[2023-01-20T18:18:55.702+0000] {standard_task_runner.py:83} INFO - Job 113: Subtask transform_stage_generation
[2023-01-20T18:18:55.805+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T18:18:55.920+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-20T18:18:55.920+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 156, in execute
    self._hook = self._get_hook()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 165, in _get_hook
    return SparkSubmitHook(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 155, in __init__
    raise RuntimeError(
RuntimeError: The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH
[2023-01-20T18:18:55.937+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230120T181855, end_date=20230120T181855
[2023-01-20T18:18:55.955+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 113 for task transform_stage_generation (The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH; 7714)
[2023-01-20T18:18:55.999+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T18:18:56.035+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T19:15:54.684+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T19:15:54.725+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T19:15:54.727+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T19:15:54.728+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-20T19:15:54.729+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T19:15:54.809+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-20T19:15:54.832+0000] {standard_task_runner.py:55} INFO - Started process 360 to run task
[2023-01-20T19:15:54.838+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '128', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp1h8wdyll']
[2023-01-20T19:15:54.840+0000] {standard_task_runner.py:83} INFO - Job 128: Subtask transform_stage_generation
[2023-01-20T19:15:55.057+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 984f5901aea4
[2023-01-20T19:15:55.331+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-20T19:15:55.331+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 156, in execute
    self._hook = self._get_hook()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 165, in _get_hook
    return SparkSubmitHook(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 155, in __init__
    raise RuntimeError(
RuntimeError: The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/usr/local/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH
[2023-01-20T19:15:55.397+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230120T191554, end_date=20230120T191555
[2023-01-20T19:15:55.458+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 128 for task transform_stage_generation (The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/usr/local/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH; 360)
[2023-01-20T19:15:55.546+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T19:15:55.717+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T03:03:17.688+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T03:03:17.710+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T03:03:17.710+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T03:03:17.711+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-21T03:03:17.711+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T03:03:17.742+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-21T03:03:17.760+0000] {standard_task_runner.py:55} INFO - Started process 324 to run task
[2023-01-21T03:03:17.771+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '140', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp942qkb5z']
[2023-01-21T03:03:17.777+0000] {standard_task_runner.py:83} INFO - Job 140: Subtask transform_stage_generation
[2023-01-21T03:03:17.928+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 2b3134e2b02d
[2023-01-21T03:03:18.087+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-21T03:03:18.089+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 156, in execute
    self._hook = self._get_hook()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 165, in _get_hook
    return SparkSubmitHook(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 155, in __init__
    raise RuntimeError(
RuntimeError: The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/usr/local/spark/bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH
[2023-01-21T03:03:18.111+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230121T030317, end_date=20230121T030318
[2023-01-21T03:03:18.136+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 140 for task transform_stage_generation (The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/usr/local/spark/bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH; 324)
[2023-01-21T03:03:18.183+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T03:03:18.220+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T07:47:28.772+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T07:47:28.783+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T07:47:28.783+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T07:47:28.784+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-21T07:47:28.784+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T07:47:28.795+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-21T07:47:28.802+0000] {standard_task_runner.py:55} INFO - Started process 71 to run task
[2023-01-21T07:47:28.807+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '162', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmps4t0igd6']
[2023-01-21T07:47:28.810+0000] {standard_task_runner.py:83} INFO - Job 162: Subtask transform_stage_generation
[2023-01-21T07:47:28.884+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 4d52b49e46b4
[2023-01-21T07:47:29.009+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-21T07:47:29.011+0000] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-01-21T07:47:29.013+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master  --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-21T07:47:29.027+0000] {spark_submit.py:495} INFO - /usr/local/spark/bin/load-spark-env.sh: line 68: ps: command not found
[2023-01-21T07:47:32.743+0000] {spark_submit.py:495} INFO - Exception in thread "main" org.apache.spark.SparkException: Master must either be yarn or start with spark, mesos, k8s, or local
[2023-01-21T07:47:32.743+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.error(SparkSubmit.scala:975)
[2023-01-21T07:47:32.744+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:238)
[2023-01-21T07:47:32.744+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:901)
[2023-01-21T07:47:32.744+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
[2023-01-21T07:47:32.744+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
[2023-01-21T07:47:32.744+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
[2023-01-21T07:47:32.745+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)
[2023-01-21T07:47:32.745+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)
[2023-01-21T07:47:32.745+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
[2023-01-21T07:47:32.787+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master  --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-21T07:47:32.793+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230121T074728, end_date=20230121T074732
[2023-01-21T07:47:32.812+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 162 for task transform_stage_generation (Cannot execute: spark-submit --master  --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 71)
[2023-01-21T07:47:32.848+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T07:47:32.874+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T08:16:38.456+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T08:16:38.469+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T08:16:38.469+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T08:16:38.470+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-21T08:16:38.470+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T08:16:38.483+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-21T08:16:38.492+0000] {standard_task_runner.py:55} INFO - Started process 2287 to run task
[2023-01-21T08:16:38.497+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '173', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpc8g6c8v8']
[2023-01-21T08:16:38.500+0000] {standard_task_runner.py:83} INFO - Job 173: Subtask transform_stage_generation
[2023-01-21T08:16:38.568+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 4d52b49e46b4
[2023-01-21T08:16:38.664+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-21T08:16:38.678+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-21T08:16:38.682+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-21T08:16:38.695+0000] {spark_submit.py:495} INFO - /usr/local/spark/bin/load-spark-env.sh: line 68: ps: command not found
[2023-01-21T08:16:41.952+0000] {spark_submit.py:495} INFO - 23/01/21 08:16:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-21T08:16:42.246+0000] {spark_submit.py:495} INFO - 23/01/21 08:16:42 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-21T08:16:42.358+0000] {spark_submit.py:495} INFO - Exception in thread "main" java.io.IOException: Cannot run program "/usr/bin/python3.10": error=2, No such file or directory
[2023-01-21T08:16:42.359+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1128)
[2023-01-21T08:16:42.359+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1071)
[2023-01-21T08:16:42.359+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.PythonRunner$.main(PythonRunner.scala:97)
[2023-01-21T08:16:42.359+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.PythonRunner.main(PythonRunner.scala)
[2023-01-21T08:16:42.359+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-21T08:16:42.359+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-21T08:16:42.360+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-21T08:16:42.360+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-21T08:16:42.360+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
[2023-01-21T08:16:42.360+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:958)
[2023-01-21T08:16:42.360+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
[2023-01-21T08:16:42.360+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
[2023-01-21T08:16:42.361+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
[2023-01-21T08:16:42.361+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)
[2023-01-21T08:16:42.361+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)
[2023-01-21T08:16:42.361+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
[2023-01-21T08:16:42.361+0000] {spark_submit.py:495} INFO - Caused by: java.io.IOException: error=2, No such file or directory
[2023-01-21T08:16:42.361+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessImpl.forkAndExec(Native Method)
[2023-01-21T08:16:42.362+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:340)
[2023-01-21T08:16:42.362+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessImpl.start(ProcessImpl.java:271)
[2023-01-21T08:16:42.362+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1107)
[2023-01-21T08:16:42.362+0000] {spark_submit.py:495} INFO - ... 15 more
[2023-01-21T08:16:42.369+0000] {spark_submit.py:495} INFO - 23/01/21 08:16:42 INFO ShutdownHookManager: Shutdown hook called
[2023-01-21T08:16:42.370+0000] {spark_submit.py:495} INFO - 23/01/21 08:16:42 INFO ShutdownHookManager: Deleting directory /tmp/spark-76ca6f76-4ff4-4a53-811a-803c704bb228
[2023-01-21T08:16:42.445+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-21T08:16:42.450+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230121T081638, end_date=20230121T081642
[2023-01-21T08:16:42.466+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 173 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 2287)
[2023-01-21T08:16:42.484+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T08:16:42.502+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T08:23:55.712+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T08:23:55.723+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T08:23:55.723+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T08:23:55.723+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-21T08:23:55.723+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T08:23:55.735+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-21T08:23:55.742+0000] {standard_task_runner.py:55} INFO - Started process 285 to run task
[2023-01-21T08:23:55.746+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '181', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpisc4tker']
[2023-01-21T08:23:55.751+0000] {standard_task_runner.py:83} INFO - Job 181: Subtask transform_stage_generation
[2023-01-21T08:23:55.821+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host d00464a30031
[2023-01-21T08:23:55.906+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-21T08:23:55.920+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-21T08:23:55.922+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-21T08:23:55.939+0000] {spark_submit.py:495} INFO - /usr/local/spark/bin/load-spark-env.sh: line 68: ps: command not found
[2023-01-21T08:23:59.685+0000] {spark_submit.py:495} INFO - 23/01/21 08:23:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-21T08:24:00.058+0000] {spark_submit.py:495} INFO - 23/01/21 08:24:00 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-21T08:24:00.253+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-21T08:24:00.254+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 2, in <module>
[2023-01-21T08:24:00.254+0000] {spark_submit.py:495} INFO - from dotenv import load_dotenv
[2023-01-21T08:24:00.254+0000] {spark_submit.py:495} INFO - ModuleNotFoundError: No module named 'dotenv'
[2023-01-21T08:24:00.276+0000] {spark_submit.py:495} INFO - 23/01/21 08:24:00 INFO ShutdownHookManager: Shutdown hook called
[2023-01-21T08:24:00.278+0000] {spark_submit.py:495} INFO - 23/01/21 08:24:00 INFO ShutdownHookManager: Deleting directory /tmp/spark-08d13d43-b8d4-486e-9a5b-0485a3af4ce8
[2023-01-21T08:24:00.355+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-21T08:24:00.360+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230121T082355, end_date=20230121T082400
[2023-01-21T08:24:00.387+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 181 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 285)
[2023-01-21T08:24:00.413+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T08:24:00.434+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T10:18:14.627+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T10:18:14.641+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T10:18:14.641+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T10:18:14.642+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-21T10:18:14.642+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T10:18:14.659+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-21T10:18:14.670+0000] {standard_task_runner.py:55} INFO - Started process 2199 to run task
[2023-01-21T10:18:14.676+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '192', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpnp4wf2gh']
[2023-01-21T10:18:14.680+0000] {standard_task_runner.py:83} INFO - Job 192: Subtask transform_stage_generation
[2023-01-21T10:18:14.783+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host b3a2c4a96689
[2023-01-21T10:18:14.959+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-21T10:18:14.984+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-21T10:18:14.990+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-21T10:18:25.378+0000] {spark_submit.py:495} INFO - 23/01/21 10:18:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-21T10:18:26.261+0000] {spark_submit.py:495} INFO - 23/01/21 10:18:26 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-21T10:18:26.384+0000] {spark_submit.py:495} INFO - Exception in thread "main" java.io.IOException: Cannot run program "/usr/bin/python3.10": error=2, No such file or directory
[2023-01-21T10:18:26.385+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1128)
[2023-01-21T10:18:26.387+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1071)
[2023-01-21T10:18:26.390+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.PythonRunner$.main(PythonRunner.scala:97)
[2023-01-21T10:18:26.391+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.PythonRunner.main(PythonRunner.scala)
[2023-01-21T10:18:26.391+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-21T10:18:26.392+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-21T10:18:26.392+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-21T10:18:26.392+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-21T10:18:26.393+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
[2023-01-21T10:18:26.393+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:958)
[2023-01-21T10:18:26.393+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
[2023-01-21T10:18:26.393+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
[2023-01-21T10:18:26.394+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
[2023-01-21T10:18:26.394+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)
[2023-01-21T10:18:26.394+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)
[2023-01-21T10:18:26.394+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
[2023-01-21T10:18:26.395+0000] {spark_submit.py:495} INFO - Caused by: java.io.IOException: error=2, No such file or directory
[2023-01-21T10:18:26.395+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessImpl.forkAndExec(Native Method)
[2023-01-21T10:18:26.395+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:340)
[2023-01-21T10:18:26.396+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessImpl.start(ProcessImpl.java:271)
[2023-01-21T10:18:26.396+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1107)
[2023-01-21T10:18:26.397+0000] {spark_submit.py:495} INFO - ... 15 more
[2023-01-21T10:18:26.404+0000] {spark_submit.py:495} INFO - 23/01/21 10:18:26 INFO ShutdownHookManager: Shutdown hook called
[2023-01-21T10:18:26.406+0000] {spark_submit.py:495} INFO - 23/01/21 10:18:26 INFO ShutdownHookManager: Deleting directory /tmp/spark-01670c95-c3fb-437f-9cfd-b697e6e02624
[2023-01-21T10:18:26.518+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-21T10:18:26.525+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230121T101814, end_date=20230121T101826
[2023-01-21T10:18:26.555+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 192 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 2199)
[2023-01-21T10:18:26.583+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T10:18:26.630+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T10:45:46.293+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T10:45:46.326+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T10:45:46.327+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T10:45:46.327+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-21T10:45:46.327+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T10:45:46.365+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-21T10:45:46.385+0000] {standard_task_runner.py:55} INFO - Started process 157 to run task
[2023-01-21T10:45:46.396+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '205', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpi6yxiejs']
[2023-01-21T10:45:46.403+0000] {standard_task_runner.py:83} INFO - Job 205: Subtask transform_stage_generation
[2023-01-21T10:45:46.564+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host b3a2c4a96689
[2023-01-21T10:45:46.755+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-21T10:45:46.779+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-21T10:45:46.785+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-21T10:45:51.731+0000] {spark_submit.py:495} INFO - 23/01/21 10:45:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-21T10:45:52.255+0000] {spark_submit.py:495} INFO - 23/01/21 10:45:52 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-21T10:45:52.337+0000] {spark_submit.py:495} INFO - Exception in thread "main" java.io.IOException: Cannot run program "/usr/bin/python3.10": error=2, No such file or directory
[2023-01-21T10:45:52.338+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1128)
[2023-01-21T10:45:52.339+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1071)
[2023-01-21T10:45:52.340+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.PythonRunner$.main(PythonRunner.scala:97)
[2023-01-21T10:45:52.340+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.PythonRunner.main(PythonRunner.scala)
[2023-01-21T10:45:52.340+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-21T10:45:52.341+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-21T10:45:52.341+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-21T10:45:52.341+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-21T10:45:52.341+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
[2023-01-21T10:45:52.342+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:958)
[2023-01-21T10:45:52.342+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
[2023-01-21T10:45:52.342+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
[2023-01-21T10:45:52.343+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
[2023-01-21T10:45:52.343+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)
[2023-01-21T10:45:52.343+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)
[2023-01-21T10:45:52.344+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
[2023-01-21T10:45:52.344+0000] {spark_submit.py:495} INFO - Caused by: java.io.IOException: error=2, No such file or directory
[2023-01-21T10:45:52.345+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessImpl.forkAndExec(Native Method)
[2023-01-21T10:45:52.345+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:340)
[2023-01-21T10:45:52.346+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessImpl.start(ProcessImpl.java:271)
[2023-01-21T10:45:52.346+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1107)
[2023-01-21T10:45:52.346+0000] {spark_submit.py:495} INFO - ... 15 more
[2023-01-21T10:45:52.356+0000] {spark_submit.py:495} INFO - 23/01/21 10:45:52 INFO ShutdownHookManager: Shutdown hook called
[2023-01-21T10:45:52.359+0000] {spark_submit.py:495} INFO - 23/01/21 10:45:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-cd243198-4111-4946-a93f-92031e66694a
[2023-01-21T10:45:52.486+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-21T10:45:52.496+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230121T104546, end_date=20230121T104552
[2023-01-21T10:45:52.534+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 205 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 157)
[2023-01-21T10:45:52.602+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T10:45:52.669+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T11:29:48.664+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T11:29:48.676+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T11:29:48.676+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T11:29:48.676+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-21T11:29:48.676+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T11:29:48.692+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-21T11:29:48.701+0000] {standard_task_runner.py:55} INFO - Started process 1124 to run task
[2023-01-21T11:29:48.707+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '218', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpnqn3n9q8']
[2023-01-21T11:29:48.711+0000] {standard_task_runner.py:83} INFO - Job 218: Subtask transform_stage_generation
[2023-01-21T11:29:48.803+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host b7c4de7fae0b
[2023-01-21T11:29:48.912+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-21T11:29:48.931+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-21T11:29:48.935+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-21T11:29:57.017+0000] {spark_submit.py:495} INFO - 23/01/21 11:29:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-21T11:29:58.039+0000] {spark_submit.py:495} INFO - 23/01/21 11:29:58 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-21T11:29:58.406+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-21T11:29:58.407+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 2, in <module>
[2023-01-21T11:29:58.407+0000] {spark_submit.py:495} INFO - from dotenv import load_dotenv, find_dotenv
[2023-01-21T11:29:58.408+0000] {spark_submit.py:495} INFO - ModuleNotFoundError: No module named 'dotenv'
[2023-01-21T11:29:58.438+0000] {spark_submit.py:495} INFO - 23/01/21 11:29:58 INFO ShutdownHookManager: Shutdown hook called
[2023-01-21T11:29:58.442+0000] {spark_submit.py:495} INFO - 23/01/21 11:29:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-316cd947-b1c9-44ba-9e79-149e10e29c8b
[2023-01-21T11:29:58.620+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-21T11:29:58.631+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230121T112948, end_date=20230121T112958
[2023-01-21T11:29:58.665+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 218 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 1124)
[2023-01-21T11:29:58.710+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T11:29:58.775+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T11:31:19.001+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T11:31:19.017+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T11:31:19.018+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T11:31:19.018+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-21T11:31:19.018+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T11:31:19.036+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-21T11:31:19.045+0000] {standard_task_runner.py:55} INFO - Started process 1335 to run task
[2023-01-21T11:31:19.052+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '222', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp053rie5o']
[2023-01-21T11:31:19.056+0000] {standard_task_runner.py:83} INFO - Job 222: Subtask transform_stage_generation
[2023-01-21T11:31:19.160+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host b7c4de7fae0b
[2023-01-21T11:31:19.291+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-21T11:31:19.305+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-21T11:31:19.308+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-21T11:31:25.015+0000] {spark_submit.py:495} INFO - 23/01/21 11:31:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-21T11:31:25.514+0000] {spark_submit.py:495} INFO - 23/01/21 11:31:25 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-21T11:31:25.697+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 2
[2023-01-21T11:31:25.698+0000] {spark_submit.py:495} INFO - from python-dotenv import load_dotenv, find_dotenv
[2023-01-21T11:31:25.698+0000] {spark_submit.py:495} INFO - ^
[2023-01-21T11:31:25.698+0000] {spark_submit.py:495} INFO - SyntaxError: invalid syntax
[2023-01-21T11:31:25.720+0000] {spark_submit.py:495} INFO - 23/01/21 11:31:25 INFO ShutdownHookManager: Shutdown hook called
[2023-01-21T11:31:25.722+0000] {spark_submit.py:495} INFO - 23/01/21 11:31:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-10c080c4-c3d9-4656-a764-e0ff73c5b187
[2023-01-21T11:31:25.829+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-21T11:31:25.836+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230121T113119, end_date=20230121T113125
[2023-01-21T11:31:25.862+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 222 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 1335)
[2023-01-21T11:31:25.918+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T11:31:25.951+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T15:52:49.622+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T15:52:49.639+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T15:52:49.640+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T15:52:49.640+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-21T15:52:49.640+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T15:52:49.661+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-21T15:52:49.673+0000] {standard_task_runner.py:55} INFO - Started process 170 to run task
[2023-01-21T15:52:49.681+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '235', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmptl1w0u1c']
[2023-01-21T15:52:49.684+0000] {standard_task_runner.py:83} INFO - Job 235: Subtask transform_stage_generation
[2023-01-21T15:52:49.772+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 673f7b70a136
[2023-01-21T15:52:49.871+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-21T15:52:49.884+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-21T15:52:49.888+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-21T15:52:56.246+0000] {spark_submit.py:495} INFO - 23/01/21 15:52:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-21T15:52:56.755+0000] {spark_submit.py:495} INFO - 23/01/21 15:52:56 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-21T15:52:56.984+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-21T15:52:56.984+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 2, in <module>
[2023-01-21T15:52:56.984+0000] {spark_submit.py:495} INFO - from dotenv import load_dotenv, find_dotenv
[2023-01-21T15:52:56.984+0000] {spark_submit.py:495} INFO - ModuleNotFoundError: No module named 'dotenv'
[2023-01-21T15:52:57.004+0000] {spark_submit.py:495} INFO - 23/01/21 15:52:57 INFO ShutdownHookManager: Shutdown hook called
[2023-01-21T15:52:57.006+0000] {spark_submit.py:495} INFO - 23/01/21 15:52:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-d7924779-f8c1-4ea3-bec1-6ccfa5b0009b
[2023-01-21T15:52:57.096+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-21T15:52:57.102+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230121T155249, end_date=20230121T155257
[2023-01-21T15:52:57.123+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 235 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 170)
[2023-01-21T15:52:57.175+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T15:52:57.215+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T16:54:30.080+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T16:54:30.094+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T16:54:30.095+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T16:54:30.095+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-21T16:54:30.095+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T16:54:30.106+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-21T16:54:30.119+0000] {standard_task_runner.py:55} INFO - Started process 358 to run task
[2023-01-21T16:54:30.123+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '247', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp_fep1hn2']
[2023-01-21T16:54:30.126+0000] {standard_task_runner.py:83} INFO - Job 247: Subtask transform_stage_generation
[2023-01-21T16:54:30.203+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 4260804338b2
[2023-01-21T16:54:30.290+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-21T16:54:30.300+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-21T16:54:30.302+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-21T16:54:33.526+0000] {spark_submit.py:495} INFO - 23/01/21 16:54:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-21T16:54:33.902+0000] {spark_submit.py:495} INFO - 23/01/21 16:54:33 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-21T16:54:34.048+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-21T16:54:34.048+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 2, in <module>
[2023-01-21T16:54:34.049+0000] {spark_submit.py:495} INFO - from dotenv import load_dotenv, find_dotenv
[2023-01-21T16:54:34.049+0000] {spark_submit.py:495} INFO - ModuleNotFoundError: No module named 'dotenv'
[2023-01-21T16:54:34.063+0000] {spark_submit.py:495} INFO - 23/01/21 16:54:34 INFO ShutdownHookManager: Shutdown hook called
[2023-01-21T16:54:34.065+0000] {spark_submit.py:495} INFO - 23/01/21 16:54:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-a0176908-db4c-49dc-b5fa-c2bd55e2a2d8
[2023-01-21T16:54:34.181+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-21T16:54:34.187+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230121T165430, end_date=20230121T165434
[2023-01-21T16:54:34.208+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 247 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 358)
[2023-01-21T16:54:34.224+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T16:54:34.248+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T23:07:14.070+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T23:07:14.088+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T23:07:14.088+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T23:07:14.088+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-21T23:07:14.088+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T23:07:14.108+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-21T23:07:14.119+0000] {standard_task_runner.py:55} INFO - Started process 4273 to run task
[2023-01-21T23:07:14.125+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '257', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp_5_c0o_x']
[2023-01-21T23:07:14.129+0000] {standard_task_runner.py:83} INFO - Job 257: Subtask transform_stage_generation
[2023-01-21T23:07:14.225+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 4260804338b2
[2023-01-21T23:07:14.360+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-21T23:07:14.380+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-21T23:07:14.383+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-21T23:07:18.776+0000] {spark_submit.py:495} INFO - 23/01/21 23:07:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-21T23:07:19.241+0000] {spark_submit.py:495} INFO - 23/01/21 23:07:19 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-21T23:07:19.411+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-21T23:07:19.413+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 2, in <module>
[2023-01-21T23:07:19.413+0000] {spark_submit.py:495} INFO - from dotenv import load_dotenv, find_dotenv
[2023-01-21T23:07:19.414+0000] {spark_submit.py:495} INFO - ModuleNotFoundError: No module named 'dotenv'
[2023-01-21T23:07:19.429+0000] {spark_submit.py:495} INFO - 23/01/21 23:07:19 INFO ShutdownHookManager: Shutdown hook called
[2023-01-21T23:07:19.430+0000] {spark_submit.py:495} INFO - 23/01/21 23:07:19 INFO ShutdownHookManager: Deleting directory /tmp/spark-a4b9cacf-efe9-44bb-ba89-80b72f3331cf
[2023-01-21T23:07:19.517+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-21T23:07:19.522+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230121T230714, end_date=20230121T230719
[2023-01-21T23:07:19.538+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 257 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 4273)
[2023-01-21T23:07:19.568+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T23:07:19.588+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T00:11:33.338+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T00:11:33.354+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T00:11:33.354+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T00:11:33.355+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-22T00:11:33.355+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T00:11:33.376+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-22T00:11:33.395+0000] {standard_task_runner.py:55} INFO - Started process 380 to run task
[2023-01-22T00:11:33.401+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '268', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpagxwi5z5']
[2023-01-22T00:11:33.405+0000] {standard_task_runner.py:83} INFO - Job 268: Subtask transform_stage_generation
[2023-01-22T00:11:33.513+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 669043c9475c
[2023-01-22T00:11:33.618+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-22T00:11:33.636+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T00:11:33.639+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-22T00:11:37.323+0000] {spark_submit.py:495} INFO - 23/01/22 00:11:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T00:11:37.695+0000] {spark_submit.py:495} INFO - 23/01/22 00:11:37 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-22T00:11:38.297+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T00:11:38.297+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 64, in <module>
[2023-01-22T00:11:38.297+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T00:11:38.298+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 16, in main
[2023-01-22T00:11:38.298+0000] {spark_submit.py:495} INFO - start_label = start.strftime("%Y%m%d%H%M")
[2023-01-22T00:11:38.298+0000] {spark_submit.py:495} INFO - AttributeError: 'str' object has no attribute 'strftime'
[2023-01-22T00:11:38.324+0000] {spark_submit.py:495} INFO - 23/01/22 00:11:38 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T00:11:38.326+0000] {spark_submit.py:495} INFO - 23/01/22 00:11:38 INFO ShutdownHookManager: Deleting directory /tmp/spark-0a6f8fe3-8b22-48e9-8206-ff7b2e1ce035
[2023-01-22T00:11:38.416+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-22T00:11:38.421+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230122T001133, end_date=20230122T001138
[2023-01-22T00:11:38.442+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 268 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 380)
[2023-01-22T00:11:38.461+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T00:11:38.484+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T03:52:52.692+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T03:52:52.704+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T03:52:52.704+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T03:52:52.704+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-22T03:52:52.704+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T03:52:52.717+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-22T03:52:52.724+0000] {standard_task_runner.py:55} INFO - Started process 835 to run task
[2023-01-22T03:52:52.729+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '280', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpg__skpk3']
[2023-01-22T03:52:52.732+0000] {standard_task_runner.py:83} INFO - Job 280: Subtask transform_stage_generation
[2023-01-22T03:52:52.815+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host e3ae05c9a97e
[2023-01-22T03:52:52.937+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-22T03:52:52.953+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T03:52:52.957+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /opt/***/spark/resources/gcs-connector-hadoop3-latest.jar, /opt/***/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-22T03:52:59.456+0000] {spark_submit.py:495} INFO - 23/01/22 03:52:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T03:52:59.901+0000] {spark_submit.py:495} INFO - 23/01/22 03:52:59 WARN DependencyUtils: Local jar /opt/***/ /opt/***/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-22T03:53:00.290+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T03:53:00.290+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 64, in <module>
[2023-01-22T03:53:00.291+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T03:53:00.291+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 16, in main
[2023-01-22T03:53:00.291+0000] {spark_submit.py:495} INFO - start_label = start.strftime("%Y%m%d%H%M")
[2023-01-22T03:53:00.291+0000] {spark_submit.py:495} INFO - AttributeError: 'str' object has no attribute 'strftime'
[2023-01-22T03:53:00.311+0000] {spark_submit.py:495} INFO - 23/01/22 03:53:00 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T03:53:00.312+0000] {spark_submit.py:495} INFO - 23/01/22 03:53:00 INFO ShutdownHookManager: Deleting directory /tmp/spark-f2f27fd8-ec71-4314-9fd7-80afc5253ec0
[2023-01-22T03:53:00.374+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /opt/***/spark/resources/gcs-connector-hadoop3-latest.jar, /opt/***/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-22T03:53:00.377+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230122T035252, end_date=20230122T035300
[2023-01-22T03:53:00.389+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 280 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /opt/***/spark/resources/gcs-connector-hadoop3-latest.jar, /opt/***/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 835)
[2023-01-22T03:53:00.437+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T03:53:00.452+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T04:19:42.626+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T04:19:42.640+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T04:19:42.640+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T04:19:42.640+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-22T04:19:42.640+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T04:19:42.656+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-22T04:19:42.667+0000] {standard_task_runner.py:55} INFO - Started process 2966 to run task
[2023-01-22T04:19:42.673+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '291', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpl9dhsm7l']
[2023-01-22T04:19:42.678+0000] {standard_task_runner.py:83} INFO - Job 291: Subtask transform_stage_generation
[2023-01-22T04:19:42.775+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host e3ae05c9a97e
[2023-01-22T04:19:42.882+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-22T04:19:42.894+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T04:19:42.895+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar, spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-22T04:19:49.531+0000] {spark_submit.py:495} INFO - 23/01/22 04:19:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T04:19:50.385+0000] {spark_submit.py:495} INFO - 23/01/22 04:19:50 WARN DependencyUtils: Local jar /opt/***/gcs-connector-hadoop3-latest.jar does not exist, skipping.
[2023-01-22T04:19:50.387+0000] {spark_submit.py:495} INFO - 23/01/22 04:19:50 WARN DependencyUtils: Local jar /opt/***/ spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-22T04:19:51.213+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T04:19:51.214+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 64, in <module>
[2023-01-22T04:19:51.215+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T04:19:51.216+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 16, in main
[2023-01-22T04:19:51.217+0000] {spark_submit.py:495} INFO - start_label = start.strftime("%Y%m%d%H%M")
[2023-01-22T04:19:51.217+0000] {spark_submit.py:495} INFO - AttributeError: 'str' object has no attribute 'strftime'
[2023-01-22T04:19:51.302+0000] {spark_submit.py:495} INFO - 23/01/22 04:19:51 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T04:19:51.311+0000] {spark_submit.py:495} INFO - 23/01/22 04:19:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-5e29b0a3-15df-4418-9ebb-f8bc87787206
[2023-01-22T04:19:51.470+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar, spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-22T04:19:51.479+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230122T041942, end_date=20230122T041951
[2023-01-22T04:19:51.524+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 291 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar, spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 2966)
[2023-01-22T04:19:51.621+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T04:19:51.657+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T06:08:47.064+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T06:08:47.156+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T06:08:47.157+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T06:08:47.158+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-22T06:08:47.165+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T06:08:47.297+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-22T06:08:47.369+0000] {standard_task_runner.py:55} INFO - Started process 243 to run task
[2023-01-22T06:08:47.404+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '305', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpzvft04h8']
[2023-01-22T06:08:47.425+0000] {standard_task_runner.py:83} INFO - Job 305: Subtask transform_stage_generation
[2023-01-22T06:08:47.857+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host d68405340607
[2023-01-22T06:08:48.147+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-22T06:08:48.186+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T06:08:48.193+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-22T06:08:48.220+0000] {spark_submit.py:495} INFO - /home/***/.local/bin/spark-submit: line 27: /opt/spark/bin/spark-class: No such file or directory
[2023-01-22T06:08:48.260+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 127.
[2023-01-22T06:08:48.276+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230122T060847, end_date=20230122T060848
[2023-01-22T06:08:48.321+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 305 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 127.; 243)
[2023-01-22T06:08:48.394+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T06:08:48.468+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T07:45:01.962+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T07:45:01.973+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T07:45:01.973+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T07:45:01.973+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-22T07:45:01.973+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T07:45:01.985+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-22T07:45:01.994+0000] {standard_task_runner.py:55} INFO - Started process 449 to run task
[2023-01-22T07:45:01.999+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '317', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpyj8sp3a4']
[2023-01-22T07:45:02.002+0000] {standard_task_runner.py:83} INFO - Job 317: Subtask transform_stage_generation
[2023-01-22T07:45:02.085+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 16233a798013
[2023-01-22T07:45:02.191+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-22T07:45:02.216+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T07:45:02.218+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-22T07:45:04.944+0000] {spark_submit.py:495} INFO - 23/01/22 07:45:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T07:45:05.209+0000] {spark_submit.py:495} INFO - 23/01/22 07:45:05 WARN DependencyUtils: Local jar /opt/***/gcs-connector-hadoop3-latest.jar does not exist, skipping.
[2023-01-22T07:45:05.209+0000] {spark_submit.py:495} INFO - 23/01/22 07:45:05 WARN DependencyUtils: Local jar /opt/***/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-22T07:45:05.543+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T07:45:05.543+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 64, in <module>
[2023-01-22T07:45:05.543+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T07:45:05.544+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 16, in main
[2023-01-22T07:45:05.544+0000] {spark_submit.py:495} INFO - start_label = start.strftime("%Y%m%d%H%M")
[2023-01-22T07:45:05.544+0000] {spark_submit.py:495} INFO - AttributeError: 'str' object has no attribute 'strftime'
[2023-01-22T07:45:05.561+0000] {spark_submit.py:495} INFO - 23/01/22 07:45:05 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T07:45:05.562+0000] {spark_submit.py:495} INFO - 23/01/22 07:45:05 INFO ShutdownHookManager: Deleting directory /tmp/spark-bc80d374-c0bd-4cf3-b34b-70c0fc3996f9
[2023-01-22T07:45:05.617+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-22T07:45:05.620+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230122T074501, end_date=20230122T074505
[2023-01-22T07:45:05.633+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 317 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 449)
[2023-01-22T07:45:05.672+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T07:45:05.690+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T07:48:20.172+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T07:48:20.188+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T07:48:20.188+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T07:48:20.188+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-22T07:48:20.189+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T07:48:20.211+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-22T07:48:20.221+0000] {standard_task_runner.py:55} INFO - Started process 803 to run task
[2023-01-22T07:48:20.226+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '321', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpbvcsd36o']
[2023-01-22T07:48:20.230+0000] {standard_task_runner.py:83} INFO - Job 321: Subtask transform_stage_generation
[2023-01-22T07:48:20.341+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 16233a798013
[2023-01-22T07:48:20.486+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-22T07:48:20.502+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T07:48:20.505+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-22T07:48:24.094+0000] {spark_submit.py:495} INFO - 23/01/22 07:48:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T07:48:24.388+0000] {spark_submit.py:495} INFO - 23/01/22 07:48:24 WARN DependencyUtils: Local jar /opt/***/gcs-connector-hadoop3-latest.jar does not exist, skipping.
[2023-01-22T07:48:24.388+0000] {spark_submit.py:495} INFO - 23/01/22 07:48:24 WARN DependencyUtils: Local jar /opt/***/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-22T07:48:24.836+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T07:48:24.836+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 64, in <module>
[2023-01-22T07:48:24.836+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T07:48:24.836+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 28, in main
[2023-01-22T07:48:24.836+0000] {spark_submit.py:495} INFO - SparkSession.builder.appName("gcp_playground")
[2023-01-22T07:48:24.836+0000] {spark_submit.py:495} INFO - NameError: name 'SparkSession' is not defined
[2023-01-22T07:48:24.853+0000] {spark_submit.py:495} INFO - 23/01/22 07:48:24 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T07:48:24.854+0000] {spark_submit.py:495} INFO - 23/01/22 07:48:24 INFO ShutdownHookManager: Deleting directory /tmp/spark-043a3e61-07d5-43d7-8672-37a1d126f084
[2023-01-22T07:48:24.931+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-22T07:48:24.937+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230122T074820, end_date=20230122T074824
[2023-01-22T07:48:24.953+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 321 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 803)
[2023-01-22T07:48:24.981+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T07:48:25.000+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T07:49:47.820+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T07:49:47.835+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T07:49:47.836+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T07:49:47.836+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-22T07:49:47.836+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T07:49:47.852+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-22T07:49:47.864+0000] {standard_task_runner.py:55} INFO - Started process 976 to run task
[2023-01-22T07:49:47.869+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '324', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmprb80g97s']
[2023-01-22T07:49:47.872+0000] {standard_task_runner.py:83} INFO - Job 324: Subtask transform_stage_generation
[2023-01-22T07:49:47.942+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 16233a798013
[2023-01-22T07:49:48.011+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-22T07:49:48.020+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T07:49:48.021+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-22T07:49:50.382+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T07:49:50.623+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:50 WARN DependencyUtils: Local jar /opt/***/gcs-connector-hadoop3-latest.jar does not exist, skipping.
[2023-01-22T07:49:50.624+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:50 WARN DependencyUtils: Local jar /opt/***/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-22T07:49:51.882+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:51 INFO SparkContext: Running Spark version 3.3.1
[2023-01-22T07:49:51.910+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:51 INFO ResourceUtils: ==============================================================
[2023-01-22T07:49:51.911+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:51 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-22T07:49:51.911+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:51 INFO ResourceUtils: ==============================================================
[2023-01-22T07:49:51.912+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:51 INFO SparkContext: Submitted application: gcp_playground
[2023-01-22T07:49:51.937+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:51 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-22T07:49:51.942+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:51 INFO ResourceProfile: Limiting resource is cpu
[2023-01-22T07:49:51.943+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:51 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-22T07:49:52.020+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:52 INFO SecurityManager: Changing view acls to: ***
[2023-01-22T07:49:52.021+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:52 INFO SecurityManager: Changing modify acls to: ***
[2023-01-22T07:49:52.022+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:52 INFO SecurityManager: Changing view acls groups to:
[2023-01-22T07:49:52.023+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:52 INFO SecurityManager: Changing modify acls groups to:
[2023-01-22T07:49:52.023+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:52 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-22T07:49:52.390+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:52 INFO Utils: Successfully started service 'sparkDriver' on port 43311.
[2023-01-22T07:49:52.468+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:52 INFO SparkEnv: Registering MapOutputTracker
[2023-01-22T07:49:52.524+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:52 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-22T07:49:52.581+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:52 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-22T07:49:52.582+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:52 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-22T07:49:52.590+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:52 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-22T07:49:52.636+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:52 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-e1ee3c25-4ca1-4503-8296-c7b52864d29c
[2023-01-22T07:49:52.662+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:52 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-22T07:49:52.687+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:52 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-22T07:49:53.104+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:53 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-01-22T07:49:53.164+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:53 INFO SparkContext: Added JAR /opt/spark/resources/gcs-connector-hadoop3-latest.jar at spark://16233a798013:43311/jars/gcs-connector-hadoop3-latest.jar with timestamp 1674373791875
[2023-01-22T07:49:53.164+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:53 INFO SparkContext: Added JAR /opt/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://16233a798013:43311/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1674373791875
[2023-01-22T07:49:53.325+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:53 INFO Executor: Starting executor ID driver on host 16233a798013
[2023-01-22T07:49:53.342+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:53 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-22T07:49:53.369+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:53 INFO Executor: Fetching spark://16233a798013:43311/jars/gcs-connector-hadoop3-latest.jar with timestamp 1674373791875
[2023-01-22T07:49:53.468+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:53 INFO TransportClientFactory: Successfully created connection to 16233a798013/192.168.32.7:43311 after 57 ms (0 ms spent in bootstraps)
[2023-01-22T07:49:53.480+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:53 INFO Utils: Fetching spark://16233a798013:43311/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-74cf75e9-0f61-4a26-a14f-296bd75c13a6/userFiles-38c58349-16f9-49f5-96bf-ce831dc032ac/fetchFileTemp14424657339347536426.tmp
[2023-01-22T07:49:53.819+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:53 INFO Executor: Adding file:/tmp/spark-74cf75e9-0f61-4a26-a14f-296bd75c13a6/userFiles-38c58349-16f9-49f5-96bf-ce831dc032ac/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-22T07:49:53.820+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:53 INFO Executor: Fetching spark://16233a798013:43311/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1674373791875
[2023-01-22T07:49:53.823+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:53 INFO Utils: Fetching spark://16233a798013:43311/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-74cf75e9-0f61-4a26-a14f-296bd75c13a6/userFiles-38c58349-16f9-49f5-96bf-ce831dc032ac/fetchFileTemp5397714438335686066.tmp
[2023-01-22T07:49:54.049+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:54 INFO Executor: Adding file:/tmp/spark-74cf75e9-0f61-4a26-a14f-296bd75c13a6/userFiles-38c58349-16f9-49f5-96bf-ce831dc032ac/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-22T07:49:54.063+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:54 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40903.
[2023-01-22T07:49:54.064+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:54 INFO NettyBlockTransferService: Server created on 16233a798013:40903
[2023-01-22T07:49:54.069+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:54 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-22T07:49:54.087+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:54 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 16233a798013, 40903, None)
[2023-01-22T07:49:54.094+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:54 INFO BlockManagerMasterEndpoint: Registering block manager 16233a798013:40903 with 434.4 MiB RAM, BlockManagerId(driver, 16233a798013, 40903, None)
[2023-01-22T07:49:54.099+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:54 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 16233a798013, 40903, None)
[2023-01-22T07:49:54.101+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:54 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 16233a798013, 40903, None)
[2023-01-22T07:49:54.847+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:54 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-22T07:49:54.852+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:54 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-22T07:49:56.420+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:56 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: gs://entsoe_analytics_1009//opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010000.json.
[2023-01-22T07:49:56.420+0000] {spark_submit.py:495} INFO - org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "gs"
[2023-01-22T07:49:56.421+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2023-01-22T07:49:56.421+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T07:49:56.421+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T07:49:56.421+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T07:49:56.421+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T07:49:56.422+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T07:49:56.422+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T07:49:56.422+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)
[2023-01-22T07:49:56.422+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:370)
[2023-01-22T07:49:56.423+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T07:49:56.423+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T07:49:56.423+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T07:49:56.423+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T07:49:56.423+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T07:49:56.426+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T07:49:56.426+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T07:49:56.426+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T07:49:56.426+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T07:49:56.427+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T07:49:56.427+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T07:49:56.427+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T07:49:56.427+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T07:49:56.428+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T07:49:56.428+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T07:49:56.428+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T07:49:56.428+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T07:49:56.480+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T07:49:56.481+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 67, in <module>
[2023-01-22T07:49:56.481+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T07:49:56.481+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 54, in main
[2023-01-22T07:49:56.482+0000] {spark_submit.py:495} INFO - df_spark.write.format("bigquery").option("project", "rafzul-analytics-1009") \
[2023-01-22T07:49:56.482+0000] {spark_submit.py:495} INFO - UnboundLocalError: local variable 'df_spark' referenced before assignment
[2023-01-22T07:49:56.583+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:56 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-22T07:49:56.616+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:56 INFO SparkUI: Stopped Spark web UI at http://16233a798013:4040
[2023-01-22T07:49:56.653+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:56 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-22T07:49:56.705+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:56 INFO MemoryStore: MemoryStore cleared
[2023-01-22T07:49:56.705+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:56 INFO BlockManager: BlockManager stopped
[2023-01-22T07:49:56.721+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:56 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-22T07:49:56.729+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:56 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-22T07:49:56.782+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:56 INFO SparkContext: Successfully stopped SparkContext
[2023-01-22T07:49:56.784+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:56 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T07:49:56.786+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:56 INFO ShutdownHookManager: Deleting directory /tmp/spark-74cf75e9-0f61-4a26-a14f-296bd75c13a6/pyspark-a7961b20-2ce8-4893-9b8b-87164ee89a0e
[2023-01-22T07:49:56.796+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:56 INFO ShutdownHookManager: Deleting directory /tmp/spark-57099318-c6ca-4956-b072-d8a726190454
[2023-01-22T07:49:56.808+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:56 INFO ShutdownHookManager: Deleting directory /tmp/spark-74cf75e9-0f61-4a26-a14f-296bd75c13a6
[2023-01-22T07:49:56.937+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-22T07:49:56.947+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230122T074947, end_date=20230122T074956
[2023-01-22T07:49:56.978+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 324 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 976)
[2023-01-22T07:49:57.018+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T07:49:57.065+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T07:58:21.576+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T07:58:21.602+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T07:58:21.602+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T07:58:21.602+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-22T07:58:21.603+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T07:58:21.631+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-22T07:58:21.650+0000] {standard_task_runner.py:55} INFO - Started process 2017 to run task
[2023-01-22T07:58:21.658+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '331', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpmt5uc4mr']
[2023-01-22T07:58:21.664+0000] {standard_task_runner.py:83} INFO - Job 331: Subtask transform_stage_generation
[2023-01-22T07:58:21.795+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 16233a798013
[2023-01-22T07:58:21.964+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-22T07:58:21.984+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T07:58:21.988+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-22T07:58:29.054+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T07:58:29.513+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:29 WARN DependencyUtils: Local jar /opt/***/gcs-connector-hadoop3-latest.jar does not exist, skipping.
[2023-01-22T07:58:29.514+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:29 WARN DependencyUtils: Local jar /opt/***/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-22T07:58:32.629+0000] {spark_submit.py:495} INFO - /opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010000.json
[2023-01-22T07:58:32.922+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:32 INFO SparkContext: Running Spark version 3.3.1
[2023-01-22T07:58:33.020+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:33 INFO ResourceUtils: ==============================================================
[2023-01-22T07:58:33.021+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:33 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-22T07:58:33.030+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:33 INFO ResourceUtils: ==============================================================
[2023-01-22T07:58:33.031+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:33 INFO SparkContext: Submitted application: gcp_playground
[2023-01-22T07:58:33.110+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:33 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-22T07:58:33.131+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:33 INFO ResourceProfile: Limiting resource is cpu
[2023-01-22T07:58:33.131+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:33 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-22T07:58:33.264+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:33 INFO SecurityManager: Changing view acls to: ***
[2023-01-22T07:58:33.265+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:33 INFO SecurityManager: Changing modify acls to: ***
[2023-01-22T07:58:33.265+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:33 INFO SecurityManager: Changing view acls groups to:
[2023-01-22T07:58:33.266+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:33 INFO SecurityManager: Changing modify acls groups to:
[2023-01-22T07:58:33.268+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:33 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-22T07:58:33.893+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:33 INFO Utils: Successfully started service 'sparkDriver' on port 42837.
[2023-01-22T07:58:34.034+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:34 INFO SparkEnv: Registering MapOutputTracker
[2023-01-22T07:58:34.184+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:34 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-22T07:58:34.283+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:34 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-22T07:58:34.285+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:34 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-22T07:58:34.300+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:34 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-22T07:58:34.371+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:34 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-5a0507a3-103c-4480-ba9f-0af6de2cec44
[2023-01-22T07:58:34.451+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:34 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-22T07:58:34.566+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:34 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-22T07:58:35.414+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:35 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-01-22T07:58:35.558+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:35 INFO SparkContext: Added JAR /opt/spark/resources/gcs-connector-hadoop3-latest.jar at spark://16233a798013:42837/jars/gcs-connector-hadoop3-latest.jar with timestamp 1674374312889
[2023-01-22T07:58:35.560+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:35 INFO SparkContext: Added JAR /opt/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://16233a798013:42837/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1674374312889
[2023-01-22T07:58:35.883+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:35 INFO Executor: Starting executor ID driver on host 16233a798013
[2023-01-22T07:58:35.947+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:35 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-22T07:58:36.032+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:36 INFO Executor: Fetching spark://16233a798013:42837/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1674374312889
[2023-01-22T07:58:36.199+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:36 INFO TransportClientFactory: Successfully created connection to 16233a798013/192.168.32.7:42837 after 87 ms (0 ms spent in bootstraps)
[2023-01-22T07:58:36.231+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:36 INFO Utils: Fetching spark://16233a798013:42837/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-33324b48-48c6-4f78-b95c-6ba59f014842/userFiles-0d50c968-31c7-4018-b0bb-fed75f79d87b/fetchFileTemp12074635087358350135.tmp
[2023-01-22T07:58:36.752+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:36 INFO Executor: Adding file:/tmp/spark-33324b48-48c6-4f78-b95c-6ba59f014842/userFiles-0d50c968-31c7-4018-b0bb-fed75f79d87b/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-22T07:58:36.752+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:36 INFO Executor: Fetching spark://16233a798013:42837/jars/gcs-connector-hadoop3-latest.jar with timestamp 1674374312889
[2023-01-22T07:58:36.758+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:36 INFO Utils: Fetching spark://16233a798013:42837/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-33324b48-48c6-4f78-b95c-6ba59f014842/userFiles-0d50c968-31c7-4018-b0bb-fed75f79d87b/fetchFileTemp5074610962313710073.tmp
[2023-01-22T07:58:37.040+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:37 INFO Executor: Adding file:/tmp/spark-33324b48-48c6-4f78-b95c-6ba59f014842/userFiles-0d50c968-31c7-4018-b0bb-fed75f79d87b/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-22T07:58:37.053+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:37 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33369.
[2023-01-22T07:58:37.055+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:37 INFO NettyBlockTransferService: Server created on 16233a798013:33369
[2023-01-22T07:58:37.058+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:37 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-22T07:58:37.076+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:37 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 16233a798013, 33369, None)
[2023-01-22T07:58:37.081+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:37 INFO BlockManagerMasterEndpoint: Registering block manager 16233a798013:33369 with 434.4 MiB RAM, BlockManagerId(driver, 16233a798013, 33369, None)
[2023-01-22T07:58:37.087+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:37 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 16233a798013, 33369, None)
[2023-01-22T07:58:37.089+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:37 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 16233a798013, 33369, None)
[2023-01-22T07:58:37.721+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:37 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-22T07:58:37.735+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:37 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-22T07:58:38.933+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:38 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: gs://entsoe_analytics_1009//opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010000.json.
[2023-01-22T07:58:38.933+0000] {spark_submit.py:495} INFO - org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "gs"
[2023-01-22T07:58:38.933+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2023-01-22T07:58:38.934+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T07:58:38.934+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T07:58:38.934+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T07:58:38.934+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T07:58:38.934+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T07:58:38.934+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T07:58:38.934+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)
[2023-01-22T07:58:38.934+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:370)
[2023-01-22T07:58:38.935+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T07:58:38.935+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T07:58:38.935+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T07:58:38.935+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T07:58:38.935+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T07:58:38.935+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T07:58:38.935+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T07:58:38.935+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T07:58:38.936+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T07:58:38.936+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T07:58:38.936+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T07:58:38.936+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T07:58:38.936+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T07:58:38.936+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T07:58:38.936+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T07:58:38.937+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T07:58:38.937+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T07:58:38.959+0000] {spark_submit.py:495} INFO - Error, ada An error occurred while calling o38.load.
[2023-01-22T07:58:38.960+0000] {spark_submit.py:495} INFO - : org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "gs"
[2023-01-22T07:58:38.960+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2023-01-22T07:58:38.960+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T07:58:38.960+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T07:58:38.961+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T07:58:38.961+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T07:58:38.961+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T07:58:38.962+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T07:58:38.962+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:752)
[2023-01-22T07:58:38.962+0000] {spark_submit.py:495} INFO - at scala.collection.immutable.List.map(List.scala:246)
[2023-01-22T07:58:38.962+0000] {spark_submit.py:495} INFO - at scala.collection.immutable.List.map(List.scala:79)
[2023-01-22T07:58:38.962+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:750)
[2023-01-22T07:58:38.962+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:579)
[2023-01-22T07:58:38.963+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:408)
[2023-01-22T07:58:38.963+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T07:58:38.963+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T07:58:38.963+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T07:58:38.963+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T07:58:38.964+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T07:58:38.964+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T07:58:38.964+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T07:58:38.964+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T07:58:38.964+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T07:58:38.964+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T07:58:38.965+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T07:58:38.965+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T07:58:38.965+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T07:58:38.965+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T07:58:38.965+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T07:58:38.965+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T07:58:38.965+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T07:58:38.966+0000] {spark_submit.py:495} INFO - 
[2023-01-22T07:58:38.966+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T07:58:38.966+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 69, in <module>
[2023-01-22T07:58:38.966+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T07:58:38.966+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 56, in main
[2023-01-22T07:58:38.966+0000] {spark_submit.py:495} INFO - df_spark.write.format("bigquery").option("project", "rafzul-analytics-1009") \
[2023-01-22T07:58:38.966+0000] {spark_submit.py:495} INFO - UnboundLocalError: local variable 'df_spark' referenced before assignment
[2023-01-22T07:58:39.028+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:39 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-22T07:58:39.041+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:39 INFO SparkUI: Stopped Spark web UI at http://16233a798013:4040
[2023-01-22T07:58:39.063+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:39 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-22T07:58:39.084+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:39 INFO MemoryStore: MemoryStore cleared
[2023-01-22T07:58:39.085+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:39 INFO BlockManager: BlockManager stopped
[2023-01-22T07:58:39.094+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:39 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-22T07:58:39.098+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:39 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-22T07:58:39.115+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:39 INFO SparkContext: Successfully stopped SparkContext
[2023-01-22T07:58:39.116+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:39 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T07:58:39.117+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-33324b48-48c6-4f78-b95c-6ba59f014842
[2023-01-22T07:58:39.121+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-1bf7a27a-efe0-4242-83a5-de15ee6ce135
[2023-01-22T07:58:39.126+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-33324b48-48c6-4f78-b95c-6ba59f014842/pyspark-3656309b-f6ee-4925-a687-849393aa2170
[2023-01-22T07:58:39.215+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-22T07:58:39.218+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230122T075821, end_date=20230122T075839
[2023-01-22T07:58:39.230+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 331 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 2017)
[2023-01-22T07:58:39.249+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T07:58:39.263+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T08:38:41.445+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T08:38:41.455+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T08:38:41.456+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T08:38:41.456+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-22T08:38:41.456+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T08:38:41.468+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-22T08:38:41.475+0000] {standard_task_runner.py:55} INFO - Started process 360 to run task
[2023-01-22T08:38:41.480+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '374', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpd1p70osm']
[2023-01-22T08:38:41.484+0000] {standard_task_runner.py:83} INFO - Job 374: Subtask transform_stage_generation
[2023-01-22T08:38:41.548+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 9d8db2d97423
[2023-01-22T08:38:41.623+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-22T08:38:41.632+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T08:38:41.634+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-22T08:38:45.753+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T08:38:46.061+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:46 WARN DependencyUtils: Local jar /opt/***/gcs-connector-hadoop3-latest.jar does not exist, skipping.
[2023-01-22T08:38:46.061+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:46 WARN DependencyUtils: Local jar /opt/***/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-22T08:38:47.290+0000] {spark_submit.py:495} INFO - /opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010000.json
[2023-01-22T08:38:47.389+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:47 INFO SparkContext: Running Spark version 3.3.1
[2023-01-22T08:38:47.427+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:47 INFO ResourceUtils: ==============================================================
[2023-01-22T08:38:47.427+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:47 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-22T08:38:47.428+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:47 INFO ResourceUtils: ==============================================================
[2023-01-22T08:38:47.429+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:47 INFO SparkContext: Submitted application: gcp_playground
[2023-01-22T08:38:47.467+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:47 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-22T08:38:47.475+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:47 INFO ResourceProfile: Limiting resource is cpu
[2023-01-22T08:38:47.477+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:47 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-22T08:38:47.565+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:47 INFO SecurityManager: Changing view acls to: ***
[2023-01-22T08:38:47.566+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:47 INFO SecurityManager: Changing modify acls to: ***
[2023-01-22T08:38:47.567+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:47 INFO SecurityManager: Changing view acls groups to:
[2023-01-22T08:38:47.568+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:47 INFO SecurityManager: Changing modify acls groups to:
[2023-01-22T08:38:47.569+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:47 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-22T08:38:47.945+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:47 INFO Utils: Successfully started service 'sparkDriver' on port 39943.
[2023-01-22T08:38:47.998+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:47 INFO SparkEnv: Registering MapOutputTracker
[2023-01-22T08:38:48.046+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-22T08:38:48.096+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-22T08:38:48.097+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-22T08:38:48.105+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-22T08:38:48.184+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-5332ee00-e429-43bd-aef0-8216c27e49dc
[2023-01-22T08:38:48.214+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-22T08:38:48.243+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-22T08:38:48.606+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-22T08:38:48.625+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2023-01-22T08:38:48.699+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 ERROR SparkContext: Failed to add file:/opt/***/gcs-connector-hadoop3-latest.jar to Spark environment
[2023-01-22T08:38:48.699+0000] {spark_submit.py:495} INFO - java.io.FileNotFoundException: Jar /opt/***/gcs-connector-hadoop3-latest.jar not found
[2023-01-22T08:38:48.699+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1949)
[2023-01-22T08:38:48.700+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.addJar(SparkContext.scala:2004)
[2023-01-22T08:38:48.700+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:507)
[2023-01-22T08:38:48.700+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:507)
[2023-01-22T08:38:48.700+0000] {spark_submit.py:495} INFO - at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
[2023-01-22T08:38:48.701+0000] {spark_submit.py:495} INFO - at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
[2023-01-22T08:38:48.701+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
[2023-01-22T08:38:48.701+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.<init>(SparkContext.scala:507)
[2023-01-22T08:38:48.701+0000] {spark_submit.py:495} INFO - at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
[2023-01-22T08:38:48.701+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
[2023-01-22T08:38:48.702+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
[2023-01-22T08:38:48.702+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
[2023-01-22T08:38:48.702+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
[2023-01-22T08:38:48.702+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
[2023-01-22T08:38:48.703+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T08:38:48.703+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:238)
[2023-01-22T08:38:48.703+0000] {spark_submit.py:495} INFO - at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
[2023-01-22T08:38:48.703+0000] {spark_submit.py:495} INFO - at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
[2023-01-22T08:38:48.704+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T08:38:48.704+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T08:38:48.704+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T08:38:48.704+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 ERROR SparkContext: Failed to add file:/opt/***/spark-bigquery-with-dependencies_2.13-0.27.1.jar to Spark environment
[2023-01-22T08:38:48.705+0000] {spark_submit.py:495} INFO - java.io.FileNotFoundException: Jar /opt/***/spark-bigquery-with-dependencies_2.13-0.27.1.jar not found
[2023-01-22T08:38:48.705+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1949)
[2023-01-22T08:38:48.705+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.addJar(SparkContext.scala:2004)
[2023-01-22T08:38:48.705+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:507)
[2023-01-22T08:38:48.706+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:507)
[2023-01-22T08:38:48.706+0000] {spark_submit.py:495} INFO - at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
[2023-01-22T08:38:48.706+0000] {spark_submit.py:495} INFO - at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
[2023-01-22T08:38:48.707+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
[2023-01-22T08:38:48.707+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.<init>(SparkContext.scala:507)
[2023-01-22T08:38:48.707+0000] {spark_submit.py:495} INFO - at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
[2023-01-22T08:38:48.707+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
[2023-01-22T08:38:48.707+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
[2023-01-22T08:38:48.707+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
[2023-01-22T08:38:48.708+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
[2023-01-22T08:38:48.708+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
[2023-01-22T08:38:48.708+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T08:38:48.708+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:238)
[2023-01-22T08:38:48.708+0000] {spark_submit.py:495} INFO - at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
[2023-01-22T08:38:48.708+0000] {spark_submit.py:495} INFO - at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
[2023-01-22T08:38:48.709+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T08:38:48.709+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T08:38:48.709+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T08:38:48.808+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO Executor: Starting executor ID driver on host 9d8db2d97423
[2023-01-22T08:38:48.818+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-22T08:38:48.851+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34655.
[2023-01-22T08:38:48.852+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO NettyBlockTransferService: Server created on 9d8db2d97423:34655
[2023-01-22T08:38:48.855+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-22T08:38:48.869+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 9d8db2d97423, 34655, None)
[2023-01-22T08:38:48.875+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO BlockManagerMasterEndpoint: Registering block manager 9d8db2d97423:34655 with 434.4 MiB RAM, BlockManagerId(driver, 9d8db2d97423, 34655, None)
[2023-01-22T08:38:48.879+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 9d8db2d97423, 34655, None)
[2023-01-22T08:38:48.881+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 9d8db2d97423, 34655, None)
[2023-01-22T08:38:49.640+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:49 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-22T08:38:49.646+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:49 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-22T08:38:51.348+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:51 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: gs://entsoe_analytics_1009//opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010000.json.
[2023-01-22T08:38:51.349+0000] {spark_submit.py:495} INFO - org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "gs"
[2023-01-22T08:38:51.349+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2023-01-22T08:38:51.349+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T08:38:51.349+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T08:38:51.349+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T08:38:51.349+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T08:38:51.350+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T08:38:51.350+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T08:38:51.350+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)
[2023-01-22T08:38:51.350+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:370)
[2023-01-22T08:38:51.350+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T08:38:51.350+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T08:38:51.351+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T08:38:51.351+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T08:38:51.351+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T08:38:51.351+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T08:38:51.351+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T08:38:51.352+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T08:38:51.352+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T08:38:51.352+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T08:38:51.352+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T08:38:51.352+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T08:38:51.352+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T08:38:51.353+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T08:38:51.353+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T08:38:51.353+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T08:38:51.353+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T08:38:51.392+0000] {spark_submit.py:495} INFO - Error, ada An error occurred while calling o37.load.
[2023-01-22T08:38:51.393+0000] {spark_submit.py:495} INFO - : org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "gs"
[2023-01-22T08:38:51.393+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2023-01-22T08:38:51.394+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T08:38:51.394+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T08:38:51.394+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T08:38:51.394+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T08:38:51.395+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T08:38:51.395+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T08:38:51.395+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:752)
[2023-01-22T08:38:51.396+0000] {spark_submit.py:495} INFO - at scala.collection.immutable.List.map(List.scala:246)
[2023-01-22T08:38:51.396+0000] {spark_submit.py:495} INFO - at scala.collection.immutable.List.map(List.scala:79)
[2023-01-22T08:38:51.396+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:750)
[2023-01-22T08:38:51.396+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:579)
[2023-01-22T08:38:51.397+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:408)
[2023-01-22T08:38:51.397+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T08:38:51.397+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T08:38:51.397+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T08:38:51.397+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T08:38:51.398+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T08:38:51.398+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T08:38:51.398+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T08:38:51.399+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T08:38:51.399+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T08:38:51.399+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T08:38:51.400+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T08:38:51.400+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T08:38:51.400+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T08:38:51.400+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T08:38:51.401+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T08:38:51.401+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T08:38:51.401+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T08:38:51.401+0000] {spark_submit.py:495} INFO - 
[2023-01-22T08:38:51.402+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T08:38:51.402+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 68, in <module>
[2023-01-22T08:38:51.402+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T08:38:51.402+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 55, in main
[2023-01-22T08:38:51.403+0000] {spark_submit.py:495} INFO - df_spark.write.format("bigquery").option("project", "rafzul-analytics-1009") \
[2023-01-22T08:38:51.403+0000] {spark_submit.py:495} INFO - UnboundLocalError: local variable 'df_spark' referenced before assignment
[2023-01-22T08:38:51.484+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:51 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-22T08:38:51.501+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:51 INFO SparkUI: Stopped Spark web UI at http://9d8db2d97423:4041
[2023-01-22T08:38:51.529+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:51 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-22T08:38:51.571+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:51 INFO MemoryStore: MemoryStore cleared
[2023-01-22T08:38:51.573+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:51 INFO BlockManager: BlockManager stopped
[2023-01-22T08:38:51.599+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:51 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-22T08:38:51.607+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:51 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-22T08:38:51.636+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:51 INFO SparkContext: Successfully stopped SparkContext
[2023-01-22T08:38:51.637+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:51 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T08:38:51.638+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-bdd5f1f1-1d65-425f-9d70-93b61f755f21/pyspark-59717ec9-87d8-496a-a22b-10f531b95371
[2023-01-22T08:38:51.655+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-bdd5f1f1-1d65-425f-9d70-93b61f755f21
[2023-01-22T08:38:51.663+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-0bd83a68-0fbb-4145-a2c0-90e32fe0f748
[2023-01-22T08:38:51.751+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-22T08:38:51.756+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230122T083841, end_date=20230122T083851
[2023-01-22T08:38:51.771+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 374 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 360)
[2023-01-22T08:38:51.794+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T08:38:51.815+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T08:48:47.935+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T08:48:47.944+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T08:48:47.945+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T08:48:47.945+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-22T08:48:47.945+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T08:48:47.959+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-22T08:48:47.966+0000] {standard_task_runner.py:55} INFO - Started process 1494 to run task
[2023-01-22T08:48:47.970+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '380', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpl8822rql']
[2023-01-22T08:48:47.973+0000] {standard_task_runner.py:83} INFO - Job 380: Subtask transform_stage_generation
[2023-01-22T08:48:48.036+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 9d8db2d97423
[2023-01-22T08:48:48.105+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-22T08:48:48.114+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T08:48:48.115+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-22T08:48:51.264+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T08:48:51.530+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:51 WARN DependencyUtils: Local jar /opt/***/gcs-connector-hadoop3-latest.jar does not exist, skipping.
[2023-01-22T08:48:51.530+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:51 WARN DependencyUtils: Local jar /opt/***/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-22T08:48:52.773+0000] {spark_submit.py:495} INFO - /opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010000.json
[2023-01-22T08:48:52.872+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:52 INFO SparkContext: Running Spark version 3.3.1
[2023-01-22T08:48:52.916+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:52 INFO ResourceUtils: ==============================================================
[2023-01-22T08:48:52.917+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:52 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-22T08:48:52.917+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:52 INFO ResourceUtils: ==============================================================
[2023-01-22T08:48:52.918+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:52 INFO SparkContext: Submitted application: gcp_playground
[2023-01-22T08:48:52.951+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:52 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-22T08:48:52.957+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:52 INFO ResourceProfile: Limiting resource is cpu
[2023-01-22T08:48:52.959+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:52 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-22T08:48:53.044+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:53 INFO SecurityManager: Changing view acls to: ***
[2023-01-22T08:48:53.044+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:53 INFO SecurityManager: Changing modify acls to: ***
[2023-01-22T08:48:53.045+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:53 INFO SecurityManager: Changing view acls groups to:
[2023-01-22T08:48:53.045+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:53 INFO SecurityManager: Changing modify acls groups to:
[2023-01-22T08:48:53.046+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-22T08:48:53.459+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:53 INFO Utils: Successfully started service 'sparkDriver' on port 34337.
[2023-01-22T08:48:53.525+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:53 INFO SparkEnv: Registering MapOutputTracker
[2023-01-22T08:48:53.591+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:53 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-22T08:48:53.649+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:53 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-22T08:48:53.651+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:53 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-22T08:48:53.665+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:53 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-22T08:48:53.716+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:53 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-10683eaa-6e5b-4fa6-95f2-acf57078f0d6
[2023-01-22T08:48:53.815+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:53 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-22T08:48:53.887+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:53 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-22T08:48:54.414+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:54 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-22T08:48:54.433+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:54 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2023-01-22T08:48:54.512+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:54 ERROR SparkContext: Failed to add file:/opt/***/gcs-connector-hadoop3-latest.jar to Spark environment
[2023-01-22T08:48:54.513+0000] {spark_submit.py:495} INFO - java.io.FileNotFoundException: Jar /opt/***/gcs-connector-hadoop3-latest.jar not found
[2023-01-22T08:48:54.513+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1949)
[2023-01-22T08:48:54.513+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.addJar(SparkContext.scala:2004)
[2023-01-22T08:48:54.513+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:507)
[2023-01-22T08:48:54.513+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:507)
[2023-01-22T08:48:54.514+0000] {spark_submit.py:495} INFO - at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
[2023-01-22T08:48:54.514+0000] {spark_submit.py:495} INFO - at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
[2023-01-22T08:48:54.514+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
[2023-01-22T08:48:54.514+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.<init>(SparkContext.scala:507)
[2023-01-22T08:48:54.514+0000] {spark_submit.py:495} INFO - at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
[2023-01-22T08:48:54.515+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
[2023-01-22T08:48:54.515+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
[2023-01-22T08:48:54.515+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
[2023-01-22T08:48:54.515+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
[2023-01-22T08:48:54.515+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
[2023-01-22T08:48:54.516+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T08:48:54.516+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:238)
[2023-01-22T08:48:54.516+0000] {spark_submit.py:495} INFO - at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
[2023-01-22T08:48:54.516+0000] {spark_submit.py:495} INFO - at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
[2023-01-22T08:48:54.516+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T08:48:54.516+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T08:48:54.517+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T08:48:54.517+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:54 ERROR SparkContext: Failed to add file:/opt/***/spark-bigquery-with-dependencies_2.13-0.27.1.jar to Spark environment
[2023-01-22T08:48:54.517+0000] {spark_submit.py:495} INFO - java.io.FileNotFoundException: Jar /opt/***/spark-bigquery-with-dependencies_2.13-0.27.1.jar not found
[2023-01-22T08:48:54.517+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1949)
[2023-01-22T08:48:54.517+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.addJar(SparkContext.scala:2004)
[2023-01-22T08:48:54.520+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:507)
[2023-01-22T08:48:54.521+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:507)
[2023-01-22T08:48:54.521+0000] {spark_submit.py:495} INFO - at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
[2023-01-22T08:48:54.521+0000] {spark_submit.py:495} INFO - at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
[2023-01-22T08:48:54.521+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
[2023-01-22T08:48:54.522+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.<init>(SparkContext.scala:507)
[2023-01-22T08:48:54.522+0000] {spark_submit.py:495} INFO - at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
[2023-01-22T08:48:54.522+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
[2023-01-22T08:48:54.522+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
[2023-01-22T08:48:54.522+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
[2023-01-22T08:48:54.522+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
[2023-01-22T08:48:54.523+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
[2023-01-22T08:48:54.523+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T08:48:54.523+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:238)
[2023-01-22T08:48:54.523+0000] {spark_submit.py:495} INFO - at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
[2023-01-22T08:48:54.524+0000] {spark_submit.py:495} INFO - at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
[2023-01-22T08:48:54.524+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T08:48:54.524+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T08:48:54.524+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T08:48:54.657+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:54 INFO Executor: Starting executor ID driver on host 9d8db2d97423
[2023-01-22T08:48:54.671+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:54 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-22T08:48:54.700+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:54 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43771.
[2023-01-22T08:48:54.701+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:54 INFO NettyBlockTransferService: Server created on 9d8db2d97423:43771
[2023-01-22T08:48:54.703+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:54 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-22T08:48:54.715+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:54 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 9d8db2d97423, 43771, None)
[2023-01-22T08:48:54.721+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:54 INFO BlockManagerMasterEndpoint: Registering block manager 9d8db2d97423:43771 with 434.4 MiB RAM, BlockManagerId(driver, 9d8db2d97423, 43771, None)
[2023-01-22T08:48:54.725+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:54 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 9d8db2d97423, 43771, None)
[2023-01-22T08:48:54.727+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:54 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 9d8db2d97423, 43771, None)
[2023-01-22T08:48:55.402+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:55 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-22T08:48:55.406+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:55 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-22T08:48:57.258+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:57 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: gs://entsoe_analytics_1009//opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010000.json.
[2023-01-22T08:48:57.259+0000] {spark_submit.py:495} INFO - org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "gs"
[2023-01-22T08:48:57.259+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2023-01-22T08:48:57.259+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T08:48:57.259+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T08:48:57.259+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T08:48:57.259+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T08:48:57.260+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T08:48:57.260+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T08:48:57.260+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)
[2023-01-22T08:48:57.260+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:370)
[2023-01-22T08:48:57.260+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T08:48:57.260+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T08:48:57.260+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T08:48:57.261+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T08:48:57.261+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T08:48:57.261+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T08:48:57.261+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T08:48:57.261+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T08:48:57.261+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T08:48:57.262+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T08:48:57.262+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T08:48:57.262+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T08:48:57.262+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T08:48:57.262+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T08:48:57.263+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T08:48:57.263+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T08:48:57.263+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T08:48:57.297+0000] {spark_submit.py:495} INFO - Error, ada An error occurred while calling o37.load.
[2023-01-22T08:48:57.297+0000] {spark_submit.py:495} INFO - : org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "gs"
[2023-01-22T08:48:57.297+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2023-01-22T08:48:57.298+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T08:48:57.298+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T08:48:57.298+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T08:48:57.298+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T08:48:57.298+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T08:48:57.298+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T08:48:57.298+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:752)
[2023-01-22T08:48:57.298+0000] {spark_submit.py:495} INFO - at scala.collection.immutable.List.map(List.scala:246)
[2023-01-22T08:48:57.299+0000] {spark_submit.py:495} INFO - at scala.collection.immutable.List.map(List.scala:79)
[2023-01-22T08:48:57.299+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:750)
[2023-01-22T08:48:57.299+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:579)
[2023-01-22T08:48:57.299+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:408)
[2023-01-22T08:48:57.299+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T08:48:57.299+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T08:48:57.299+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T08:48:57.300+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T08:48:57.300+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T08:48:57.304+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T08:48:57.305+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T08:48:57.307+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T08:48:57.307+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T08:48:57.308+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T08:48:57.308+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T08:48:57.308+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T08:48:57.308+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T08:48:57.308+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T08:48:57.309+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T08:48:57.309+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T08:48:57.309+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T08:48:57.309+0000] {spark_submit.py:495} INFO - 
[2023-01-22T08:48:57.309+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T08:48:57.309+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 67, in <module>
[2023-01-22T08:48:57.310+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T08:48:57.310+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 54, in main
[2023-01-22T08:48:57.310+0000] {spark_submit.py:495} INFO - df_spark.write.format("bigquery").option("project", "rafzul-analytics-1009") \
[2023-01-22T08:48:57.310+0000] {spark_submit.py:495} INFO - UnboundLocalError: local variable 'df_spark' referenced before assignment
[2023-01-22T08:48:57.392+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:57 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-22T08:48:57.459+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:57 INFO SparkUI: Stopped Spark web UI at http://9d8db2d97423:4041
[2023-01-22T08:48:57.541+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:57 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-22T08:48:57.575+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:57 INFO MemoryStore: MemoryStore cleared
[2023-01-22T08:48:57.576+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:57 INFO BlockManager: BlockManager stopped
[2023-01-22T08:48:57.590+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:57 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-22T08:48:57.595+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:57 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-22T08:48:57.606+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:57 INFO SparkContext: Successfully stopped SparkContext
[2023-01-22T08:48:57.607+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:57 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T08:48:57.608+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-9a0447f3-08ff-47a0-8e05-46a665c2c8d9/pyspark-78b5cf95-12c7-49ee-a0e8-f857ef7bbbc5
[2023-01-22T08:48:57.614+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-916d54e9-b14c-4c44-8488-6825964b451a
[2023-01-22T08:48:57.621+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-9a0447f3-08ff-47a0-8e05-46a665c2c8d9
[2023-01-22T08:48:57.732+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-22T08:48:57.736+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230122T084847, end_date=20230122T084857
[2023-01-22T08:48:57.757+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 380 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 1494)
[2023-01-22T08:48:57.786+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T08:48:57.801+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
