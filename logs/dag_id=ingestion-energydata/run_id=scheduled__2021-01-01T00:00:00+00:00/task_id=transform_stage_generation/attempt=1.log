[2023-01-20T16:16:53.917+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T16:16:53.930+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T16:16:53.930+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T16:16:53.930+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-20T16:16:53.930+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T16:16:53.941+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-20T16:16:53.951+0000] {standard_task_runner.py:55} INFO - Started process 197 to run task
[2023-01-20T16:16:53.956+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '61', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpueq872vm']
[2023-01-20T16:16:53.959+0000] {standard_task_runner.py:83} INFO - Job 61: Subtask transform_stage_generation
[2023-01-20T16:16:54.025+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T16:16:54.112+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-20T16:16:54.120+0000] {spark_submit.py:222} INFO - Could not load connection string spark_default, defaulting to yarn
[2023-01-20T16:16:54.121+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master yarn --name arrow-spark /opt/***/plugins/scripts/transform_stage.py total_generation 202101010000 202101010100 Europe/Berlin DE_TENNET
[2023-01-20T16:16:54.127+0000] {spark_submit.py:495} INFO - /usr/local/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit: line 27: /opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-class: No such file or directory
[2023-01-20T16:16:54.138+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master yarn --name arrow-spark /opt/***/plugins/scripts/transform_stage.py total_generation 202101010000 202101010100 Europe/Berlin DE_TENNET. Error code is: 127.
[2023-01-20T16:16:54.142+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230120T161653, end_date=20230120T161654
[2023-01-20T16:16:54.155+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 61 for task transform_stage_generation (Cannot execute: spark-submit --master yarn --name arrow-spark /opt/***/plugins/scripts/transform_stage.py total_generation 202101010000 202101010100 Europe/Berlin DE_TENNET. Error code is: 127.; 197)
[2023-01-20T16:16:54.207+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T16:16:54.228+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T16:19:06.401+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T16:19:06.413+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T16:19:06.414+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T16:19:06.414+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-20T16:19:06.414+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T16:19:06.428+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-20T16:19:06.437+0000] {standard_task_runner.py:55} INFO - Started process 337 to run task
[2023-01-20T16:19:06.442+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '65', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp15kc1xg3']
[2023-01-20T16:19:06.445+0000] {standard_task_runner.py:83} INFO - Job 65: Subtask transform_stage_generation
[2023-01-20T16:19:06.518+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T16:19:06.594+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-20T16:19:06.605+0000] {spark_submit.py:222} INFO - Could not load connection string spark_default, defaulting to yarn
[2023-01-20T16:19:06.606+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master yarn --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 Europe/Berlin DE_TENNET
[2023-01-20T16:19:06.612+0000] {spark_submit.py:495} INFO - /usr/local/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit: line 27: /opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-class: No such file or directory
[2023-01-20T16:19:06.624+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master yarn --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 Europe/Berlin DE_TENNET. Error code is: 127.
[2023-01-20T16:19:06.628+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230120T161906, end_date=20230120T161906
[2023-01-20T16:19:06.643+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 65 for task transform_stage_generation (Cannot execute: spark-submit --master yarn --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 Europe/Berlin DE_TENNET. Error code is: 127.; 337)
[2023-01-20T16:19:06.694+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T16:19:06.717+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T16:22:27.122+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T16:22:27.156+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T16:22:27.157+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T16:22:27.158+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-20T16:22:27.159+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T16:22:27.197+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-20T16:22:27.228+0000] {standard_task_runner.py:55} INFO - Started process 552 to run task
[2023-01-20T16:22:27.246+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '68', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp0horvmzi']
[2023-01-20T16:22:27.255+0000] {standard_task_runner.py:83} INFO - Job 68: Subtask transform_stage_generation
[2023-01-20T16:22:27.472+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T16:22:27.669+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-20T16:22:27.695+0000] {spark_submit.py:222} INFO - Could not load connection string spark_default, defaulting to yarn
[2023-01-20T16:22:27.698+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master yarn --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-20T16:22:27.715+0000] {spark_submit.py:495} INFO - /usr/local/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit: line 27: /opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-class: No such file or directory
[2023-01-20T16:22:27.743+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master yarn --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 127.
[2023-01-20T16:22:27.754+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230120T162227, end_date=20230120T162227
[2023-01-20T16:22:27.799+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 68 for task transform_stage_generation (Cannot execute: spark-submit --master yarn --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 127.; 552)
[2023-01-20T16:22:27.860+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T16:22:27.952+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T16:31:17.792+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T16:31:17.809+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T16:31:17.809+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T16:31:17.809+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-20T16:31:17.809+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T16:31:17.823+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-20T16:31:17.847+0000] {standard_task_runner.py:55} INFO - Started process 1114 to run task
[2023-01-20T16:31:17.852+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '75', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpsk_yetc7']
[2023-01-20T16:31:17.855+0000] {standard_task_runner.py:83} INFO - Job 75: Subtask transform_stage_generation
[2023-01-20T16:31:17.938+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T16:31:18.018+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-20T16:31:18.027+0000] {spark_submit.py:222} INFO - Could not load connection string spark_default, defaulting to yarn
[2023-01-20T16:31:18.030+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master yarn --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-20T16:31:18.036+0000] {spark_submit.py:495} INFO - /usr/local/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit: line 27: /opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-class: No such file or directory
[2023-01-20T16:31:18.046+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master yarn --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 127.
[2023-01-20T16:31:18.050+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230120T163117, end_date=20230120T163118
[2023-01-20T16:31:18.063+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 75 for task transform_stage_generation (Cannot execute: spark-submit --master yarn --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 127.; 1114)
[2023-01-20T16:31:18.103+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T16:31:18.124+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T17:55:57.464+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T17:55:57.476+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T17:55:57.476+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T17:55:57.476+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-20T17:55:57.476+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T17:55:57.491+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-20T17:55:57.500+0000] {standard_task_runner.py:55} INFO - Started process 6213 to run task
[2023-01-20T17:55:57.505+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '84', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpnytjgm5k']
[2023-01-20T17:55:57.508+0000] {standard_task_runner.py:83} INFO - Job 84: Subtask transform_stage_generation
[2023-01-20T17:55:57.575+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T17:55:57.662+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-20T17:55:57.664+0000] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-01-20T17:55:57.666+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master  --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-20T17:55:57.679+0000] {spark_submit.py:495} INFO - /usr/local/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit: line 27: /opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-class: No such file or directory
[2023-01-20T17:55:57.688+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master  --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 127.
[2023-01-20T17:55:57.693+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230120T175557, end_date=20230120T175557
[2023-01-20T17:55:57.706+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 84 for task transform_stage_generation (Cannot execute: spark-submit --master  --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 127.; 6213)
[2023-01-20T17:55:57.756+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T17:55:57.776+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T17:59:00.638+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T17:59:00.651+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T17:59:00.651+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T17:59:00.652+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-20T17:59:00.652+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T17:59:00.669+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-20T17:59:00.681+0000] {standard_task_runner.py:55} INFO - Started process 6426 to run task
[2023-01-20T17:59:00.687+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '89', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmplxuv_u9q']
[2023-01-20T17:59:00.692+0000] {standard_task_runner.py:83} INFO - Job 89: Subtask transform_stage_generation
[2023-01-20T17:59:00.809+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T17:59:00.900+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-20T17:59:00.902+0000] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-01-20T17:59:00.905+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master  --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-20T17:59:00.912+0000] {spark_submit.py:495} INFO - /usr/local/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit: line 27: /opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-class: No such file or directory
[2023-01-20T17:59:00.927+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master  --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 127.
[2023-01-20T17:59:00.933+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230120T175900, end_date=20230120T175900
[2023-01-20T17:59:00.949+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 89 for task transform_stage_generation (Cannot execute: spark-submit --master  --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 127.; 6426)
[2023-01-20T17:59:00.979+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T17:59:01.001+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T18:05:02.149+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T18:05:02.164+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T18:05:02.164+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T18:05:02.164+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-20T18:05:02.164+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T18:05:02.184+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-20T18:05:02.201+0000] {standard_task_runner.py:55} INFO - Started process 6786 to run task
[2023-01-20T18:05:02.209+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '94', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpplc320p7']
[2023-01-20T18:05:02.214+0000] {standard_task_runner.py:83} INFO - Job 94: Subtask transform_stage_generation
[2023-01-20T18:05:02.324+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T18:05:02.449+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-20T18:05:02.451+0000] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-01-20T18:05:02.454+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master  --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-20T18:05:02.464+0000] {spark_submit.py:495} INFO - /usr/local/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit: line 27: /opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-class: No such file or directory
[2023-01-20T18:05:02.479+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master  --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 127.
[2023-01-20T18:05:02.484+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230120T180502, end_date=20230120T180502
[2023-01-20T18:05:02.505+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 94 for task transform_stage_generation (Cannot execute: spark-submit --master  --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 127.; 6786)
[2023-01-20T18:05:02.555+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T18:05:02.578+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T18:11:15.403+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T18:11:15.414+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T18:11:15.414+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T18:11:15.414+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-20T18:11:15.414+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T18:11:15.426+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-20T18:11:15.435+0000] {standard_task_runner.py:55} INFO - Started process 7212 to run task
[2023-01-20T18:11:15.439+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '100', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpkg9tefe2']
[2023-01-20T18:11:15.442+0000] {standard_task_runner.py:83} INFO - Job 100: Subtask transform_stage_generation
[2023-01-20T18:11:15.514+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T18:11:15.581+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-20T18:11:15.581+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 156, in execute
    self._hook = self._get_hook()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 165, in _get_hook
    return SparkSubmitHook(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 155, in __init__
    raise RuntimeError(
RuntimeError: The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/opt/spark-3.3.1-bin-hadoop3-scala2.13bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH
[2023-01-20T18:11:15.590+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230120T181115, end_date=20230120T181115
[2023-01-20T18:11:15.599+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 100 for task transform_stage_generation (The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/opt/spark-3.3.1-bin-hadoop3-scala2.13bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH; 7212)
[2023-01-20T18:11:15.651+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T18:11:15.665+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T18:12:43.110+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T18:12:43.146+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T18:12:43.147+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T18:12:43.147+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-20T18:12:43.147+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T18:12:43.187+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-20T18:12:43.204+0000] {standard_task_runner.py:55} INFO - Started process 7307 to run task
[2023-01-20T18:12:43.214+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '104', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpwfwt1dxc']
[2023-01-20T18:12:43.222+0000] {standard_task_runner.py:83} INFO - Job 104: Subtask transform_stage_generation
[2023-01-20T18:12:43.392+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T18:12:43.537+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-20T18:12:43.538+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 156, in execute
    self._hook = self._get_hook()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 165, in _get_hook
    return SparkSubmitHook(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 155, in __init__
    raise RuntimeError(
RuntimeError: The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH
[2023-01-20T18:12:43.558+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230120T181243, end_date=20230120T181243
[2023-01-20T18:12:43.580+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 104 for task transform_stage_generation (The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH; 7307)
[2023-01-20T18:12:43.625+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T18:12:43.659+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T18:16:15.968+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T18:16:15.996+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T18:16:15.996+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T18:16:15.997+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-20T18:16:15.998+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T18:16:16.028+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-20T18:16:16.061+0000] {standard_task_runner.py:55} INFO - Started process 7543 to run task
[2023-01-20T18:16:16.068+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '108', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpc1whcyu5']
[2023-01-20T18:16:16.076+0000] {standard_task_runner.py:83} INFO - Job 108: Subtask transform_stage_generation
[2023-01-20T18:16:16.231+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T18:16:16.410+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-20T18:16:16.411+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 156, in execute
    self._hook = self._get_hook()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 165, in _get_hook
    return SparkSubmitHook(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 155, in __init__
    raise RuntimeError(
RuntimeError: The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH
[2023-01-20T18:16:16.437+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230120T181615, end_date=20230120T181616
[2023-01-20T18:16:16.472+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 108 for task transform_stage_generation (The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH; 7543)
[2023-01-20T18:16:16.518+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T18:16:16.558+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T18:18:55.636+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T18:18:55.654+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T18:18:55.654+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T18:18:55.654+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-20T18:18:55.654+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T18:18:55.675+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-20T18:18:55.690+0000] {standard_task_runner.py:55} INFO - Started process 7714 to run task
[2023-01-20T18:18:55.698+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '113', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpm9vipqnn']
[2023-01-20T18:18:55.702+0000] {standard_task_runner.py:83} INFO - Job 113: Subtask transform_stage_generation
[2023-01-20T18:18:55.805+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T18:18:55.920+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-20T18:18:55.920+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 156, in execute
    self._hook = self._get_hook()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 165, in _get_hook
    return SparkSubmitHook(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 155, in __init__
    raise RuntimeError(
RuntimeError: The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH
[2023-01-20T18:18:55.937+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230120T181855, end_date=20230120T181855
[2023-01-20T18:18:55.955+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 113 for task transform_stage_generation (The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH; 7714)
[2023-01-20T18:18:55.999+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T18:18:56.035+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T19:15:54.684+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T19:15:54.725+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T19:15:54.727+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T19:15:54.728+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-20T19:15:54.729+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T19:15:54.809+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-20T19:15:54.832+0000] {standard_task_runner.py:55} INFO - Started process 360 to run task
[2023-01-20T19:15:54.838+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '128', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp1h8wdyll']
[2023-01-20T19:15:54.840+0000] {standard_task_runner.py:83} INFO - Job 128: Subtask transform_stage_generation
[2023-01-20T19:15:55.057+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 984f5901aea4
[2023-01-20T19:15:55.331+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-20T19:15:55.331+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 156, in execute
    self._hook = self._get_hook()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 165, in _get_hook
    return SparkSubmitHook(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 155, in __init__
    raise RuntimeError(
RuntimeError: The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/usr/local/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH
[2023-01-20T19:15:55.397+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230120T191554, end_date=20230120T191555
[2023-01-20T19:15:55.458+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 128 for task transform_stage_generation (The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/usr/local/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH; 360)
[2023-01-20T19:15:55.546+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T19:15:55.717+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T03:03:17.688+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T03:03:17.710+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T03:03:17.710+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T03:03:17.711+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-21T03:03:17.711+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T03:03:17.742+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-21T03:03:17.760+0000] {standard_task_runner.py:55} INFO - Started process 324 to run task
[2023-01-21T03:03:17.771+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '140', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp942qkb5z']
[2023-01-21T03:03:17.777+0000] {standard_task_runner.py:83} INFO - Job 140: Subtask transform_stage_generation
[2023-01-21T03:03:17.928+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 2b3134e2b02d
[2023-01-21T03:03:18.087+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-21T03:03:18.089+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 156, in execute
    self._hook = self._get_hook()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 165, in _get_hook
    return SparkSubmitHook(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 155, in __init__
    raise RuntimeError(
RuntimeError: The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/usr/local/spark/bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH
[2023-01-21T03:03:18.111+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230121T030317, end_date=20230121T030318
[2023-01-21T03:03:18.136+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 140 for task transform_stage_generation (The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/usr/local/spark/bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH; 324)
[2023-01-21T03:03:18.183+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T03:03:18.220+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T07:47:28.772+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T07:47:28.783+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T07:47:28.783+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T07:47:28.784+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-21T07:47:28.784+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T07:47:28.795+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-21T07:47:28.802+0000] {standard_task_runner.py:55} INFO - Started process 71 to run task
[2023-01-21T07:47:28.807+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '162', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmps4t0igd6']
[2023-01-21T07:47:28.810+0000] {standard_task_runner.py:83} INFO - Job 162: Subtask transform_stage_generation
[2023-01-21T07:47:28.884+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 4d52b49e46b4
[2023-01-21T07:47:29.009+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-21T07:47:29.011+0000] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-01-21T07:47:29.013+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master  --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-21T07:47:29.027+0000] {spark_submit.py:495} INFO - /usr/local/spark/bin/load-spark-env.sh: line 68: ps: command not found
[2023-01-21T07:47:32.743+0000] {spark_submit.py:495} INFO - Exception in thread "main" org.apache.spark.SparkException: Master must either be yarn or start with spark, mesos, k8s, or local
[2023-01-21T07:47:32.743+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.error(SparkSubmit.scala:975)
[2023-01-21T07:47:32.744+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:238)
[2023-01-21T07:47:32.744+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:901)
[2023-01-21T07:47:32.744+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
[2023-01-21T07:47:32.744+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
[2023-01-21T07:47:32.744+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
[2023-01-21T07:47:32.745+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)
[2023-01-21T07:47:32.745+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)
[2023-01-21T07:47:32.745+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
[2023-01-21T07:47:32.787+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master  --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-21T07:47:32.793+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230121T074728, end_date=20230121T074732
[2023-01-21T07:47:32.812+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 162 for task transform_stage_generation (Cannot execute: spark-submit --master  --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 71)
[2023-01-21T07:47:32.848+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T07:47:32.874+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T08:16:38.456+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T08:16:38.469+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T08:16:38.469+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T08:16:38.470+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-21T08:16:38.470+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T08:16:38.483+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-21T08:16:38.492+0000] {standard_task_runner.py:55} INFO - Started process 2287 to run task
[2023-01-21T08:16:38.497+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '173', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpc8g6c8v8']
[2023-01-21T08:16:38.500+0000] {standard_task_runner.py:83} INFO - Job 173: Subtask transform_stage_generation
[2023-01-21T08:16:38.568+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 4d52b49e46b4
[2023-01-21T08:16:38.664+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-21T08:16:38.678+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-21T08:16:38.682+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-21T08:16:38.695+0000] {spark_submit.py:495} INFO - /usr/local/spark/bin/load-spark-env.sh: line 68: ps: command not found
[2023-01-21T08:16:41.952+0000] {spark_submit.py:495} INFO - 23/01/21 08:16:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-21T08:16:42.246+0000] {spark_submit.py:495} INFO - 23/01/21 08:16:42 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-21T08:16:42.358+0000] {spark_submit.py:495} INFO - Exception in thread "main" java.io.IOException: Cannot run program "/usr/bin/python3.10": error=2, No such file or directory
[2023-01-21T08:16:42.359+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1128)
[2023-01-21T08:16:42.359+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1071)
[2023-01-21T08:16:42.359+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.PythonRunner$.main(PythonRunner.scala:97)
[2023-01-21T08:16:42.359+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.PythonRunner.main(PythonRunner.scala)
[2023-01-21T08:16:42.359+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-21T08:16:42.359+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-21T08:16:42.360+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-21T08:16:42.360+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-21T08:16:42.360+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
[2023-01-21T08:16:42.360+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:958)
[2023-01-21T08:16:42.360+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
[2023-01-21T08:16:42.360+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
[2023-01-21T08:16:42.361+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
[2023-01-21T08:16:42.361+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)
[2023-01-21T08:16:42.361+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)
[2023-01-21T08:16:42.361+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
[2023-01-21T08:16:42.361+0000] {spark_submit.py:495} INFO - Caused by: java.io.IOException: error=2, No such file or directory
[2023-01-21T08:16:42.361+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessImpl.forkAndExec(Native Method)
[2023-01-21T08:16:42.362+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:340)
[2023-01-21T08:16:42.362+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessImpl.start(ProcessImpl.java:271)
[2023-01-21T08:16:42.362+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1107)
[2023-01-21T08:16:42.362+0000] {spark_submit.py:495} INFO - ... 15 more
[2023-01-21T08:16:42.369+0000] {spark_submit.py:495} INFO - 23/01/21 08:16:42 INFO ShutdownHookManager: Shutdown hook called
[2023-01-21T08:16:42.370+0000] {spark_submit.py:495} INFO - 23/01/21 08:16:42 INFO ShutdownHookManager: Deleting directory /tmp/spark-76ca6f76-4ff4-4a53-811a-803c704bb228
[2023-01-21T08:16:42.445+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-21T08:16:42.450+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230121T081638, end_date=20230121T081642
[2023-01-21T08:16:42.466+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 173 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 2287)
[2023-01-21T08:16:42.484+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T08:16:42.502+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T08:23:55.712+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T08:23:55.723+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T08:23:55.723+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T08:23:55.723+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-21T08:23:55.723+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T08:23:55.735+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-21T08:23:55.742+0000] {standard_task_runner.py:55} INFO - Started process 285 to run task
[2023-01-21T08:23:55.746+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '181', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpisc4tker']
[2023-01-21T08:23:55.751+0000] {standard_task_runner.py:83} INFO - Job 181: Subtask transform_stage_generation
[2023-01-21T08:23:55.821+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host d00464a30031
[2023-01-21T08:23:55.906+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-21T08:23:55.920+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-21T08:23:55.922+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-21T08:23:55.939+0000] {spark_submit.py:495} INFO - /usr/local/spark/bin/load-spark-env.sh: line 68: ps: command not found
[2023-01-21T08:23:59.685+0000] {spark_submit.py:495} INFO - 23/01/21 08:23:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-21T08:24:00.058+0000] {spark_submit.py:495} INFO - 23/01/21 08:24:00 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-21T08:24:00.253+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-21T08:24:00.254+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 2, in <module>
[2023-01-21T08:24:00.254+0000] {spark_submit.py:495} INFO - from dotenv import load_dotenv
[2023-01-21T08:24:00.254+0000] {spark_submit.py:495} INFO - ModuleNotFoundError: No module named 'dotenv'
[2023-01-21T08:24:00.276+0000] {spark_submit.py:495} INFO - 23/01/21 08:24:00 INFO ShutdownHookManager: Shutdown hook called
[2023-01-21T08:24:00.278+0000] {spark_submit.py:495} INFO - 23/01/21 08:24:00 INFO ShutdownHookManager: Deleting directory /tmp/spark-08d13d43-b8d4-486e-9a5b-0485a3af4ce8
[2023-01-21T08:24:00.355+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-21T08:24:00.360+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230121T082355, end_date=20230121T082400
[2023-01-21T08:24:00.387+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 181 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 285)
[2023-01-21T08:24:00.413+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T08:24:00.434+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T10:18:14.627+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T10:18:14.641+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T10:18:14.641+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T10:18:14.642+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-21T10:18:14.642+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T10:18:14.659+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-21T10:18:14.670+0000] {standard_task_runner.py:55} INFO - Started process 2199 to run task
[2023-01-21T10:18:14.676+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '192', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpnp4wf2gh']
[2023-01-21T10:18:14.680+0000] {standard_task_runner.py:83} INFO - Job 192: Subtask transform_stage_generation
[2023-01-21T10:18:14.783+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host b3a2c4a96689
[2023-01-21T10:18:14.959+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-21T10:18:14.984+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-21T10:18:14.990+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-21T10:18:25.378+0000] {spark_submit.py:495} INFO - 23/01/21 10:18:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-21T10:18:26.261+0000] {spark_submit.py:495} INFO - 23/01/21 10:18:26 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-21T10:18:26.384+0000] {spark_submit.py:495} INFO - Exception in thread "main" java.io.IOException: Cannot run program "/usr/bin/python3.10": error=2, No such file or directory
[2023-01-21T10:18:26.385+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1128)
[2023-01-21T10:18:26.387+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1071)
[2023-01-21T10:18:26.390+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.PythonRunner$.main(PythonRunner.scala:97)
[2023-01-21T10:18:26.391+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.PythonRunner.main(PythonRunner.scala)
[2023-01-21T10:18:26.391+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-21T10:18:26.392+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-21T10:18:26.392+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-21T10:18:26.392+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-21T10:18:26.393+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
[2023-01-21T10:18:26.393+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:958)
[2023-01-21T10:18:26.393+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
[2023-01-21T10:18:26.393+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
[2023-01-21T10:18:26.394+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
[2023-01-21T10:18:26.394+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)
[2023-01-21T10:18:26.394+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)
[2023-01-21T10:18:26.394+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
[2023-01-21T10:18:26.395+0000] {spark_submit.py:495} INFO - Caused by: java.io.IOException: error=2, No such file or directory
[2023-01-21T10:18:26.395+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessImpl.forkAndExec(Native Method)
[2023-01-21T10:18:26.395+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:340)
[2023-01-21T10:18:26.396+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessImpl.start(ProcessImpl.java:271)
[2023-01-21T10:18:26.396+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1107)
[2023-01-21T10:18:26.397+0000] {spark_submit.py:495} INFO - ... 15 more
[2023-01-21T10:18:26.404+0000] {spark_submit.py:495} INFO - 23/01/21 10:18:26 INFO ShutdownHookManager: Shutdown hook called
[2023-01-21T10:18:26.406+0000] {spark_submit.py:495} INFO - 23/01/21 10:18:26 INFO ShutdownHookManager: Deleting directory /tmp/spark-01670c95-c3fb-437f-9cfd-b697e6e02624
[2023-01-21T10:18:26.518+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-21T10:18:26.525+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230121T101814, end_date=20230121T101826
[2023-01-21T10:18:26.555+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 192 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 2199)
[2023-01-21T10:18:26.583+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T10:18:26.630+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T10:45:46.293+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T10:45:46.326+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T10:45:46.327+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T10:45:46.327+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-21T10:45:46.327+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T10:45:46.365+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-21T10:45:46.385+0000] {standard_task_runner.py:55} INFO - Started process 157 to run task
[2023-01-21T10:45:46.396+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '205', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpi6yxiejs']
[2023-01-21T10:45:46.403+0000] {standard_task_runner.py:83} INFO - Job 205: Subtask transform_stage_generation
[2023-01-21T10:45:46.564+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host b3a2c4a96689
[2023-01-21T10:45:46.755+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-21T10:45:46.779+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-21T10:45:46.785+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-21T10:45:51.731+0000] {spark_submit.py:495} INFO - 23/01/21 10:45:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-21T10:45:52.255+0000] {spark_submit.py:495} INFO - 23/01/21 10:45:52 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-21T10:45:52.337+0000] {spark_submit.py:495} INFO - Exception in thread "main" java.io.IOException: Cannot run program "/usr/bin/python3.10": error=2, No such file or directory
[2023-01-21T10:45:52.338+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1128)
[2023-01-21T10:45:52.339+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1071)
[2023-01-21T10:45:52.340+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.PythonRunner$.main(PythonRunner.scala:97)
[2023-01-21T10:45:52.340+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.PythonRunner.main(PythonRunner.scala)
[2023-01-21T10:45:52.340+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-21T10:45:52.341+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-21T10:45:52.341+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-21T10:45:52.341+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-21T10:45:52.341+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
[2023-01-21T10:45:52.342+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:958)
[2023-01-21T10:45:52.342+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
[2023-01-21T10:45:52.342+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
[2023-01-21T10:45:52.343+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
[2023-01-21T10:45:52.343+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)
[2023-01-21T10:45:52.343+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)
[2023-01-21T10:45:52.344+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
[2023-01-21T10:45:52.344+0000] {spark_submit.py:495} INFO - Caused by: java.io.IOException: error=2, No such file or directory
[2023-01-21T10:45:52.345+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessImpl.forkAndExec(Native Method)
[2023-01-21T10:45:52.345+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:340)
[2023-01-21T10:45:52.346+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessImpl.start(ProcessImpl.java:271)
[2023-01-21T10:45:52.346+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1107)
[2023-01-21T10:45:52.346+0000] {spark_submit.py:495} INFO - ... 15 more
[2023-01-21T10:45:52.356+0000] {spark_submit.py:495} INFO - 23/01/21 10:45:52 INFO ShutdownHookManager: Shutdown hook called
[2023-01-21T10:45:52.359+0000] {spark_submit.py:495} INFO - 23/01/21 10:45:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-cd243198-4111-4946-a93f-92031e66694a
[2023-01-21T10:45:52.486+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-21T10:45:52.496+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230121T104546, end_date=20230121T104552
[2023-01-21T10:45:52.534+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 205 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 157)
[2023-01-21T10:45:52.602+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T10:45:52.669+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T11:29:48.664+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T11:29:48.676+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T11:29:48.676+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T11:29:48.676+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-21T11:29:48.676+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T11:29:48.692+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-21T11:29:48.701+0000] {standard_task_runner.py:55} INFO - Started process 1124 to run task
[2023-01-21T11:29:48.707+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '218', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpnqn3n9q8']
[2023-01-21T11:29:48.711+0000] {standard_task_runner.py:83} INFO - Job 218: Subtask transform_stage_generation
[2023-01-21T11:29:48.803+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host b7c4de7fae0b
[2023-01-21T11:29:48.912+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-21T11:29:48.931+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-21T11:29:48.935+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-21T11:29:57.017+0000] {spark_submit.py:495} INFO - 23/01/21 11:29:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-21T11:29:58.039+0000] {spark_submit.py:495} INFO - 23/01/21 11:29:58 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-21T11:29:58.406+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-21T11:29:58.407+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 2, in <module>
[2023-01-21T11:29:58.407+0000] {spark_submit.py:495} INFO - from dotenv import load_dotenv, find_dotenv
[2023-01-21T11:29:58.408+0000] {spark_submit.py:495} INFO - ModuleNotFoundError: No module named 'dotenv'
[2023-01-21T11:29:58.438+0000] {spark_submit.py:495} INFO - 23/01/21 11:29:58 INFO ShutdownHookManager: Shutdown hook called
[2023-01-21T11:29:58.442+0000] {spark_submit.py:495} INFO - 23/01/21 11:29:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-316cd947-b1c9-44ba-9e79-149e10e29c8b
[2023-01-21T11:29:58.620+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-21T11:29:58.631+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230121T112948, end_date=20230121T112958
[2023-01-21T11:29:58.665+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 218 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 1124)
[2023-01-21T11:29:58.710+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T11:29:58.775+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T11:31:19.001+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T11:31:19.017+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T11:31:19.018+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T11:31:19.018+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-21T11:31:19.018+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T11:31:19.036+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-21T11:31:19.045+0000] {standard_task_runner.py:55} INFO - Started process 1335 to run task
[2023-01-21T11:31:19.052+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '222', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp053rie5o']
[2023-01-21T11:31:19.056+0000] {standard_task_runner.py:83} INFO - Job 222: Subtask transform_stage_generation
[2023-01-21T11:31:19.160+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host b7c4de7fae0b
[2023-01-21T11:31:19.291+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-21T11:31:19.305+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-21T11:31:19.308+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-21T11:31:25.015+0000] {spark_submit.py:495} INFO - 23/01/21 11:31:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-21T11:31:25.514+0000] {spark_submit.py:495} INFO - 23/01/21 11:31:25 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-21T11:31:25.697+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 2
[2023-01-21T11:31:25.698+0000] {spark_submit.py:495} INFO - from python-dotenv import load_dotenv, find_dotenv
[2023-01-21T11:31:25.698+0000] {spark_submit.py:495} INFO - ^
[2023-01-21T11:31:25.698+0000] {spark_submit.py:495} INFO - SyntaxError: invalid syntax
[2023-01-21T11:31:25.720+0000] {spark_submit.py:495} INFO - 23/01/21 11:31:25 INFO ShutdownHookManager: Shutdown hook called
[2023-01-21T11:31:25.722+0000] {spark_submit.py:495} INFO - 23/01/21 11:31:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-10c080c4-c3d9-4656-a764-e0ff73c5b187
[2023-01-21T11:31:25.829+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-21T11:31:25.836+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230121T113119, end_date=20230121T113125
[2023-01-21T11:31:25.862+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 222 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 1335)
[2023-01-21T11:31:25.918+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T11:31:25.951+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T15:52:49.622+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T15:52:49.639+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T15:52:49.640+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T15:52:49.640+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-21T15:52:49.640+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T15:52:49.661+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-21T15:52:49.673+0000] {standard_task_runner.py:55} INFO - Started process 170 to run task
[2023-01-21T15:52:49.681+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '235', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmptl1w0u1c']
[2023-01-21T15:52:49.684+0000] {standard_task_runner.py:83} INFO - Job 235: Subtask transform_stage_generation
[2023-01-21T15:52:49.772+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 673f7b70a136
[2023-01-21T15:52:49.871+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-21T15:52:49.884+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-21T15:52:49.888+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-21T15:52:56.246+0000] {spark_submit.py:495} INFO - 23/01/21 15:52:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-21T15:52:56.755+0000] {spark_submit.py:495} INFO - 23/01/21 15:52:56 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-21T15:52:56.984+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-21T15:52:56.984+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 2, in <module>
[2023-01-21T15:52:56.984+0000] {spark_submit.py:495} INFO - from dotenv import load_dotenv, find_dotenv
[2023-01-21T15:52:56.984+0000] {spark_submit.py:495} INFO - ModuleNotFoundError: No module named 'dotenv'
[2023-01-21T15:52:57.004+0000] {spark_submit.py:495} INFO - 23/01/21 15:52:57 INFO ShutdownHookManager: Shutdown hook called
[2023-01-21T15:52:57.006+0000] {spark_submit.py:495} INFO - 23/01/21 15:52:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-d7924779-f8c1-4ea3-bec1-6ccfa5b0009b
[2023-01-21T15:52:57.096+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-21T15:52:57.102+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230121T155249, end_date=20230121T155257
[2023-01-21T15:52:57.123+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 235 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 170)
[2023-01-21T15:52:57.175+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T15:52:57.215+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T16:54:30.080+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T16:54:30.094+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T16:54:30.095+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T16:54:30.095+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-21T16:54:30.095+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T16:54:30.106+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-21T16:54:30.119+0000] {standard_task_runner.py:55} INFO - Started process 358 to run task
[2023-01-21T16:54:30.123+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '247', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp_fep1hn2']
[2023-01-21T16:54:30.126+0000] {standard_task_runner.py:83} INFO - Job 247: Subtask transform_stage_generation
[2023-01-21T16:54:30.203+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 4260804338b2
[2023-01-21T16:54:30.290+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-21T16:54:30.300+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-21T16:54:30.302+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-21T16:54:33.526+0000] {spark_submit.py:495} INFO - 23/01/21 16:54:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-21T16:54:33.902+0000] {spark_submit.py:495} INFO - 23/01/21 16:54:33 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-21T16:54:34.048+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-21T16:54:34.048+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 2, in <module>
[2023-01-21T16:54:34.049+0000] {spark_submit.py:495} INFO - from dotenv import load_dotenv, find_dotenv
[2023-01-21T16:54:34.049+0000] {spark_submit.py:495} INFO - ModuleNotFoundError: No module named 'dotenv'
[2023-01-21T16:54:34.063+0000] {spark_submit.py:495} INFO - 23/01/21 16:54:34 INFO ShutdownHookManager: Shutdown hook called
[2023-01-21T16:54:34.065+0000] {spark_submit.py:495} INFO - 23/01/21 16:54:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-a0176908-db4c-49dc-b5fa-c2bd55e2a2d8
[2023-01-21T16:54:34.181+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-21T16:54:34.187+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230121T165430, end_date=20230121T165434
[2023-01-21T16:54:34.208+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 247 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 358)
[2023-01-21T16:54:34.224+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T16:54:34.248+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T23:07:14.070+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T23:07:14.088+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T23:07:14.088+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T23:07:14.088+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-21T23:07:14.088+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T23:07:14.108+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-21T23:07:14.119+0000] {standard_task_runner.py:55} INFO - Started process 4273 to run task
[2023-01-21T23:07:14.125+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '257', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp_5_c0o_x']
[2023-01-21T23:07:14.129+0000] {standard_task_runner.py:83} INFO - Job 257: Subtask transform_stage_generation
[2023-01-21T23:07:14.225+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 4260804338b2
[2023-01-21T23:07:14.360+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-21T23:07:14.380+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-21T23:07:14.383+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-21T23:07:18.776+0000] {spark_submit.py:495} INFO - 23/01/21 23:07:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-21T23:07:19.241+0000] {spark_submit.py:495} INFO - 23/01/21 23:07:19 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-21T23:07:19.411+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-21T23:07:19.413+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 2, in <module>
[2023-01-21T23:07:19.413+0000] {spark_submit.py:495} INFO - from dotenv import load_dotenv, find_dotenv
[2023-01-21T23:07:19.414+0000] {spark_submit.py:495} INFO - ModuleNotFoundError: No module named 'dotenv'
[2023-01-21T23:07:19.429+0000] {spark_submit.py:495} INFO - 23/01/21 23:07:19 INFO ShutdownHookManager: Shutdown hook called
[2023-01-21T23:07:19.430+0000] {spark_submit.py:495} INFO - 23/01/21 23:07:19 INFO ShutdownHookManager: Deleting directory /tmp/spark-a4b9cacf-efe9-44bb-ba89-80b72f3331cf
[2023-01-21T23:07:19.517+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-21T23:07:19.522+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230121T230714, end_date=20230121T230719
[2023-01-21T23:07:19.538+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 257 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 4273)
[2023-01-21T23:07:19.568+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T23:07:19.588+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T00:11:33.338+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T00:11:33.354+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T00:11:33.354+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T00:11:33.355+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-22T00:11:33.355+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T00:11:33.376+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-22T00:11:33.395+0000] {standard_task_runner.py:55} INFO - Started process 380 to run task
[2023-01-22T00:11:33.401+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '268', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpagxwi5z5']
[2023-01-22T00:11:33.405+0000] {standard_task_runner.py:83} INFO - Job 268: Subtask transform_stage_generation
[2023-01-22T00:11:33.513+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 669043c9475c
[2023-01-22T00:11:33.618+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-22T00:11:33.636+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T00:11:33.639+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-22T00:11:37.323+0000] {spark_submit.py:495} INFO - 23/01/22 00:11:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T00:11:37.695+0000] {spark_submit.py:495} INFO - 23/01/22 00:11:37 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-22T00:11:38.297+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T00:11:38.297+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 64, in <module>
[2023-01-22T00:11:38.297+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T00:11:38.298+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 16, in main
[2023-01-22T00:11:38.298+0000] {spark_submit.py:495} INFO - start_label = start.strftime("%Y%m%d%H%M")
[2023-01-22T00:11:38.298+0000] {spark_submit.py:495} INFO - AttributeError: 'str' object has no attribute 'strftime'
[2023-01-22T00:11:38.324+0000] {spark_submit.py:495} INFO - 23/01/22 00:11:38 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T00:11:38.326+0000] {spark_submit.py:495} INFO - 23/01/22 00:11:38 INFO ShutdownHookManager: Deleting directory /tmp/spark-0a6f8fe3-8b22-48e9-8206-ff7b2e1ce035
[2023-01-22T00:11:38.416+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-22T00:11:38.421+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230122T001133, end_date=20230122T001138
[2023-01-22T00:11:38.442+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 268 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 380)
[2023-01-22T00:11:38.461+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T00:11:38.484+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T03:52:52.692+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T03:52:52.704+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T03:52:52.704+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T03:52:52.704+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-22T03:52:52.704+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T03:52:52.717+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-22T03:52:52.724+0000] {standard_task_runner.py:55} INFO - Started process 835 to run task
[2023-01-22T03:52:52.729+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '280', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpg__skpk3']
[2023-01-22T03:52:52.732+0000] {standard_task_runner.py:83} INFO - Job 280: Subtask transform_stage_generation
[2023-01-22T03:52:52.815+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host e3ae05c9a97e
[2023-01-22T03:52:52.937+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-22T03:52:52.953+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T03:52:52.957+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /opt/***/spark/resources/gcs-connector-hadoop3-latest.jar, /opt/***/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-22T03:52:59.456+0000] {spark_submit.py:495} INFO - 23/01/22 03:52:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T03:52:59.901+0000] {spark_submit.py:495} INFO - 23/01/22 03:52:59 WARN DependencyUtils: Local jar /opt/***/ /opt/***/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-22T03:53:00.290+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T03:53:00.290+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 64, in <module>
[2023-01-22T03:53:00.291+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T03:53:00.291+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 16, in main
[2023-01-22T03:53:00.291+0000] {spark_submit.py:495} INFO - start_label = start.strftime("%Y%m%d%H%M")
[2023-01-22T03:53:00.291+0000] {spark_submit.py:495} INFO - AttributeError: 'str' object has no attribute 'strftime'
[2023-01-22T03:53:00.311+0000] {spark_submit.py:495} INFO - 23/01/22 03:53:00 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T03:53:00.312+0000] {spark_submit.py:495} INFO - 23/01/22 03:53:00 INFO ShutdownHookManager: Deleting directory /tmp/spark-f2f27fd8-ec71-4314-9fd7-80afc5253ec0
[2023-01-22T03:53:00.374+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /opt/***/spark/resources/gcs-connector-hadoop3-latest.jar, /opt/***/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-22T03:53:00.377+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230122T035252, end_date=20230122T035300
[2023-01-22T03:53:00.389+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 280 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /opt/***/spark/resources/gcs-connector-hadoop3-latest.jar, /opt/***/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 835)
[2023-01-22T03:53:00.437+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T03:53:00.452+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T04:19:42.626+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T04:19:42.640+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T04:19:42.640+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T04:19:42.640+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-22T04:19:42.640+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T04:19:42.656+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-22T04:19:42.667+0000] {standard_task_runner.py:55} INFO - Started process 2966 to run task
[2023-01-22T04:19:42.673+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '291', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpl9dhsm7l']
[2023-01-22T04:19:42.678+0000] {standard_task_runner.py:83} INFO - Job 291: Subtask transform_stage_generation
[2023-01-22T04:19:42.775+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host e3ae05c9a97e
[2023-01-22T04:19:42.882+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-22T04:19:42.894+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T04:19:42.895+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar, spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-22T04:19:49.531+0000] {spark_submit.py:495} INFO - 23/01/22 04:19:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T04:19:50.385+0000] {spark_submit.py:495} INFO - 23/01/22 04:19:50 WARN DependencyUtils: Local jar /opt/***/gcs-connector-hadoop3-latest.jar does not exist, skipping.
[2023-01-22T04:19:50.387+0000] {spark_submit.py:495} INFO - 23/01/22 04:19:50 WARN DependencyUtils: Local jar /opt/***/ spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-22T04:19:51.213+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T04:19:51.214+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 64, in <module>
[2023-01-22T04:19:51.215+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T04:19:51.216+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 16, in main
[2023-01-22T04:19:51.217+0000] {spark_submit.py:495} INFO - start_label = start.strftime("%Y%m%d%H%M")
[2023-01-22T04:19:51.217+0000] {spark_submit.py:495} INFO - AttributeError: 'str' object has no attribute 'strftime'
[2023-01-22T04:19:51.302+0000] {spark_submit.py:495} INFO - 23/01/22 04:19:51 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T04:19:51.311+0000] {spark_submit.py:495} INFO - 23/01/22 04:19:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-5e29b0a3-15df-4418-9ebb-f8bc87787206
[2023-01-22T04:19:51.470+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar, spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-22T04:19:51.479+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230122T041942, end_date=20230122T041951
[2023-01-22T04:19:51.524+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 291 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar, spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 2966)
[2023-01-22T04:19:51.621+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T04:19:51.657+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T06:08:47.064+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T06:08:47.156+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T06:08:47.157+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T06:08:47.158+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-22T06:08:47.165+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T06:08:47.297+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-22T06:08:47.369+0000] {standard_task_runner.py:55} INFO - Started process 243 to run task
[2023-01-22T06:08:47.404+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '305', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpzvft04h8']
[2023-01-22T06:08:47.425+0000] {standard_task_runner.py:83} INFO - Job 305: Subtask transform_stage_generation
[2023-01-22T06:08:47.857+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host d68405340607
[2023-01-22T06:08:48.147+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-22T06:08:48.186+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T06:08:48.193+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-22T06:08:48.220+0000] {spark_submit.py:495} INFO - /home/***/.local/bin/spark-submit: line 27: /opt/spark/bin/spark-class: No such file or directory
[2023-01-22T06:08:48.260+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 127.
[2023-01-22T06:08:48.276+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230122T060847, end_date=20230122T060848
[2023-01-22T06:08:48.321+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 305 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 127.; 243)
[2023-01-22T06:08:48.394+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T06:08:48.468+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T07:45:01.962+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T07:45:01.973+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T07:45:01.973+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T07:45:01.973+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-22T07:45:01.973+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T07:45:01.985+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-22T07:45:01.994+0000] {standard_task_runner.py:55} INFO - Started process 449 to run task
[2023-01-22T07:45:01.999+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '317', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpyj8sp3a4']
[2023-01-22T07:45:02.002+0000] {standard_task_runner.py:83} INFO - Job 317: Subtask transform_stage_generation
[2023-01-22T07:45:02.085+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 16233a798013
[2023-01-22T07:45:02.191+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-22T07:45:02.216+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T07:45:02.218+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-22T07:45:04.944+0000] {spark_submit.py:495} INFO - 23/01/22 07:45:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T07:45:05.209+0000] {spark_submit.py:495} INFO - 23/01/22 07:45:05 WARN DependencyUtils: Local jar /opt/***/gcs-connector-hadoop3-latest.jar does not exist, skipping.
[2023-01-22T07:45:05.209+0000] {spark_submit.py:495} INFO - 23/01/22 07:45:05 WARN DependencyUtils: Local jar /opt/***/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-22T07:45:05.543+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T07:45:05.543+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 64, in <module>
[2023-01-22T07:45:05.543+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T07:45:05.544+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 16, in main
[2023-01-22T07:45:05.544+0000] {spark_submit.py:495} INFO - start_label = start.strftime("%Y%m%d%H%M")
[2023-01-22T07:45:05.544+0000] {spark_submit.py:495} INFO - AttributeError: 'str' object has no attribute 'strftime'
[2023-01-22T07:45:05.561+0000] {spark_submit.py:495} INFO - 23/01/22 07:45:05 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T07:45:05.562+0000] {spark_submit.py:495} INFO - 23/01/22 07:45:05 INFO ShutdownHookManager: Deleting directory /tmp/spark-bc80d374-c0bd-4cf3-b34b-70c0fc3996f9
[2023-01-22T07:45:05.617+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-22T07:45:05.620+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230122T074501, end_date=20230122T074505
[2023-01-22T07:45:05.633+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 317 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 449)
[2023-01-22T07:45:05.672+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T07:45:05.690+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T07:48:20.172+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T07:48:20.188+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T07:48:20.188+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T07:48:20.188+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-22T07:48:20.189+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T07:48:20.211+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-22T07:48:20.221+0000] {standard_task_runner.py:55} INFO - Started process 803 to run task
[2023-01-22T07:48:20.226+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '321', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpbvcsd36o']
[2023-01-22T07:48:20.230+0000] {standard_task_runner.py:83} INFO - Job 321: Subtask transform_stage_generation
[2023-01-22T07:48:20.341+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 16233a798013
[2023-01-22T07:48:20.486+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-22T07:48:20.502+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T07:48:20.505+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-22T07:48:24.094+0000] {spark_submit.py:495} INFO - 23/01/22 07:48:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T07:48:24.388+0000] {spark_submit.py:495} INFO - 23/01/22 07:48:24 WARN DependencyUtils: Local jar /opt/***/gcs-connector-hadoop3-latest.jar does not exist, skipping.
[2023-01-22T07:48:24.388+0000] {spark_submit.py:495} INFO - 23/01/22 07:48:24 WARN DependencyUtils: Local jar /opt/***/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-22T07:48:24.836+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T07:48:24.836+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 64, in <module>
[2023-01-22T07:48:24.836+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T07:48:24.836+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 28, in main
[2023-01-22T07:48:24.836+0000] {spark_submit.py:495} INFO - SparkSession.builder.appName("gcp_playground")
[2023-01-22T07:48:24.836+0000] {spark_submit.py:495} INFO - NameError: name 'SparkSession' is not defined
[2023-01-22T07:48:24.853+0000] {spark_submit.py:495} INFO - 23/01/22 07:48:24 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T07:48:24.854+0000] {spark_submit.py:495} INFO - 23/01/22 07:48:24 INFO ShutdownHookManager: Deleting directory /tmp/spark-043a3e61-07d5-43d7-8672-37a1d126f084
[2023-01-22T07:48:24.931+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-22T07:48:24.937+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230122T074820, end_date=20230122T074824
[2023-01-22T07:48:24.953+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 321 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 803)
[2023-01-22T07:48:24.981+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T07:48:25.000+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T07:49:47.820+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T07:49:47.835+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T07:49:47.836+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T07:49:47.836+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-22T07:49:47.836+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T07:49:47.852+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-22T07:49:47.864+0000] {standard_task_runner.py:55} INFO - Started process 976 to run task
[2023-01-22T07:49:47.869+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '324', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmprb80g97s']
[2023-01-22T07:49:47.872+0000] {standard_task_runner.py:83} INFO - Job 324: Subtask transform_stage_generation
[2023-01-22T07:49:47.942+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 16233a798013
[2023-01-22T07:49:48.011+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-22T07:49:48.020+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T07:49:48.021+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-22T07:49:50.382+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T07:49:50.623+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:50 WARN DependencyUtils: Local jar /opt/***/gcs-connector-hadoop3-latest.jar does not exist, skipping.
[2023-01-22T07:49:50.624+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:50 WARN DependencyUtils: Local jar /opt/***/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-22T07:49:51.882+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:51 INFO SparkContext: Running Spark version 3.3.1
[2023-01-22T07:49:51.910+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:51 INFO ResourceUtils: ==============================================================
[2023-01-22T07:49:51.911+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:51 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-22T07:49:51.911+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:51 INFO ResourceUtils: ==============================================================
[2023-01-22T07:49:51.912+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:51 INFO SparkContext: Submitted application: gcp_playground
[2023-01-22T07:49:51.937+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:51 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-22T07:49:51.942+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:51 INFO ResourceProfile: Limiting resource is cpu
[2023-01-22T07:49:51.943+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:51 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-22T07:49:52.020+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:52 INFO SecurityManager: Changing view acls to: ***
[2023-01-22T07:49:52.021+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:52 INFO SecurityManager: Changing modify acls to: ***
[2023-01-22T07:49:52.022+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:52 INFO SecurityManager: Changing view acls groups to:
[2023-01-22T07:49:52.023+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:52 INFO SecurityManager: Changing modify acls groups to:
[2023-01-22T07:49:52.023+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:52 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-22T07:49:52.390+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:52 INFO Utils: Successfully started service 'sparkDriver' on port 43311.
[2023-01-22T07:49:52.468+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:52 INFO SparkEnv: Registering MapOutputTracker
[2023-01-22T07:49:52.524+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:52 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-22T07:49:52.581+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:52 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-22T07:49:52.582+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:52 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-22T07:49:52.590+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:52 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-22T07:49:52.636+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:52 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-e1ee3c25-4ca1-4503-8296-c7b52864d29c
[2023-01-22T07:49:52.662+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:52 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-22T07:49:52.687+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:52 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-22T07:49:53.104+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:53 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-01-22T07:49:53.164+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:53 INFO SparkContext: Added JAR /opt/spark/resources/gcs-connector-hadoop3-latest.jar at spark://16233a798013:43311/jars/gcs-connector-hadoop3-latest.jar with timestamp 1674373791875
[2023-01-22T07:49:53.164+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:53 INFO SparkContext: Added JAR /opt/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://16233a798013:43311/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1674373791875
[2023-01-22T07:49:53.325+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:53 INFO Executor: Starting executor ID driver on host 16233a798013
[2023-01-22T07:49:53.342+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:53 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-22T07:49:53.369+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:53 INFO Executor: Fetching spark://16233a798013:43311/jars/gcs-connector-hadoop3-latest.jar with timestamp 1674373791875
[2023-01-22T07:49:53.468+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:53 INFO TransportClientFactory: Successfully created connection to 16233a798013/192.168.32.7:43311 after 57 ms (0 ms spent in bootstraps)
[2023-01-22T07:49:53.480+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:53 INFO Utils: Fetching spark://16233a798013:43311/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-74cf75e9-0f61-4a26-a14f-296bd75c13a6/userFiles-38c58349-16f9-49f5-96bf-ce831dc032ac/fetchFileTemp14424657339347536426.tmp
[2023-01-22T07:49:53.819+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:53 INFO Executor: Adding file:/tmp/spark-74cf75e9-0f61-4a26-a14f-296bd75c13a6/userFiles-38c58349-16f9-49f5-96bf-ce831dc032ac/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-22T07:49:53.820+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:53 INFO Executor: Fetching spark://16233a798013:43311/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1674373791875
[2023-01-22T07:49:53.823+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:53 INFO Utils: Fetching spark://16233a798013:43311/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-74cf75e9-0f61-4a26-a14f-296bd75c13a6/userFiles-38c58349-16f9-49f5-96bf-ce831dc032ac/fetchFileTemp5397714438335686066.tmp
[2023-01-22T07:49:54.049+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:54 INFO Executor: Adding file:/tmp/spark-74cf75e9-0f61-4a26-a14f-296bd75c13a6/userFiles-38c58349-16f9-49f5-96bf-ce831dc032ac/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-22T07:49:54.063+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:54 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40903.
[2023-01-22T07:49:54.064+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:54 INFO NettyBlockTransferService: Server created on 16233a798013:40903
[2023-01-22T07:49:54.069+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:54 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-22T07:49:54.087+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:54 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 16233a798013, 40903, None)
[2023-01-22T07:49:54.094+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:54 INFO BlockManagerMasterEndpoint: Registering block manager 16233a798013:40903 with 434.4 MiB RAM, BlockManagerId(driver, 16233a798013, 40903, None)
[2023-01-22T07:49:54.099+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:54 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 16233a798013, 40903, None)
[2023-01-22T07:49:54.101+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:54 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 16233a798013, 40903, None)
[2023-01-22T07:49:54.847+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:54 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-22T07:49:54.852+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:54 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-22T07:49:56.420+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:56 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: gs://entsoe_analytics_1009//opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010000.json.
[2023-01-22T07:49:56.420+0000] {spark_submit.py:495} INFO - org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "gs"
[2023-01-22T07:49:56.421+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2023-01-22T07:49:56.421+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T07:49:56.421+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T07:49:56.421+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T07:49:56.421+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T07:49:56.422+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T07:49:56.422+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T07:49:56.422+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)
[2023-01-22T07:49:56.422+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:370)
[2023-01-22T07:49:56.423+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T07:49:56.423+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T07:49:56.423+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T07:49:56.423+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T07:49:56.423+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T07:49:56.426+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T07:49:56.426+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T07:49:56.426+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T07:49:56.426+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T07:49:56.427+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T07:49:56.427+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T07:49:56.427+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T07:49:56.427+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T07:49:56.428+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T07:49:56.428+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T07:49:56.428+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T07:49:56.428+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T07:49:56.480+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T07:49:56.481+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 67, in <module>
[2023-01-22T07:49:56.481+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T07:49:56.481+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 54, in main
[2023-01-22T07:49:56.482+0000] {spark_submit.py:495} INFO - df_spark.write.format("bigquery").option("project", "rafzul-analytics-1009") \
[2023-01-22T07:49:56.482+0000] {spark_submit.py:495} INFO - UnboundLocalError: local variable 'df_spark' referenced before assignment
[2023-01-22T07:49:56.583+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:56 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-22T07:49:56.616+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:56 INFO SparkUI: Stopped Spark web UI at http://16233a798013:4040
[2023-01-22T07:49:56.653+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:56 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-22T07:49:56.705+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:56 INFO MemoryStore: MemoryStore cleared
[2023-01-22T07:49:56.705+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:56 INFO BlockManager: BlockManager stopped
[2023-01-22T07:49:56.721+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:56 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-22T07:49:56.729+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:56 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-22T07:49:56.782+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:56 INFO SparkContext: Successfully stopped SparkContext
[2023-01-22T07:49:56.784+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:56 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T07:49:56.786+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:56 INFO ShutdownHookManager: Deleting directory /tmp/spark-74cf75e9-0f61-4a26-a14f-296bd75c13a6/pyspark-a7961b20-2ce8-4893-9b8b-87164ee89a0e
[2023-01-22T07:49:56.796+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:56 INFO ShutdownHookManager: Deleting directory /tmp/spark-57099318-c6ca-4956-b072-d8a726190454
[2023-01-22T07:49:56.808+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:56 INFO ShutdownHookManager: Deleting directory /tmp/spark-74cf75e9-0f61-4a26-a14f-296bd75c13a6
[2023-01-22T07:49:56.937+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-22T07:49:56.947+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230122T074947, end_date=20230122T074956
[2023-01-22T07:49:56.978+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 324 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 976)
[2023-01-22T07:49:57.018+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T07:49:57.065+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T07:58:21.576+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T07:58:21.602+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T07:58:21.602+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T07:58:21.602+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-22T07:58:21.603+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T07:58:21.631+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-22T07:58:21.650+0000] {standard_task_runner.py:55} INFO - Started process 2017 to run task
[2023-01-22T07:58:21.658+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '331', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpmt5uc4mr']
[2023-01-22T07:58:21.664+0000] {standard_task_runner.py:83} INFO - Job 331: Subtask transform_stage_generation
[2023-01-22T07:58:21.795+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 16233a798013
[2023-01-22T07:58:21.964+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-22T07:58:21.984+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T07:58:21.988+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-22T07:58:29.054+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T07:58:29.513+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:29 WARN DependencyUtils: Local jar /opt/***/gcs-connector-hadoop3-latest.jar does not exist, skipping.
[2023-01-22T07:58:29.514+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:29 WARN DependencyUtils: Local jar /opt/***/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-22T07:58:32.629+0000] {spark_submit.py:495} INFO - /opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010000.json
[2023-01-22T07:58:32.922+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:32 INFO SparkContext: Running Spark version 3.3.1
[2023-01-22T07:58:33.020+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:33 INFO ResourceUtils: ==============================================================
[2023-01-22T07:58:33.021+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:33 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-22T07:58:33.030+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:33 INFO ResourceUtils: ==============================================================
[2023-01-22T07:58:33.031+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:33 INFO SparkContext: Submitted application: gcp_playground
[2023-01-22T07:58:33.110+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:33 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-22T07:58:33.131+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:33 INFO ResourceProfile: Limiting resource is cpu
[2023-01-22T07:58:33.131+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:33 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-22T07:58:33.264+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:33 INFO SecurityManager: Changing view acls to: ***
[2023-01-22T07:58:33.265+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:33 INFO SecurityManager: Changing modify acls to: ***
[2023-01-22T07:58:33.265+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:33 INFO SecurityManager: Changing view acls groups to:
[2023-01-22T07:58:33.266+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:33 INFO SecurityManager: Changing modify acls groups to:
[2023-01-22T07:58:33.268+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:33 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-22T07:58:33.893+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:33 INFO Utils: Successfully started service 'sparkDriver' on port 42837.
[2023-01-22T07:58:34.034+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:34 INFO SparkEnv: Registering MapOutputTracker
[2023-01-22T07:58:34.184+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:34 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-22T07:58:34.283+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:34 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-22T07:58:34.285+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:34 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-22T07:58:34.300+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:34 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-22T07:58:34.371+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:34 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-5a0507a3-103c-4480-ba9f-0af6de2cec44
[2023-01-22T07:58:34.451+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:34 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-22T07:58:34.566+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:34 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-22T07:58:35.414+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:35 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-01-22T07:58:35.558+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:35 INFO SparkContext: Added JAR /opt/spark/resources/gcs-connector-hadoop3-latest.jar at spark://16233a798013:42837/jars/gcs-connector-hadoop3-latest.jar with timestamp 1674374312889
[2023-01-22T07:58:35.560+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:35 INFO SparkContext: Added JAR /opt/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://16233a798013:42837/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1674374312889
[2023-01-22T07:58:35.883+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:35 INFO Executor: Starting executor ID driver on host 16233a798013
[2023-01-22T07:58:35.947+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:35 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-22T07:58:36.032+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:36 INFO Executor: Fetching spark://16233a798013:42837/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1674374312889
[2023-01-22T07:58:36.199+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:36 INFO TransportClientFactory: Successfully created connection to 16233a798013/192.168.32.7:42837 after 87 ms (0 ms spent in bootstraps)
[2023-01-22T07:58:36.231+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:36 INFO Utils: Fetching spark://16233a798013:42837/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-33324b48-48c6-4f78-b95c-6ba59f014842/userFiles-0d50c968-31c7-4018-b0bb-fed75f79d87b/fetchFileTemp12074635087358350135.tmp
[2023-01-22T07:58:36.752+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:36 INFO Executor: Adding file:/tmp/spark-33324b48-48c6-4f78-b95c-6ba59f014842/userFiles-0d50c968-31c7-4018-b0bb-fed75f79d87b/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-22T07:58:36.752+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:36 INFO Executor: Fetching spark://16233a798013:42837/jars/gcs-connector-hadoop3-latest.jar with timestamp 1674374312889
[2023-01-22T07:58:36.758+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:36 INFO Utils: Fetching spark://16233a798013:42837/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-33324b48-48c6-4f78-b95c-6ba59f014842/userFiles-0d50c968-31c7-4018-b0bb-fed75f79d87b/fetchFileTemp5074610962313710073.tmp
[2023-01-22T07:58:37.040+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:37 INFO Executor: Adding file:/tmp/spark-33324b48-48c6-4f78-b95c-6ba59f014842/userFiles-0d50c968-31c7-4018-b0bb-fed75f79d87b/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-22T07:58:37.053+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:37 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33369.
[2023-01-22T07:58:37.055+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:37 INFO NettyBlockTransferService: Server created on 16233a798013:33369
[2023-01-22T07:58:37.058+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:37 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-22T07:58:37.076+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:37 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 16233a798013, 33369, None)
[2023-01-22T07:58:37.081+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:37 INFO BlockManagerMasterEndpoint: Registering block manager 16233a798013:33369 with 434.4 MiB RAM, BlockManagerId(driver, 16233a798013, 33369, None)
[2023-01-22T07:58:37.087+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:37 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 16233a798013, 33369, None)
[2023-01-22T07:58:37.089+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:37 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 16233a798013, 33369, None)
[2023-01-22T07:58:37.721+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:37 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-22T07:58:37.735+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:37 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-22T07:58:38.933+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:38 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: gs://entsoe_analytics_1009//opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010000.json.
[2023-01-22T07:58:38.933+0000] {spark_submit.py:495} INFO - org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "gs"
[2023-01-22T07:58:38.933+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2023-01-22T07:58:38.934+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T07:58:38.934+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T07:58:38.934+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T07:58:38.934+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T07:58:38.934+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T07:58:38.934+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T07:58:38.934+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)
[2023-01-22T07:58:38.934+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:370)
[2023-01-22T07:58:38.935+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T07:58:38.935+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T07:58:38.935+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T07:58:38.935+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T07:58:38.935+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T07:58:38.935+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T07:58:38.935+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T07:58:38.935+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T07:58:38.936+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T07:58:38.936+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T07:58:38.936+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T07:58:38.936+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T07:58:38.936+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T07:58:38.936+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T07:58:38.936+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T07:58:38.937+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T07:58:38.937+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T07:58:38.959+0000] {spark_submit.py:495} INFO - Error, ada An error occurred while calling o38.load.
[2023-01-22T07:58:38.960+0000] {spark_submit.py:495} INFO - : org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "gs"
[2023-01-22T07:58:38.960+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2023-01-22T07:58:38.960+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T07:58:38.960+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T07:58:38.961+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T07:58:38.961+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T07:58:38.961+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T07:58:38.962+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T07:58:38.962+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:752)
[2023-01-22T07:58:38.962+0000] {spark_submit.py:495} INFO - at scala.collection.immutable.List.map(List.scala:246)
[2023-01-22T07:58:38.962+0000] {spark_submit.py:495} INFO - at scala.collection.immutable.List.map(List.scala:79)
[2023-01-22T07:58:38.962+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:750)
[2023-01-22T07:58:38.962+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:579)
[2023-01-22T07:58:38.963+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:408)
[2023-01-22T07:58:38.963+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T07:58:38.963+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T07:58:38.963+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T07:58:38.963+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T07:58:38.964+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T07:58:38.964+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T07:58:38.964+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T07:58:38.964+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T07:58:38.964+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T07:58:38.964+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T07:58:38.965+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T07:58:38.965+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T07:58:38.965+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T07:58:38.965+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T07:58:38.965+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T07:58:38.965+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T07:58:38.965+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T07:58:38.966+0000] {spark_submit.py:495} INFO - 
[2023-01-22T07:58:38.966+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T07:58:38.966+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 69, in <module>
[2023-01-22T07:58:38.966+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T07:58:38.966+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 56, in main
[2023-01-22T07:58:38.966+0000] {spark_submit.py:495} INFO - df_spark.write.format("bigquery").option("project", "rafzul-analytics-1009") \
[2023-01-22T07:58:38.966+0000] {spark_submit.py:495} INFO - UnboundLocalError: local variable 'df_spark' referenced before assignment
[2023-01-22T07:58:39.028+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:39 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-22T07:58:39.041+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:39 INFO SparkUI: Stopped Spark web UI at http://16233a798013:4040
[2023-01-22T07:58:39.063+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:39 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-22T07:58:39.084+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:39 INFO MemoryStore: MemoryStore cleared
[2023-01-22T07:58:39.085+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:39 INFO BlockManager: BlockManager stopped
[2023-01-22T07:58:39.094+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:39 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-22T07:58:39.098+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:39 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-22T07:58:39.115+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:39 INFO SparkContext: Successfully stopped SparkContext
[2023-01-22T07:58:39.116+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:39 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T07:58:39.117+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-33324b48-48c6-4f78-b95c-6ba59f014842
[2023-01-22T07:58:39.121+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-1bf7a27a-efe0-4242-83a5-de15ee6ce135
[2023-01-22T07:58:39.126+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-33324b48-48c6-4f78-b95c-6ba59f014842/pyspark-3656309b-f6ee-4925-a687-849393aa2170
[2023-01-22T07:58:39.215+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-22T07:58:39.218+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230122T075821, end_date=20230122T075839
[2023-01-22T07:58:39.230+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 331 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 2017)
[2023-01-22T07:58:39.249+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T07:58:39.263+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T08:38:41.445+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T08:38:41.455+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T08:38:41.456+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T08:38:41.456+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-22T08:38:41.456+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T08:38:41.468+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-22T08:38:41.475+0000] {standard_task_runner.py:55} INFO - Started process 360 to run task
[2023-01-22T08:38:41.480+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '374', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpd1p70osm']
[2023-01-22T08:38:41.484+0000] {standard_task_runner.py:83} INFO - Job 374: Subtask transform_stage_generation
[2023-01-22T08:38:41.548+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 9d8db2d97423
[2023-01-22T08:38:41.623+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-22T08:38:41.632+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T08:38:41.634+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-22T08:38:45.753+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T08:38:46.061+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:46 WARN DependencyUtils: Local jar /opt/***/gcs-connector-hadoop3-latest.jar does not exist, skipping.
[2023-01-22T08:38:46.061+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:46 WARN DependencyUtils: Local jar /opt/***/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-22T08:38:47.290+0000] {spark_submit.py:495} INFO - /opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010000.json
[2023-01-22T08:38:47.389+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:47 INFO SparkContext: Running Spark version 3.3.1
[2023-01-22T08:38:47.427+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:47 INFO ResourceUtils: ==============================================================
[2023-01-22T08:38:47.427+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:47 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-22T08:38:47.428+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:47 INFO ResourceUtils: ==============================================================
[2023-01-22T08:38:47.429+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:47 INFO SparkContext: Submitted application: gcp_playground
[2023-01-22T08:38:47.467+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:47 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-22T08:38:47.475+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:47 INFO ResourceProfile: Limiting resource is cpu
[2023-01-22T08:38:47.477+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:47 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-22T08:38:47.565+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:47 INFO SecurityManager: Changing view acls to: ***
[2023-01-22T08:38:47.566+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:47 INFO SecurityManager: Changing modify acls to: ***
[2023-01-22T08:38:47.567+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:47 INFO SecurityManager: Changing view acls groups to:
[2023-01-22T08:38:47.568+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:47 INFO SecurityManager: Changing modify acls groups to:
[2023-01-22T08:38:47.569+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:47 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-22T08:38:47.945+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:47 INFO Utils: Successfully started service 'sparkDriver' on port 39943.
[2023-01-22T08:38:47.998+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:47 INFO SparkEnv: Registering MapOutputTracker
[2023-01-22T08:38:48.046+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-22T08:38:48.096+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-22T08:38:48.097+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-22T08:38:48.105+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-22T08:38:48.184+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-5332ee00-e429-43bd-aef0-8216c27e49dc
[2023-01-22T08:38:48.214+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-22T08:38:48.243+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-22T08:38:48.606+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-22T08:38:48.625+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2023-01-22T08:38:48.699+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 ERROR SparkContext: Failed to add file:/opt/***/gcs-connector-hadoop3-latest.jar to Spark environment
[2023-01-22T08:38:48.699+0000] {spark_submit.py:495} INFO - java.io.FileNotFoundException: Jar /opt/***/gcs-connector-hadoop3-latest.jar not found
[2023-01-22T08:38:48.699+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1949)
[2023-01-22T08:38:48.700+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.addJar(SparkContext.scala:2004)
[2023-01-22T08:38:48.700+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:507)
[2023-01-22T08:38:48.700+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:507)
[2023-01-22T08:38:48.700+0000] {spark_submit.py:495} INFO - at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
[2023-01-22T08:38:48.701+0000] {spark_submit.py:495} INFO - at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
[2023-01-22T08:38:48.701+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
[2023-01-22T08:38:48.701+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.<init>(SparkContext.scala:507)
[2023-01-22T08:38:48.701+0000] {spark_submit.py:495} INFO - at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
[2023-01-22T08:38:48.701+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
[2023-01-22T08:38:48.702+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
[2023-01-22T08:38:48.702+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
[2023-01-22T08:38:48.702+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
[2023-01-22T08:38:48.702+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
[2023-01-22T08:38:48.703+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T08:38:48.703+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:238)
[2023-01-22T08:38:48.703+0000] {spark_submit.py:495} INFO - at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
[2023-01-22T08:38:48.703+0000] {spark_submit.py:495} INFO - at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
[2023-01-22T08:38:48.704+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T08:38:48.704+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T08:38:48.704+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T08:38:48.704+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 ERROR SparkContext: Failed to add file:/opt/***/spark-bigquery-with-dependencies_2.13-0.27.1.jar to Spark environment
[2023-01-22T08:38:48.705+0000] {spark_submit.py:495} INFO - java.io.FileNotFoundException: Jar /opt/***/spark-bigquery-with-dependencies_2.13-0.27.1.jar not found
[2023-01-22T08:38:48.705+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1949)
[2023-01-22T08:38:48.705+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.addJar(SparkContext.scala:2004)
[2023-01-22T08:38:48.705+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:507)
[2023-01-22T08:38:48.706+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:507)
[2023-01-22T08:38:48.706+0000] {spark_submit.py:495} INFO - at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
[2023-01-22T08:38:48.706+0000] {spark_submit.py:495} INFO - at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
[2023-01-22T08:38:48.707+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
[2023-01-22T08:38:48.707+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.<init>(SparkContext.scala:507)
[2023-01-22T08:38:48.707+0000] {spark_submit.py:495} INFO - at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
[2023-01-22T08:38:48.707+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
[2023-01-22T08:38:48.707+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
[2023-01-22T08:38:48.707+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
[2023-01-22T08:38:48.708+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
[2023-01-22T08:38:48.708+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
[2023-01-22T08:38:48.708+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T08:38:48.708+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:238)
[2023-01-22T08:38:48.708+0000] {spark_submit.py:495} INFO - at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
[2023-01-22T08:38:48.708+0000] {spark_submit.py:495} INFO - at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
[2023-01-22T08:38:48.709+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T08:38:48.709+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T08:38:48.709+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T08:38:48.808+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO Executor: Starting executor ID driver on host 9d8db2d97423
[2023-01-22T08:38:48.818+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-22T08:38:48.851+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34655.
[2023-01-22T08:38:48.852+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO NettyBlockTransferService: Server created on 9d8db2d97423:34655
[2023-01-22T08:38:48.855+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-22T08:38:48.869+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 9d8db2d97423, 34655, None)
[2023-01-22T08:38:48.875+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO BlockManagerMasterEndpoint: Registering block manager 9d8db2d97423:34655 with 434.4 MiB RAM, BlockManagerId(driver, 9d8db2d97423, 34655, None)
[2023-01-22T08:38:48.879+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 9d8db2d97423, 34655, None)
[2023-01-22T08:38:48.881+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 9d8db2d97423, 34655, None)
[2023-01-22T08:38:49.640+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:49 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-22T08:38:49.646+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:49 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-22T08:38:51.348+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:51 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: gs://entsoe_analytics_1009//opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010000.json.
[2023-01-22T08:38:51.349+0000] {spark_submit.py:495} INFO - org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "gs"
[2023-01-22T08:38:51.349+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2023-01-22T08:38:51.349+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T08:38:51.349+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T08:38:51.349+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T08:38:51.349+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T08:38:51.350+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T08:38:51.350+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T08:38:51.350+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)
[2023-01-22T08:38:51.350+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:370)
[2023-01-22T08:38:51.350+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T08:38:51.350+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T08:38:51.351+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T08:38:51.351+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T08:38:51.351+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T08:38:51.351+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T08:38:51.351+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T08:38:51.352+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T08:38:51.352+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T08:38:51.352+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T08:38:51.352+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T08:38:51.352+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T08:38:51.352+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T08:38:51.353+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T08:38:51.353+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T08:38:51.353+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T08:38:51.353+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T08:38:51.392+0000] {spark_submit.py:495} INFO - Error, ada An error occurred while calling o37.load.
[2023-01-22T08:38:51.393+0000] {spark_submit.py:495} INFO - : org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "gs"
[2023-01-22T08:38:51.393+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2023-01-22T08:38:51.394+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T08:38:51.394+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T08:38:51.394+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T08:38:51.394+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T08:38:51.395+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T08:38:51.395+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T08:38:51.395+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:752)
[2023-01-22T08:38:51.396+0000] {spark_submit.py:495} INFO - at scala.collection.immutable.List.map(List.scala:246)
[2023-01-22T08:38:51.396+0000] {spark_submit.py:495} INFO - at scala.collection.immutable.List.map(List.scala:79)
[2023-01-22T08:38:51.396+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:750)
[2023-01-22T08:38:51.396+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:579)
[2023-01-22T08:38:51.397+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:408)
[2023-01-22T08:38:51.397+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T08:38:51.397+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T08:38:51.397+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T08:38:51.397+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T08:38:51.398+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T08:38:51.398+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T08:38:51.398+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T08:38:51.399+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T08:38:51.399+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T08:38:51.399+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T08:38:51.400+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T08:38:51.400+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T08:38:51.400+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T08:38:51.400+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T08:38:51.401+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T08:38:51.401+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T08:38:51.401+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T08:38:51.401+0000] {spark_submit.py:495} INFO - 
[2023-01-22T08:38:51.402+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T08:38:51.402+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 68, in <module>
[2023-01-22T08:38:51.402+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T08:38:51.402+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 55, in main
[2023-01-22T08:38:51.403+0000] {spark_submit.py:495} INFO - df_spark.write.format("bigquery").option("project", "rafzul-analytics-1009") \
[2023-01-22T08:38:51.403+0000] {spark_submit.py:495} INFO - UnboundLocalError: local variable 'df_spark' referenced before assignment
[2023-01-22T08:38:51.484+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:51 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-22T08:38:51.501+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:51 INFO SparkUI: Stopped Spark web UI at http://9d8db2d97423:4041
[2023-01-22T08:38:51.529+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:51 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-22T08:38:51.571+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:51 INFO MemoryStore: MemoryStore cleared
[2023-01-22T08:38:51.573+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:51 INFO BlockManager: BlockManager stopped
[2023-01-22T08:38:51.599+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:51 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-22T08:38:51.607+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:51 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-22T08:38:51.636+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:51 INFO SparkContext: Successfully stopped SparkContext
[2023-01-22T08:38:51.637+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:51 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T08:38:51.638+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-bdd5f1f1-1d65-425f-9d70-93b61f755f21/pyspark-59717ec9-87d8-496a-a22b-10f531b95371
[2023-01-22T08:38:51.655+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-bdd5f1f1-1d65-425f-9d70-93b61f755f21
[2023-01-22T08:38:51.663+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-0bd83a68-0fbb-4145-a2c0-90e32fe0f748
[2023-01-22T08:38:51.751+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-22T08:38:51.756+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230122T083841, end_date=20230122T083851
[2023-01-22T08:38:51.771+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 374 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 360)
[2023-01-22T08:38:51.794+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T08:38:51.815+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T08:48:47.935+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T08:48:47.944+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T08:48:47.945+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T08:48:47.945+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-22T08:48:47.945+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T08:48:47.959+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-22T08:48:47.966+0000] {standard_task_runner.py:55} INFO - Started process 1494 to run task
[2023-01-22T08:48:47.970+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '380', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpl8822rql']
[2023-01-22T08:48:47.973+0000] {standard_task_runner.py:83} INFO - Job 380: Subtask transform_stage_generation
[2023-01-22T08:48:48.036+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 9d8db2d97423
[2023-01-22T08:48:48.105+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-22T08:48:48.114+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T08:48:48.115+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-22T08:48:51.264+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T08:48:51.530+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:51 WARN DependencyUtils: Local jar /opt/***/gcs-connector-hadoop3-latest.jar does not exist, skipping.
[2023-01-22T08:48:51.530+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:51 WARN DependencyUtils: Local jar /opt/***/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-22T08:48:52.773+0000] {spark_submit.py:495} INFO - /opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010000.json
[2023-01-22T08:48:52.872+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:52 INFO SparkContext: Running Spark version 3.3.1
[2023-01-22T08:48:52.916+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:52 INFO ResourceUtils: ==============================================================
[2023-01-22T08:48:52.917+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:52 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-22T08:48:52.917+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:52 INFO ResourceUtils: ==============================================================
[2023-01-22T08:48:52.918+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:52 INFO SparkContext: Submitted application: gcp_playground
[2023-01-22T08:48:52.951+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:52 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-22T08:48:52.957+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:52 INFO ResourceProfile: Limiting resource is cpu
[2023-01-22T08:48:52.959+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:52 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-22T08:48:53.044+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:53 INFO SecurityManager: Changing view acls to: ***
[2023-01-22T08:48:53.044+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:53 INFO SecurityManager: Changing modify acls to: ***
[2023-01-22T08:48:53.045+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:53 INFO SecurityManager: Changing view acls groups to:
[2023-01-22T08:48:53.045+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:53 INFO SecurityManager: Changing modify acls groups to:
[2023-01-22T08:48:53.046+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-22T08:48:53.459+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:53 INFO Utils: Successfully started service 'sparkDriver' on port 34337.
[2023-01-22T08:48:53.525+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:53 INFO SparkEnv: Registering MapOutputTracker
[2023-01-22T08:48:53.591+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:53 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-22T08:48:53.649+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:53 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-22T08:48:53.651+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:53 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-22T08:48:53.665+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:53 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-22T08:48:53.716+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:53 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-10683eaa-6e5b-4fa6-95f2-acf57078f0d6
[2023-01-22T08:48:53.815+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:53 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-22T08:48:53.887+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:53 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-22T08:48:54.414+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:54 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-22T08:48:54.433+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:54 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2023-01-22T08:48:54.512+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:54 ERROR SparkContext: Failed to add file:/opt/***/gcs-connector-hadoop3-latest.jar to Spark environment
[2023-01-22T08:48:54.513+0000] {spark_submit.py:495} INFO - java.io.FileNotFoundException: Jar /opt/***/gcs-connector-hadoop3-latest.jar not found
[2023-01-22T08:48:54.513+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1949)
[2023-01-22T08:48:54.513+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.addJar(SparkContext.scala:2004)
[2023-01-22T08:48:54.513+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:507)
[2023-01-22T08:48:54.513+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:507)
[2023-01-22T08:48:54.514+0000] {spark_submit.py:495} INFO - at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
[2023-01-22T08:48:54.514+0000] {spark_submit.py:495} INFO - at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
[2023-01-22T08:48:54.514+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
[2023-01-22T08:48:54.514+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.<init>(SparkContext.scala:507)
[2023-01-22T08:48:54.514+0000] {spark_submit.py:495} INFO - at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
[2023-01-22T08:48:54.515+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
[2023-01-22T08:48:54.515+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
[2023-01-22T08:48:54.515+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
[2023-01-22T08:48:54.515+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
[2023-01-22T08:48:54.515+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
[2023-01-22T08:48:54.516+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T08:48:54.516+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:238)
[2023-01-22T08:48:54.516+0000] {spark_submit.py:495} INFO - at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
[2023-01-22T08:48:54.516+0000] {spark_submit.py:495} INFO - at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
[2023-01-22T08:48:54.516+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T08:48:54.516+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T08:48:54.517+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T08:48:54.517+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:54 ERROR SparkContext: Failed to add file:/opt/***/spark-bigquery-with-dependencies_2.13-0.27.1.jar to Spark environment
[2023-01-22T08:48:54.517+0000] {spark_submit.py:495} INFO - java.io.FileNotFoundException: Jar /opt/***/spark-bigquery-with-dependencies_2.13-0.27.1.jar not found
[2023-01-22T08:48:54.517+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1949)
[2023-01-22T08:48:54.517+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.addJar(SparkContext.scala:2004)
[2023-01-22T08:48:54.520+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:507)
[2023-01-22T08:48:54.521+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:507)
[2023-01-22T08:48:54.521+0000] {spark_submit.py:495} INFO - at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
[2023-01-22T08:48:54.521+0000] {spark_submit.py:495} INFO - at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
[2023-01-22T08:48:54.521+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
[2023-01-22T08:48:54.522+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.<init>(SparkContext.scala:507)
[2023-01-22T08:48:54.522+0000] {spark_submit.py:495} INFO - at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
[2023-01-22T08:48:54.522+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
[2023-01-22T08:48:54.522+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
[2023-01-22T08:48:54.522+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
[2023-01-22T08:48:54.522+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
[2023-01-22T08:48:54.523+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
[2023-01-22T08:48:54.523+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T08:48:54.523+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:238)
[2023-01-22T08:48:54.523+0000] {spark_submit.py:495} INFO - at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
[2023-01-22T08:48:54.524+0000] {spark_submit.py:495} INFO - at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
[2023-01-22T08:48:54.524+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T08:48:54.524+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T08:48:54.524+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T08:48:54.657+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:54 INFO Executor: Starting executor ID driver on host 9d8db2d97423
[2023-01-22T08:48:54.671+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:54 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-22T08:48:54.700+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:54 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43771.
[2023-01-22T08:48:54.701+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:54 INFO NettyBlockTransferService: Server created on 9d8db2d97423:43771
[2023-01-22T08:48:54.703+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:54 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-22T08:48:54.715+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:54 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 9d8db2d97423, 43771, None)
[2023-01-22T08:48:54.721+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:54 INFO BlockManagerMasterEndpoint: Registering block manager 9d8db2d97423:43771 with 434.4 MiB RAM, BlockManagerId(driver, 9d8db2d97423, 43771, None)
[2023-01-22T08:48:54.725+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:54 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 9d8db2d97423, 43771, None)
[2023-01-22T08:48:54.727+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:54 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 9d8db2d97423, 43771, None)
[2023-01-22T08:48:55.402+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:55 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-22T08:48:55.406+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:55 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-22T08:48:57.258+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:57 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: gs://entsoe_analytics_1009//opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010000.json.
[2023-01-22T08:48:57.259+0000] {spark_submit.py:495} INFO - org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "gs"
[2023-01-22T08:48:57.259+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2023-01-22T08:48:57.259+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T08:48:57.259+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T08:48:57.259+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T08:48:57.259+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T08:48:57.260+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T08:48:57.260+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T08:48:57.260+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)
[2023-01-22T08:48:57.260+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:370)
[2023-01-22T08:48:57.260+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T08:48:57.260+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T08:48:57.260+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T08:48:57.261+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T08:48:57.261+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T08:48:57.261+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T08:48:57.261+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T08:48:57.261+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T08:48:57.261+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T08:48:57.262+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T08:48:57.262+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T08:48:57.262+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T08:48:57.262+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T08:48:57.262+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T08:48:57.263+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T08:48:57.263+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T08:48:57.263+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T08:48:57.297+0000] {spark_submit.py:495} INFO - Error, ada An error occurred while calling o37.load.
[2023-01-22T08:48:57.297+0000] {spark_submit.py:495} INFO - : org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "gs"
[2023-01-22T08:48:57.297+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2023-01-22T08:48:57.298+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T08:48:57.298+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T08:48:57.298+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T08:48:57.298+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T08:48:57.298+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T08:48:57.298+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T08:48:57.298+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:752)
[2023-01-22T08:48:57.298+0000] {spark_submit.py:495} INFO - at scala.collection.immutable.List.map(List.scala:246)
[2023-01-22T08:48:57.299+0000] {spark_submit.py:495} INFO - at scala.collection.immutable.List.map(List.scala:79)
[2023-01-22T08:48:57.299+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:750)
[2023-01-22T08:48:57.299+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:579)
[2023-01-22T08:48:57.299+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:408)
[2023-01-22T08:48:57.299+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T08:48:57.299+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T08:48:57.299+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T08:48:57.300+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T08:48:57.300+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T08:48:57.304+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T08:48:57.305+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T08:48:57.307+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T08:48:57.307+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T08:48:57.308+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T08:48:57.308+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T08:48:57.308+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T08:48:57.308+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T08:48:57.308+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T08:48:57.309+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T08:48:57.309+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T08:48:57.309+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T08:48:57.309+0000] {spark_submit.py:495} INFO - 
[2023-01-22T08:48:57.309+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T08:48:57.309+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 67, in <module>
[2023-01-22T08:48:57.310+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T08:48:57.310+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 54, in main
[2023-01-22T08:48:57.310+0000] {spark_submit.py:495} INFO - df_spark.write.format("bigquery").option("project", "rafzul-analytics-1009") \
[2023-01-22T08:48:57.310+0000] {spark_submit.py:495} INFO - UnboundLocalError: local variable 'df_spark' referenced before assignment
[2023-01-22T08:48:57.392+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:57 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-22T08:48:57.459+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:57 INFO SparkUI: Stopped Spark web UI at http://9d8db2d97423:4041
[2023-01-22T08:48:57.541+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:57 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-22T08:48:57.575+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:57 INFO MemoryStore: MemoryStore cleared
[2023-01-22T08:48:57.576+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:57 INFO BlockManager: BlockManager stopped
[2023-01-22T08:48:57.590+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:57 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-22T08:48:57.595+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:57 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-22T08:48:57.606+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:57 INFO SparkContext: Successfully stopped SparkContext
[2023-01-22T08:48:57.607+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:57 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T08:48:57.608+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-9a0447f3-08ff-47a0-8e05-46a665c2c8d9/pyspark-78b5cf95-12c7-49ee-a0e8-f857ef7bbbc5
[2023-01-22T08:48:57.614+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-916d54e9-b14c-4c44-8488-6825964b451a
[2023-01-22T08:48:57.621+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-9a0447f3-08ff-47a0-8e05-46a665c2c8d9
[2023-01-22T08:48:57.732+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-22T08:48:57.736+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230122T084847, end_date=20230122T084857
[2023-01-22T08:48:57.757+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 380 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 1494)
[2023-01-22T08:48:57.786+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T08:48:57.801+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T09:18:08.369+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T09:18:08.382+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T09:18:08.382+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T09:18:08.382+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-22T09:18:08.382+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T09:18:08.398+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-22T09:18:08.420+0000] {standard_task_runner.py:55} INFO - Started process 998 to run task
[2023-01-22T09:18:08.426+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '389', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpccceliov']
[2023-01-22T09:18:08.430+0000] {standard_task_runner.py:83} INFO - Job 389: Subtask transform_stage_generation
[2023-01-22T09:18:08.540+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 266f60b86faa
[2023-01-22T09:18:08.797+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-22T09:18:08.814+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T09:18:08.815+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-22T09:18:22.642+0000] {spark_submit.py:495} INFO - /opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010000.json
[2023-01-22T09:18:22.893+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:22 INFO SparkContext: Running Spark version 3.3.1
[2023-01-22T09:18:23.083+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T09:18:23.409+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:23 INFO ResourceUtils: ==============================================================
[2023-01-22T09:18:23.411+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:23 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-22T09:18:23.412+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:23 INFO ResourceUtils: ==============================================================
[2023-01-22T09:18:23.414+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:23 INFO SparkContext: Submitted application: gcp_playground
[2023-01-22T09:18:23.493+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:23 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-22T09:18:23.507+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:23 INFO ResourceProfile: Limiting resource is cpu
[2023-01-22T09:18:23.511+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:23 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-22T09:18:23.708+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:23 INFO SecurityManager: Changing view acls to: ***
[2023-01-22T09:18:23.710+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:23 INFO SecurityManager: Changing modify acls to: ***
[2023-01-22T09:18:23.712+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:23 INFO SecurityManager: Changing view acls groups to:
[2023-01-22T09:18:23.713+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:23 INFO SecurityManager: Changing modify acls groups to:
[2023-01-22T09:18:23.714+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-22T09:18:24.946+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:24 INFO Utils: Successfully started service 'sparkDriver' on port 42289.
[2023-01-22T09:18:25.143+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:25 INFO SparkEnv: Registering MapOutputTracker
[2023-01-22T09:18:25.364+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:25 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-22T09:18:25.480+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:25 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-22T09:18:25.482+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:25 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-22T09:18:25.492+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:25 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-22T09:18:25.563+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:25 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-704363d7-ce59-4b9d-83ec-ae64610484c2
[2023-01-22T09:18:25.623+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:25 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-22T09:18:25.688+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:25 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-22T09:18:26.580+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:26 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-01-22T09:18:26.675+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:26 INFO SparkContext: Added JAR /opt/spark/gcs-connector-hadoop3-latest.jar at spark://266f60b86faa:42289/jars/gcs-connector-hadoop3-latest.jar with timestamp 1674379102870
[2023-01-22T09:18:26.676+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:26 INFO SparkContext: Added JAR /opt/spark/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://266f60b86faa:42289/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1674379102870
[2023-01-22T09:18:26.864+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:26 INFO Executor: Starting executor ID driver on host 266f60b86faa
[2023-01-22T09:18:26.894+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:26 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-22T09:18:26.961+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:26 INFO Executor: Fetching spark://266f60b86faa:42289/jars/gcs-connector-hadoop3-latest.jar with timestamp 1674379102870
[2023-01-22T09:18:27.147+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:27 INFO TransportClientFactory: Successfully created connection to 266f60b86faa/192.168.64.4:42289 after 99 ms (0 ms spent in bootstraps)
[2023-01-22T09:18:27.179+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:27 INFO Utils: Fetching spark://266f60b86faa:42289/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-de604445-915a-47ed-b55c-74b7698d99d1/userFiles-da7e363b-2de4-45bb-8dbf-bc498fbed436/fetchFileTemp11304287788666287321.tmp
[2023-01-22T09:18:28.017+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:28 INFO Executor: Adding file:/tmp/spark-de604445-915a-47ed-b55c-74b7698d99d1/userFiles-da7e363b-2de4-45bb-8dbf-bc498fbed436/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-22T09:18:28.018+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:28 INFO Executor: Fetching spark://266f60b86faa:42289/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1674379102870
[2023-01-22T09:18:28.021+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:28 INFO Utils: Fetching spark://266f60b86faa:42289/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-de604445-915a-47ed-b55c-74b7698d99d1/userFiles-da7e363b-2de4-45bb-8dbf-bc498fbed436/fetchFileTemp18430610829554697502.tmp
[2023-01-22T09:18:28.674+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:28 INFO Executor: Adding file:/tmp/spark-de604445-915a-47ed-b55c-74b7698d99d1/userFiles-da7e363b-2de4-45bb-8dbf-bc498fbed436/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-22T09:18:28.703+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:28 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45583.
[2023-01-22T09:18:28.705+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:28 INFO NettyBlockTransferService: Server created on 266f60b86faa:45583
[2023-01-22T09:18:28.709+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:28 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-22T09:18:28.737+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:28 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 266f60b86faa, 45583, None)
[2023-01-22T09:18:28.749+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:28 INFO BlockManagerMasterEndpoint: Registering block manager 266f60b86faa:45583 with 434.4 MiB RAM, BlockManagerId(driver, 266f60b86faa, 45583, None)
[2023-01-22T09:18:28.764+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:28 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 266f60b86faa, 45583, None)
[2023-01-22T09:18:28.769+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:28 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 266f60b86faa, 45583, None)
[2023-01-22T09:18:31.236+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:31 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-22T09:18:31.266+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:31 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-22T09:18:34.690+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:34 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: gs://entsoe_analytics_1009//opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010000.json.
[2023-01-22T09:18:34.691+0000] {spark_submit.py:495} INFO - org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "gs"
[2023-01-22T09:18:34.691+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2023-01-22T09:18:34.692+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T09:18:34.692+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T09:18:34.693+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T09:18:34.693+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T09:18:34.693+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T09:18:34.694+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T09:18:34.701+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)
[2023-01-22T09:18:34.702+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:370)
[2023-01-22T09:18:34.702+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T09:18:34.738+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T09:18:34.738+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T09:18:34.739+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T09:18:34.739+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T09:18:34.740+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T09:18:34.740+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T09:18:34.741+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T09:18:34.741+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T09:18:34.741+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T09:18:34.742+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T09:18:34.762+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T09:18:34.763+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T09:18:34.763+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T09:18:34.764+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T09:18:34.764+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T09:18:34.765+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T09:18:34.933+0000] {spark_submit.py:495} INFO - Error, ada An error occurred while calling o37.load.
[2023-01-22T09:18:34.934+0000] {spark_submit.py:495} INFO - : org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "gs"
[2023-01-22T09:18:34.934+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2023-01-22T09:18:34.935+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T09:18:34.935+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T09:18:34.936+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T09:18:34.936+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T09:18:34.936+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T09:18:34.937+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T09:18:34.945+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:752)
[2023-01-22T09:18:34.946+0000] {spark_submit.py:495} INFO - at scala.collection.immutable.List.map(List.scala:246)
[2023-01-22T09:18:34.946+0000] {spark_submit.py:495} INFO - at scala.collection.immutable.List.map(List.scala:79)
[2023-01-22T09:18:34.947+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:750)
[2023-01-22T09:18:34.947+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:579)
[2023-01-22T09:18:34.947+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:408)
[2023-01-22T09:18:34.948+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T09:18:34.948+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T09:18:34.948+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T09:18:34.949+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T09:18:34.955+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T09:18:34.955+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T09:18:34.955+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T09:18:34.956+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T09:18:34.956+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T09:18:34.965+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T09:18:34.965+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T09:18:34.968+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T09:18:34.969+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T09:18:34.969+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T09:18:34.970+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T09:18:34.978+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T09:18:34.984+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T09:18:34.984+0000] {spark_submit.py:495} INFO - 
[2023-01-22T09:18:34.985+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T09:18:34.985+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 68, in <module>
[2023-01-22T09:18:34.986+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T09:18:34.986+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 55, in main
[2023-01-22T09:18:34.989+0000] {spark_submit.py:495} INFO - df_spark.write.format("bigquery").option("project", "rafzul-analytics-1009") \
[2023-01-22T09:18:34.990+0000] {spark_submit.py:495} INFO - UnboundLocalError: local variable 'df_spark' referenced before assignment
[2023-01-22T09:18:35.210+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:35 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-22T09:18:35.343+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:35 INFO SparkUI: Stopped Spark web UI at http://266f60b86faa:4040
[2023-01-22T09:18:35.553+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:35 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-22T09:18:35.694+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:35 INFO MemoryStore: MemoryStore cleared
[2023-01-22T09:18:35.710+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:35 INFO BlockManager: BlockManager stopped
[2023-01-22T09:18:35.738+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:35 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-22T09:18:35.750+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:35 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-22T09:18:35.842+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:35 INFO SparkContext: Successfully stopped SparkContext
[2023-01-22T09:18:35.843+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:35 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T09:18:35.844+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:35 INFO ShutdownHookManager: Deleting directory /tmp/spark-de604445-915a-47ed-b55c-74b7698d99d1
[2023-01-22T09:18:35.872+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:35 INFO ShutdownHookManager: Deleting directory /tmp/spark-c935f4ba-b5a3-41aa-a01d-24e212079766
[2023-01-22T09:18:35.900+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:35 INFO ShutdownHookManager: Deleting directory /tmp/spark-de604445-915a-47ed-b55c-74b7698d99d1/pyspark-64c21324-4e13-44d0-b10b-2830392c613a
[2023-01-22T09:18:36.082+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-22T09:18:36.093+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230122T091808, end_date=20230122T091836
[2023-01-22T09:18:36.143+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 389 for task transform_stage_generation (Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 998)
[2023-01-22T09:18:36.193+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T09:18:36.244+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T10:00:18.374+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T10:00:18.386+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T10:00:18.386+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T10:00:18.386+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-22T10:00:18.386+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T10:00:18.398+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-22T10:00:18.406+0000] {standard_task_runner.py:55} INFO - Started process 4616 to run task
[2023-01-22T10:00:18.409+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '400', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp8k3h5qdz']
[2023-01-22T10:00:18.413+0000] {standard_task_runner.py:83} INFO - Job 400: Subtask transform_stage_generation
[2023-01-22T10:00:18.480+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 266f60b86faa
[2023-01-22T10:00:18.578+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-22T10:00:18.588+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T10:00:18.589+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-22T10:00:21.278+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 36
[2023-01-22T10:00:21.278+0000] {spark_submit.py:495} INFO - .config("spark.hadoop.fs.AbstractFileSystem.gs.impl","com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS",) \
[2023-01-22T10:00:21.278+0000] {spark_submit.py:495} INFO - IndentationError: unexpected indent
[2023-01-22T10:00:21.296+0000] {spark_submit.py:495} INFO - 23/01/22 10:00:21 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T10:00:21.300+0000] {spark_submit.py:495} INFO - 23/01/22 10:00:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-d32c9133-a1e4-4537-89f0-3ced6e6e815e
[2023-01-22T10:00:21.366+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-22T10:00:21.373+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230122T100018, end_date=20230122T100021
[2023-01-22T10:00:21.397+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 400 for task transform_stage_generation (Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 4616)
[2023-01-22T10:00:21.441+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T10:00:21.477+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T10:01:45.464+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T10:01:45.479+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T10:01:45.479+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T10:01:45.480+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-22T10:01:45.480+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T10:01:45.499+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-22T10:01:45.512+0000] {standard_task_runner.py:55} INFO - Started process 4826 to run task
[2023-01-22T10:01:45.518+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '404', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpc4pscu51']
[2023-01-22T10:01:45.523+0000] {standard_task_runner.py:83} INFO - Job 404: Subtask transform_stage_generation
[2023-01-22T10:01:45.626+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 266f60b86faa
[2023-01-22T10:01:45.737+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-22T10:01:45.749+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T10:01:45.752+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-22T10:01:50.922+0000] {spark_submit.py:495} INFO - /opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010000.json
[2023-01-22T10:01:51.041+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:51 INFO SparkContext: Running Spark version 3.3.1
[2023-01-22T10:01:51.185+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T10:01:51.384+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:51 INFO ResourceUtils: ==============================================================
[2023-01-22T10:01:51.386+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:51 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-22T10:01:51.387+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:51 INFO ResourceUtils: ==============================================================
[2023-01-22T10:01:51.389+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:51 INFO SparkContext: Submitted application: gcp_playground
[2023-01-22T10:01:51.435+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:51 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-22T10:01:51.446+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:51 INFO ResourceProfile: Limiting resource is cpu
[2023-01-22T10:01:51.449+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:51 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-22T10:01:51.622+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:51 INFO SecurityManager: Changing view acls to: ***
[2023-01-22T10:01:51.632+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:51 INFO SecurityManager: Changing modify acls to: ***
[2023-01-22T10:01:51.633+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:51 INFO SecurityManager: Changing view acls groups to:
[2023-01-22T10:01:51.633+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:51 INFO SecurityManager: Changing modify acls groups to:
[2023-01-22T10:01:51.633+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:51 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-22T10:01:52.299+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:52 INFO Utils: Successfully started service 'sparkDriver' on port 39475.
[2023-01-22T10:01:52.422+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:52 INFO SparkEnv: Registering MapOutputTracker
[2023-01-22T10:01:52.614+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:52 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-22T10:01:52.698+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:52 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-22T10:01:52.699+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:52 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-22T10:01:52.709+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:52 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-22T10:01:52.765+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:52 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-9017ff8e-9a2f-4993-91c0-bb8dc2eed554
[2023-01-22T10:01:52.806+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:52 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-22T10:01:52.855+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:52 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-22T10:01:53.353+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:53 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-22T10:01:53.368+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:53 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2023-01-22T10:01:53.448+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:53 INFO SparkContext: Added JAR /opt/spark/gcs-connector-hadoop3-latest.jar at spark://266f60b86faa:39475/jars/gcs-connector-hadoop3-latest.jar with timestamp 1674381711025
[2023-01-22T10:01:53.448+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:53 INFO SparkContext: Added JAR /opt/spark/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://266f60b86faa:39475/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1674381711025
[2023-01-22T10:01:53.590+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:53 INFO Executor: Starting executor ID driver on host 266f60b86faa
[2023-01-22T10:01:53.606+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:53 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-22T10:01:53.626+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:53 INFO Executor: Fetching spark://266f60b86faa:39475/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1674381711025
[2023-01-22T10:01:53.721+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:53 INFO TransportClientFactory: Successfully created connection to 266f60b86faa/192.168.64.4:39475 after 53 ms (0 ms spent in bootstraps)
[2023-01-22T10:01:53.736+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:53 INFO Utils: Fetching spark://266f60b86faa:39475/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-8669c611-d465-47ed-afcd-b42f91d619c1/userFiles-4d6ad7c3-515f-4e95-af02-17a55281f217/fetchFileTemp8969019435306679076.tmp
[2023-01-22T10:01:54.066+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:54 INFO Executor: Adding file:/tmp/spark-8669c611-d465-47ed-afcd-b42f91d619c1/userFiles-4d6ad7c3-515f-4e95-af02-17a55281f217/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-22T10:01:54.066+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:54 INFO Executor: Fetching spark://266f60b86faa:39475/jars/gcs-connector-hadoop3-latest.jar with timestamp 1674381711025
[2023-01-22T10:01:54.070+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:54 INFO Utils: Fetching spark://266f60b86faa:39475/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-8669c611-d465-47ed-afcd-b42f91d619c1/userFiles-4d6ad7c3-515f-4e95-af02-17a55281f217/fetchFileTemp15375495149630908688.tmp
[2023-01-22T10:01:54.463+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:54 INFO Executor: Adding file:/tmp/spark-8669c611-d465-47ed-afcd-b42f91d619c1/userFiles-4d6ad7c3-515f-4e95-af02-17a55281f217/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-22T10:01:54.515+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:54 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36025.
[2023-01-22T10:01:54.517+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:54 INFO NettyBlockTransferService: Server created on 266f60b86faa:36025
[2023-01-22T10:01:54.519+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:54 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-22T10:01:54.538+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:54 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 266f60b86faa, 36025, None)
[2023-01-22T10:01:54.547+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:54 INFO BlockManagerMasterEndpoint: Registering block manager 266f60b86faa:36025 with 434.4 MiB RAM, BlockManagerId(driver, 266f60b86faa, 36025, None)
[2023-01-22T10:01:54.568+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:54 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 266f60b86faa, 36025, None)
[2023-01-22T10:01:54.569+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:54 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 266f60b86faa, 36025, None)
[2023-01-22T10:01:56.008+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:56 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-22T10:01:56.023+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:56 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-22T10:01:57.583+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:57 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: gs://entsoe_analytics_1009//opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010000.json.
[2023-01-22T10:01:57.584+0000] {spark_submit.py:495} INFO - java.lang.RuntimeException: java.lang.ClassNotFoundException: Class com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem not found
[2023-01-22T10:01:57.584+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)
[2023-01-22T10:01:57.584+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)
[2023-01-22T10:01:57.585+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T10:01:57.585+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T10:01:57.585+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T10:01:57.585+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T10:01:57.585+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T10:01:57.585+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T10:01:57.586+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)
[2023-01-22T10:01:57.586+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:370)
[2023-01-22T10:01:57.586+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T10:01:57.586+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T10:01:57.586+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T10:01:57.586+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T10:01:57.587+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T10:01:57.587+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T10:01:57.587+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T10:01:57.587+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T10:01:57.587+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T10:01:57.587+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T10:01:57.588+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T10:01:57.588+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T10:01:57.588+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T10:01:57.588+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T10:01:57.588+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T10:01:57.588+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T10:01:57.589+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T10:01:57.589+0000] {spark_submit.py:495} INFO - Caused by: java.lang.ClassNotFoundException: Class com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem not found
[2023-01-22T10:01:57.589+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)
[2023-01-22T10:01:57.589+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)
[2023-01-22T10:01:57.589+0000] {spark_submit.py:495} INFO - ... 26 more
[2023-01-22T10:01:57.658+0000] {spark_submit.py:495} INFO - Error, ada An error occurred while calling o39.load.
[2023-01-22T10:01:57.659+0000] {spark_submit.py:495} INFO - : java.lang.RuntimeException: java.lang.ClassNotFoundException: Class com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem not found
[2023-01-22T10:01:57.659+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)
[2023-01-22T10:01:57.659+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)
[2023-01-22T10:01:57.659+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T10:01:57.660+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T10:01:57.660+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T10:01:57.660+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T10:01:57.660+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T10:01:57.660+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T10:01:57.661+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:752)
[2023-01-22T10:01:57.661+0000] {spark_submit.py:495} INFO - at scala.collection.immutable.List.map(List.scala:246)
[2023-01-22T10:01:57.661+0000] {spark_submit.py:495} INFO - at scala.collection.immutable.List.map(List.scala:79)
[2023-01-22T10:01:57.661+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:750)
[2023-01-22T10:01:57.662+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:579)
[2023-01-22T10:01:57.662+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:408)
[2023-01-22T10:01:57.662+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T10:01:57.663+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T10:01:57.663+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T10:01:57.663+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T10:01:57.664+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T10:01:57.664+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T10:01:57.664+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T10:01:57.664+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T10:01:57.664+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T10:01:57.665+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T10:01:57.665+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T10:01:57.665+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T10:01:57.666+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T10:01:57.666+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T10:01:57.666+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T10:01:57.666+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T10:01:57.667+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T10:01:57.667+0000] {spark_submit.py:495} INFO - Caused by: java.lang.ClassNotFoundException: Class com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem not found
[2023-01-22T10:01:57.667+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)
[2023-01-22T10:01:57.667+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)
[2023-01-22T10:01:57.668+0000] {spark_submit.py:495} INFO - ... 30 more
[2023-01-22T10:01:57.668+0000] {spark_submit.py:495} INFO - 
[2023-01-22T10:01:57.668+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T10:01:57.668+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 69, in <module>
[2023-01-22T10:01:57.669+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T10:01:57.669+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 56, in main
[2023-01-22T10:01:57.669+0000] {spark_submit.py:495} INFO - df_spark.write.format("bigquery").option("project", "rafzul-analytics-1009") \
[2023-01-22T10:01:57.669+0000] {spark_submit.py:495} INFO - UnboundLocalError: local variable 'df_spark' referenced before assignment
[2023-01-22T10:01:57.797+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:57 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-22T10:01:57.812+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:57 INFO SparkUI: Stopped Spark web UI at http://266f60b86faa:4041
[2023-01-22T10:01:57.829+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:57 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-22T10:01:57.847+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:57 INFO MemoryStore: MemoryStore cleared
[2023-01-22T10:01:57.848+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:57 INFO BlockManager: BlockManager stopped
[2023-01-22T10:01:57.858+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:57 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-22T10:01:57.864+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:57 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-22T10:01:57.889+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:57 INFO SparkContext: Successfully stopped SparkContext
[2023-01-22T10:01:57.891+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:57 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T10:01:57.891+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-8669c611-d465-47ed-afcd-b42f91d619c1
[2023-01-22T10:01:57.898+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-8669c611-d465-47ed-afcd-b42f91d619c1/pyspark-a60dbd05-0906-41a4-beca-50c01c726581
[2023-01-22T10:01:57.905+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-8e3d3acb-e2da-4d9e-9f7a-a2b966bbd722
[2023-01-22T10:01:57.992+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-22T10:01:57.998+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230122T100145, end_date=20230122T100157
[2023-01-22T10:01:58.029+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 404 for task transform_stage_generation (Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 4826)
[2023-01-22T10:01:58.068+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T10:01:58.085+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T10:03:45.352+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T10:03:45.371+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T10:03:45.372+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T10:03:45.372+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-22T10:03:45.372+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T10:03:45.402+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-22T10:03:45.418+0000] {standard_task_runner.py:55} INFO - Started process 5173 to run task
[2023-01-22T10:03:45.426+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '408', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmptz2dj8er']
[2023-01-22T10:03:45.432+0000] {standard_task_runner.py:83} INFO - Job 408: Subtask transform_stage_generation
[2023-01-22T10:03:45.531+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 266f60b86faa
[2023-01-22T10:03:45.621+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-22T10:03:45.632+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T10:03:45.635+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-22T10:03:50.942+0000] {spark_submit.py:495} INFO - /opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010000.json
[2023-01-22T10:03:51.055+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:51 INFO SparkContext: Running Spark version 3.3.1
[2023-01-22T10:03:51.184+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T10:03:51.471+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:51 INFO ResourceUtils: ==============================================================
[2023-01-22T10:03:51.472+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:51 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-22T10:03:51.473+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:51 INFO ResourceUtils: ==============================================================
[2023-01-22T10:03:51.475+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:51 INFO SparkContext: Submitted application: gcp_playground
[2023-01-22T10:03:51.536+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:51 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-22T10:03:51.553+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:51 INFO ResourceProfile: Limiting resource is cpu
[2023-01-22T10:03:51.556+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:51 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-22T10:03:51.712+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:51 INFO SecurityManager: Changing view acls to: ***
[2023-01-22T10:03:51.713+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:51 INFO SecurityManager: Changing modify acls to: ***
[2023-01-22T10:03:51.715+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:51 INFO SecurityManager: Changing view acls groups to:
[2023-01-22T10:03:51.716+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:51 INFO SecurityManager: Changing modify acls groups to:
[2023-01-22T10:03:51.718+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:51 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-22T10:03:52.262+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:52 INFO Utils: Successfully started service 'sparkDriver' on port 46355.
[2023-01-22T10:03:52.315+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:52 INFO SparkEnv: Registering MapOutputTracker
[2023-01-22T10:03:52.435+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:52 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-22T10:03:52.514+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:52 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-22T10:03:52.518+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:52 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-22T10:03:52.541+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:52 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-22T10:03:52.601+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:52 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f0a75c94-3fbc-4045-ba48-a2ac9cd1f2dc
[2023-01-22T10:03:52.679+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:52 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-22T10:03:52.716+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:52 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-22T10:03:53.202+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:53 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-01-22T10:03:53.278+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:53 INFO SparkContext: Added JAR /opt/spark/gcs-connector-hadoop3-latest.jar at spark://266f60b86faa:46355/jars/gcs-connector-hadoop3-latest.jar with timestamp 1674381831040
[2023-01-22T10:03:53.278+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:53 INFO SparkContext: Added JAR /opt/spark/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://266f60b86faa:46355/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1674381831040
[2023-01-22T10:03:53.455+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:53 INFO Executor: Starting executor ID driver on host 266f60b86faa
[2023-01-22T10:03:53.474+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:53 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-22T10:03:53.515+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:53 INFO Executor: Fetching spark://266f60b86faa:46355/jars/gcs-connector-hadoop3-latest.jar with timestamp 1674381831040
[2023-01-22T10:03:53.616+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:53 INFO TransportClientFactory: Successfully created connection to 266f60b86faa/192.168.64.4:46355 after 48 ms (0 ms spent in bootstraps)
[2023-01-22T10:03:53.630+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:53 INFO Utils: Fetching spark://266f60b86faa:46355/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-5219229e-9c7e-48d4-ac0f-a160d74c578a/userFiles-ca3d657e-f94e-475f-bfda-7ea91c617568/fetchFileTemp14996334942250513068.tmp
[2023-01-22T10:03:53.901+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:53 INFO Executor: Adding file:/tmp/spark-5219229e-9c7e-48d4-ac0f-a160d74c578a/userFiles-ca3d657e-f94e-475f-bfda-7ea91c617568/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-22T10:03:53.902+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:53 INFO Executor: Fetching spark://266f60b86faa:46355/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1674381831040
[2023-01-22T10:03:53.903+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:53 INFO Utils: Fetching spark://266f60b86faa:46355/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-5219229e-9c7e-48d4-ac0f-a160d74c578a/userFiles-ca3d657e-f94e-475f-bfda-7ea91c617568/fetchFileTemp7612250137691620900.tmp
[2023-01-22T10:03:54.228+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:54 INFO Executor: Adding file:/tmp/spark-5219229e-9c7e-48d4-ac0f-a160d74c578a/userFiles-ca3d657e-f94e-475f-bfda-7ea91c617568/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-22T10:03:54.240+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:54 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37583.
[2023-01-22T10:03:54.240+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:54 INFO NettyBlockTransferService: Server created on 266f60b86faa:37583
[2023-01-22T10:03:54.244+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:54 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-22T10:03:54.255+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:54 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 266f60b86faa, 37583, None)
[2023-01-22T10:03:54.262+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:54 INFO BlockManagerMasterEndpoint: Registering block manager 266f60b86faa:37583 with 434.4 MiB RAM, BlockManagerId(driver, 266f60b86faa, 37583, None)
[2023-01-22T10:03:54.268+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:54 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 266f60b86faa, 37583, None)
[2023-01-22T10:03:54.270+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:54 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 266f60b86faa, 37583, None)
[2023-01-22T10:03:55.549+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:55 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-22T10:03:55.574+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:55 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-22T10:03:57.848+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:57 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: gs://entsoe_analytics_1009//opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010000.json.
[2023-01-22T10:03:57.848+0000] {spark_submit.py:495} INFO - org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "gs"
[2023-01-22T10:03:57.848+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2023-01-22T10:03:57.849+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T10:03:57.849+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T10:03:57.849+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T10:03:57.849+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T10:03:57.849+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T10:03:57.850+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T10:03:57.850+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)
[2023-01-22T10:03:57.850+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:370)
[2023-01-22T10:03:57.850+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T10:03:57.851+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T10:03:57.851+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T10:03:57.851+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T10:03:57.851+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T10:03:57.852+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T10:03:57.852+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T10:03:57.852+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T10:03:57.853+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T10:03:57.853+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T10:03:57.853+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T10:03:57.853+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T10:03:57.854+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T10:03:57.854+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T10:03:57.854+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T10:03:57.854+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T10:03:57.854+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T10:03:57.892+0000] {spark_submit.py:495} INFO - Error, ada An error occurred while calling o37.load.
[2023-01-22T10:03:57.892+0000] {spark_submit.py:495} INFO - : org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "gs"
[2023-01-22T10:03:57.892+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2023-01-22T10:03:57.893+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T10:03:57.893+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T10:03:57.894+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T10:03:57.894+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T10:03:57.894+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T10:03:57.894+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T10:03:57.895+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:752)
[2023-01-22T10:03:57.895+0000] {spark_submit.py:495} INFO - at scala.collection.immutable.List.map(List.scala:246)
[2023-01-22T10:03:57.895+0000] {spark_submit.py:495} INFO - at scala.collection.immutable.List.map(List.scala:79)
[2023-01-22T10:03:57.895+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:750)
[2023-01-22T10:03:57.896+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:579)
[2023-01-22T10:03:57.896+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:408)
[2023-01-22T10:03:57.896+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T10:03:57.897+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T10:03:57.897+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T10:03:57.897+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T10:03:57.897+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T10:03:57.898+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T10:03:57.898+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T10:03:57.899+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T10:03:57.900+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T10:03:57.900+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T10:03:57.901+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T10:03:57.901+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T10:03:57.901+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T10:03:57.901+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T10:03:57.902+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T10:03:57.902+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T10:03:57.902+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T10:03:57.902+0000] {spark_submit.py:495} INFO - 
[2023-01-22T10:03:57.903+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T10:03:57.903+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 68, in <module>
[2023-01-22T10:03:57.903+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T10:03:57.903+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 55, in main
[2023-01-22T10:03:57.904+0000] {spark_submit.py:495} INFO - df_spark.write.format("bigquery").option("project", "rafzul-analytics-1009") \
[2023-01-22T10:03:57.904+0000] {spark_submit.py:495} INFO - UnboundLocalError: local variable 'df_spark' referenced before assignment
[2023-01-22T10:03:57.993+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:57 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-22T10:03:58.018+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:58 INFO SparkUI: Stopped Spark web UI at http://266f60b86faa:4040
[2023-01-22T10:03:58.054+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:58 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-22T10:03:58.080+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:58 INFO MemoryStore: MemoryStore cleared
[2023-01-22T10:03:58.081+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:58 INFO BlockManager: BlockManager stopped
[2023-01-22T10:03:58.103+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:58 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-22T10:03:58.112+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:58 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-22T10:03:58.153+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:58 INFO SparkContext: Successfully stopped SparkContext
[2023-01-22T10:03:58.154+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:58 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T10:03:58.155+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-5219229e-9c7e-48d4-ac0f-a160d74c578a/pyspark-889be6e1-0ab3-419a-b224-80fc54a700b3
[2023-01-22T10:03:58.166+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-fd95c2fa-ff3a-4126-9bcc-0ade240415ce
[2023-01-22T10:03:58.194+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-5219229e-9c7e-48d4-ac0f-a160d74c578a
[2023-01-22T10:03:58.379+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-22T10:03:58.387+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230122T100345, end_date=20230122T100358
[2023-01-22T10:03:58.413+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 408 for task transform_stage_generation (Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 5173)
[2023-01-22T10:03:58.438+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T10:03:58.468+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T10:15:51.610+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T10:15:51.622+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T10:15:51.623+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T10:15:51.623+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-22T10:15:51.623+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T10:15:51.637+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-22T10:15:51.646+0000] {standard_task_runner.py:55} INFO - Started process 6724 to run task
[2023-01-22T10:15:51.650+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '416', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp9t3hrmpb']
[2023-01-22T10:15:51.653+0000] {standard_task_runner.py:83} INFO - Job 416: Subtask transform_stage_generation
[2023-01-22T10:15:51.718+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 266f60b86faa
[2023-01-22T10:15:51.787+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-22T10:15:51.797+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T10:15:51.798+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-22T10:15:57.350+0000] {spark_submit.py:495} INFO - /opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010000.json
[2023-01-22T10:15:57.463+0000] {spark_submit.py:495} INFO - 23/01/22 10:15:57 INFO SparkContext: Running Spark version 3.3.1
[2023-01-22T10:15:57.633+0000] {spark_submit.py:495} INFO - 23/01/22 10:15:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T10:15:57.799+0000] {spark_submit.py:495} INFO - 23/01/22 10:15:57 INFO ResourceUtils: ==============================================================
[2023-01-22T10:15:57.800+0000] {spark_submit.py:495} INFO - 23/01/22 10:15:57 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-22T10:15:57.802+0000] {spark_submit.py:495} INFO - 23/01/22 10:15:57 INFO ResourceUtils: ==============================================================
[2023-01-22T10:15:57.803+0000] {spark_submit.py:495} INFO - 23/01/22 10:15:57 INFO SparkContext: Submitted application: gcp_playground
[2023-01-22T10:15:57.849+0000] {spark_submit.py:495} INFO - 23/01/22 10:15:57 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-22T10:15:57.857+0000] {spark_submit.py:495} INFO - 23/01/22 10:15:57 INFO ResourceProfile: Limiting resource is cpu
[2023-01-22T10:15:57.860+0000] {spark_submit.py:495} INFO - 23/01/22 10:15:57 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-22T10:15:57.997+0000] {spark_submit.py:495} INFO - 23/01/22 10:15:57 INFO SecurityManager: Changing view acls to: ***
[2023-01-22T10:15:57.999+0000] {spark_submit.py:495} INFO - 23/01/22 10:15:57 INFO SecurityManager: Changing modify acls to: ***
[2023-01-22T10:15:58.001+0000] {spark_submit.py:495} INFO - 23/01/22 10:15:58 INFO SecurityManager: Changing view acls groups to:
[2023-01-22T10:15:58.002+0000] {spark_submit.py:495} INFO - 23/01/22 10:15:58 INFO SecurityManager: Changing modify acls groups to:
[2023-01-22T10:15:58.005+0000] {spark_submit.py:495} INFO - 23/01/22 10:15:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-22T10:15:59.671+0000] {spark_submit.py:495} INFO - 23/01/22 10:15:59 INFO Utils: Successfully started service 'sparkDriver' on port 36513.
[2023-01-22T10:15:59.856+0000] {spark_submit.py:495} INFO - 23/01/22 10:15:59 INFO SparkEnv: Registering MapOutputTracker
[2023-01-22T10:16:00.033+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:00 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-22T10:16:00.140+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:00 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-22T10:16:00.143+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:00 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-22T10:16:00.156+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:00 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-22T10:16:00.243+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:00 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1f075d62-8da6-4f2b-b3ab-84ed723b58e4
[2023-01-22T10:16:00.302+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:00 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-22T10:16:00.359+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:00 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-22T10:16:01.218+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:01 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-22T10:16:01.282+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:01 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2023-01-22T10:16:01.590+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:01 INFO SparkContext: Added JAR /opt/spark/gcs-connector-hadoop3-latest.jar at spark://266f60b86faa:36513/jars/gcs-connector-hadoop3-latest.jar with timestamp 1674382557447
[2023-01-22T10:16:01.591+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:01 INFO SparkContext: Added JAR /opt/spark/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://266f60b86faa:36513/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1674382557447
[2023-01-22T10:16:01.980+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:01 INFO Executor: Starting executor ID driver on host 266f60b86faa
[2023-01-22T10:16:02.012+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:02 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-22T10:16:02.070+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:02 INFO Executor: Fetching spark://266f60b86faa:36513/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1674382557447
[2023-01-22T10:16:02.408+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:02 INFO TransportClientFactory: Successfully created connection to 266f60b86faa/192.168.64.4:36513 after 227 ms (0 ms spent in bootstraps)
[2023-01-22T10:16:02.454+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:02 INFO Utils: Fetching spark://266f60b86faa:36513/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-c726ba4c-2b42-4449-8cf4-c9625b1af488/userFiles-06a0b35a-8a2c-48e3-a8a4-7a463a592935/fetchFileTemp16933798279010517298.tmp
[2023-01-22T10:16:03.197+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:03 INFO Executor: Adding file:/tmp/spark-c726ba4c-2b42-4449-8cf4-c9625b1af488/userFiles-06a0b35a-8a2c-48e3-a8a4-7a463a592935/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-22T10:16:03.198+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:03 INFO Executor: Fetching spark://266f60b86faa:36513/jars/gcs-connector-hadoop3-latest.jar with timestamp 1674382557447
[2023-01-22T10:16:03.202+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:03 INFO Utils: Fetching spark://266f60b86faa:36513/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-c726ba4c-2b42-4449-8cf4-c9625b1af488/userFiles-06a0b35a-8a2c-48e3-a8a4-7a463a592935/fetchFileTemp13143286828746188846.tmp
[2023-01-22T10:16:03.576+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:03 INFO Executor: Adding file:/tmp/spark-c726ba4c-2b42-4449-8cf4-c9625b1af488/userFiles-06a0b35a-8a2c-48e3-a8a4-7a463a592935/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-22T10:16:03.599+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:03 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46661.
[2023-01-22T10:16:03.600+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:03 INFO NettyBlockTransferService: Server created on 266f60b86faa:46661
[2023-01-22T10:16:03.606+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:03 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-22T10:16:03.632+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 266f60b86faa, 46661, None)
[2023-01-22T10:16:03.641+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:03 INFO BlockManagerMasterEndpoint: Registering block manager 266f60b86faa:46661 with 434.4 MiB RAM, BlockManagerId(driver, 266f60b86faa, 46661, None)
[2023-01-22T10:16:03.650+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:03 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 266f60b86faa, 46661, None)
[2023-01-22T10:16:03.653+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:03 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 266f60b86faa, 46661, None)
[2023-01-22T10:16:05.384+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:05 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-22T10:16:05.400+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:05 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-22T10:16:07.012+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:07 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: gs://entsoe_analytics_1009//opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010000.json.
[2023-01-22T10:16:07.012+0000] {spark_submit.py:495} INFO - org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "gs"
[2023-01-22T10:16:07.012+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2023-01-22T10:16:07.013+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T10:16:07.013+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T10:16:07.013+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T10:16:07.013+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T10:16:07.013+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T10:16:07.013+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T10:16:07.013+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)
[2023-01-22T10:16:07.014+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:370)
[2023-01-22T10:16:07.014+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T10:16:07.014+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T10:16:07.015+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T10:16:07.015+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T10:16:07.015+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T10:16:07.015+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T10:16:07.015+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T10:16:07.015+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T10:16:07.015+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T10:16:07.016+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T10:16:07.016+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T10:16:07.016+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T10:16:07.016+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T10:16:07.016+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T10:16:07.016+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T10:16:07.016+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T10:16:07.016+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T10:16:07.055+0000] {spark_submit.py:495} INFO - Error, ada An error occurred while calling o37.load.
[2023-01-22T10:16:07.055+0000] {spark_submit.py:495} INFO - : org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "gs"
[2023-01-22T10:16:07.055+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2023-01-22T10:16:07.056+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T10:16:07.056+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T10:16:07.056+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T10:16:07.056+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T10:16:07.056+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T10:16:07.056+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T10:16:07.056+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:752)
[2023-01-22T10:16:07.057+0000] {spark_submit.py:495} INFO - at scala.collection.immutable.List.map(List.scala:246)
[2023-01-22T10:16:07.057+0000] {spark_submit.py:495} INFO - at scala.collection.immutable.List.map(List.scala:79)
[2023-01-22T10:16:07.057+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:750)
[2023-01-22T10:16:07.057+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:579)
[2023-01-22T10:16:07.057+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:408)
[2023-01-22T10:16:07.057+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T10:16:07.057+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T10:16:07.057+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T10:16:07.057+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T10:16:07.057+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T10:16:07.058+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T10:16:07.058+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T10:16:07.058+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T10:16:07.058+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T10:16:07.058+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T10:16:07.058+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T10:16:07.058+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T10:16:07.058+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T10:16:07.059+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T10:16:07.059+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T10:16:07.059+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T10:16:07.059+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T10:16:07.059+0000] {spark_submit.py:495} INFO - 
[2023-01-22T10:16:07.059+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T10:16:07.059+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 68, in <module>
[2023-01-22T10:16:07.059+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T10:16:07.060+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 55, in main
[2023-01-22T10:16:07.060+0000] {spark_submit.py:495} INFO - df_spark.write.format("bigquery").option("project", "rafzul-analytics-1009") \
[2023-01-22T10:16:07.060+0000] {spark_submit.py:495} INFO - UnboundLocalError: local variable 'df_spark' referenced before assignment
[2023-01-22T10:16:07.121+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:07 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-22T10:16:07.137+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:07 INFO SparkUI: Stopped Spark web UI at http://266f60b86faa:4041
[2023-01-22T10:16:07.156+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:07 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-22T10:16:07.170+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:07 INFO MemoryStore: MemoryStore cleared
[2023-01-22T10:16:07.171+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:07 INFO BlockManager: BlockManager stopped
[2023-01-22T10:16:07.179+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:07 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-22T10:16:07.183+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:07 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-22T10:16:07.203+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:07 INFO SparkContext: Successfully stopped SparkContext
[2023-01-22T10:16:07.204+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:07 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T10:16:07.204+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-612e74fd-94df-488a-9652-eaa517a8fe86
[2023-01-22T10:16:07.210+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-c726ba4c-2b42-4449-8cf4-c9625b1af488
[2023-01-22T10:16:07.215+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-c726ba4c-2b42-4449-8cf4-c9625b1af488/pyspark-e368d290-0eeb-4a0f-8313-12bdc4578293
[2023-01-22T10:16:07.312+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-22T10:16:07.315+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230122T101551, end_date=20230122T101607
[2023-01-22T10:16:07.331+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 416 for task transform_stage_generation (Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 6724)
[2023-01-22T10:16:07.362+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T10:16:07.392+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T10:19:19.382+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T10:19:19.391+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T10:19:19.392+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T10:19:19.392+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-22T10:19:19.392+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T10:19:19.405+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-22T10:19:19.414+0000] {standard_task_runner.py:55} INFO - Started process 7212 to run task
[2023-01-22T10:19:19.418+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '419', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpvsle82yt']
[2023-01-22T10:19:19.421+0000] {standard_task_runner.py:83} INFO - Job 419: Subtask transform_stage_generation
[2023-01-22T10:19:19.482+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 266f60b86faa
[2023-01-22T10:19:19.555+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-22T10:19:19.565+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T10:19:19.566+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-22T10:19:24.037+0000] {spark_submit.py:495} INFO - /opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010000.json
[2023-01-22T10:19:24.112+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:24 INFO SparkContext: Running Spark version 3.3.1
[2023-01-22T10:19:24.191+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T10:19:24.292+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:24 INFO ResourceUtils: ==============================================================
[2023-01-22T10:19:24.293+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:24 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-22T10:19:24.293+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:24 INFO ResourceUtils: ==============================================================
[2023-01-22T10:19:24.293+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:24 INFO SparkContext: Submitted application: gcp_playground
[2023-01-22T10:19:24.319+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:24 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-22T10:19:24.325+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:24 INFO ResourceProfile: Limiting resource is cpu
[2023-01-22T10:19:24.327+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:24 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-22T10:19:24.389+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:24 INFO SecurityManager: Changing view acls to: ***
[2023-01-22T10:19:24.390+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:24 INFO SecurityManager: Changing modify acls to: ***
[2023-01-22T10:19:24.390+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:24 INFO SecurityManager: Changing view acls groups to:
[2023-01-22T10:19:24.391+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:24 INFO SecurityManager: Changing modify acls groups to:
[2023-01-22T10:19:24.391+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:24 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-22T10:19:24.836+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:24 INFO Utils: Successfully started service 'sparkDriver' on port 34371.
[2023-01-22T10:19:24.893+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:24 INFO SparkEnv: Registering MapOutputTracker
[2023-01-22T10:19:24.966+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:24 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-22T10:19:25.028+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:25 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-22T10:19:25.029+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:25 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-22T10:19:25.039+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:25 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-22T10:19:25.087+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:25 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-52738733-54c3-4e84-b9fa-14d7bbc00747
[2023-01-22T10:19:25.125+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:25 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-22T10:19:25.151+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:25 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-22T10:19:25.590+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:25 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-01-22T10:19:25.652+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:25 INFO SparkContext: Added JAR /opt/spark/gcs-connector-hadoop3-latest.jar at spark://266f60b86faa:34371/jars/gcs-connector-hadoop3-latest.jar with timestamp 1674382764103
[2023-01-22T10:19:25.653+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:25 INFO SparkContext: Added JAR /opt/spark/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://266f60b86faa:34371/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1674382764103
[2023-01-22T10:19:25.799+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:25 INFO Executor: Starting executor ID driver on host 266f60b86faa
[2023-01-22T10:19:25.812+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:25 INFO Executor: Starting executor with user classpath (userClassPathFirst = true): ''
[2023-01-22T10:19:25.853+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:25 INFO Executor: Fetching spark://266f60b86faa:34371/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1674382764103
[2023-01-22T10:19:25.942+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:25 INFO TransportClientFactory: Successfully created connection to 266f60b86faa/192.168.64.4:34371 after 52 ms (0 ms spent in bootstraps)
[2023-01-22T10:19:25.952+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:25 INFO Utils: Fetching spark://266f60b86faa:34371/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-54cd3a6c-2b1c-4310-9c60-e03da5447b36/userFiles-c56612b1-5594-4861-9ae0-a2030a830228/fetchFileTemp16481152021263256733.tmp
[2023-01-22T10:19:26.218+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:26 INFO Executor: Adding file:/tmp/spark-54cd3a6c-2b1c-4310-9c60-e03da5447b36/userFiles-c56612b1-5594-4861-9ae0-a2030a830228/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-22T10:19:26.219+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:26 INFO Executor: Fetching spark://266f60b86faa:34371/jars/gcs-connector-hadoop3-latest.jar with timestamp 1674382764103
[2023-01-22T10:19:26.221+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:26 INFO Utils: Fetching spark://266f60b86faa:34371/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-54cd3a6c-2b1c-4310-9c60-e03da5447b36/userFiles-c56612b1-5594-4861-9ae0-a2030a830228/fetchFileTemp14184597007200844860.tmp
[2023-01-22T10:19:26.520+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:26 INFO Executor: Adding file:/tmp/spark-54cd3a6c-2b1c-4310-9c60-e03da5447b36/userFiles-c56612b1-5594-4861-9ae0-a2030a830228/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-22T10:19:26.540+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:26 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36591.
[2023-01-22T10:19:26.540+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:26 INFO NettyBlockTransferService: Server created on 266f60b86faa:36591
[2023-01-22T10:19:26.544+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:26 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-22T10:19:26.556+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:26 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 266f60b86faa, 36591, None)
[2023-01-22T10:19:26.560+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:26 INFO BlockManagerMasterEndpoint: Registering block manager 266f60b86faa:36591 with 434.4 MiB RAM, BlockManagerId(driver, 266f60b86faa, 36591, None)
[2023-01-22T10:19:26.565+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:26 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 266f60b86faa, 36591, None)
[2023-01-22T10:19:26.567+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:26 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 266f60b86faa, 36591, None)
[2023-01-22T10:19:27.342+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:27 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-22T10:19:27.365+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:27 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-22T10:19:28.758+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:28 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: gs://entsoe_analytics_1009//opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010000.json.
[2023-01-22T10:19:28.759+0000] {spark_submit.py:495} INFO - org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "gs"
[2023-01-22T10:19:28.759+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2023-01-22T10:19:28.759+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T10:19:28.759+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T10:19:28.760+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T10:19:28.760+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T10:19:28.760+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T10:19:28.760+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T10:19:28.760+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)
[2023-01-22T10:19:28.761+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:370)
[2023-01-22T10:19:28.761+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T10:19:28.761+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T10:19:28.761+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T10:19:28.761+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T10:19:28.762+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T10:19:28.762+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T10:19:28.762+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T10:19:28.762+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T10:19:28.762+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T10:19:28.762+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T10:19:28.763+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T10:19:28.763+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T10:19:28.763+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T10:19:28.763+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T10:19:28.763+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T10:19:28.763+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T10:19:28.763+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T10:19:28.790+0000] {spark_submit.py:495} INFO - Error, ada An error occurred while calling o39.load.
[2023-01-22T10:19:28.791+0000] {spark_submit.py:495} INFO - : org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "gs"
[2023-01-22T10:19:28.791+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2023-01-22T10:19:28.791+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T10:19:28.792+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T10:19:28.792+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T10:19:28.792+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T10:19:28.792+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T10:19:28.793+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T10:19:28.793+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:752)
[2023-01-22T10:19:28.793+0000] {spark_submit.py:495} INFO - at scala.collection.immutable.List.map(List.scala:246)
[2023-01-22T10:19:28.793+0000] {spark_submit.py:495} INFO - at scala.collection.immutable.List.map(List.scala:79)
[2023-01-22T10:19:28.793+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:750)
[2023-01-22T10:19:28.794+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:579)
[2023-01-22T10:19:28.794+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:408)
[2023-01-22T10:19:28.794+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T10:19:28.794+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T10:19:28.794+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T10:19:28.795+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T10:19:28.795+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T10:19:28.795+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T10:19:28.795+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T10:19:28.795+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T10:19:28.796+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T10:19:28.796+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T10:19:28.796+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T10:19:28.796+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T10:19:28.796+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T10:19:28.797+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T10:19:28.797+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T10:19:28.797+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T10:19:28.797+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T10:19:28.797+0000] {spark_submit.py:495} INFO - 
[2023-01-22T10:19:28.798+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T10:19:28.798+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 69, in <module>
[2023-01-22T10:19:28.798+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T10:19:28.798+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 56, in main
[2023-01-22T10:19:28.798+0000] {spark_submit.py:495} INFO - df_spark.write.format("bigquery").option("project", "rafzul-analytics-1009") \
[2023-01-22T10:19:28.799+0000] {spark_submit.py:495} INFO - UnboundLocalError: local variable 'df_spark' referenced before assignment
[2023-01-22T10:19:28.860+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:28 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-22T10:19:28.881+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:28 INFO SparkUI: Stopped Spark web UI at http://266f60b86faa:4040
[2023-01-22T10:19:28.904+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:28 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-22T10:19:28.926+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:28 INFO MemoryStore: MemoryStore cleared
[2023-01-22T10:19:28.928+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:28 INFO BlockManager: BlockManager stopped
[2023-01-22T10:19:28.942+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:28 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-22T10:19:28.946+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:28 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-22T10:19:29.014+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:29 INFO SparkContext: Successfully stopped SparkContext
[2023-01-22T10:19:29.015+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:29 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T10:19:29.016+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-c3c05cb3-6242-4554-ac4f-c401291f53b3
[2023-01-22T10:19:29.038+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-54cd3a6c-2b1c-4310-9c60-e03da5447b36
[2023-01-22T10:19:29.106+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-54cd3a6c-2b1c-4310-9c60-e03da5447b36/pyspark-101872ae-6d8d-4ff5-abc9-3efe61d8c530
[2023-01-22T10:19:29.291+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-22T10:19:29.297+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230122T101919, end_date=20230122T101929
[2023-01-22T10:19:29.315+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 419 for task transform_stage_generation (Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 7212)
[2023-01-22T10:19:29.343+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T10:19:29.366+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T10:36:57.143+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T10:36:57.155+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T10:36:57.155+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T10:36:57.155+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-22T10:36:57.155+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T10:36:57.167+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-22T10:36:57.175+0000] {standard_task_runner.py:55} INFO - Started process 436 to run task
[2023-01-22T10:36:57.181+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '456', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp_56ksm3q']
[2023-01-22T10:36:57.184+0000] {standard_task_runner.py:83} INFO - Job 456: Subtask transform_stage_generation
[2023-01-22T10:36:57.254+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host ce1344c6e6d4
[2023-01-22T10:36:57.333+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-22T10:36:57.342+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T10:36:57.344+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-22T10:37:01.933+0000] {spark_submit.py:495} INFO - /opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010000.json
[2023-01-22T10:37:02.082+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:02 INFO SparkContext: Running Spark version 3.3.1
[2023-01-22T10:37:02.202+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T10:37:02.488+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:02 INFO ResourceUtils: ==============================================================
[2023-01-22T10:37:02.491+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:02 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-22T10:37:02.494+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:02 INFO ResourceUtils: ==============================================================
[2023-01-22T10:37:02.498+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:02 INFO SparkContext: Submitted application: gcp_playground
[2023-01-22T10:37:02.568+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:02 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-22T10:37:02.582+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:02 INFO ResourceProfile: Limiting resource is cpu
[2023-01-22T10:37:02.587+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:02 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-22T10:37:02.704+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:02 INFO SecurityManager: Changing view acls to: ***
[2023-01-22T10:37:02.705+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:02 INFO SecurityManager: Changing modify acls to: ***
[2023-01-22T10:37:02.706+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:02 INFO SecurityManager: Changing view acls groups to:
[2023-01-22T10:37:02.707+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:02 INFO SecurityManager: Changing modify acls groups to:
[2023-01-22T10:37:02.709+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:02 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-22T10:37:03.484+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:03 INFO Utils: Successfully started service 'sparkDriver' on port 35317.
[2023-01-22T10:37:03.776+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:03 INFO SparkEnv: Registering MapOutputTracker
[2023-01-22T10:37:03.889+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:03 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-22T10:37:03.944+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:03 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-22T10:37:03.945+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:03 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-22T10:37:03.959+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:03 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-22T10:37:04.026+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:04 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-84851b84-06ed-49e1-b623-018689401c1d
[2023-01-22T10:37:04.064+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:04 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-22T10:37:04.100+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:04 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-22T10:37:04.902+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:04 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-01-22T10:37:05.018+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:05 ERROR SparkContext: Failed to add /opt/spark/gcs-connector-hadoop3-latest.jar to Spark environment
[2023-01-22T10:37:05.018+0000] {spark_submit.py:495} INFO - java.io.FileNotFoundException: Jar /opt/spark/gcs-connector-hadoop3-latest.jar not found
[2023-01-22T10:37:05.019+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1949)
[2023-01-22T10:37:05.019+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.addJar(SparkContext.scala:2004)
[2023-01-22T10:37:05.019+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:507)
[2023-01-22T10:37:05.019+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:507)
[2023-01-22T10:37:05.019+0000] {spark_submit.py:495} INFO - at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
[2023-01-22T10:37:05.020+0000] {spark_submit.py:495} INFO - at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
[2023-01-22T10:37:05.020+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
[2023-01-22T10:37:05.020+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.<init>(SparkContext.scala:507)
[2023-01-22T10:37:05.020+0000] {spark_submit.py:495} INFO - at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
[2023-01-22T10:37:05.020+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
[2023-01-22T10:37:05.021+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
[2023-01-22T10:37:05.021+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
[2023-01-22T10:37:05.021+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
[2023-01-22T10:37:05.021+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
[2023-01-22T10:37:05.021+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T10:37:05.022+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:238)
[2023-01-22T10:37:05.022+0000] {spark_submit.py:495} INFO - at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
[2023-01-22T10:37:05.022+0000] {spark_submit.py:495} INFO - at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
[2023-01-22T10:37:05.027+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T10:37:05.027+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T10:37:05.027+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T10:37:05.027+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:05 ERROR SparkContext: Failed to add /opt/spark/spark-bigquery-with-dependencies_2.13-0.27.1.jar to Spark environment
[2023-01-22T10:37:05.028+0000] {spark_submit.py:495} INFO - java.io.FileNotFoundException: Jar /opt/spark/spark-bigquery-with-dependencies_2.13-0.27.1.jar not found
[2023-01-22T10:37:05.028+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1949)
[2023-01-22T10:37:05.028+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.addJar(SparkContext.scala:2004)
[2023-01-22T10:37:05.028+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:507)
[2023-01-22T10:37:05.028+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:507)
[2023-01-22T10:37:05.029+0000] {spark_submit.py:495} INFO - at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
[2023-01-22T10:37:05.030+0000] {spark_submit.py:495} INFO - at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
[2023-01-22T10:37:05.030+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
[2023-01-22T10:37:05.030+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.<init>(SparkContext.scala:507)
[2023-01-22T10:37:05.030+0000] {spark_submit.py:495} INFO - at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
[2023-01-22T10:37:05.030+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
[2023-01-22T10:37:05.031+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
[2023-01-22T10:37:05.031+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
[2023-01-22T10:37:05.031+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
[2023-01-22T10:37:05.031+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
[2023-01-22T10:37:05.031+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T10:37:05.032+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:238)
[2023-01-22T10:37:05.032+0000] {spark_submit.py:495} INFO - at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
[2023-01-22T10:37:05.032+0000] {spark_submit.py:495} INFO - at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
[2023-01-22T10:37:05.032+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T10:37:05.032+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T10:37:05.033+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T10:37:05.210+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:05 INFO Executor: Starting executor ID driver on host ce1344c6e6d4
[2023-01-22T10:37:05.224+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:05 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-22T10:37:05.259+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:05 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45467.
[2023-01-22T10:37:05.260+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:05 INFO NettyBlockTransferService: Server created on ce1344c6e6d4:45467
[2023-01-22T10:37:05.262+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:05 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-22T10:37:05.272+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:05 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ce1344c6e6d4, 45467, None)
[2023-01-22T10:37:05.284+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:05 INFO BlockManagerMasterEndpoint: Registering block manager ce1344c6e6d4:45467 with 434.4 MiB RAM, BlockManagerId(driver, ce1344c6e6d4, 45467, None)
[2023-01-22T10:37:05.290+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:05 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ce1344c6e6d4, 45467, None)
[2023-01-22T10:37:05.293+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:05 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ce1344c6e6d4, 45467, None)
[2023-01-22T10:37:07.105+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:07 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-22T10:37:07.129+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:07 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-22T10:37:10.857+0000] {spark_submit.py:495} INFO - Error, ada Path does not exist: gs://entsoe_analytics_1009/opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010000.json
[2023-01-22T10:37:10.857+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T10:37:10.858+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 68, in <module>
[2023-01-22T10:37:10.858+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T10:37:10.858+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 55, in main
[2023-01-22T10:37:10.858+0000] {spark_submit.py:495} INFO - df_spark.write.format("bigquery").option("project", "rafzul-analytics-1009") \
[2023-01-22T10:37:10.859+0000] {spark_submit.py:495} INFO - UnboundLocalError: local variable 'df_spark' referenced before assignment
[2023-01-22T10:37:10.974+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:10 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-22T10:37:10.994+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:10 INFO SparkUI: Stopped Spark web UI at http://ce1344c6e6d4:4040
[2023-01-22T10:37:11.016+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-22T10:37:11.033+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:11 INFO MemoryStore: MemoryStore cleared
[2023-01-22T10:37:11.033+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:11 INFO BlockManager: BlockManager stopped
[2023-01-22T10:37:11.045+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:11 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-22T10:37:11.050+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-22T10:37:11.057+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:11 INFO SparkContext: Successfully stopped SparkContext
[2023-01-22T10:37:11.057+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:11 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T10:37:11.058+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:11 INFO ShutdownHookManager: Deleting directory /tmp/spark-cbbbb6e9-56b8-4d91-9578-dd1c0799390f/pyspark-2cabba31-3eaa-484a-8412-4ca086c90b5d
[2023-01-22T10:37:11.064+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:11 INFO ShutdownHookManager: Deleting directory /tmp/spark-cbbbb6e9-56b8-4d91-9578-dd1c0799390f
[2023-01-22T10:37:11.069+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:11 INFO ShutdownHookManager: Deleting directory /tmp/spark-51b37b7b-804f-44d3-9311-8fba4680b05e
[2023-01-22T10:37:11.176+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-22T10:37:11.180+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230122T103657, end_date=20230122T103711
[2023-01-22T10:37:11.195+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 456 for task transform_stage_generation (Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 436)
[2023-01-22T10:37:11.249+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T10:37:11.265+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T10:38:10.290+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T10:38:10.304+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T10:38:10.304+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T10:38:10.305+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-22T10:38:10.305+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T10:38:10.318+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-22T10:38:10.326+0000] {standard_task_runner.py:55} INFO - Started process 787 to run task
[2023-01-22T10:38:10.332+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '461', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpwabv6rsy']
[2023-01-22T10:38:10.335+0000] {standard_task_runner.py:83} INFO - Job 461: Subtask transform_stage_generation
[2023-01-22T10:38:10.435+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host ce1344c6e6d4
[2023-01-22T10:38:10.560+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-22T10:38:10.571+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T10:38:10.573+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-22T10:38:16.527+0000] {spark_submit.py:495} INFO - /opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010000.json
[2023-01-22T10:38:16.794+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:16 INFO SparkContext: Running Spark version 3.3.1
[2023-01-22T10:38:16.933+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T10:38:17.279+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:17 INFO ResourceUtils: ==============================================================
[2023-01-22T10:38:17.281+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:17 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-22T10:38:17.283+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:17 INFO ResourceUtils: ==============================================================
[2023-01-22T10:38:17.286+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:17 INFO SparkContext: Submitted application: gcp_playground
[2023-01-22T10:38:17.381+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:17 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-22T10:38:17.398+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:17 INFO ResourceProfile: Limiting resource is cpu
[2023-01-22T10:38:17.410+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:17 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-22T10:38:17.566+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:17 INFO SecurityManager: Changing view acls to: ***
[2023-01-22T10:38:17.567+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:17 INFO SecurityManager: Changing modify acls to: ***
[2023-01-22T10:38:17.571+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:17 INFO SecurityManager: Changing view acls groups to:
[2023-01-22T10:38:17.572+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:17 INFO SecurityManager: Changing modify acls groups to:
[2023-01-22T10:38:17.573+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-22T10:38:18.476+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:18 INFO Utils: Successfully started service 'sparkDriver' on port 34265.
[2023-01-22T10:38:18.569+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:18 INFO SparkEnv: Registering MapOutputTracker
[2023-01-22T10:38:18.679+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:18 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-22T10:38:18.907+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:18 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-22T10:38:18.914+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:18 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-22T10:38:18.939+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:18 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-22T10:38:19.026+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:19 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-be48d34f-d89d-44e7-82f9-cac736994fce
[2023-01-22T10:38:19.094+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:19 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-22T10:38:19.144+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:19 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-22T10:38:19.909+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:19 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-22T10:38:19.940+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:19 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2023-01-22T10:38:20.032+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:20 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://ce1344c6e6d4:34265/jars/gcs-connector-hadoop3-latest.jar with timestamp 1674383896777
[2023-01-22T10:38:20.033+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:20 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://ce1344c6e6d4:34265/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1674383896777
[2023-01-22T10:38:20.309+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:20 INFO Executor: Starting executor ID driver on host ce1344c6e6d4
[2023-01-22T10:38:20.326+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:20 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-22T10:38:20.357+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:20 INFO Executor: Fetching spark://ce1344c6e6d4:34265/jars/gcs-connector-hadoop3-latest.jar with timestamp 1674383896777
[2023-01-22T10:38:20.479+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:20 INFO TransportClientFactory: Successfully created connection to ce1344c6e6d4/192.168.80.8:34265 after 65 ms (0 ms spent in bootstraps)
[2023-01-22T10:38:20.499+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:20 INFO Utils: Fetching spark://ce1344c6e6d4:34265/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-51e0bf2b-16e2-4d8d-a640-b7540686273a/userFiles-c7142639-2d1a-4839-9f08-76a1cfe4b7d2/fetchFileTemp6871523177594393126.tmp
[2023-01-22T10:38:21.048+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:21 INFO Executor: Adding file:/tmp/spark-51e0bf2b-16e2-4d8d-a640-b7540686273a/userFiles-c7142639-2d1a-4839-9f08-76a1cfe4b7d2/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-22T10:38:21.051+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:21 INFO Executor: Fetching spark://ce1344c6e6d4:34265/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1674383896777
[2023-01-22T10:38:21.072+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:21 INFO Utils: Fetching spark://ce1344c6e6d4:34265/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-51e0bf2b-16e2-4d8d-a640-b7540686273a/userFiles-c7142639-2d1a-4839-9f08-76a1cfe4b7d2/fetchFileTemp17932527026581969696.tmp
[2023-01-22T10:38:21.549+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:21 INFO Executor: Adding file:/tmp/spark-51e0bf2b-16e2-4d8d-a640-b7540686273a/userFiles-c7142639-2d1a-4839-9f08-76a1cfe4b7d2/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-22T10:38:21.567+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43207.
[2023-01-22T10:38:21.568+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:21 INFO NettyBlockTransferService: Server created on ce1344c6e6d4:43207
[2023-01-22T10:38:21.571+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:21 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-22T10:38:21.589+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:21 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ce1344c6e6d4, 43207, None)
[2023-01-22T10:38:21.595+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:21 INFO BlockManagerMasterEndpoint: Registering block manager ce1344c6e6d4:43207 with 434.4 MiB RAM, BlockManagerId(driver, ce1344c6e6d4, 43207, None)
[2023-01-22T10:38:21.601+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:21 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ce1344c6e6d4, 43207, None)
[2023-01-22T10:38:21.603+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:21 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ce1344c6e6d4, 43207, None)
[2023-01-22T10:38:24.123+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:24 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-22T10:38:24.139+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:24 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-22T10:38:28.469+0000] {spark_submit.py:495} INFO - Error, ada Path does not exist: gs://entsoe_analytics_1009/opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010000.json
[2023-01-22T10:38:28.470+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T10:38:28.470+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 68, in <module>
[2023-01-22T10:38:28.470+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T10:38:28.471+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 55, in main
[2023-01-22T10:38:28.471+0000] {spark_submit.py:495} INFO - df_spark.write.format("bigquery").option("project", "rafzul-analytics-1009") \
[2023-01-22T10:38:28.471+0000] {spark_submit.py:495} INFO - UnboundLocalError: local variable 'df_spark' referenced before assignment
[2023-01-22T10:38:28.596+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:28 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-22T10:38:28.623+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:28 INFO SparkUI: Stopped Spark web UI at http://ce1344c6e6d4:4041
[2023-01-22T10:38:28.653+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:28 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-22T10:38:28.683+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:28 INFO MemoryStore: MemoryStore cleared
[2023-01-22T10:38:28.684+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:28 INFO BlockManager: BlockManager stopped
[2023-01-22T10:38:28.700+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:28 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-22T10:38:28.711+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:28 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-22T10:38:28.756+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:28 INFO SparkContext: Successfully stopped SparkContext
[2023-01-22T10:38:28.757+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:28 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T10:38:28.760+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:28 INFO ShutdownHookManager: Deleting directory /tmp/spark-51e0bf2b-16e2-4d8d-a640-b7540686273a
[2023-01-22T10:38:28.771+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:28 INFO ShutdownHookManager: Deleting directory /tmp/spark-32538b17-7249-4090-9d57-60c498c1f223
[2023-01-22T10:38:28.791+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:28 INFO ShutdownHookManager: Deleting directory /tmp/spark-51e0bf2b-16e2-4d8d-a640-b7540686273a/pyspark-4f3dc5ba-8f96-4cf4-9fe6-13e26f02ae55
[2023-01-22T10:38:29.018+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-22T10:38:29.028+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230122T103810, end_date=20230122T103829
[2023-01-22T10:38:29.060+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 461 for task transform_stage_generation (Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 787)
[2023-01-22T10:38:29.128+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T10:38:29.167+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T10:46:57.696+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T10:46:57.707+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T10:46:57.707+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T10:46:57.708+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-22T10:46:57.708+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T10:46:57.723+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-22T10:46:57.732+0000] {standard_task_runner.py:55} INFO - Started process 1872 to run task
[2023-01-22T10:46:57.737+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '467', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpbad379lw']
[2023-01-22T10:46:57.740+0000] {standard_task_runner.py:83} INFO - Job 467: Subtask transform_stage_generation
[2023-01-22T10:46:57.809+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host ce1344c6e6d4
[2023-01-22T10:46:57.905+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-22T10:46:57.918+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T10:46:57.919+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-22T10:47:03.840+0000] {spark_submit.py:495} INFO - total_generation__DE_TENNET__202101010000__202101010100.json
[2023-01-22T10:47:04.004+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:03 INFO SparkContext: Running Spark version 3.3.1
[2023-01-22T10:47:04.313+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T10:47:04.771+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:04 INFO ResourceUtils: ==============================================================
[2023-01-22T10:47:04.773+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:04 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-22T10:47:04.775+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:04 INFO ResourceUtils: ==============================================================
[2023-01-22T10:47:04.777+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:04 INFO SparkContext: Submitted application: gcp_playground
[2023-01-22T10:47:04.907+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:04 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-22T10:47:04.934+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:04 INFO ResourceProfile: Limiting resource is cpu
[2023-01-22T10:47:04.938+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:04 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-22T10:47:05.118+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:05 INFO SecurityManager: Changing view acls to: ***
[2023-01-22T10:47:05.120+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:05 INFO SecurityManager: Changing modify acls to: ***
[2023-01-22T10:47:05.123+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:05 INFO SecurityManager: Changing view acls groups to:
[2023-01-22T10:47:05.125+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:05 INFO SecurityManager: Changing modify acls groups to:
[2023-01-22T10:47:05.127+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:05 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-22T10:47:06.400+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:06 INFO Utils: Successfully started service 'sparkDriver' on port 40305.
[2023-01-22T10:47:06.578+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:06 INFO SparkEnv: Registering MapOutputTracker
[2023-01-22T10:47:06.754+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:06 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-22T10:47:06.876+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:06 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-22T10:47:06.881+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:06 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-22T10:47:06.895+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:06 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-22T10:47:07.015+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:07 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d6c61378-fa62-4db1-89e0-0c66d41d9f5d
[2023-01-22T10:47:07.107+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:07 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-22T10:47:07.249+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:07 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-22T10:47:08.310+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:08 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-22T10:47:08.384+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:08 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2023-01-22T10:47:08.622+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:08 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://ce1344c6e6d4:40305/jars/gcs-connector-hadoop3-latest.jar with timestamp 1674384423981
[2023-01-22T10:47:08.629+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:08 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://ce1344c6e6d4:40305/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1674384423981
[2023-01-22T10:47:09.450+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:09 INFO Executor: Starting executor ID driver on host ce1344c6e6d4
[2023-01-22T10:47:09.490+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:09 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-22T10:47:09.611+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:09 INFO Executor: Fetching spark://ce1344c6e6d4:40305/jars/gcs-connector-hadoop3-latest.jar with timestamp 1674384423981
[2023-01-22T10:47:09.886+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:09 INFO TransportClientFactory: Successfully created connection to ce1344c6e6d4/192.168.80.8:40305 after 138 ms (0 ms spent in bootstraps)
[2023-01-22T10:47:09.933+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:09 INFO Utils: Fetching spark://ce1344c6e6d4:40305/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-16822bd0-3f8e-4163-bb9f-2744fb2a1a8a/userFiles-d05debaa-d04d-4254-9047-e7898442378a/fetchFileTemp5058298234839422821.tmp
[2023-01-22T10:47:10.758+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:10 INFO Executor: Adding file:/tmp/spark-16822bd0-3f8e-4163-bb9f-2744fb2a1a8a/userFiles-d05debaa-d04d-4254-9047-e7898442378a/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-22T10:47:10.759+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:10 INFO Executor: Fetching spark://ce1344c6e6d4:40305/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1674384423981
[2023-01-22T10:47:10.763+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:10 INFO Utils: Fetching spark://ce1344c6e6d4:40305/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-16822bd0-3f8e-4163-bb9f-2744fb2a1a8a/userFiles-d05debaa-d04d-4254-9047-e7898442378a/fetchFileTemp12567157502259377766.tmp
[2023-01-22T10:47:11.386+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:11 INFO Executor: Adding file:/tmp/spark-16822bd0-3f8e-4163-bb9f-2744fb2a1a8a/userFiles-d05debaa-d04d-4254-9047-e7898442378a/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-22T10:47:11.416+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:11 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44397.
[2023-01-22T10:47:11.418+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:11 INFO NettyBlockTransferService: Server created on ce1344c6e6d4:44397
[2023-01-22T10:47:11.423+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:11 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-22T10:47:11.449+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:11 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ce1344c6e6d4, 44397, None)
[2023-01-22T10:47:11.462+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:11 INFO BlockManagerMasterEndpoint: Registering block manager ce1344c6e6d4:44397 with 434.4 MiB RAM, BlockManagerId(driver, ce1344c6e6d4, 44397, None)
[2023-01-22T10:47:11.472+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:11 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ce1344c6e6d4, 44397, None)
[2023-01-22T10:47:11.475+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:11 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ce1344c6e6d4, 44397, None)
[2023-01-22T10:47:13.994+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:13 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-22T10:47:14.015+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:14 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-22T10:47:19.029+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:19 INFO InMemoryFileIndex: It took 208 ms to list leaf files for 1 paths.
[2023-01-22T10:47:19.405+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:19 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.8 KiB, free 434.2 MiB)
[2023-01-22T10:47:19.531+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:19 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-22T10:47:19.538+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:19 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ce1344c6e6d4:44397 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-22T10:47:19.547+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:19 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-22T10:47:20.374+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:20 INFO FileInputFormat: Total input files to process : 1
[2023-01-22T10:47:20.422+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:20 INFO FileInputFormat: Total input files to process : 1
[2023-01-22T10:47:20.458+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:20 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-22T10:47:20.495+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:20 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-22T10:47:20.496+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:20 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-22T10:47:20.497+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:20 INFO DAGScheduler: Parents of final stage: List()
[2023-01-22T10:47:20.503+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:20 INFO DAGScheduler: Missing parents: List()
[2023-01-22T10:47:20.514+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:20 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-22T10:47:20.612+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:20 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-22T10:47:20.621+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:20 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.2 MiB)
[2023-01-22T10:47:20.623+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:20 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on ce1344c6e6d4:44397 (size: 4.5 KiB, free: 434.4 MiB)
[2023-01-22T10:47:20.625+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:20 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-22T10:47:20.661+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-22T10:47:20.664+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:20 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-22T10:47:20.884+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:20 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (ce1344c6e6d4, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-22T10:47:20.934+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:20 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-22T10:47:21.155+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:21 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010000__202101010100.json:0+9369
[2023-01-22T10:47:21.560+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:21 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3437 bytes result sent to driver
[2023-01-22T10:47:21.580+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:21 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 742 ms on ce1344c6e6d4 (executor driver) (1/1)
[2023-01-22T10:47:21.584+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:21 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-22T10:47:21.594+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:21 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 1.051 s
[2023-01-22T10:47:21.600+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:21 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-22T10:47:21.601+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-22T10:47:21.605+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:21 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 1.146374 s
[2023-01-22T10:47:23.480+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:23 INFO BlockManagerInfo: Removed broadcast_0_piece0 on ce1344c6e6d4:44397 in memory (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-22T10:47:23.493+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:23 INFO BlockManagerInfo: Removed broadcast_1_piece0 on ce1344c6e6d4:44397 in memory (size: 4.5 KiB, free: 434.4 MiB)
[2023-01-22T10:47:32.660+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:32 INFO FileSourceStrategy: Pushed Filters:
[2023-01-22T10:47:32.666+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:32 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-22T10:47:32.673+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:32 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-22T10:47:33.104+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:33 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-22T10:47:33.237+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-22T10:47:33.239+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-22T10:47:33.251+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:33 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-22T10:47:33.253+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-22T10:47:33.254+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-22T10:47:33.260+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-22T10:47:33.799+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:33 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.2 KiB, free 434.2 MiB)
[2023-01-22T10:47:33.835+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:33 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-22T10:47:33.838+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:33 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on ce1344c6e6d4:44397 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-22T10:47:33.840+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:33 INFO SparkContext: Created broadcast 2 from save at BigQueryWriteHelper.java:105
[2023-01-22T10:47:33.866+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-22T10:47:34.127+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:34 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-22T10:47:34.129+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:34 INFO DAGScheduler: Got job 1 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-22T10:47:34.129+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:34 INFO DAGScheduler: Final stage: ResultStage 1 (save at BigQueryWriteHelper.java:105)
[2023-01-22T10:47:34.130+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:34 INFO DAGScheduler: Parents of final stage: List()
[2023-01-22T10:47:34.133+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:34 INFO DAGScheduler: Missing parents: List()
[2023-01-22T10:47:34.136+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:34 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-22T10:47:34.232+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:34 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 215.9 KiB, free 434.0 MiB)
[2023-01-22T10:47:34.242+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:34 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 77.2 KiB, free 433.9 MiB)
[2023-01-22T10:47:34.244+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:34 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on ce1344c6e6d4:44397 (size: 77.2 KiB, free: 434.3 MiB)
[2023-01-22T10:47:34.246+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:34 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-22T10:47:34.248+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-22T10:47:34.249+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:34 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-22T10:47:34.258+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:34 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (ce1344c6e6d4, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-22T10:47:34.261+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:34 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-22T10:47:34.482+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-22T10:47:34.482+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-22T10:47:34.484+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:34 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-22T10:47:34.485+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-22T10:47:34.485+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-22T10:47:34.486+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-22T10:47:34.497+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:34 INFO CodecConfig: Compression: SNAPPY
[2023-01-22T10:47:34.503+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:34 INFO CodecConfig: Compression: SNAPPY
[2023-01-22T10:47:34.564+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:34 INFO ParquetOutputFormat: Parquet block size to 134217728
[2023-01-22T10:47:34.565+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:34 INFO ParquetOutputFormat: Validation is off
[2023-01-22T10:47:34.565+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:34 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-01-22T10:47:34.566+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:34 INFO ParquetOutputFormat: Parquet properties are:
[2023-01-22T10:47:34.566+0000] {spark_submit.py:495} INFO - Parquet page size to 1048576
[2023-01-22T10:47:34.566+0000] {spark_submit.py:495} INFO - Parquet dictionary page size to 1048576
[2023-01-22T10:47:34.567+0000] {spark_submit.py:495} INFO - Dictionary is true
[2023-01-22T10:47:34.567+0000] {spark_submit.py:495} INFO - Writer version is: PARQUET_1_0
[2023-01-22T10:47:34.567+0000] {spark_submit.py:495} INFO - Page size checking is: estimated
[2023-01-22T10:47:34.568+0000] {spark_submit.py:495} INFO - Min row count for page size check is: 100
[2023-01-22T10:47:34.568+0000] {spark_submit.py:495} INFO - Max row count for page size check is: 10000
[2023-01-22T10:47:34.569+0000] {spark_submit.py:495} INFO - Truncate length for column indexes is: 64
[2023-01-22T10:47:34.569+0000] {spark_submit.py:495} INFO - Truncate length for statistics min/max  is: 2147483647
[2023-01-22T10:47:34.569+0000] {spark_submit.py:495} INFO - Bloom filter enabled: false
[2023-01-22T10:47:34.570+0000] {spark_submit.py:495} INFO - Max Bloom filter size for a column is 1048576
[2023-01-22T10:47:34.570+0000] {spark_submit.py:495} INFO - Bloom filter expected number of distinct values are: null
[2023-01-22T10:47:34.570+0000] {spark_submit.py:495} INFO - Page row count limit to 20000
[2023-01-22T10:47:34.571+0000] {spark_submit.py:495} INFO - Writing page checksums is: on
[2023-01-22T10:47:34.681+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:34 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-01-22T10:47:34.682+0000] {spark_submit.py:495} INFO - {
[2023-01-22T10:47:34.682+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-22T10:47:34.682+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-22T10:47:34.683+0000] {spark_submit.py:495} INFO - "name" : "GL_MarketDocument",
[2023-01-22T10:47:34.683+0000] {spark_submit.py:495} INFO - "type" : {
[2023-01-22T10:47:34.683+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-22T10:47:34.684+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-22T10:47:34.684+0000] {spark_submit.py:495} INFO - "name" : "@xmlns",
[2023-01-22T10:47:34.684+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:47:34.684+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.685+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.685+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:47:34.685+0000] {spark_submit.py:495} INFO - "name" : "TimeSeries",
[2023-01-22T10:47:34.686+0000] {spark_submit.py:495} INFO - "type" : {
[2023-01-22T10:47:34.686+0000] {spark_submit.py:495} INFO - "type" : "array",
[2023-01-22T10:47:34.686+0000] {spark_submit.py:495} INFO - "elementType" : {
[2023-01-22T10:47:34.687+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-22T10:47:34.687+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-22T10:47:34.687+0000] {spark_submit.py:495} INFO - "name" : "MktPSRType",
[2023-01-22T10:47:34.688+0000] {spark_submit.py:495} INFO - "type" : {
[2023-01-22T10:47:34.688+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-22T10:47:34.689+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-22T10:47:34.689+0000] {spark_submit.py:495} INFO - "name" : "psrType",
[2023-01-22T10:47:34.690+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:47:34.690+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.690+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.691+0000] {spark_submit.py:495} INFO - } ]
[2023-01-22T10:47:34.691+0000] {spark_submit.py:495} INFO - },
[2023-01-22T10:47:34.692+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.692+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.692+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:47:34.693+0000] {spark_submit.py:495} INFO - "name" : "Period",
[2023-01-22T10:47:34.693+0000] {spark_submit.py:495} INFO - "type" : {
[2023-01-22T10:47:34.694+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-22T10:47:34.694+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-22T10:47:34.694+0000] {spark_submit.py:495} INFO - "name" : "Point",
[2023-01-22T10:47:34.695+0000] {spark_submit.py:495} INFO - "type" : {
[2023-01-22T10:47:34.695+0000] {spark_submit.py:495} INFO - "type" : "array",
[2023-01-22T10:47:34.695+0000] {spark_submit.py:495} INFO - "elementType" : {
[2023-01-22T10:47:34.696+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-22T10:47:34.696+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-22T10:47:34.697+0000] {spark_submit.py:495} INFO - "name" : "position",
[2023-01-22T10:47:34.697+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:47:34.697+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.698+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.698+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:47:34.698+0000] {spark_submit.py:495} INFO - "name" : "quantity",
[2023-01-22T10:47:34.699+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:47:34.699+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.699+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.700+0000] {spark_submit.py:495} INFO - } ]
[2023-01-22T10:47:34.700+0000] {spark_submit.py:495} INFO - },
[2023-01-22T10:47:34.700+0000] {spark_submit.py:495} INFO - "containsNull" : true
[2023-01-22T10:47:34.700+0000] {spark_submit.py:495} INFO - },
[2023-01-22T10:47:34.701+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.701+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.701+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:47:34.702+0000] {spark_submit.py:495} INFO - "name" : "resolution",
[2023-01-22T10:47:34.702+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:47:34.702+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.702+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.703+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:47:34.703+0000] {spark_submit.py:495} INFO - "name" : "timeInterval",
[2023-01-22T10:47:34.703+0000] {spark_submit.py:495} INFO - "type" : {
[2023-01-22T10:47:34.703+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-22T10:47:34.704+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-22T10:47:34.704+0000] {spark_submit.py:495} INFO - "name" : "end",
[2023-01-22T10:47:34.704+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:47:34.704+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.705+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.705+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:47:34.705+0000] {spark_submit.py:495} INFO - "name" : "start",
[2023-01-22T10:47:34.706+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:47:34.706+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.706+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.706+0000] {spark_submit.py:495} INFO - } ]
[2023-01-22T10:47:34.707+0000] {spark_submit.py:495} INFO - },
[2023-01-22T10:47:34.707+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.707+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.707+0000] {spark_submit.py:495} INFO - } ]
[2023-01-22T10:47:34.708+0000] {spark_submit.py:495} INFO - },
[2023-01-22T10:47:34.708+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.708+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.708+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:47:34.709+0000] {spark_submit.py:495} INFO - "name" : "businessType",
[2023-01-22T10:47:34.709+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:47:34.709+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.709+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.710+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:47:34.710+0000] {spark_submit.py:495} INFO - "name" : "curveType",
[2023-01-22T10:47:34.710+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:47:34.710+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.711+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.711+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:47:34.711+0000] {spark_submit.py:495} INFO - "name" : "inBiddingZone_Domain.mRID",
[2023-01-22T10:47:34.712+0000] {spark_submit.py:495} INFO - "type" : {
[2023-01-22T10:47:34.712+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-22T10:47:34.712+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-22T10:47:34.713+0000] {spark_submit.py:495} INFO - "name" : "#text",
[2023-01-22T10:47:34.713+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:47:34.714+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.714+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.714+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:47:34.715+0000] {spark_submit.py:495} INFO - "name" : "@codingScheme",
[2023-01-22T10:47:34.715+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:47:34.716+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.716+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.716+0000] {spark_submit.py:495} INFO - } ]
[2023-01-22T10:47:34.716+0000] {spark_submit.py:495} INFO - },
[2023-01-22T10:47:34.717+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.717+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.717+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:47:34.718+0000] {spark_submit.py:495} INFO - "name" : "mRID",
[2023-01-22T10:47:34.718+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:47:34.718+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.718+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.719+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:47:34.719+0000] {spark_submit.py:495} INFO - "name" : "objectAggregation",
[2023-01-22T10:47:34.719+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:47:34.720+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.720+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.720+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:47:34.720+0000] {spark_submit.py:495} INFO - "name" : "outBiddingZone_Domain.mRID",
[2023-01-22T10:47:34.721+0000] {spark_submit.py:495} INFO - "type" : {
[2023-01-22T10:47:34.721+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-22T10:47:34.721+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-22T10:47:34.721+0000] {spark_submit.py:495} INFO - "name" : "#text",
[2023-01-22T10:47:34.722+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:47:34.722+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.722+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.723+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:47:34.723+0000] {spark_submit.py:495} INFO - "name" : "@codingScheme",
[2023-01-22T10:47:34.723+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:47:34.723+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.724+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.724+0000] {spark_submit.py:495} INFO - } ]
[2023-01-22T10:47:34.724+0000] {spark_submit.py:495} INFO - },
[2023-01-22T10:47:34.724+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.725+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.725+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:47:34.725+0000] {spark_submit.py:495} INFO - "name" : "quantity_Measure_Unit.name",
[2023-01-22T10:47:34.725+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:47:34.726+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.726+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.726+0000] {spark_submit.py:495} INFO - } ]
[2023-01-22T10:47:34.726+0000] {spark_submit.py:495} INFO - },
[2023-01-22T10:47:34.726+0000] {spark_submit.py:495} INFO - "containsNull" : true
[2023-01-22T10:47:34.727+0000] {spark_submit.py:495} INFO - },
[2023-01-22T10:47:34.727+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.727+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.727+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:47:34.728+0000] {spark_submit.py:495} INFO - "name" : "createdDateTime",
[2023-01-22T10:47:34.728+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:47:34.728+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.728+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.729+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:47:34.729+0000] {spark_submit.py:495} INFO - "name" : "mRID",
[2023-01-22T10:47:34.729+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:47:34.729+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.730+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.730+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:47:34.731+0000] {spark_submit.py:495} INFO - "name" : "process.processType",
[2023-01-22T10:47:34.731+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:47:34.731+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.731+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.732+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:47:34.732+0000] {spark_submit.py:495} INFO - "name" : "receiver_MarketParticipant.mRID",
[2023-01-22T10:47:34.732+0000] {spark_submit.py:495} INFO - "type" : {
[2023-01-22T10:47:34.732+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-22T10:47:34.733+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-22T10:47:34.733+0000] {spark_submit.py:495} INFO - "name" : "#text",
[2023-01-22T10:47:34.733+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:47:34.733+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.734+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.734+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:47:34.734+0000] {spark_submit.py:495} INFO - "name" : "@codingScheme",
[2023-01-22T10:47:34.734+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:47:34.734+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.735+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.735+0000] {spark_submit.py:495} INFO - } ]
[2023-01-22T10:47:34.735+0000] {spark_submit.py:495} INFO - },
[2023-01-22T10:47:34.735+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.736+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.736+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:47:34.736+0000] {spark_submit.py:495} INFO - "name" : "receiver_MarketParticipant.marketRole.type",
[2023-01-22T10:47:34.736+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:47:34.736+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.737+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.737+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:47:34.737+0000] {spark_submit.py:495} INFO - "name" : "revisionNumber",
[2023-01-22T10:47:34.737+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:47:34.738+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.738+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.739+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:47:34.739+0000] {spark_submit.py:495} INFO - "name" : "sender_MarketParticipant.mRID",
[2023-01-22T10:47:34.739+0000] {spark_submit.py:495} INFO - "type" : {
[2023-01-22T10:47:34.739+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-22T10:47:34.739+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-22T10:47:34.740+0000] {spark_submit.py:495} INFO - "name" : "#text",
[2023-01-22T10:47:34.741+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:47:34.741+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.741+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.741+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:47:34.742+0000] {spark_submit.py:495} INFO - "name" : "@codingScheme",
[2023-01-22T10:47:34.742+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:47:34.742+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.743+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.743+0000] {spark_submit.py:495} INFO - } ]
[2023-01-22T10:47:34.743+0000] {spark_submit.py:495} INFO - },
[2023-01-22T10:47:34.743+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.743+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.744+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:47:34.744+0000] {spark_submit.py:495} INFO - "name" : "sender_MarketParticipant.marketRole.type",
[2023-01-22T10:47:34.744+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:47:34.744+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.745+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.745+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:47:34.745+0000] {spark_submit.py:495} INFO - "name" : "time_Period.timeInterval",
[2023-01-22T10:47:34.745+0000] {spark_submit.py:495} INFO - "type" : {
[2023-01-22T10:47:34.746+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-22T10:47:34.746+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-22T10:47:34.746+0000] {spark_submit.py:495} INFO - "name" : "end",
[2023-01-22T10:47:34.746+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:47:34.746+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.747+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.747+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:47:34.747+0000] {spark_submit.py:495} INFO - "name" : "start",
[2023-01-22T10:47:34.747+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:47:34.748+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.748+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.748+0000] {spark_submit.py:495} INFO - } ]
[2023-01-22T10:47:34.748+0000] {spark_submit.py:495} INFO - },
[2023-01-22T10:47:34.749+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.749+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.749+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:47:34.749+0000] {spark_submit.py:495} INFO - "name" : "type",
[2023-01-22T10:47:34.750+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:47:34.750+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.750+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.750+0000] {spark_submit.py:495} INFO - } ]
[2023-01-22T10:47:34.751+0000] {spark_submit.py:495} INFO - },
[2023-01-22T10:47:34.751+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.751+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.751+0000] {spark_submit.py:495} INFO - } ]
[2023-01-22T10:47:34.751+0000] {spark_submit.py:495} INFO - }
[2023-01-22T10:47:34.752+0000] {spark_submit.py:495} INFO - and corresponding Parquet message type:
[2023-01-22T10:47:34.752+0000] {spark_submit.py:495} INFO - message spark_schema {
[2023-01-22T10:47:34.752+0000] {spark_submit.py:495} INFO - optional group GL_MarketDocument {
[2023-01-22T10:47:34.752+0000] {spark_submit.py:495} INFO - optional binary @xmlns (STRING);
[2023-01-22T10:47:34.753+0000] {spark_submit.py:495} INFO - optional group TimeSeries (LIST) {
[2023-01-22T10:47:34.753+0000] {spark_submit.py:495} INFO - repeated group list {
[2023-01-22T10:47:34.753+0000] {spark_submit.py:495} INFO - optional group element {
[2023-01-22T10:47:34.753+0000] {spark_submit.py:495} INFO - optional group MktPSRType {
[2023-01-22T10:47:34.754+0000] {spark_submit.py:495} INFO - optional binary psrType (STRING);
[2023-01-22T10:47:34.754+0000] {spark_submit.py:495} INFO - }
[2023-01-22T10:47:34.754+0000] {spark_submit.py:495} INFO - optional group Period {
[2023-01-22T10:47:34.754+0000] {spark_submit.py:495} INFO - optional group Point (LIST) {
[2023-01-22T10:47:34.755+0000] {spark_submit.py:495} INFO - repeated group list {
[2023-01-22T10:47:34.755+0000] {spark_submit.py:495} INFO - optional group element {
[2023-01-22T10:47:34.755+0000] {spark_submit.py:495} INFO - optional binary position (STRING);
[2023-01-22T10:47:34.755+0000] {spark_submit.py:495} INFO - optional binary quantity (STRING);
[2023-01-22T10:47:34.755+0000] {spark_submit.py:495} INFO - }
[2023-01-22T10:47:34.755+0000] {spark_submit.py:495} INFO - }
[2023-01-22T10:47:34.755+0000] {spark_submit.py:495} INFO - }
[2023-01-22T10:47:34.755+0000] {spark_submit.py:495} INFO - optional binary resolution (STRING);
[2023-01-22T10:47:34.756+0000] {spark_submit.py:495} INFO - optional group timeInterval {
[2023-01-22T10:47:34.756+0000] {spark_submit.py:495} INFO - optional binary end (STRING);
[2023-01-22T10:47:34.756+0000] {spark_submit.py:495} INFO - optional binary start (STRING);
[2023-01-22T10:47:34.756+0000] {spark_submit.py:495} INFO - }
[2023-01-22T10:47:34.756+0000] {spark_submit.py:495} INFO - }
[2023-01-22T10:47:34.757+0000] {spark_submit.py:495} INFO - optional binary businessType (STRING);
[2023-01-22T10:47:34.757+0000] {spark_submit.py:495} INFO - optional binary curveType (STRING);
[2023-01-22T10:47:34.757+0000] {spark_submit.py:495} INFO - optional group inBiddingZone_Domain.mRID {
[2023-01-22T10:47:34.757+0000] {spark_submit.py:495} INFO - optional binary #text (STRING);
[2023-01-22T10:47:34.757+0000] {spark_submit.py:495} INFO - optional binary @codingScheme (STRING);
[2023-01-22T10:47:34.758+0000] {spark_submit.py:495} INFO - }
[2023-01-22T10:47:34.758+0000] {spark_submit.py:495} INFO - optional binary mRID (STRING);
[2023-01-22T10:47:34.758+0000] {spark_submit.py:495} INFO - optional binary objectAggregation (STRING);
[2023-01-22T10:47:34.758+0000] {spark_submit.py:495} INFO - optional group outBiddingZone_Domain.mRID {
[2023-01-22T10:47:34.759+0000] {spark_submit.py:495} INFO - optional binary #text (STRING);
[2023-01-22T10:47:34.759+0000] {spark_submit.py:495} INFO - optional binary @codingScheme (STRING);
[2023-01-22T10:47:34.759+0000] {spark_submit.py:495} INFO - }
[2023-01-22T10:47:34.759+0000] {spark_submit.py:495} INFO - optional binary quantity_Measure_Unit.name (STRING);
[2023-01-22T10:47:34.760+0000] {spark_submit.py:495} INFO - }
[2023-01-22T10:47:34.760+0000] {spark_submit.py:495} INFO - }
[2023-01-22T10:47:34.760+0000] {spark_submit.py:495} INFO - }
[2023-01-22T10:47:34.760+0000] {spark_submit.py:495} INFO - optional binary createdDateTime (STRING);
[2023-01-22T10:47:34.760+0000] {spark_submit.py:495} INFO - optional binary mRID (STRING);
[2023-01-22T10:47:34.760+0000] {spark_submit.py:495} INFO - optional binary process.processType (STRING);
[2023-01-22T10:47:34.761+0000] {spark_submit.py:495} INFO - optional group receiver_MarketParticipant.mRID {
[2023-01-22T10:47:34.761+0000] {spark_submit.py:495} INFO - optional binary #text (STRING);
[2023-01-22T10:47:34.761+0000] {spark_submit.py:495} INFO - optional binary @codingScheme (STRING);
[2023-01-22T10:47:34.761+0000] {spark_submit.py:495} INFO - }
[2023-01-22T10:47:34.761+0000] {spark_submit.py:495} INFO - optional binary receiver_MarketParticipant.marketRole.type (STRING);
[2023-01-22T10:47:34.762+0000] {spark_submit.py:495} INFO - optional binary revisionNumber (STRING);
[2023-01-22T10:47:34.762+0000] {spark_submit.py:495} INFO - optional group sender_MarketParticipant.mRID {
[2023-01-22T10:47:34.762+0000] {spark_submit.py:495} INFO - optional binary #text (STRING);
[2023-01-22T10:47:34.762+0000] {spark_submit.py:495} INFO - optional binary @codingScheme (STRING);
[2023-01-22T10:47:34.762+0000] {spark_submit.py:495} INFO - }
[2023-01-22T10:47:34.763+0000] {spark_submit.py:495} INFO - optional binary sender_MarketParticipant.marketRole.type (STRING);
[2023-01-22T10:47:34.763+0000] {spark_submit.py:495} INFO - optional group time_Period.timeInterval {
[2023-01-22T10:47:34.763+0000] {spark_submit.py:495} INFO - optional binary end (STRING);
[2023-01-22T10:47:34.763+0000] {spark_submit.py:495} INFO - optional binary start (STRING);
[2023-01-22T10:47:34.763+0000] {spark_submit.py:495} INFO - }
[2023-01-22T10:47:34.763+0000] {spark_submit.py:495} INFO - optional binary type (STRING);
[2023-01-22T10:47:34.764+0000] {spark_submit.py:495} INFO - }
[2023-01-22T10:47:34.764+0000] {spark_submit.py:495} INFO - }
[2023-01-22T10:47:34.764+0000] {spark_submit.py:495} INFO - 
[2023-01-22T10:47:34.764+0000] {spark_submit.py:495} INFO - 
[2023-01-22T10:47:35.027+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:35 INFO CodecPool: Got brand-new compressor [.snappy]
[2023-01-22T10:47:35.458+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:35 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-22T10:47:35.900+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:35 INFO CodeGenerator: Code generated in 365.830833 ms
[2023-01-22T10:47:37.513+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:37 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1674384429204-83d9a345-dbf2-418b-86ea-52db103f09d6/_temporary/0/_temporary/' directory.
[2023-01-22T10:47:37.514+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:37 INFO FileOutputCommitter: Saved output of task 'attempt_202301221047333789128522810807192_0001_m_000000_1' to gs://entsoe_temp_1009/.spark-bigquery-local-1674384429204-83d9a345-dbf2-418b-86ea-52db103f09d6/_temporary/0/task_202301221047333789128522810807192_0001_m_000000
[2023-01-22T10:47:37.514+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:37 INFO SparkHadoopMapRedUtil: attempt_202301221047333789128522810807192_0001_m_000000_1: Committed. Elapsed time: 516 ms.
[2023-01-22T10:47:37.522+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:37 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2799 bytes result sent to driver
[2023-01-22T10:47:37.524+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:37 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 3273 ms on ce1344c6e6d4 (executor driver) (1/1)
[2023-01-22T10:47:37.525+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:37 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-22T10:47:37.526+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:37 INFO DAGScheduler: ResultStage 1 (save at BigQueryWriteHelper.java:105) finished in 3.381 s
[2023-01-22T10:47:37.526+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:37 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-22T10:47:37.526+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-22T10:47:37.527+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:37 INFO DAGScheduler: Job 1 finished: save at BigQueryWriteHelper.java:105, took 3.399702 s
[2023-01-22T10:47:37.529+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:37 INFO FileFormatWriter: Start to commit write Job fd4a7bed-1646-4003-a0cf-3562ac500ff9.
[2023-01-22T10:47:38.045+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:38 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1674384429204-83d9a345-dbf2-418b-86ea-52db103f09d6/_temporary/0/task_202301221047333789128522810807192_0001_m_000000/' directory.
[2023-01-22T10:47:38.296+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:38 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1674384429204-83d9a345-dbf2-418b-86ea-52db103f09d6/' directory.
[2023-01-22T10:47:38.617+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:38 INFO BlockManagerInfo: Removed broadcast_3_piece0 on ce1344c6e6d4:44397 in memory (size: 77.2 KiB, free: 434.4 MiB)
[2023-01-22T10:47:39.093+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:39 INFO FileFormatWriter: Write Job fd4a7bed-1646-4003-a0cf-3562ac500ff9 committed. Elapsed time: 1563 ms.
[2023-01-22T10:47:39.096+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:39 INFO FileFormatWriter: Finished processing stats for write job fd4a7bed-1646-4003-a0cf-3562ac500ff9.
[2023-01-22T10:47:39.922+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:39 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=entsoe_playground, projectId=rafzul-analytics-1009, tableId=TEST_total_generation_staging}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=null, ignoreUnknownValue=null, sourceUris=[gs://entsoe_temp_1009/.spark-bigquery-local-1674384429204-83d9a345-dbf2-418b-86ea-52db103f09d6/part-00000-8f331e0f-c4e6-4849-8d67-43c353237355-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=true, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=rafzul-analytics-1009, job=e2cc1202-d2f9-4dfd-907d-f57b274f4a19, location=asia-southeast1}
[2023-01-22T10:47:42.919+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:42 ERROR BigQueryClient: Unable to create the job to load to rafzul-analytics-1009.entsoe_playground.TEST_total_generation_staging
[2023-01-22T10:47:43.191+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T10:47:43.191+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 68, in <module>
[2023-01-22T10:47:43.192+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T10:47:43.192+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 58, in main
[2023-01-22T10:47:43.192+0000] {spark_submit.py:495} INFO - .save("rafzul-analytics-1009.entsoe_playground.TEST_total_generation_staging")
[2023-01-22T10:47:43.192+0000] {spark_submit.py:495} INFO - File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 968, in save
[2023-01-22T10:47:43.192+0000] {spark_submit.py:495} INFO - File "/opt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py", line 1321, in __call__
[2023-01-22T10:47:43.192+0000] {spark_submit.py:495} INFO - File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 190, in deco
[2023-01-22T10:47:43.192+0000] {spark_submit.py:495} INFO - File "/opt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py", line 326, in get_return_value
[2023-01-22T10:47:43.192+0000] {spark_submit.py:495} INFO - py4j.protocol.Py4JJavaError: An error occurred while calling o43.save.
[2023-01-22T10:47:43.192+0000] {spark_submit.py:495} INFO - : com.google.cloud.bigquery.connector.common.BigQueryConnectorException: Failed to write to BigQuery
[2023-01-22T10:47:43.193+0000] {spark_submit.py:495} INFO - at com.google.cloud.spark.bigquery.write.BigQueryWriteHelper.writeDataFrameToBigQuery(BigQueryWriteHelper.java:110)
[2023-01-22T10:47:43.193+0000] {spark_submit.py:495} INFO - at com.google.cloud.spark.bigquery.write.BigQueryDeprecatedIndirectInsertableRelation.insert(BigQueryDeprecatedIndirectInsertableRelation.java:43)
[2023-01-22T10:47:43.193+0000] {spark_submit.py:495} INFO - at com.google.cloud.spark.bigquery.write.CreatableRelationProviderHelper.createRelation(CreatableRelationProviderHelper.java:51)
[2023-01-22T10:47:43.193+0000] {spark_submit.py:495} INFO - at com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelation(BigQueryRelationProvider.scala:106)
[2023-01-22T10:47:43.193+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
[2023-01-22T10:47:43.193+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
[2023-01-22T10:47:43.193+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
[2023-01-22T10:47:43.193+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
[2023-01-22T10:47:43.193+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
[2023-01-22T10:47:43.193+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-22T10:47:43.193+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-22T10:47:43.194+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-22T10:47:43.194+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-22T10:47:43.194+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-22T10:47:43.194+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
[2023-01-22T10:47:43.194+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
[2023-01-22T10:47:43.194+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
[2023-01-22T10:47:43.194+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
[2023-01-22T10:47:43.194+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
[2023-01-22T10:47:43.194+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-22T10:47:43.194+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2023-01-22T10:47:43.194+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2023-01-22T10:47:43.195+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-22T10:47:43.195+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-22T10:47:43.195+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
[2023-01-22T10:47:43.195+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
[2023-01-22T10:47:43.195+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
[2023-01-22T10:47:43.195+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
[2023-01-22T10:47:43.195+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)
[2023-01-22T10:47:43.195+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)
[2023-01-22T10:47:43.195+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)
[2023-01-22T10:47:43.195+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)
[2023-01-22T10:47:43.195+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
[2023-01-22T10:47:43.195+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T10:47:43.195+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T10:47:43.195+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T10:47:43.195+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T10:47:43.195+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T10:47:43.195+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T10:47:43.196+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T10:47:43.196+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T10:47:43.196+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T10:47:43.196+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T10:47:43.196+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T10:47:43.196+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T10:47:43.196+0000] {spark_submit.py:495} INFO - Caused by: com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryException: Character '.' found in field name: inBiddingZone_Domain.mRID, parquet file: /bigstore/entsoe_temp_1009/.spark-bigquery-local-1674384429204-83d9a345-dbf2-418b-86ea-52db103f09d6/part-00000-8f331e0f-c4e6-4849-8d67-43c353237355-c000.snappy.parquet.Reading such fields is not yet supported.
[2023-01-22T10:47:43.196+0000] {spark_submit.py:495} INFO - at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.Job.reload(Job.java:419)
[2023-01-22T10:47:43.196+0000] {spark_submit.py:495} INFO - at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.Job.waitFor(Job.java:252)
[2023-01-22T10:47:43.196+0000] {spark_submit.py:495} INFO - at com.google.cloud.bigquery.connector.common.BigQueryClient.createAndWaitFor(BigQueryClient.java:333)
[2023-01-22T10:47:43.196+0000] {spark_submit.py:495} INFO - at com.google.cloud.bigquery.connector.common.BigQueryClient.createAndWaitFor(BigQueryClient.java:323)
[2023-01-22T10:47:43.196+0000] {spark_submit.py:495} INFO - at com.google.cloud.bigquery.connector.common.BigQueryClient.loadDataIntoTable(BigQueryClient.java:553)
[2023-01-22T10:47:43.196+0000] {spark_submit.py:495} INFO - at com.google.cloud.spark.bigquery.write.BigQueryWriteHelper.loadDataToBigQuery(BigQueryWriteHelper.java:130)
[2023-01-22T10:47:43.196+0000] {spark_submit.py:495} INFO - at com.google.cloud.spark.bigquery.write.BigQueryWriteHelper.writeDataFrameToBigQuery(BigQueryWriteHelper.java:107)
[2023-01-22T10:47:43.196+0000] {spark_submit.py:495} INFO - ... 44 more
[2023-01-22T10:47:43.196+0000] {spark_submit.py:495} INFO - 
[2023-01-22T10:47:43.231+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:43 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-22T10:47:43.240+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:43 INFO SparkUI: Stopped Spark web UI at http://ce1344c6e6d4:4041
[2023-01-22T10:47:43.253+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:43 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-22T10:47:43.266+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:43 INFO MemoryStore: MemoryStore cleared
[2023-01-22T10:47:43.267+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:43 INFO BlockManager: BlockManager stopped
[2023-01-22T10:47:43.269+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:43 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-22T10:47:43.272+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:43 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-22T10:47:43.277+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:43 INFO SparkContext: Successfully stopped SparkContext
[2023-01-22T10:47:43.277+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:43 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T10:47:43.278+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:43 INFO ShutdownHookManager: Deleting directory /tmp/spark-f7e52ea1-6a41-4453-a9c3-d7d29f163b8b
[2023-01-22T10:47:43.281+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:43 INFO ShutdownHookManager: Deleting directory /tmp/spark-16822bd0-3f8e-4163-bb9f-2744fb2a1a8a/pyspark-10a945d6-ad91-43c2-bce4-1e7050cb1bac
[2023-01-22T10:47:43.285+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:43 INFO ShutdownHookManager: Deleting directory /tmp/spark-16822bd0-3f8e-4163-bb9f-2744fb2a1a8a
[2023-01-22T10:47:43.368+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-22T10:47:43.371+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230122T104657, end_date=20230122T104743
[2023-01-22T10:47:43.391+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 467 for task transform_stage_generation (Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 1872)
[2023-01-22T10:47:43.444+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T10:47:43.455+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
