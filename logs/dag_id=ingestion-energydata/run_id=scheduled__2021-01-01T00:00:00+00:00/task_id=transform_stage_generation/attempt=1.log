[2023-01-20T16:16:53.917+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T16:16:53.930+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T16:16:53.930+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T16:16:53.930+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-20T16:16:53.930+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T16:16:53.941+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-20T16:16:53.951+0000] {standard_task_runner.py:55} INFO - Started process 197 to run task
[2023-01-20T16:16:53.956+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '61', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpueq872vm']
[2023-01-20T16:16:53.959+0000] {standard_task_runner.py:83} INFO - Job 61: Subtask transform_stage_generation
[2023-01-20T16:16:54.025+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T16:16:54.112+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-20T16:16:54.120+0000] {spark_submit.py:222} INFO - Could not load connection string spark_default, defaulting to yarn
[2023-01-20T16:16:54.121+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master yarn --name arrow-spark /opt/***/plugins/scripts/transform_stage.py total_generation 202101010000 202101010100 Europe/Berlin DE_TENNET
[2023-01-20T16:16:54.127+0000] {spark_submit.py:495} INFO - /usr/local/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit: line 27: /opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-class: No such file or directory
[2023-01-20T16:16:54.138+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master yarn --name arrow-spark /opt/***/plugins/scripts/transform_stage.py total_generation 202101010000 202101010100 Europe/Berlin DE_TENNET. Error code is: 127.
[2023-01-20T16:16:54.142+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230120T161653, end_date=20230120T161654
[2023-01-20T16:16:54.155+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 61 for task transform_stage_generation (Cannot execute: spark-submit --master yarn --name arrow-spark /opt/***/plugins/scripts/transform_stage.py total_generation 202101010000 202101010100 Europe/Berlin DE_TENNET. Error code is: 127.; 197)
[2023-01-20T16:16:54.207+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T16:16:54.228+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T16:19:06.401+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T16:19:06.413+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T16:19:06.414+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T16:19:06.414+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-20T16:19:06.414+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T16:19:06.428+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-20T16:19:06.437+0000] {standard_task_runner.py:55} INFO - Started process 337 to run task
[2023-01-20T16:19:06.442+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '65', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp15kc1xg3']
[2023-01-20T16:19:06.445+0000] {standard_task_runner.py:83} INFO - Job 65: Subtask transform_stage_generation
[2023-01-20T16:19:06.518+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T16:19:06.594+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-20T16:19:06.605+0000] {spark_submit.py:222} INFO - Could not load connection string spark_default, defaulting to yarn
[2023-01-20T16:19:06.606+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master yarn --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 Europe/Berlin DE_TENNET
[2023-01-20T16:19:06.612+0000] {spark_submit.py:495} INFO - /usr/local/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit: line 27: /opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-class: No such file or directory
[2023-01-20T16:19:06.624+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master yarn --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 Europe/Berlin DE_TENNET. Error code is: 127.
[2023-01-20T16:19:06.628+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230120T161906, end_date=20230120T161906
[2023-01-20T16:19:06.643+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 65 for task transform_stage_generation (Cannot execute: spark-submit --master yarn --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 Europe/Berlin DE_TENNET. Error code is: 127.; 337)
[2023-01-20T16:19:06.694+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T16:19:06.717+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T16:22:27.122+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T16:22:27.156+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T16:22:27.157+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T16:22:27.158+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-20T16:22:27.159+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T16:22:27.197+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-20T16:22:27.228+0000] {standard_task_runner.py:55} INFO - Started process 552 to run task
[2023-01-20T16:22:27.246+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '68', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp0horvmzi']
[2023-01-20T16:22:27.255+0000] {standard_task_runner.py:83} INFO - Job 68: Subtask transform_stage_generation
[2023-01-20T16:22:27.472+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T16:22:27.669+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-20T16:22:27.695+0000] {spark_submit.py:222} INFO - Could not load connection string spark_default, defaulting to yarn
[2023-01-20T16:22:27.698+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master yarn --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-20T16:22:27.715+0000] {spark_submit.py:495} INFO - /usr/local/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit: line 27: /opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-class: No such file or directory
[2023-01-20T16:22:27.743+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master yarn --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 127.
[2023-01-20T16:22:27.754+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230120T162227, end_date=20230120T162227
[2023-01-20T16:22:27.799+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 68 for task transform_stage_generation (Cannot execute: spark-submit --master yarn --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 127.; 552)
[2023-01-20T16:22:27.860+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T16:22:27.952+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T16:31:17.792+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T16:31:17.809+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T16:31:17.809+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T16:31:17.809+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-20T16:31:17.809+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T16:31:17.823+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-20T16:31:17.847+0000] {standard_task_runner.py:55} INFO - Started process 1114 to run task
[2023-01-20T16:31:17.852+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '75', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpsk_yetc7']
[2023-01-20T16:31:17.855+0000] {standard_task_runner.py:83} INFO - Job 75: Subtask transform_stage_generation
[2023-01-20T16:31:17.938+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T16:31:18.018+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-20T16:31:18.027+0000] {spark_submit.py:222} INFO - Could not load connection string spark_default, defaulting to yarn
[2023-01-20T16:31:18.030+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master yarn --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-20T16:31:18.036+0000] {spark_submit.py:495} INFO - /usr/local/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit: line 27: /opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-class: No such file or directory
[2023-01-20T16:31:18.046+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master yarn --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 127.
[2023-01-20T16:31:18.050+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230120T163117, end_date=20230120T163118
[2023-01-20T16:31:18.063+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 75 for task transform_stage_generation (Cannot execute: spark-submit --master yarn --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 127.; 1114)
[2023-01-20T16:31:18.103+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T16:31:18.124+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T17:55:57.464+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T17:55:57.476+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T17:55:57.476+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T17:55:57.476+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-20T17:55:57.476+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T17:55:57.491+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-20T17:55:57.500+0000] {standard_task_runner.py:55} INFO - Started process 6213 to run task
[2023-01-20T17:55:57.505+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '84', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpnytjgm5k']
[2023-01-20T17:55:57.508+0000] {standard_task_runner.py:83} INFO - Job 84: Subtask transform_stage_generation
[2023-01-20T17:55:57.575+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T17:55:57.662+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-20T17:55:57.664+0000] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-01-20T17:55:57.666+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master  --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-20T17:55:57.679+0000] {spark_submit.py:495} INFO - /usr/local/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit: line 27: /opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-class: No such file or directory
[2023-01-20T17:55:57.688+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master  --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 127.
[2023-01-20T17:55:57.693+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230120T175557, end_date=20230120T175557
[2023-01-20T17:55:57.706+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 84 for task transform_stage_generation (Cannot execute: spark-submit --master  --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 127.; 6213)
[2023-01-20T17:55:57.756+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T17:55:57.776+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T17:59:00.638+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T17:59:00.651+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T17:59:00.651+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T17:59:00.652+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-20T17:59:00.652+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T17:59:00.669+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-20T17:59:00.681+0000] {standard_task_runner.py:55} INFO - Started process 6426 to run task
[2023-01-20T17:59:00.687+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '89', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmplxuv_u9q']
[2023-01-20T17:59:00.692+0000] {standard_task_runner.py:83} INFO - Job 89: Subtask transform_stage_generation
[2023-01-20T17:59:00.809+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T17:59:00.900+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-20T17:59:00.902+0000] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-01-20T17:59:00.905+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master  --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-20T17:59:00.912+0000] {spark_submit.py:495} INFO - /usr/local/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit: line 27: /opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-class: No such file or directory
[2023-01-20T17:59:00.927+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master  --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 127.
[2023-01-20T17:59:00.933+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230120T175900, end_date=20230120T175900
[2023-01-20T17:59:00.949+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 89 for task transform_stage_generation (Cannot execute: spark-submit --master  --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 127.; 6426)
[2023-01-20T17:59:00.979+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T17:59:01.001+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T18:05:02.149+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T18:05:02.164+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T18:05:02.164+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T18:05:02.164+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-20T18:05:02.164+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T18:05:02.184+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-20T18:05:02.201+0000] {standard_task_runner.py:55} INFO - Started process 6786 to run task
[2023-01-20T18:05:02.209+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '94', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpplc320p7']
[2023-01-20T18:05:02.214+0000] {standard_task_runner.py:83} INFO - Job 94: Subtask transform_stage_generation
[2023-01-20T18:05:02.324+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T18:05:02.449+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-20T18:05:02.451+0000] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-01-20T18:05:02.454+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master  --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-20T18:05:02.464+0000] {spark_submit.py:495} INFO - /usr/local/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit: line 27: /opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-class: No such file or directory
[2023-01-20T18:05:02.479+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master  --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 127.
[2023-01-20T18:05:02.484+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230120T180502, end_date=20230120T180502
[2023-01-20T18:05:02.505+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 94 for task transform_stage_generation (Cannot execute: spark-submit --master  --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 127.; 6786)
[2023-01-20T18:05:02.555+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T18:05:02.578+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T18:11:15.403+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T18:11:15.414+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T18:11:15.414+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T18:11:15.414+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-20T18:11:15.414+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T18:11:15.426+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-20T18:11:15.435+0000] {standard_task_runner.py:55} INFO - Started process 7212 to run task
[2023-01-20T18:11:15.439+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '100', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpkg9tefe2']
[2023-01-20T18:11:15.442+0000] {standard_task_runner.py:83} INFO - Job 100: Subtask transform_stage_generation
[2023-01-20T18:11:15.514+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T18:11:15.581+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-20T18:11:15.581+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 156, in execute
    self._hook = self._get_hook()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 165, in _get_hook
    return SparkSubmitHook(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 155, in __init__
    raise RuntimeError(
RuntimeError: The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/opt/spark-3.3.1-bin-hadoop3-scala2.13bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH
[2023-01-20T18:11:15.590+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230120T181115, end_date=20230120T181115
[2023-01-20T18:11:15.599+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 100 for task transform_stage_generation (The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/opt/spark-3.3.1-bin-hadoop3-scala2.13bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH; 7212)
[2023-01-20T18:11:15.651+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T18:11:15.665+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T18:12:43.110+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T18:12:43.146+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T18:12:43.147+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T18:12:43.147+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-20T18:12:43.147+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T18:12:43.187+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-20T18:12:43.204+0000] {standard_task_runner.py:55} INFO - Started process 7307 to run task
[2023-01-20T18:12:43.214+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '104', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpwfwt1dxc']
[2023-01-20T18:12:43.222+0000] {standard_task_runner.py:83} INFO - Job 104: Subtask transform_stage_generation
[2023-01-20T18:12:43.392+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T18:12:43.537+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-20T18:12:43.538+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 156, in execute
    self._hook = self._get_hook()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 165, in _get_hook
    return SparkSubmitHook(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 155, in __init__
    raise RuntimeError(
RuntimeError: The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH
[2023-01-20T18:12:43.558+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230120T181243, end_date=20230120T181243
[2023-01-20T18:12:43.580+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 104 for task transform_stage_generation (The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH; 7307)
[2023-01-20T18:12:43.625+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T18:12:43.659+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T18:16:15.968+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T18:16:15.996+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T18:16:15.996+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T18:16:15.997+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-20T18:16:15.998+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T18:16:16.028+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-20T18:16:16.061+0000] {standard_task_runner.py:55} INFO - Started process 7543 to run task
[2023-01-20T18:16:16.068+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '108', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpc1whcyu5']
[2023-01-20T18:16:16.076+0000] {standard_task_runner.py:83} INFO - Job 108: Subtask transform_stage_generation
[2023-01-20T18:16:16.231+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T18:16:16.410+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-20T18:16:16.411+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 156, in execute
    self._hook = self._get_hook()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 165, in _get_hook
    return SparkSubmitHook(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 155, in __init__
    raise RuntimeError(
RuntimeError: The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH
[2023-01-20T18:16:16.437+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230120T181615, end_date=20230120T181616
[2023-01-20T18:16:16.472+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 108 for task transform_stage_generation (The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH; 7543)
[2023-01-20T18:16:16.518+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T18:16:16.558+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T18:18:55.636+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T18:18:55.654+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T18:18:55.654+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T18:18:55.654+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-20T18:18:55.654+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T18:18:55.675+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-20T18:18:55.690+0000] {standard_task_runner.py:55} INFO - Started process 7714 to run task
[2023-01-20T18:18:55.698+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '113', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpm9vipqnn']
[2023-01-20T18:18:55.702+0000] {standard_task_runner.py:83} INFO - Job 113: Subtask transform_stage_generation
[2023-01-20T18:18:55.805+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T18:18:55.920+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-20T18:18:55.920+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 156, in execute
    self._hook = self._get_hook()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 165, in _get_hook
    return SparkSubmitHook(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 155, in __init__
    raise RuntimeError(
RuntimeError: The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH
[2023-01-20T18:18:55.937+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230120T181855, end_date=20230120T181855
[2023-01-20T18:18:55.955+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 113 for task transform_stage_generation (The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH; 7714)
[2023-01-20T18:18:55.999+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T18:18:56.035+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T19:15:54.684+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T19:15:54.725+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T19:15:54.727+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T19:15:54.728+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-20T19:15:54.729+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T19:15:54.809+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-20T19:15:54.832+0000] {standard_task_runner.py:55} INFO - Started process 360 to run task
[2023-01-20T19:15:54.838+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '128', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp1h8wdyll']
[2023-01-20T19:15:54.840+0000] {standard_task_runner.py:83} INFO - Job 128: Subtask transform_stage_generation
[2023-01-20T19:15:55.057+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 984f5901aea4
[2023-01-20T19:15:55.331+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-20T19:15:55.331+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 156, in execute
    self._hook = self._get_hook()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 165, in _get_hook
    return SparkSubmitHook(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 155, in __init__
    raise RuntimeError(
RuntimeError: The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/usr/local/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH
[2023-01-20T19:15:55.397+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230120T191554, end_date=20230120T191555
[2023-01-20T19:15:55.458+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 128 for task transform_stage_generation (The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/usr/local/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH; 360)
[2023-01-20T19:15:55.546+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T19:15:55.717+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T03:03:17.688+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T03:03:17.710+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T03:03:17.710+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T03:03:17.711+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-21T03:03:17.711+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T03:03:17.742+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-21T03:03:17.760+0000] {standard_task_runner.py:55} INFO - Started process 324 to run task
[2023-01-21T03:03:17.771+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '140', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp942qkb5z']
[2023-01-21T03:03:17.777+0000] {standard_task_runner.py:83} INFO - Job 140: Subtask transform_stage_generation
[2023-01-21T03:03:17.928+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 2b3134e2b02d
[2023-01-21T03:03:18.087+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-21T03:03:18.089+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 156, in execute
    self._hook = self._get_hook()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 165, in _get_hook
    return SparkSubmitHook(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 155, in __init__
    raise RuntimeError(
RuntimeError: The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/usr/local/spark/bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH
[2023-01-21T03:03:18.111+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230121T030317, end_date=20230121T030318
[2023-01-21T03:03:18.136+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 140 for task transform_stage_generation (The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/usr/local/spark/bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH; 324)
[2023-01-21T03:03:18.183+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T03:03:18.220+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T07:47:28.772+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T07:47:28.783+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T07:47:28.783+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T07:47:28.784+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-21T07:47:28.784+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T07:47:28.795+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-21T07:47:28.802+0000] {standard_task_runner.py:55} INFO - Started process 71 to run task
[2023-01-21T07:47:28.807+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '162', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmps4t0igd6']
[2023-01-21T07:47:28.810+0000] {standard_task_runner.py:83} INFO - Job 162: Subtask transform_stage_generation
[2023-01-21T07:47:28.884+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 4d52b49e46b4
[2023-01-21T07:47:29.009+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-21T07:47:29.011+0000] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-01-21T07:47:29.013+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master  --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-21T07:47:29.027+0000] {spark_submit.py:495} INFO - /usr/local/spark/bin/load-spark-env.sh: line 68: ps: command not found
[2023-01-21T07:47:32.743+0000] {spark_submit.py:495} INFO - Exception in thread "main" org.apache.spark.SparkException: Master must either be yarn or start with spark, mesos, k8s, or local
[2023-01-21T07:47:32.743+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.error(SparkSubmit.scala:975)
[2023-01-21T07:47:32.744+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:238)
[2023-01-21T07:47:32.744+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:901)
[2023-01-21T07:47:32.744+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
[2023-01-21T07:47:32.744+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
[2023-01-21T07:47:32.744+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
[2023-01-21T07:47:32.745+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)
[2023-01-21T07:47:32.745+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)
[2023-01-21T07:47:32.745+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
[2023-01-21T07:47:32.787+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master  --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-21T07:47:32.793+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230121T074728, end_date=20230121T074732
[2023-01-21T07:47:32.812+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 162 for task transform_stage_generation (Cannot execute: spark-submit --master  --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 71)
[2023-01-21T07:47:32.848+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T07:47:32.874+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T08:16:38.456+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T08:16:38.469+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T08:16:38.469+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T08:16:38.470+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-21T08:16:38.470+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T08:16:38.483+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-21T08:16:38.492+0000] {standard_task_runner.py:55} INFO - Started process 2287 to run task
[2023-01-21T08:16:38.497+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '173', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpc8g6c8v8']
[2023-01-21T08:16:38.500+0000] {standard_task_runner.py:83} INFO - Job 173: Subtask transform_stage_generation
[2023-01-21T08:16:38.568+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 4d52b49e46b4
[2023-01-21T08:16:38.664+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-21T08:16:38.678+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-21T08:16:38.682+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-21T08:16:38.695+0000] {spark_submit.py:495} INFO - /usr/local/spark/bin/load-spark-env.sh: line 68: ps: command not found
[2023-01-21T08:16:41.952+0000] {spark_submit.py:495} INFO - 23/01/21 08:16:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-21T08:16:42.246+0000] {spark_submit.py:495} INFO - 23/01/21 08:16:42 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-21T08:16:42.358+0000] {spark_submit.py:495} INFO - Exception in thread "main" java.io.IOException: Cannot run program "/usr/bin/python3.10": error=2, No such file or directory
[2023-01-21T08:16:42.359+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1128)
[2023-01-21T08:16:42.359+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1071)
[2023-01-21T08:16:42.359+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.PythonRunner$.main(PythonRunner.scala:97)
[2023-01-21T08:16:42.359+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.PythonRunner.main(PythonRunner.scala)
[2023-01-21T08:16:42.359+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-21T08:16:42.359+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-21T08:16:42.360+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-21T08:16:42.360+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-21T08:16:42.360+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
[2023-01-21T08:16:42.360+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:958)
[2023-01-21T08:16:42.360+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
[2023-01-21T08:16:42.360+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
[2023-01-21T08:16:42.361+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
[2023-01-21T08:16:42.361+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)
[2023-01-21T08:16:42.361+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)
[2023-01-21T08:16:42.361+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
[2023-01-21T08:16:42.361+0000] {spark_submit.py:495} INFO - Caused by: java.io.IOException: error=2, No such file or directory
[2023-01-21T08:16:42.361+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessImpl.forkAndExec(Native Method)
[2023-01-21T08:16:42.362+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:340)
[2023-01-21T08:16:42.362+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessImpl.start(ProcessImpl.java:271)
[2023-01-21T08:16:42.362+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1107)
[2023-01-21T08:16:42.362+0000] {spark_submit.py:495} INFO - ... 15 more
[2023-01-21T08:16:42.369+0000] {spark_submit.py:495} INFO - 23/01/21 08:16:42 INFO ShutdownHookManager: Shutdown hook called
[2023-01-21T08:16:42.370+0000] {spark_submit.py:495} INFO - 23/01/21 08:16:42 INFO ShutdownHookManager: Deleting directory /tmp/spark-76ca6f76-4ff4-4a53-811a-803c704bb228
[2023-01-21T08:16:42.445+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-21T08:16:42.450+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230121T081638, end_date=20230121T081642
[2023-01-21T08:16:42.466+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 173 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 2287)
[2023-01-21T08:16:42.484+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T08:16:42.502+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T08:23:55.712+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T08:23:55.723+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T08:23:55.723+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T08:23:55.723+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-21T08:23:55.723+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T08:23:55.735+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-21T08:23:55.742+0000] {standard_task_runner.py:55} INFO - Started process 285 to run task
[2023-01-21T08:23:55.746+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '181', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpisc4tker']
[2023-01-21T08:23:55.751+0000] {standard_task_runner.py:83} INFO - Job 181: Subtask transform_stage_generation
[2023-01-21T08:23:55.821+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host d00464a30031
[2023-01-21T08:23:55.906+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-21T08:23:55.920+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-21T08:23:55.922+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-21T08:23:55.939+0000] {spark_submit.py:495} INFO - /usr/local/spark/bin/load-spark-env.sh: line 68: ps: command not found
[2023-01-21T08:23:59.685+0000] {spark_submit.py:495} INFO - 23/01/21 08:23:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-21T08:24:00.058+0000] {spark_submit.py:495} INFO - 23/01/21 08:24:00 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-21T08:24:00.253+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-21T08:24:00.254+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 2, in <module>
[2023-01-21T08:24:00.254+0000] {spark_submit.py:495} INFO - from dotenv import load_dotenv
[2023-01-21T08:24:00.254+0000] {spark_submit.py:495} INFO - ModuleNotFoundError: No module named 'dotenv'
[2023-01-21T08:24:00.276+0000] {spark_submit.py:495} INFO - 23/01/21 08:24:00 INFO ShutdownHookManager: Shutdown hook called
[2023-01-21T08:24:00.278+0000] {spark_submit.py:495} INFO - 23/01/21 08:24:00 INFO ShutdownHookManager: Deleting directory /tmp/spark-08d13d43-b8d4-486e-9a5b-0485a3af4ce8
[2023-01-21T08:24:00.355+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-21T08:24:00.360+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230121T082355, end_date=20230121T082400
[2023-01-21T08:24:00.387+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 181 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 285)
[2023-01-21T08:24:00.413+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T08:24:00.434+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T10:18:14.627+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T10:18:14.641+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T10:18:14.641+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T10:18:14.642+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-21T10:18:14.642+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T10:18:14.659+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-21T10:18:14.670+0000] {standard_task_runner.py:55} INFO - Started process 2199 to run task
[2023-01-21T10:18:14.676+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '192', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpnp4wf2gh']
[2023-01-21T10:18:14.680+0000] {standard_task_runner.py:83} INFO - Job 192: Subtask transform_stage_generation
[2023-01-21T10:18:14.783+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host b3a2c4a96689
[2023-01-21T10:18:14.959+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-21T10:18:14.984+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-21T10:18:14.990+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-21T10:18:25.378+0000] {spark_submit.py:495} INFO - 23/01/21 10:18:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-21T10:18:26.261+0000] {spark_submit.py:495} INFO - 23/01/21 10:18:26 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-21T10:18:26.384+0000] {spark_submit.py:495} INFO - Exception in thread "main" java.io.IOException: Cannot run program "/usr/bin/python3.10": error=2, No such file or directory
[2023-01-21T10:18:26.385+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1128)
[2023-01-21T10:18:26.387+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1071)
[2023-01-21T10:18:26.390+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.PythonRunner$.main(PythonRunner.scala:97)
[2023-01-21T10:18:26.391+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.PythonRunner.main(PythonRunner.scala)
[2023-01-21T10:18:26.391+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-21T10:18:26.392+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-21T10:18:26.392+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-21T10:18:26.392+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-21T10:18:26.393+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
[2023-01-21T10:18:26.393+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:958)
[2023-01-21T10:18:26.393+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
[2023-01-21T10:18:26.393+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
[2023-01-21T10:18:26.394+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
[2023-01-21T10:18:26.394+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)
[2023-01-21T10:18:26.394+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)
[2023-01-21T10:18:26.394+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
[2023-01-21T10:18:26.395+0000] {spark_submit.py:495} INFO - Caused by: java.io.IOException: error=2, No such file or directory
[2023-01-21T10:18:26.395+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessImpl.forkAndExec(Native Method)
[2023-01-21T10:18:26.395+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:340)
[2023-01-21T10:18:26.396+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessImpl.start(ProcessImpl.java:271)
[2023-01-21T10:18:26.396+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1107)
[2023-01-21T10:18:26.397+0000] {spark_submit.py:495} INFO - ... 15 more
[2023-01-21T10:18:26.404+0000] {spark_submit.py:495} INFO - 23/01/21 10:18:26 INFO ShutdownHookManager: Shutdown hook called
[2023-01-21T10:18:26.406+0000] {spark_submit.py:495} INFO - 23/01/21 10:18:26 INFO ShutdownHookManager: Deleting directory /tmp/spark-01670c95-c3fb-437f-9cfd-b697e6e02624
[2023-01-21T10:18:26.518+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-21T10:18:26.525+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230121T101814, end_date=20230121T101826
[2023-01-21T10:18:26.555+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 192 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 2199)
[2023-01-21T10:18:26.583+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T10:18:26.630+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T10:45:46.293+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T10:45:46.326+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T10:45:46.327+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T10:45:46.327+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-21T10:45:46.327+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T10:45:46.365+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-21T10:45:46.385+0000] {standard_task_runner.py:55} INFO - Started process 157 to run task
[2023-01-21T10:45:46.396+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '205', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpi6yxiejs']
[2023-01-21T10:45:46.403+0000] {standard_task_runner.py:83} INFO - Job 205: Subtask transform_stage_generation
[2023-01-21T10:45:46.564+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host b3a2c4a96689
[2023-01-21T10:45:46.755+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-21T10:45:46.779+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-21T10:45:46.785+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-21T10:45:51.731+0000] {spark_submit.py:495} INFO - 23/01/21 10:45:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-21T10:45:52.255+0000] {spark_submit.py:495} INFO - 23/01/21 10:45:52 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-21T10:45:52.337+0000] {spark_submit.py:495} INFO - Exception in thread "main" java.io.IOException: Cannot run program "/usr/bin/python3.10": error=2, No such file or directory
[2023-01-21T10:45:52.338+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1128)
[2023-01-21T10:45:52.339+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1071)
[2023-01-21T10:45:52.340+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.PythonRunner$.main(PythonRunner.scala:97)
[2023-01-21T10:45:52.340+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.PythonRunner.main(PythonRunner.scala)
[2023-01-21T10:45:52.340+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-21T10:45:52.341+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-21T10:45:52.341+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-21T10:45:52.341+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-21T10:45:52.341+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
[2023-01-21T10:45:52.342+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:958)
[2023-01-21T10:45:52.342+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
[2023-01-21T10:45:52.342+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
[2023-01-21T10:45:52.343+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
[2023-01-21T10:45:52.343+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)
[2023-01-21T10:45:52.343+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)
[2023-01-21T10:45:52.344+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
[2023-01-21T10:45:52.344+0000] {spark_submit.py:495} INFO - Caused by: java.io.IOException: error=2, No such file or directory
[2023-01-21T10:45:52.345+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessImpl.forkAndExec(Native Method)
[2023-01-21T10:45:52.345+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:340)
[2023-01-21T10:45:52.346+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessImpl.start(ProcessImpl.java:271)
[2023-01-21T10:45:52.346+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1107)
[2023-01-21T10:45:52.346+0000] {spark_submit.py:495} INFO - ... 15 more
[2023-01-21T10:45:52.356+0000] {spark_submit.py:495} INFO - 23/01/21 10:45:52 INFO ShutdownHookManager: Shutdown hook called
[2023-01-21T10:45:52.359+0000] {spark_submit.py:495} INFO - 23/01/21 10:45:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-cd243198-4111-4946-a93f-92031e66694a
[2023-01-21T10:45:52.486+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-21T10:45:52.496+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230121T104546, end_date=20230121T104552
[2023-01-21T10:45:52.534+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 205 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 157)
[2023-01-21T10:45:52.602+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T10:45:52.669+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T11:29:48.664+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T11:29:48.676+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T11:29:48.676+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T11:29:48.676+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-21T11:29:48.676+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T11:29:48.692+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-21T11:29:48.701+0000] {standard_task_runner.py:55} INFO - Started process 1124 to run task
[2023-01-21T11:29:48.707+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '218', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpnqn3n9q8']
[2023-01-21T11:29:48.711+0000] {standard_task_runner.py:83} INFO - Job 218: Subtask transform_stage_generation
[2023-01-21T11:29:48.803+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host b7c4de7fae0b
[2023-01-21T11:29:48.912+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-21T11:29:48.931+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-21T11:29:48.935+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-21T11:29:57.017+0000] {spark_submit.py:495} INFO - 23/01/21 11:29:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-21T11:29:58.039+0000] {spark_submit.py:495} INFO - 23/01/21 11:29:58 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-21T11:29:58.406+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-21T11:29:58.407+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 2, in <module>
[2023-01-21T11:29:58.407+0000] {spark_submit.py:495} INFO - from dotenv import load_dotenv, find_dotenv
[2023-01-21T11:29:58.408+0000] {spark_submit.py:495} INFO - ModuleNotFoundError: No module named 'dotenv'
[2023-01-21T11:29:58.438+0000] {spark_submit.py:495} INFO - 23/01/21 11:29:58 INFO ShutdownHookManager: Shutdown hook called
[2023-01-21T11:29:58.442+0000] {spark_submit.py:495} INFO - 23/01/21 11:29:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-316cd947-b1c9-44ba-9e79-149e10e29c8b
[2023-01-21T11:29:58.620+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-21T11:29:58.631+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230121T112948, end_date=20230121T112958
[2023-01-21T11:29:58.665+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 218 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 1124)
[2023-01-21T11:29:58.710+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T11:29:58.775+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T11:31:19.001+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T11:31:19.017+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T11:31:19.018+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T11:31:19.018+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-21T11:31:19.018+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T11:31:19.036+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-21T11:31:19.045+0000] {standard_task_runner.py:55} INFO - Started process 1335 to run task
[2023-01-21T11:31:19.052+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '222', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp053rie5o']
[2023-01-21T11:31:19.056+0000] {standard_task_runner.py:83} INFO - Job 222: Subtask transform_stage_generation
[2023-01-21T11:31:19.160+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host b7c4de7fae0b
[2023-01-21T11:31:19.291+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-21T11:31:19.305+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-21T11:31:19.308+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-21T11:31:25.015+0000] {spark_submit.py:495} INFO - 23/01/21 11:31:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-21T11:31:25.514+0000] {spark_submit.py:495} INFO - 23/01/21 11:31:25 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-21T11:31:25.697+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 2
[2023-01-21T11:31:25.698+0000] {spark_submit.py:495} INFO - from python-dotenv import load_dotenv, find_dotenv
[2023-01-21T11:31:25.698+0000] {spark_submit.py:495} INFO - ^
[2023-01-21T11:31:25.698+0000] {spark_submit.py:495} INFO - SyntaxError: invalid syntax
[2023-01-21T11:31:25.720+0000] {spark_submit.py:495} INFO - 23/01/21 11:31:25 INFO ShutdownHookManager: Shutdown hook called
[2023-01-21T11:31:25.722+0000] {spark_submit.py:495} INFO - 23/01/21 11:31:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-10c080c4-c3d9-4656-a764-e0ff73c5b187
[2023-01-21T11:31:25.829+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-21T11:31:25.836+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230121T113119, end_date=20230121T113125
[2023-01-21T11:31:25.862+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 222 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 1335)
[2023-01-21T11:31:25.918+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T11:31:25.951+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
