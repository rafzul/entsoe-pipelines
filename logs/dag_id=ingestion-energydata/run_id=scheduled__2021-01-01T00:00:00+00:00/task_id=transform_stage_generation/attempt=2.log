[2023-01-20T16:27:30.832+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T16:27:30.889+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T16:27:30.890+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T16:27:30.890+0000] {taskinstance.py:1284} INFO - Starting attempt 2 of 4
[2023-01-20T16:27:30.891+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T16:27:30.956+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-20T16:27:31.010+0000] {standard_task_runner.py:55} INFO - Started process 876 to run task
[2023-01-20T16:27:31.038+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '71', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp53w3d8je']
[2023-01-20T16:27:31.053+0000] {standard_task_runner.py:83} INFO - Job 71: Subtask transform_stage_generation
[2023-01-20T16:27:31.321+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T16:27:31.670+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=2
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-20T16:27:31.718+0000] {spark_submit.py:222} INFO - Could not load connection string spark_default, defaulting to yarn
[2023-01-20T16:27:31.725+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master yarn --jars {SPARK_HOME}/resources/gcs-connector-hadoop3-latest.jar, {SPARK_HOME}/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-20T16:27:31.748+0000] {spark_submit.py:495} INFO - /usr/local/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit: line 27: /opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-class: No such file or directory
[2023-01-20T16:27:31.784+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master yarn --jars {SPARK_HOME}/resources/gcs-connector-hadoop3-latest.jar, {SPARK_HOME}/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 127.
[2023-01-20T16:27:31.796+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230120T162730, end_date=20230120T162731
[2023-01-20T16:27:31.839+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 71 for task transform_stage_generation (Cannot execute: spark-submit --master yarn --jars {SPARK_HOME}/resources/gcs-connector-hadoop3-latest.jar, {SPARK_HOME}/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 127.; 876)
[2023-01-20T16:27:31.890+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T16:27:31.932+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T16:36:21.936+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T16:36:21.948+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T16:36:21.948+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T16:36:21.948+0000] {taskinstance.py:1284} INFO - Starting attempt 2 of 4
[2023-01-20T16:36:21.948+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T16:36:21.981+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-20T16:36:21.988+0000] {standard_task_runner.py:55} INFO - Started process 1414 to run task
[2023-01-20T16:36:21.992+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '77', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmppwpmlsa7']
[2023-01-20T16:36:21.994+0000] {standard_task_runner.py:83} INFO - Job 77: Subtask transform_stage_generation
[2023-01-20T16:36:22.178+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T16:36:22.447+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=2
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-20T16:36:22.462+0000] {spark_submit.py:222} INFO - Could not load connection string spark_default, defaulting to yarn
[2023-01-20T16:36:22.475+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master yarn --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-20T16:36:22.488+0000] {spark_submit.py:495} INFO - /usr/local/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit: line 27: /opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-class: No such file or directory
[2023-01-20T16:36:22.530+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master yarn --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 127.
[2023-01-20T16:36:22.542+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230120T163621, end_date=20230120T163622
[2023-01-20T16:36:22.571+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 77 for task transform_stage_generation (Cannot execute: spark-submit --master yarn --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 127.; 1414)
[2023-01-20T16:36:22.609+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T16:36:22.670+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T18:04:03.194+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T18:04:03.206+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T18:04:03.207+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T18:04:03.207+0000] {taskinstance.py:1284} INFO - Starting attempt 2 of 4
[2023-01-20T18:04:03.207+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T18:04:03.222+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-20T18:04:03.232+0000] {standard_task_runner.py:55} INFO - Started process 6702 to run task
[2023-01-20T18:04:03.235+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '90', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpkpa02uoi']
[2023-01-20T18:04:03.238+0000] {standard_task_runner.py:83} INFO - Job 90: Subtask transform_stage_generation
[2023-01-20T18:04:03.300+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T18:04:03.373+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=2
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-20T18:04:03.374+0000] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-01-20T18:04:03.375+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master  --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-20T18:04:03.386+0000] {spark_submit.py:495} INFO - /usr/local/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit: line 27: /opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-class: No such file or directory
[2023-01-20T18:04:03.401+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master  --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 127.
[2023-01-20T18:04:03.405+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230120T180403, end_date=20230120T180403
[2023-01-20T18:04:03.416+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 90 for task transform_stage_generation (Cannot execute: spark-submit --master  --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 127.; 6702)
[2023-01-20T18:04:03.448+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T18:04:03.474+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T18:10:04.105+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T18:10:04.115+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T18:10:04.115+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T18:10:04.115+0000] {taskinstance.py:1284} INFO - Starting attempt 2 of 4
[2023-01-20T18:10:04.116+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T18:10:04.144+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-20T18:10:04.157+0000] {standard_task_runner.py:55} INFO - Started process 7117 to run task
[2023-01-20T18:10:04.162+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '97', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmplvfkxos5']
[2023-01-20T18:10:04.166+0000] {standard_task_runner.py:83} INFO - Job 97: Subtask transform_stage_generation
[2023-01-20T18:10:04.254+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T18:10:04.345+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=2
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-20T18:10:04.346+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 156, in execute
    self._hook = self._get_hook()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 165, in _get_hook
    return SparkSubmitHook(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 155, in __init__
    raise RuntimeError(
RuntimeError: The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/opt/spark-3.3.1-bin-hadoop3-scala2.13bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH
[2023-01-20T18:10:04.355+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230120T181004, end_date=20230120T181004
[2023-01-20T18:10:04.368+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 97 for task transform_stage_generation (The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/opt/spark-3.3.1-bin-hadoop3-scala2.13bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH; 7117)
[2023-01-20T18:10:04.413+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T18:10:04.431+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T18:23:57.776+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T18:23:57.792+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T18:23:57.792+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T18:23:57.792+0000] {taskinstance.py:1284} INFO - Starting attempt 2 of 4
[2023-01-20T18:23:57.792+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T18:23:57.807+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-20T18:23:57.814+0000] {standard_task_runner.py:55} INFO - Started process 8048 to run task
[2023-01-20T18:23:57.819+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '114', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpbalubv9e']
[2023-01-20T18:23:57.821+0000] {standard_task_runner.py:83} INFO - Job 114: Subtask transform_stage_generation
[2023-01-20T18:23:57.880+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T18:23:57.945+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=2
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-20T18:23:57.946+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 156, in execute
    self._hook = self._get_hook()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 165, in _get_hook
    return SparkSubmitHook(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 155, in __init__
    raise RuntimeError(
RuntimeError: The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH
[2023-01-20T18:23:57.953+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230120T182357, end_date=20230120T182357
[2023-01-20T18:23:57.963+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 114 for task transform_stage_generation (The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH; 8048)
[2023-01-20T18:23:57.990+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T18:23:58.005+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T19:20:59.065+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T19:20:59.091+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-20T19:20:59.092+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T19:20:59.092+0000] {taskinstance.py:1284} INFO - Starting attempt 2 of 4
[2023-01-20T19:20:59.092+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T19:20:59.126+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-20T19:20:59.142+0000] {standard_task_runner.py:55} INFO - Started process 660 to run task
[2023-01-20T19:20:59.157+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '131', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpygaxdfrm']
[2023-01-20T19:20:59.159+0000] {standard_task_runner.py:83} INFO - Job 131: Subtask transform_stage_generation
[2023-01-20T19:20:59.338+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 984f5901aea4
[2023-01-20T19:20:59.647+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=2
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-20T19:20:59.648+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 156, in execute
    self._hook = self._get_hook()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 165, in _get_hook
    return SparkSubmitHook(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 155, in __init__
    raise RuntimeError(
RuntimeError: The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/usr/local/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH
[2023-01-20T19:20:59.668+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230120T192059, end_date=20230120T192059
[2023-01-20T19:20:59.687+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 131 for task transform_stage_generation (The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/usr/local/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH; 660)
[2023-01-20T19:20:59.725+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T19:20:59.759+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T03:08:20.407+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T03:08:20.431+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T03:08:20.432+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T03:08:20.432+0000] {taskinstance.py:1284} INFO - Starting attempt 2 of 4
[2023-01-21T03:08:20.432+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T03:08:20.456+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-21T03:08:20.469+0000] {standard_task_runner.py:55} INFO - Started process 660 to run task
[2023-01-21T03:08:20.477+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '142', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmph8onnbu4']
[2023-01-21T03:08:20.481+0000] {standard_task_runner.py:83} INFO - Job 142: Subtask transform_stage_generation
[2023-01-21T03:08:20.600+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 2b3134e2b02d
[2023-01-21T03:08:20.736+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=2
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-21T03:08:20.739+0000] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-01-21T03:08:20.743+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master localhost --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-21T03:08:20.770+0000] {spark_submit.py:495} INFO - /usr/local/spark/bin/load-spark-env.sh: line 68: ps: command not found
[2023-01-21T03:08:28.047+0000] {spark_submit.py:495} INFO - 23/01/21 03:08:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-21T03:08:28.614+0000] {spark_submit.py:495} INFO - 23/01/21 03:08:28 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-21T03:08:28.771+0000] {spark_submit.py:495} INFO - Exception in thread "main" java.io.IOException: Cannot run program "/usr/bin/python3.10": error=2, No such file or directory
[2023-01-21T03:08:28.771+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1128)
[2023-01-21T03:08:28.771+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1071)
[2023-01-21T03:08:28.771+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.PythonRunner$.main(PythonRunner.scala:97)
[2023-01-21T03:08:28.772+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.PythonRunner.main(PythonRunner.scala)
[2023-01-21T03:08:28.772+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-21T03:08:28.772+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-21T03:08:28.772+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-21T03:08:28.772+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-21T03:08:28.773+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
[2023-01-21T03:08:28.773+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:958)
[2023-01-21T03:08:28.773+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
[2023-01-21T03:08:28.773+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
[2023-01-21T03:08:28.773+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
[2023-01-21T03:08:28.774+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)
[2023-01-21T03:08:28.774+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)
[2023-01-21T03:08:28.774+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
[2023-01-21T03:08:28.776+0000] {spark_submit.py:495} INFO - Caused by: java.io.IOException: error=2, No such file or directory
[2023-01-21T03:08:28.777+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessImpl.forkAndExec(Native Method)
[2023-01-21T03:08:28.777+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:340)
[2023-01-21T03:08:28.778+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessImpl.start(ProcessImpl.java:271)
[2023-01-21T03:08:28.779+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1107)
[2023-01-21T03:08:28.779+0000] {spark_submit.py:495} INFO - ... 15 more
[2023-01-21T03:08:28.805+0000] {spark_submit.py:495} INFO - 23/01/21 03:08:28 INFO ShutdownHookManager: Shutdown hook called
[2023-01-21T03:08:28.807+0000] {spark_submit.py:495} INFO - 23/01/21 03:08:28 INFO ShutdownHookManager: Deleting directory /tmp/spark-950fb3ad-18cb-400f-befb-1595e8828f24
[2023-01-21T03:08:28.949+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master localhost --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-21T03:08:28.956+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230121T030820, end_date=20230121T030828
[2023-01-21T03:08:28.983+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 142 for task transform_stage_generation (Cannot execute: spark-submit --master localhost --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 660)
[2023-01-21T03:08:29.029+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T03:08:29.089+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T07:52:38.828+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T07:52:38.888+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T07:52:38.888+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T07:52:38.889+0000] {taskinstance.py:1284} INFO - Starting attempt 2 of 4
[2023-01-21T07:52:38.889+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T07:52:38.947+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-21T07:52:38.986+0000] {standard_task_runner.py:55} INFO - Started process 479 to run task
[2023-01-21T07:52:39.033+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '165', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpirki0p9k']
[2023-01-21T07:52:39.048+0000] {standard_task_runner.py:83} INFO - Job 165: Subtask transform_stage_generation
[2023-01-21T07:52:39.403+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 4d52b49e46b4
[2023-01-21T07:52:39.715+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=2
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-21T07:52:39.719+0000] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-01-21T07:52:39.724+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master  --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-21T07:52:39.772+0000] {spark_submit.py:495} INFO - /usr/local/spark/bin/load-spark-env.sh: line 68: ps: command not found
[2023-01-21T07:52:52.630+0000] {spark_submit.py:495} INFO - Exception in thread "main" org.apache.spark.SparkException: Master must either be yarn or start with spark, mesos, k8s, or local
[2023-01-21T07:52:52.631+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.error(SparkSubmit.scala:975)
[2023-01-21T07:52:52.631+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:238)
[2023-01-21T07:52:52.632+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:901)
[2023-01-21T07:52:52.632+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
[2023-01-21T07:52:52.635+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
[2023-01-21T07:52:52.636+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
[2023-01-21T07:52:52.636+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)
[2023-01-21T07:52:52.637+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)
[2023-01-21T07:52:52.638+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
[2023-01-21T07:52:52.770+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master  --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-21T07:52:52.785+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230121T075238, end_date=20230121T075252
[2023-01-21T07:52:52.833+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 165 for task transform_stage_generation (Cannot execute: spark-submit --master  --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 479)
[2023-01-21T07:52:52.919+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T07:52:52.994+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T08:22:17.489+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T08:22:17.504+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T08:22:17.504+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T08:22:17.504+0000] {taskinstance.py:1284} INFO - Starting attempt 2 of 4
[2023-01-21T08:22:17.504+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T08:22:17.522+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-21T08:22:17.530+0000] {standard_task_runner.py:55} INFO - Started process 67 to run task
[2023-01-21T08:22:17.536+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '176', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp18zsm5mt']
[2023-01-21T08:22:17.540+0000] {standard_task_runner.py:83} INFO - Job 176: Subtask transform_stage_generation
[2023-01-21T08:22:17.616+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host d00464a30031
[2023-01-21T08:22:17.688+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=2
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-21T08:22:17.699+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-21T08:22:17.702+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-21T08:22:17.717+0000] {spark_submit.py:495} INFO - /usr/local/spark/bin/load-spark-env.sh: line 68: ps: command not found
[2023-01-21T08:22:22.891+0000] {spark_submit.py:495} INFO - 23/01/21 08:22:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-21T08:22:23.369+0000] {spark_submit.py:495} INFO - 23/01/21 08:22:23 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-21T08:22:23.555+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-21T08:22:23.556+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 2, in <module>
[2023-01-21T08:22:23.556+0000] {spark_submit.py:495} INFO - from dotenv import load_dotenv
[2023-01-21T08:22:23.556+0000] {spark_submit.py:495} INFO - ModuleNotFoundError: No module named 'dotenv'
[2023-01-21T08:22:23.570+0000] {spark_submit.py:495} INFO - 23/01/21 08:22:23 INFO ShutdownHookManager: Shutdown hook called
[2023-01-21T08:22:23.572+0000] {spark_submit.py:495} INFO - 23/01/21 08:22:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-dc78ba14-3f06-44bc-9133-b863a0f4bdd1
[2023-01-21T08:22:23.691+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-21T08:22:23.696+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230121T082217, end_date=20230121T082223
[2023-01-21T08:22:23.714+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 176 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 67)
[2023-01-21T08:22:23.756+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T08:22:23.785+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T08:29:01.903+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T08:29:01.910+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T08:29:01.910+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T08:29:01.911+0000] {taskinstance.py:1284} INFO - Starting attempt 2 of 4
[2023-01-21T08:29:01.911+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T08:29:01.920+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-21T08:29:01.926+0000] {standard_task_runner.py:55} INFO - Started process 728 to run task
[2023-01-21T08:29:01.929+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '182', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpdzoey8p4']
[2023-01-21T08:29:01.932+0000] {standard_task_runner.py:83} INFO - Job 182: Subtask transform_stage_generation
[2023-01-21T08:29:01.985+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host d00464a30031
[2023-01-21T08:29:02.045+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=2
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-21T08:29:02.052+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-21T08:29:02.053+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-21T08:29:02.062+0000] {spark_submit.py:495} INFO - /usr/local/spark/bin/load-spark-env.sh: line 68: ps: command not found
[2023-01-21T08:29:04.307+0000] {spark_submit.py:495} INFO - 23/01/21 08:29:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-21T08:29:04.569+0000] {spark_submit.py:495} INFO - 23/01/21 08:29:04 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-21T08:29:04.674+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-21T08:29:04.675+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 2, in <module>
[2023-01-21T08:29:04.675+0000] {spark_submit.py:495} INFO - from dotenv import load_dotenv
[2023-01-21T08:29:04.675+0000] {spark_submit.py:495} INFO - ModuleNotFoundError: No module named 'dotenv'
[2023-01-21T08:29:04.683+0000] {spark_submit.py:495} INFO - 23/01/21 08:29:04 INFO ShutdownHookManager: Shutdown hook called
[2023-01-21T08:29:04.684+0000] {spark_submit.py:495} INFO - 23/01/21 08:29:04 INFO ShutdownHookManager: Deleting directory /tmp/spark-e5b8f713-555f-4a49-8353-002a82c3fa55
[2023-01-21T08:29:04.735+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-21T08:29:04.738+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230121T082901, end_date=20230121T082904
[2023-01-21T08:29:04.748+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 182 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 728)
[2023-01-21T08:29:04.777+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T08:29:04.790+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T10:23:27.666+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T10:23:27.682+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T10:23:27.682+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T10:23:27.682+0000] {taskinstance.py:1284} INFO - Starting attempt 2 of 4
[2023-01-21T10:23:27.682+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T10:23:27.700+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-21T10:23:27.712+0000] {standard_task_runner.py:55} INFO - Started process 2635 to run task
[2023-01-21T10:23:27.718+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '195', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp4cvh653_']
[2023-01-21T10:23:27.724+0000] {standard_task_runner.py:83} INFO - Job 195: Subtask transform_stage_generation
[2023-01-21T10:23:27.843+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host b3a2c4a96689
[2023-01-21T10:23:27.919+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=2
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-21T10:23:27.929+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-21T10:23:27.931+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-21T10:23:31.638+0000] {spark_submit.py:495} INFO - 23/01/21 10:23:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-21T10:23:32.049+0000] {spark_submit.py:495} INFO - 23/01/21 10:23:32 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-21T10:23:32.162+0000] {spark_submit.py:495} INFO - Exception in thread "main" java.io.IOException: Cannot run program "/usr/bin/python3.10": error=2, No such file or directory
[2023-01-21T10:23:32.162+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1128)
[2023-01-21T10:23:32.163+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1071)
[2023-01-21T10:23:32.163+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.PythonRunner$.main(PythonRunner.scala:97)
[2023-01-21T10:23:32.163+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.PythonRunner.main(PythonRunner.scala)
[2023-01-21T10:23:32.164+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-21T10:23:32.164+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-21T10:23:32.164+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-21T10:23:32.165+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-21T10:23:32.165+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
[2023-01-21T10:23:32.165+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:958)
[2023-01-21T10:23:32.165+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
[2023-01-21T10:23:32.165+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
[2023-01-21T10:23:32.165+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
[2023-01-21T10:23:32.165+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)
[2023-01-21T10:23:32.165+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)
[2023-01-21T10:23:32.166+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
[2023-01-21T10:23:32.166+0000] {spark_submit.py:495} INFO - Caused by: java.io.IOException: error=2, No such file or directory
[2023-01-21T10:23:32.166+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessImpl.forkAndExec(Native Method)
[2023-01-21T10:23:32.166+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:340)
[2023-01-21T10:23:32.167+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessImpl.start(ProcessImpl.java:271)
[2023-01-21T10:23:32.167+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1107)
[2023-01-21T10:23:32.168+0000] {spark_submit.py:495} INFO - ... 15 more
[2023-01-21T10:23:32.181+0000] {spark_submit.py:495} INFO - 23/01/21 10:23:32 INFO ShutdownHookManager: Shutdown hook called
[2023-01-21T10:23:32.184+0000] {spark_submit.py:495} INFO - 23/01/21 10:23:32 INFO ShutdownHookManager: Deleting directory /tmp/spark-9147ff91-8425-477f-88d5-38e3b1527bc7
[2023-01-21T10:23:32.270+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-21T10:23:32.275+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230121T102327, end_date=20230121T102332
[2023-01-21T10:23:32.289+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 195 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 2635)
[2023-01-21T10:23:32.340+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T10:23:32.364+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T10:50:59.769+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T10:50:59.801+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T10:50:59.802+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T10:50:59.802+0000] {taskinstance.py:1284} INFO - Starting attempt 2 of 4
[2023-01-21T10:50:59.802+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T10:50:59.855+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-21T10:50:59.918+0000] {standard_task_runner.py:55} INFO - Started process 215 to run task
[2023-01-21T10:50:59.932+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '209', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp263u8cwd']
[2023-01-21T10:50:59.940+0000] {standard_task_runner.py:83} INFO - Job 209: Subtask transform_stage_generation
[2023-01-21T10:51:00.127+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host a0147a73c87a
[2023-01-21T10:51:00.321+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=2
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-21T10:51:00.347+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-21T10:51:00.351+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-21T10:51:11.599+0000] {spark_submit.py:495} INFO - 23/01/21 10:51:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-21T10:51:12.655+0000] {spark_submit.py:495} INFO - 23/01/21 10:51:12 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-21T10:51:13.088+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-21T10:51:13.089+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 2, in <module>
[2023-01-21T10:51:13.090+0000] {spark_submit.py:495} INFO - from dotenv import load_dotenv
[2023-01-21T10:51:13.090+0000] {spark_submit.py:495} INFO - ModuleNotFoundError: No module named 'dotenv'
[2023-01-21T10:51:13.126+0000] {spark_submit.py:495} INFO - 23/01/21 10:51:13 INFO ShutdownHookManager: Shutdown hook called
[2023-01-21T10:51:13.130+0000] {spark_submit.py:495} INFO - 23/01/21 10:51:13 INFO ShutdownHookManager: Deleting directory /tmp/spark-23c3d154-25d0-4200-9fb1-e9d377bf738b
[2023-01-21T10:51:13.292+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-21T10:51:13.305+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230121T105059, end_date=20230121T105113
[2023-01-21T10:51:13.351+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 209 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 215)
[2023-01-21T10:51:13.417+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T10:51:13.464+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T11:36:27.657+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T11:36:27.666+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T11:36:27.666+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T11:36:27.666+0000] {taskinstance.py:1284} INFO - Starting attempt 2 of 4
[2023-01-21T11:36:27.666+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T11:36:27.675+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-21T11:36:27.682+0000] {standard_task_runner.py:55} INFO - Started process 1778 to run task
[2023-01-21T11:36:27.685+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '224', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp5d2epi2f']
[2023-01-21T11:36:27.688+0000] {standard_task_runner.py:83} INFO - Job 224: Subtask transform_stage_generation
[2023-01-21T11:36:27.735+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host b7c4de7fae0b
[2023-01-21T11:36:27.783+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=2
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-21T11:36:27.790+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-21T11:36:27.791+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-21T11:36:31.054+0000] {spark_submit.py:495} INFO - 23/01/21 11:36:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-21T11:36:31.251+0000] {spark_submit.py:495} INFO - 23/01/21 11:36:31 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-21T11:36:31.334+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-21T11:36:31.334+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 2, in <module>
[2023-01-21T11:36:31.335+0000] {spark_submit.py:495} INFO - from dotenv import load_dotenv, find_dotenv
[2023-01-21T11:36:31.335+0000] {spark_submit.py:495} INFO - ModuleNotFoundError: No module named 'dotenv'
[2023-01-21T11:36:31.344+0000] {spark_submit.py:495} INFO - 23/01/21 11:36:31 INFO ShutdownHookManager: Shutdown hook called
[2023-01-21T11:36:31.346+0000] {spark_submit.py:495} INFO - 23/01/21 11:36:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-721817e1-6c71-4f9e-82ee-b7b61406d6eb
[2023-01-21T11:36:31.414+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-21T11:36:31.419+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230121T113627, end_date=20230121T113631
[2023-01-21T11:36:31.434+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 224 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 1778)
[2023-01-21T11:36:31.449+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T11:36:31.472+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T15:57:58.388+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T15:57:58.401+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T15:57:58.402+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T15:57:58.402+0000] {taskinstance.py:1284} INFO - Starting attempt 2 of 4
[2023-01-21T15:57:58.402+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T15:57:58.416+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-21T15:57:58.424+0000] {standard_task_runner.py:55} INFO - Started process 618 to run task
[2023-01-21T15:57:58.428+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '236', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp9j6ccm1i']
[2023-01-21T15:57:58.431+0000] {standard_task_runner.py:83} INFO - Job 236: Subtask transform_stage_generation
[2023-01-21T15:57:58.498+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 673f7b70a136
[2023-01-21T15:57:58.571+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=2
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-21T15:57:58.582+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-21T15:57:58.585+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-21T15:58:01.610+0000] {spark_submit.py:495} INFO - 23/01/21 15:58:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-21T15:58:01.843+0000] {spark_submit.py:495} INFO - 23/01/21 15:58:01 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-21T15:58:01.985+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-21T15:58:01.986+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 2, in <module>
[2023-01-21T15:58:01.986+0000] {spark_submit.py:495} INFO - from dotenv import load_dotenv, find_dotenv
[2023-01-21T15:58:01.987+0000] {spark_submit.py:495} INFO - ModuleNotFoundError: No module named 'dotenv'
[2023-01-21T15:58:02.005+0000] {spark_submit.py:495} INFO - 23/01/21 15:58:02 INFO ShutdownHookManager: Shutdown hook called
[2023-01-21T15:58:02.009+0000] {spark_submit.py:495} INFO - 23/01/21 15:58:02 INFO ShutdownHookManager: Deleting directory /tmp/spark-26b84bb1-497e-40fa-8aab-8dba5d0c7c40
[2023-01-21T15:58:02.091+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-21T15:58:02.096+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230121T155758, end_date=20230121T155802
[2023-01-21T15:58:02.117+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 236 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 618)
[2023-01-21T15:58:02.152+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T15:58:02.181+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T22:02:07.669+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T22:02:07.702+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T22:02:07.703+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T22:02:07.703+0000] {taskinstance.py:1284} INFO - Starting attempt 2 of 4
[2023-01-21T22:02:07.703+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T22:02:07.722+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-21T22:02:07.732+0000] {standard_task_runner.py:55} INFO - Started process 557 to run task
[2023-01-21T22:02:07.747+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '248', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpmhnxp4o1']
[2023-01-21T22:02:07.750+0000] {standard_task_runner.py:83} INFO - Job 248: Subtask transform_stage_generation
[2023-01-21T22:02:07.911+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 4260804338b2
[2023-01-21T22:02:08.121+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=2
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-21T22:02:08.158+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-21T22:02:08.164+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-21T22:13:07.710+0000] {spark_submit.py:495} INFO - 23/01/21 22:13:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-21T22:13:08.762+0000] {spark_submit.py:495} INFO - 23/01/21 22:13:08 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-21T22:13:09.134+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-21T22:13:09.135+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 2, in <module>
[2023-01-21T22:13:09.135+0000] {spark_submit.py:495} INFO - from dotenv import load_dotenv, find_dotenv
[2023-01-21T22:13:09.135+0000] {spark_submit.py:495} INFO - ModuleNotFoundError: No module named 'dotenv'
[2023-01-21T22:13:09.167+0000] {spark_submit.py:495} INFO - 23/01/21 22:13:09 INFO ShutdownHookManager: Shutdown hook called
[2023-01-21T22:13:09.168+0000] {spark_submit.py:495} INFO - 23/01/21 22:13:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-612ee540-8064-446c-b900-76f32ae09c50
[2023-01-21T22:13:09.259+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-21T22:13:09.270+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230121T220207, end_date=20230121T221309
[2023-01-21T22:13:09.291+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 248 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 557)
[2023-01-21T22:13:09.326+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T22:13:09.356+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T23:12:20.826+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T23:12:20.841+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-21T23:12:20.841+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T23:12:20.841+0000] {taskinstance.py:1284} INFO - Starting attempt 2 of 4
[2023-01-21T23:12:20.841+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T23:12:20.860+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-21T23:12:20.868+0000] {standard_task_runner.py:55} INFO - Started process 4719 to run task
[2023-01-21T23:12:20.872+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '258', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp30dgz8bj']
[2023-01-21T23:12:20.875+0000] {standard_task_runner.py:83} INFO - Job 258: Subtask transform_stage_generation
[2023-01-21T23:12:20.949+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 4260804338b2
[2023-01-21T23:12:21.021+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=2
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-21T23:12:21.032+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-21T23:12:21.034+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-21T23:12:24.539+0000] {spark_submit.py:495} INFO - 23/01/21 23:12:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-21T23:12:24.845+0000] {spark_submit.py:495} INFO - 23/01/21 23:12:24 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-21T23:12:24.932+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-21T23:12:24.932+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 2, in <module>
[2023-01-21T23:12:24.932+0000] {spark_submit.py:495} INFO - from dotenv import load_dotenv, find_dotenv
[2023-01-21T23:12:24.932+0000] {spark_submit.py:495} INFO - ModuleNotFoundError: No module named 'dotenv'
[2023-01-21T23:12:24.942+0000] {spark_submit.py:495} INFO - 23/01/21 23:12:24 INFO ShutdownHookManager: Shutdown hook called
[2023-01-21T23:12:24.943+0000] {spark_submit.py:495} INFO - 23/01/21 23:12:24 INFO ShutdownHookManager: Deleting directory /tmp/spark-48a4aa82-fac0-4581-ae0a-81a805f9ea12
[2023-01-21T23:12:25.000+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-21T23:12:25.003+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230121T231220, end_date=20230121T231225
[2023-01-21T23:12:25.019+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 258 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 4719)
[2023-01-21T23:12:25.039+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T23:12:25.059+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T00:16:40.928+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T00:16:40.942+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T00:16:40.942+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T00:16:40.942+0000] {taskinstance.py:1284} INFO - Starting attempt 2 of 4
[2023-01-22T00:16:40.942+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T00:16:40.958+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-22T00:16:40.970+0000] {standard_task_runner.py:55} INFO - Started process 831 to run task
[2023-01-22T00:16:40.974+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '271', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpagyxk_f8']
[2023-01-22T00:16:40.976+0000] {standard_task_runner.py:83} INFO - Job 271: Subtask transform_stage_generation
[2023-01-22T00:16:41.056+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 669043c9475c
[2023-01-22T00:16:41.140+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=2
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-22T00:16:41.151+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T00:16:41.153+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-22T00:16:45.001+0000] {spark_submit.py:495} INFO - 23/01/22 00:16:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T00:16:45.304+0000] {spark_submit.py:495} INFO - 23/01/22 00:16:45 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-22T00:16:45.722+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T00:16:45.722+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 64, in <module>
[2023-01-22T00:16:45.722+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T00:16:45.722+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 16, in main
[2023-01-22T00:16:45.722+0000] {spark_submit.py:495} INFO - start_label = start.strftime("%Y%m%d%H%M")
[2023-01-22T00:16:45.723+0000] {spark_submit.py:495} INFO - AttributeError: 'str' object has no attribute 'strftime'
[2023-01-22T00:16:45.746+0000] {spark_submit.py:495} INFO - 23/01/22 00:16:45 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T00:16:45.746+0000] {spark_submit.py:495} INFO - 23/01/22 00:16:45 INFO ShutdownHookManager: Deleting directory /tmp/spark-52beea80-fe0a-4026-b897-e871ded04d6c
[2023-01-22T00:16:45.816+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-22T00:16:45.819+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230122T001640, end_date=20230122T001645
[2023-01-22T00:16:45.834+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 271 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 831)
[2023-01-22T00:16:45.868+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T00:16:45.890+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T03:58:02.075+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T03:58:02.084+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T03:58:02.085+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T03:58:02.085+0000] {taskinstance.py:1284} INFO - Starting attempt 2 of 4
[2023-01-22T03:58:02.085+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T03:58:02.099+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-22T03:58:02.108+0000] {standard_task_runner.py:55} INFO - Started process 1284 to run task
[2023-01-22T03:58:02.112+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '283', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpi2yxc55k']
[2023-01-22T03:58:02.114+0000] {standard_task_runner.py:83} INFO - Job 283: Subtask transform_stage_generation
[2023-01-22T03:58:02.172+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host e3ae05c9a97e
[2023-01-22T03:58:02.285+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=2
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-22T03:58:02.295+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T03:58:02.297+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /opt/***/spark/resources/gcs-connector-hadoop3-latest.jar, /opt/***/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-22T03:58:06.618+0000] {spark_submit.py:495} INFO - 23/01/22 03:58:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T03:58:06.998+0000] {spark_submit.py:495} INFO - 23/01/22 03:58:06 WARN DependencyUtils: Local jar /opt/***/ /opt/***/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-22T03:58:07.480+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T03:58:07.481+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 64, in <module>
[2023-01-22T03:58:07.483+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T03:58:07.483+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 16, in main
[2023-01-22T03:58:07.484+0000] {spark_submit.py:495} INFO - start_label = start.strftime("%Y%m%d%H%M")
[2023-01-22T03:58:07.484+0000] {spark_submit.py:495} INFO - AttributeError: 'str' object has no attribute 'strftime'
[2023-01-22T03:58:07.516+0000] {spark_submit.py:495} INFO - 23/01/22 03:58:07 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T03:58:07.517+0000] {spark_submit.py:495} INFO - 23/01/22 03:58:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-be9d1f5e-1f49-4c28-bf12-383730cb66af
[2023-01-22T03:58:07.652+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /opt/***/spark/resources/gcs-connector-hadoop3-latest.jar, /opt/***/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-22T03:58:07.677+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230122T035802, end_date=20230122T035807
[2023-01-22T03:58:07.725+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 283 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /opt/***/spark/resources/gcs-connector-hadoop3-latest.jar, /opt/***/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 1284)
[2023-01-22T03:58:07.793+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T03:58:07.826+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T04:24:54.236+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T04:24:54.273+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T04:24:54.274+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T04:24:54.274+0000] {taskinstance.py:1284} INFO - Starting attempt 2 of 4
[2023-01-22T04:24:54.275+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T04:24:54.310+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-22T04:24:54.342+0000] {standard_task_runner.py:55} INFO - Started process 3402 to run task
[2023-01-22T04:24:54.355+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '292', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpby9gqvgx']
[2023-01-22T04:24:54.364+0000] {standard_task_runner.py:83} INFO - Job 292: Subtask transform_stage_generation
[2023-01-22T04:24:54.614+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host e3ae05c9a97e
[2023-01-22T04:24:54.820+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=2
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-22T04:24:54.847+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T04:24:54.851+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars spark/resources/gcs-connector-hadoop3-latest.jar, spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-22T04:25:06.193+0000] {spark_submit.py:495} INFO - 23/01/22 04:25:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T04:25:07.028+0000] {spark_submit.py:495} INFO - 23/01/22 04:25:07 WARN DependencyUtils: Local jar /opt/***/ spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-22T04:25:08.192+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T04:25:08.192+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 64, in <module>
[2023-01-22T04:25:08.193+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T04:25:08.193+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 16, in main
[2023-01-22T04:25:08.194+0000] {spark_submit.py:495} INFO - start_label = start.strftime("%Y%m%d%H%M")
[2023-01-22T04:25:08.194+0000] {spark_submit.py:495} INFO - AttributeError: 'str' object has no attribute 'strftime'
[2023-01-22T04:25:08.244+0000] {spark_submit.py:495} INFO - 23/01/22 04:25:08 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T04:25:08.247+0000] {spark_submit.py:495} INFO - 23/01/22 04:25:08 INFO ShutdownHookManager: Deleting directory /tmp/spark-290cabf8-4220-43fb-95a5-ac06416925c7
[2023-01-22T04:25:08.384+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars spark/resources/gcs-connector-hadoop3-latest.jar, spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-22T04:25:08.394+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230122T042454, end_date=20230122T042508
[2023-01-22T04:25:08.437+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 292 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars spark/resources/gcs-connector-hadoop3-latest.jar, spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 3402)
[2023-01-22T04:25:08.482+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T04:25:08.534+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T06:13:52.329+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T06:13:52.367+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T06:13:52.368+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T06:13:52.369+0000] {taskinstance.py:1284} INFO - Starting attempt 2 of 4
[2023-01-22T06:13:52.369+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T06:13:52.419+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-22T06:13:52.439+0000] {standard_task_runner.py:55} INFO - Started process 512 to run task
[2023-01-22T06:13:52.453+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '307', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp49079728']
[2023-01-22T06:13:52.466+0000] {standard_task_runner.py:83} INFO - Job 307: Subtask transform_stage_generation
[2023-01-22T06:13:52.661+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host d68405340607
[2023-01-22T06:13:52.839+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=2
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-22T06:13:52.862+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T06:13:52.865+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-22T06:13:52.884+0000] {spark_submit.py:495} INFO - /home/***/.local/bin/spark-submit: line 27: /opt/spark/bin/spark-class: No such file or directory
[2023-01-22T06:13:52.912+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 127.
[2023-01-22T06:13:52.921+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230122T061352, end_date=20230122T061352
[2023-01-22T06:13:52.965+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 307 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 127.; 512)
[2023-01-22T06:13:53.023+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T06:13:53.087+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T07:54:59.339+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T07:54:59.348+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T07:54:59.349+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T07:54:59.349+0000] {taskinstance.py:1284} INFO - Starting attempt 2 of 4
[2023-01-22T07:54:59.349+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T07:54:59.359+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-22T07:54:59.366+0000] {standard_task_runner.py:55} INFO - Started process 1548 to run task
[2023-01-22T07:54:59.369+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '326', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp95mstm3n']
[2023-01-22T07:54:59.372+0000] {standard_task_runner.py:83} INFO - Job 326: Subtask transform_stage_generation
[2023-01-22T07:54:59.458+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 16233a798013
[2023-01-22T07:54:59.540+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=2
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-22T07:54:59.550+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T07:54:59.552+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-22T07:55:03.319+0000] {spark_submit.py:495} INFO - 23/01/22 07:55:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T07:55:03.625+0000] {spark_submit.py:495} INFO - 23/01/22 07:55:03 WARN DependencyUtils: Local jar /opt/***/gcs-connector-hadoop3-latest.jar does not exist, skipping.
[2023-01-22T07:55:03.626+0000] {spark_submit.py:495} INFO - 23/01/22 07:55:03 WARN DependencyUtils: Local jar /opt/***/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-22T07:55:04.861+0000] {spark_submit.py:495} INFO - 23/01/22 07:55:04 INFO SparkContext: Running Spark version 3.3.1
[2023-01-22T07:55:04.893+0000] {spark_submit.py:495} INFO - 23/01/22 07:55:04 INFO ResourceUtils: ==============================================================
[2023-01-22T07:55:04.893+0000] {spark_submit.py:495} INFO - 23/01/22 07:55:04 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-22T07:55:04.894+0000] {spark_submit.py:495} INFO - 23/01/22 07:55:04 INFO ResourceUtils: ==============================================================
[2023-01-22T07:55:04.895+0000] {spark_submit.py:495} INFO - 23/01/22 07:55:04 INFO SparkContext: Submitted application: gcp_playground
[2023-01-22T07:55:04.925+0000] {spark_submit.py:495} INFO - 23/01/22 07:55:04 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-22T07:55:04.931+0000] {spark_submit.py:495} INFO - 23/01/22 07:55:04 INFO ResourceProfile: Limiting resource is cpu
[2023-01-22T07:55:04.932+0000] {spark_submit.py:495} INFO - 23/01/22 07:55:04 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-22T07:55:05.000+0000] {spark_submit.py:495} INFO - 23/01/22 07:55:05 INFO SecurityManager: Changing view acls to: ***
[2023-01-22T07:55:05.000+0000] {spark_submit.py:495} INFO - 23/01/22 07:55:05 INFO SecurityManager: Changing modify acls to: ***
[2023-01-22T07:55:05.001+0000] {spark_submit.py:495} INFO - 23/01/22 07:55:05 INFO SecurityManager: Changing view acls groups to:
[2023-01-22T07:55:05.002+0000] {spark_submit.py:495} INFO - 23/01/22 07:55:05 INFO SecurityManager: Changing modify acls groups to:
[2023-01-22T07:55:05.002+0000] {spark_submit.py:495} INFO - 23/01/22 07:55:05 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-22T07:55:05.331+0000] {spark_submit.py:495} INFO - 23/01/22 07:55:05 INFO Utils: Successfully started service 'sparkDriver' on port 34689.
[2023-01-22T07:55:05.372+0000] {spark_submit.py:495} INFO - 23/01/22 07:55:05 INFO SparkEnv: Registering MapOutputTracker
[2023-01-22T07:55:05.442+0000] {spark_submit.py:495} INFO - 23/01/22 07:55:05 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-22T07:55:05.504+0000] {spark_submit.py:495} INFO - 23/01/22 07:55:05 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-22T07:55:05.505+0000] {spark_submit.py:495} INFO - 23/01/22 07:55:05 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-22T07:55:05.511+0000] {spark_submit.py:495} INFO - 23/01/22 07:55:05 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-22T07:55:05.545+0000] {spark_submit.py:495} INFO - 23/01/22 07:55:05 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1fa72cd4-9b18-460b-ac6f-2fbe170a205e
[2023-01-22T07:55:05.568+0000] {spark_submit.py:495} INFO - 23/01/22 07:55:05 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-22T07:55:05.593+0000] {spark_submit.py:495} INFO - 23/01/22 07:55:05 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-22T07:55:05.921+0000] {spark_submit.py:495} INFO - 23/01/22 07:55:05 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-01-22T07:55:05.991+0000] {spark_submit.py:495} INFO - 23/01/22 07:55:05 INFO SparkContext: Added JAR /opt/spark/resources/gcs-connector-hadoop3-latest.jar at spark://16233a798013:34689/jars/gcs-connector-hadoop3-latest.jar with timestamp 1674374104855
[2023-01-22T07:55:05.992+0000] {spark_submit.py:495} INFO - 23/01/22 07:55:05 INFO SparkContext: Added JAR /opt/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://16233a798013:34689/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1674374104855
[2023-01-22T07:55:06.107+0000] {spark_submit.py:495} INFO - 23/01/22 07:55:06 INFO Executor: Starting executor ID driver on host 16233a798013
[2023-01-22T07:55:06.119+0000] {spark_submit.py:495} INFO - 23/01/22 07:55:06 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-22T07:55:06.140+0000] {spark_submit.py:495} INFO - 23/01/22 07:55:06 INFO Executor: Fetching spark://16233a798013:34689/jars/gcs-connector-hadoop3-latest.jar with timestamp 1674374104855
[2023-01-22T07:55:06.234+0000] {spark_submit.py:495} INFO - 23/01/22 07:55:06 INFO TransportClientFactory: Successfully created connection to 16233a798013/192.168.32.7:34689 after 53 ms (0 ms spent in bootstraps)
[2023-01-22T07:55:06.245+0000] {spark_submit.py:495} INFO - 23/01/22 07:55:06 INFO Utils: Fetching spark://16233a798013:34689/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-403c2eb7-95c6-4ad6-a46e-6f85d3a7dff8/userFiles-780f18a7-6af4-44fb-a524-e089c678f0dd/fetchFileTemp17583988596992603257.tmp
[2023-01-22T07:55:06.479+0000] {spark_submit.py:495} INFO - 23/01/22 07:55:06 INFO Executor: Adding file:/tmp/spark-403c2eb7-95c6-4ad6-a46e-6f85d3a7dff8/userFiles-780f18a7-6af4-44fb-a524-e089c678f0dd/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-22T07:55:06.479+0000] {spark_submit.py:495} INFO - 23/01/22 07:55:06 INFO Executor: Fetching spark://16233a798013:34689/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1674374104855
[2023-01-22T07:55:06.481+0000] {spark_submit.py:495} INFO - 23/01/22 07:55:06 INFO Utils: Fetching spark://16233a798013:34689/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-403c2eb7-95c6-4ad6-a46e-6f85d3a7dff8/userFiles-780f18a7-6af4-44fb-a524-e089c678f0dd/fetchFileTemp3339122695772320667.tmp
[2023-01-22T07:55:06.671+0000] {spark_submit.py:495} INFO - 23/01/22 07:55:06 INFO Executor: Adding file:/tmp/spark-403c2eb7-95c6-4ad6-a46e-6f85d3a7dff8/userFiles-780f18a7-6af4-44fb-a524-e089c678f0dd/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-22T07:55:06.682+0000] {spark_submit.py:495} INFO - 23/01/22 07:55:06 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44751.
[2023-01-22T07:55:06.682+0000] {spark_submit.py:495} INFO - 23/01/22 07:55:06 INFO NettyBlockTransferService: Server created on 16233a798013:44751
[2023-01-22T07:55:06.685+0000] {spark_submit.py:495} INFO - 23/01/22 07:55:06 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-22T07:55:06.698+0000] {spark_submit.py:495} INFO - 23/01/22 07:55:06 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 16233a798013, 44751, None)
[2023-01-22T07:55:06.703+0000] {spark_submit.py:495} INFO - 23/01/22 07:55:06 INFO BlockManagerMasterEndpoint: Registering block manager 16233a798013:44751 with 434.4 MiB RAM, BlockManagerId(driver, 16233a798013, 44751, None)
[2023-01-22T07:55:06.708+0000] {spark_submit.py:495} INFO - 23/01/22 07:55:06 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 16233a798013, 44751, None)
[2023-01-22T07:55:06.710+0000] {spark_submit.py:495} INFO - 23/01/22 07:55:06 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 16233a798013, 44751, None)
[2023-01-22T07:55:07.416+0000] {spark_submit.py:495} INFO - 23/01/22 07:55:07 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-22T07:55:07.425+0000] {spark_submit.py:495} INFO - 23/01/22 07:55:07 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-22T07:55:08.872+0000] {spark_submit.py:495} INFO - 23/01/22 07:55:08 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: gs://entsoe_analytics_1009//opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010000.json.
[2023-01-22T07:55:08.872+0000] {spark_submit.py:495} INFO - org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "gs"
[2023-01-22T07:55:08.872+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2023-01-22T07:55:08.873+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T07:55:08.873+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T07:55:08.873+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T07:55:08.873+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T07:55:08.874+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T07:55:08.874+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T07:55:08.874+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)
[2023-01-22T07:55:08.875+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:370)
[2023-01-22T07:55:08.875+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T07:55:08.875+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T07:55:08.875+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T07:55:08.876+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T07:55:08.876+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T07:55:08.876+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T07:55:08.877+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T07:55:08.877+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T07:55:08.877+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T07:55:08.877+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T07:55:08.878+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T07:55:08.878+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T07:55:08.878+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T07:55:08.878+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T07:55:08.879+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T07:55:08.879+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T07:55:08.879+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T07:55:08.902+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T07:55:08.903+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 67, in <module>
[2023-01-22T07:55:08.903+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T07:55:08.903+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 54, in main
[2023-01-22T07:55:08.904+0000] {spark_submit.py:495} INFO - df_spark.write.format("bigquery").option("project", "rafzul-analytics-1009") \
[2023-01-22T07:55:08.904+0000] {spark_submit.py:495} INFO - UnboundLocalError: local variable 'df_spark' referenced before assignment
[2023-01-22T07:55:08.970+0000] {spark_submit.py:495} INFO - 23/01/22 07:55:08 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-22T07:55:08.996+0000] {spark_submit.py:495} INFO - 23/01/22 07:55:08 INFO SparkUI: Stopped Spark web UI at http://16233a798013:4040
[2023-01-22T07:55:09.024+0000] {spark_submit.py:495} INFO - 23/01/22 07:55:09 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-22T07:55:09.056+0000] {spark_submit.py:495} INFO - 23/01/22 07:55:09 INFO MemoryStore: MemoryStore cleared
[2023-01-22T07:55:09.057+0000] {spark_submit.py:495} INFO - 23/01/22 07:55:09 INFO BlockManager: BlockManager stopped
[2023-01-22T07:55:09.069+0000] {spark_submit.py:495} INFO - 23/01/22 07:55:09 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-22T07:55:09.076+0000] {spark_submit.py:495} INFO - 23/01/22 07:55:09 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-22T07:55:09.108+0000] {spark_submit.py:495} INFO - 23/01/22 07:55:09 INFO SparkContext: Successfully stopped SparkContext
[2023-01-22T07:55:09.108+0000] {spark_submit.py:495} INFO - 23/01/22 07:55:09 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T07:55:09.109+0000] {spark_submit.py:495} INFO - 23/01/22 07:55:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-403c2eb7-95c6-4ad6-a46e-6f85d3a7dff8
[2023-01-22T07:55:09.118+0000] {spark_submit.py:495} INFO - 23/01/22 07:55:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-9506dfba-ff60-4a23-bbc2-1fcbef9b223b
[2023-01-22T07:55:09.127+0000] {spark_submit.py:495} INFO - 23/01/22 07:55:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-403c2eb7-95c6-4ad6-a46e-6f85d3a7dff8/pyspark-813d5856-8cf8-4250-9631-f85b296fd011
[2023-01-22T07:55:09.264+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-22T07:55:09.270+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230122T075459, end_date=20230122T075509
[2023-01-22T07:55:09.288+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 326 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 1548)
[2023-01-22T07:55:09.316+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T07:55:09.340+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T08:03:40.597+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T08:03:40.606+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T08:03:40.606+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T08:03:40.606+0000] {taskinstance.py:1284} INFO - Starting attempt 2 of 4
[2023-01-22T08:03:40.606+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T08:03:40.617+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-22T08:03:40.626+0000] {standard_task_runner.py:55} INFO - Started process 2614 to run task
[2023-01-22T08:03:40.630+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '332', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpw0y9q3ae']
[2023-01-22T08:03:40.633+0000] {standard_task_runner.py:83} INFO - Job 332: Subtask transform_stage_generation
[2023-01-22T08:03:40.706+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 16233a798013
[2023-01-22T08:03:40.787+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=2
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-22T08:03:40.796+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T08:03:40.798+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-22T08:03:45.725+0000] {spark_submit.py:495} INFO - 23/01/22 08:03:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T08:03:46.171+0000] {spark_submit.py:495} INFO - 23/01/22 08:03:46 WARN DependencyUtils: Local jar /opt/***/gcs-connector-hadoop3-latest.jar does not exist, skipping.
[2023-01-22T08:03:46.172+0000] {spark_submit.py:495} INFO - 23/01/22 08:03:46 WARN DependencyUtils: Local jar /opt/***/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-22T08:03:47.941+0000] {spark_submit.py:495} INFO - /opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010000.json
[2023-01-22T08:03:48.178+0000] {spark_submit.py:495} INFO - 23/01/22 08:03:48 INFO SparkContext: Running Spark version 3.3.1
[2023-01-22T08:03:48.240+0000] {spark_submit.py:495} INFO - 23/01/22 08:03:48 INFO ResourceUtils: ==============================================================
[2023-01-22T08:03:48.240+0000] {spark_submit.py:495} INFO - 23/01/22 08:03:48 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-22T08:03:48.241+0000] {spark_submit.py:495} INFO - 23/01/22 08:03:48 INFO ResourceUtils: ==============================================================
[2023-01-22T08:03:48.242+0000] {spark_submit.py:495} INFO - 23/01/22 08:03:48 INFO SparkContext: Submitted application: gcp_playground
[2023-01-22T08:03:48.284+0000] {spark_submit.py:495} INFO - 23/01/22 08:03:48 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-22T08:03:48.295+0000] {spark_submit.py:495} INFO - 23/01/22 08:03:48 INFO ResourceProfile: Limiting resource is cpu
[2023-01-22T08:03:48.298+0000] {spark_submit.py:495} INFO - 23/01/22 08:03:48 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-22T08:03:48.430+0000] {spark_submit.py:495} INFO - 23/01/22 08:03:48 INFO SecurityManager: Changing view acls to: ***
[2023-01-22T08:03:48.431+0000] {spark_submit.py:495} INFO - 23/01/22 08:03:48 INFO SecurityManager: Changing modify acls to: ***
[2023-01-22T08:03:48.433+0000] {spark_submit.py:495} INFO - 23/01/22 08:03:48 INFO SecurityManager: Changing view acls groups to:
[2023-01-22T08:03:48.434+0000] {spark_submit.py:495} INFO - 23/01/22 08:03:48 INFO SecurityManager: Changing modify acls groups to:
[2023-01-22T08:03:48.436+0000] {spark_submit.py:495} INFO - 23/01/22 08:03:48 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-22T08:03:49.238+0000] {spark_submit.py:495} INFO - 23/01/22 08:03:49 INFO Utils: Successfully started service 'sparkDriver' on port 44569.
[2023-01-22T08:03:49.307+0000] {spark_submit.py:495} INFO - 23/01/22 08:03:49 INFO SparkEnv: Registering MapOutputTracker
[2023-01-22T08:03:49.404+0000] {spark_submit.py:495} INFO - 23/01/22 08:03:49 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-22T08:03:49.460+0000] {spark_submit.py:495} INFO - 23/01/22 08:03:49 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-22T08:03:49.462+0000] {spark_submit.py:495} INFO - 23/01/22 08:03:49 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-22T08:03:49.468+0000] {spark_submit.py:495} INFO - 23/01/22 08:03:49 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-22T08:03:49.507+0000] {spark_submit.py:495} INFO - 23/01/22 08:03:49 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-4cd876c2-0b65-47af-b3bb-d33592ce5f41
[2023-01-22T08:03:49.539+0000] {spark_submit.py:495} INFO - 23/01/22 08:03:49 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-22T08:03:49.570+0000] {spark_submit.py:495} INFO - 23/01/22 08:03:49 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-22T08:03:50.062+0000] {spark_submit.py:495} INFO - 23/01/22 08:03:50 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-01-22T08:03:50.150+0000] {spark_submit.py:495} INFO - 23/01/22 08:03:50 INFO SparkContext: Added JAR /opt/spark/resources/gcs-connector-hadoop3-latest.jar at spark://16233a798013:44569/jars/gcs-connector-hadoop3-latest.jar with timestamp 1674374628160
[2023-01-22T08:03:50.151+0000] {spark_submit.py:495} INFO - 23/01/22 08:03:50 INFO SparkContext: Added JAR /opt/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://16233a798013:44569/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1674374628160
[2023-01-22T08:03:50.353+0000] {spark_submit.py:495} INFO - 23/01/22 08:03:50 INFO Executor: Starting executor ID driver on host 16233a798013
[2023-01-22T08:03:50.371+0000] {spark_submit.py:495} INFO - 23/01/22 08:03:50 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-22T08:03:50.421+0000] {spark_submit.py:495} INFO - 23/01/22 08:03:50 INFO Executor: Fetching spark://16233a798013:44569/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1674374628160
[2023-01-22T08:03:50.586+0000] {spark_submit.py:495} INFO - 23/01/22 08:03:50 INFO TransportClientFactory: Successfully created connection to 16233a798013/192.168.32.7:44569 after 59 ms (0 ms spent in bootstraps)
[2023-01-22T08:03:50.603+0000] {spark_submit.py:495} INFO - 23/01/22 08:03:50 INFO Utils: Fetching spark://16233a798013:44569/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-2f0d85a5-cf9e-469c-8c23-20f6323d2be4/userFiles-6f223ecc-59d9-4ff8-b51a-01165c38861a/fetchFileTemp10258298167370475541.tmp
[2023-01-22T08:03:51.170+0000] {spark_submit.py:495} INFO - 23/01/22 08:03:51 INFO Executor: Adding file:/tmp/spark-2f0d85a5-cf9e-469c-8c23-20f6323d2be4/userFiles-6f223ecc-59d9-4ff8-b51a-01165c38861a/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-22T08:03:51.171+0000] {spark_submit.py:495} INFO - 23/01/22 08:03:51 INFO Executor: Fetching spark://16233a798013:44569/jars/gcs-connector-hadoop3-latest.jar with timestamp 1674374628160
[2023-01-22T08:03:51.172+0000] {spark_submit.py:495} INFO - 23/01/22 08:03:51 INFO Utils: Fetching spark://16233a798013:44569/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-2f0d85a5-cf9e-469c-8c23-20f6323d2be4/userFiles-6f223ecc-59d9-4ff8-b51a-01165c38861a/fetchFileTemp934039865707113856.tmp
[2023-01-22T08:03:51.646+0000] {spark_submit.py:495} INFO - 23/01/22 08:03:51 INFO Executor: Adding file:/tmp/spark-2f0d85a5-cf9e-469c-8c23-20f6323d2be4/userFiles-6f223ecc-59d9-4ff8-b51a-01165c38861a/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-22T08:03:51.678+0000] {spark_submit.py:495} INFO - 23/01/22 08:03:51 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46791.
[2023-01-22T08:03:51.679+0000] {spark_submit.py:495} INFO - 23/01/22 08:03:51 INFO NettyBlockTransferService: Server created on 16233a798013:46791
[2023-01-22T08:03:51.683+0000] {spark_submit.py:495} INFO - 23/01/22 08:03:51 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-22T08:03:51.705+0000] {spark_submit.py:495} INFO - 23/01/22 08:03:51 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 16233a798013, 46791, None)
[2023-01-22T08:03:51.717+0000] {spark_submit.py:495} INFO - 23/01/22 08:03:51 INFO BlockManagerMasterEndpoint: Registering block manager 16233a798013:46791 with 434.4 MiB RAM, BlockManagerId(driver, 16233a798013, 46791, None)
[2023-01-22T08:03:51.725+0000] {spark_submit.py:495} INFO - 23/01/22 08:03:51 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 16233a798013, 46791, None)
[2023-01-22T08:03:51.728+0000] {spark_submit.py:495} INFO - 23/01/22 08:03:51 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 16233a798013, 46791, None)
[2023-01-22T08:03:52.901+0000] {spark_submit.py:495} INFO - 23/01/22 08:03:52 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-22T08:03:52.908+0000] {spark_submit.py:495} INFO - 23/01/22 08:03:52 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-22T08:03:55.029+0000] {spark_submit.py:495} INFO - 23/01/22 08:03:55 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: gs://entsoe_analytics_1009//opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010000.json.
[2023-01-22T08:03:55.029+0000] {spark_submit.py:495} INFO - org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "gs"
[2023-01-22T08:03:55.030+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2023-01-22T08:03:55.030+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T08:03:55.030+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T08:03:55.030+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T08:03:55.031+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T08:03:55.031+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T08:03:55.031+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T08:03:55.031+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)
[2023-01-22T08:03:55.031+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:370)
[2023-01-22T08:03:55.032+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T08:03:55.032+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T08:03:55.032+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T08:03:55.032+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T08:03:55.032+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T08:03:55.033+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T08:03:55.033+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T08:03:55.033+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T08:03:55.033+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T08:03:55.034+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T08:03:55.034+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T08:03:55.034+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T08:03:55.035+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T08:03:55.035+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T08:03:55.035+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T08:03:55.035+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T08:03:55.036+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T08:03:55.105+0000] {spark_submit.py:495} INFO - Error, ada An error occurred while calling o38.load.
[2023-01-22T08:03:55.106+0000] {spark_submit.py:495} INFO - : org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "gs"
[2023-01-22T08:03:55.106+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2023-01-22T08:03:55.106+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T08:03:55.106+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T08:03:55.107+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T08:03:55.107+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T08:03:55.107+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T08:03:55.107+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T08:03:55.107+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:752)
[2023-01-22T08:03:55.108+0000] {spark_submit.py:495} INFO - at scala.collection.immutable.List.map(List.scala:246)
[2023-01-22T08:03:55.108+0000] {spark_submit.py:495} INFO - at scala.collection.immutable.List.map(List.scala:79)
[2023-01-22T08:03:55.108+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:750)
[2023-01-22T08:03:55.108+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:579)
[2023-01-22T08:03:55.108+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:408)
[2023-01-22T08:03:55.109+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T08:03:55.109+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T08:03:55.109+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T08:03:55.109+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T08:03:55.109+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T08:03:55.110+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T08:03:55.110+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T08:03:55.110+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T08:03:55.114+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T08:03:55.114+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T08:03:55.114+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T08:03:55.115+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T08:03:55.115+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T08:03:55.115+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T08:03:55.115+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T08:03:55.116+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T08:03:55.116+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T08:03:55.116+0000] {spark_submit.py:495} INFO - 
[2023-01-22T08:03:55.117+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T08:03:55.117+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 69, in <module>
[2023-01-22T08:03:55.117+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T08:03:55.117+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 56, in main
[2023-01-22T08:03:55.118+0000] {spark_submit.py:495} INFO - df_spark.write.format("bigquery").option("project", "rafzul-analytics-1009") \
[2023-01-22T08:03:55.118+0000] {spark_submit.py:495} INFO - UnboundLocalError: local variable 'df_spark' referenced before assignment
[2023-01-22T08:03:55.231+0000] {spark_submit.py:495} INFO - 23/01/22 08:03:55 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-22T08:03:55.257+0000] {spark_submit.py:495} INFO - 23/01/22 08:03:55 INFO SparkUI: Stopped Spark web UI at http://16233a798013:4040
[2023-01-22T08:03:55.290+0000] {spark_submit.py:495} INFO - 23/01/22 08:03:55 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-22T08:03:55.315+0000] {spark_submit.py:495} INFO - 23/01/22 08:03:55 INFO MemoryStore: MemoryStore cleared
[2023-01-22T08:03:55.316+0000] {spark_submit.py:495} INFO - 23/01/22 08:03:55 INFO BlockManager: BlockManager stopped
[2023-01-22T08:03:55.333+0000] {spark_submit.py:495} INFO - 23/01/22 08:03:55 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-22T08:03:55.339+0000] {spark_submit.py:495} INFO - 23/01/22 08:03:55 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-22T08:03:55.365+0000] {spark_submit.py:495} INFO - 23/01/22 08:03:55 INFO SparkContext: Successfully stopped SparkContext
[2023-01-22T08:03:55.367+0000] {spark_submit.py:495} INFO - 23/01/22 08:03:55 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T08:03:55.369+0000] {spark_submit.py:495} INFO - 23/01/22 08:03:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-2f0d85a5-cf9e-469c-8c23-20f6323d2be4/pyspark-3cea11b4-aa65-4c7b-990e-7fd292da406c
[2023-01-22T08:03:55.381+0000] {spark_submit.py:495} INFO - 23/01/22 08:03:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-2f0d85a5-cf9e-469c-8c23-20f6323d2be4
[2023-01-22T08:03:55.396+0000] {spark_submit.py:495} INFO - 23/01/22 08:03:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-c4e51583-7c1d-4ef5-b9a5-c96d1aa0e5b2
[2023-01-22T08:03:55.570+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-22T08:03:55.575+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230122T080340, end_date=20230122T080355
[2023-01-22T08:03:55.593+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 332 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 2614)
[2023-01-22T08:03:55.616+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T08:03:55.664+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T08:43:54.524+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T08:43:54.546+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-22T08:43:54.546+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T08:43:54.547+0000] {taskinstance.py:1284} INFO - Starting attempt 2 of 4
[2023-01-22T08:43:54.547+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T08:43:54.573+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 00:00:00+00:00
[2023-01-22T08:43:54.591+0000] {standard_task_runner.py:55} INFO - Started process 923 to run task
[2023-01-22T08:43:54.599+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '376', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpid76qjr3']
[2023-01-22T08:43:54.605+0000] {standard_task_runner.py:83} INFO - Job 376: Subtask transform_stage_generation
[2023-01-22T08:43:54.755+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 9d8db2d97423
[2023-01-22T08:43:54.941+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=2
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-22T08:43:54.964+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T08:43:54.970+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-22T08:44:04.701+0000] {spark_submit.py:495} INFO - 23/01/22 08:44:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T08:44:05.993+0000] {spark_submit.py:495} INFO - 23/01/22 08:44:05 WARN DependencyUtils: Local jar /opt/***/gcs-connector-hadoop3-latest.jar does not exist, skipping.
[2023-01-22T08:44:05.996+0000] {spark_submit.py:495} INFO - 23/01/22 08:44:05 WARN DependencyUtils: Local jar /opt/***/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-22T08:44:08.248+0000] {spark_submit.py:495} INFO - /opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010000.json
[2023-01-22T08:44:08.426+0000] {spark_submit.py:495} INFO - 23/01/22 08:44:08 INFO SparkContext: Running Spark version 3.3.1
[2023-01-22T08:44:08.482+0000] {spark_submit.py:495} INFO - 23/01/22 08:44:08 INFO ResourceUtils: ==============================================================
[2023-01-22T08:44:08.483+0000] {spark_submit.py:495} INFO - 23/01/22 08:44:08 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-22T08:44:08.484+0000] {spark_submit.py:495} INFO - 23/01/22 08:44:08 INFO ResourceUtils: ==============================================================
[2023-01-22T08:44:08.485+0000] {spark_submit.py:495} INFO - 23/01/22 08:44:08 INFO SparkContext: Submitted application: gcp_playground
[2023-01-22T08:44:08.537+0000] {spark_submit.py:495} INFO - 23/01/22 08:44:08 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-22T08:44:08.547+0000] {spark_submit.py:495} INFO - 23/01/22 08:44:08 INFO ResourceProfile: Limiting resource is cpu
[2023-01-22T08:44:08.549+0000] {spark_submit.py:495} INFO - 23/01/22 08:44:08 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-22T08:44:08.646+0000] {spark_submit.py:495} INFO - 23/01/22 08:44:08 INFO SecurityManager: Changing view acls to: ***
[2023-01-22T08:44:08.647+0000] {spark_submit.py:495} INFO - 23/01/22 08:44:08 INFO SecurityManager: Changing modify acls to: ***
[2023-01-22T08:44:08.647+0000] {spark_submit.py:495} INFO - 23/01/22 08:44:08 INFO SecurityManager: Changing view acls groups to:
[2023-01-22T08:44:08.648+0000] {spark_submit.py:495} INFO - 23/01/22 08:44:08 INFO SecurityManager: Changing modify acls groups to:
[2023-01-22T08:44:08.649+0000] {spark_submit.py:495} INFO - 23/01/22 08:44:08 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-22T08:44:09.325+0000] {spark_submit.py:495} INFO - 23/01/22 08:44:09 INFO Utils: Successfully started service 'sparkDriver' on port 36525.
[2023-01-22T08:44:09.419+0000] {spark_submit.py:495} INFO - 23/01/22 08:44:09 INFO SparkEnv: Registering MapOutputTracker
[2023-01-22T08:44:09.518+0000] {spark_submit.py:495} INFO - 23/01/22 08:44:09 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-22T08:44:09.587+0000] {spark_submit.py:495} INFO - 23/01/22 08:44:09 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-22T08:44:09.588+0000] {spark_submit.py:495} INFO - 23/01/22 08:44:09 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-22T08:44:09.596+0000] {spark_submit.py:495} INFO - 23/01/22 08:44:09 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-22T08:44:09.643+0000] {spark_submit.py:495} INFO - 23/01/22 08:44:09 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-febb779d-9c03-416e-8d15-7b0940daf697
[2023-01-22T08:44:09.675+0000] {spark_submit.py:495} INFO - 23/01/22 08:44:09 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-22T08:44:09.711+0000] {spark_submit.py:495} INFO - 23/01/22 08:44:09 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-22T08:44:10.273+0000] {spark_submit.py:495} INFO - 23/01/22 08:44:10 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-22T08:44:10.288+0000] {spark_submit.py:495} INFO - 23/01/22 08:44:10 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2023-01-22T08:44:10.355+0000] {spark_submit.py:495} INFO - 23/01/22 08:44:10 ERROR SparkContext: Failed to add file:/opt/***/gcs-connector-hadoop3-latest.jar to Spark environment
[2023-01-22T08:44:10.356+0000] {spark_submit.py:495} INFO - java.io.FileNotFoundException: Jar /opt/***/gcs-connector-hadoop3-latest.jar not found
[2023-01-22T08:44:10.356+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1949)
[2023-01-22T08:44:10.356+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.addJar(SparkContext.scala:2004)
[2023-01-22T08:44:10.357+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:507)
[2023-01-22T08:44:10.357+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:507)
[2023-01-22T08:44:10.357+0000] {spark_submit.py:495} INFO - at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
[2023-01-22T08:44:10.357+0000] {spark_submit.py:495} INFO - at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
[2023-01-22T08:44:10.358+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
[2023-01-22T08:44:10.358+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.<init>(SparkContext.scala:507)
[2023-01-22T08:44:10.358+0000] {spark_submit.py:495} INFO - at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
[2023-01-22T08:44:10.359+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
[2023-01-22T08:44:10.359+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
[2023-01-22T08:44:10.359+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
[2023-01-22T08:44:10.359+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
[2023-01-22T08:44:10.360+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
[2023-01-22T08:44:10.360+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T08:44:10.360+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:238)
[2023-01-22T08:44:10.360+0000] {spark_submit.py:495} INFO - at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
[2023-01-22T08:44:10.361+0000] {spark_submit.py:495} INFO - at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
[2023-01-22T08:44:10.361+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T08:44:10.361+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T08:44:10.362+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T08:44:10.362+0000] {spark_submit.py:495} INFO - 23/01/22 08:44:10 ERROR SparkContext: Failed to add file:/opt/***/spark-bigquery-with-dependencies_2.13-0.27.1.jar to Spark environment
[2023-01-22T08:44:10.362+0000] {spark_submit.py:495} INFO - java.io.FileNotFoundException: Jar /opt/***/spark-bigquery-with-dependencies_2.13-0.27.1.jar not found
[2023-01-22T08:44:10.362+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1949)
[2023-01-22T08:44:10.363+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.addJar(SparkContext.scala:2004)
[2023-01-22T08:44:10.363+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:507)
[2023-01-22T08:44:10.363+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:507)
[2023-01-22T08:44:10.363+0000] {spark_submit.py:495} INFO - at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
[2023-01-22T08:44:10.364+0000] {spark_submit.py:495} INFO - at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
[2023-01-22T08:44:10.364+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
[2023-01-22T08:44:10.364+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.<init>(SparkContext.scala:507)
[2023-01-22T08:44:10.365+0000] {spark_submit.py:495} INFO - at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
[2023-01-22T08:44:10.365+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
[2023-01-22T08:44:10.365+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
[2023-01-22T08:44:10.365+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
[2023-01-22T08:44:10.366+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
[2023-01-22T08:44:10.366+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
[2023-01-22T08:44:10.366+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T08:44:10.367+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:238)
[2023-01-22T08:44:10.367+0000] {spark_submit.py:495} INFO - at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
[2023-01-22T08:44:10.367+0000] {spark_submit.py:495} INFO - at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
[2023-01-22T08:44:10.367+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T08:44:10.368+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T08:44:10.368+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T08:44:10.490+0000] {spark_submit.py:495} INFO - 23/01/22 08:44:10 INFO Executor: Starting executor ID driver on host 9d8db2d97423
[2023-01-22T08:44:10.502+0000] {spark_submit.py:495} INFO - 23/01/22 08:44:10 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-22T08:44:10.561+0000] {spark_submit.py:495} INFO - 23/01/22 08:44:10 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40629.
[2023-01-22T08:44:10.561+0000] {spark_submit.py:495} INFO - 23/01/22 08:44:10 INFO NettyBlockTransferService: Server created on 9d8db2d97423:40629
[2023-01-22T08:44:10.564+0000] {spark_submit.py:495} INFO - 23/01/22 08:44:10 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-22T08:44:10.581+0000] {spark_submit.py:495} INFO - 23/01/22 08:44:10 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 9d8db2d97423, 40629, None)
[2023-01-22T08:44:10.596+0000] {spark_submit.py:495} INFO - 23/01/22 08:44:10 INFO BlockManagerMasterEndpoint: Registering block manager 9d8db2d97423:40629 with 434.4 MiB RAM, BlockManagerId(driver, 9d8db2d97423, 40629, None)
[2023-01-22T08:44:10.605+0000] {spark_submit.py:495} INFO - 23/01/22 08:44:10 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 9d8db2d97423, 40629, None)
[2023-01-22T08:44:10.608+0000] {spark_submit.py:495} INFO - 23/01/22 08:44:10 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 9d8db2d97423, 40629, None)
[2023-01-22T08:44:11.526+0000] {spark_submit.py:495} INFO - 23/01/22 08:44:11 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-22T08:44:11.532+0000] {spark_submit.py:495} INFO - 23/01/22 08:44:11 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-22T08:44:12.851+0000] {spark_submit.py:495} INFO - 23/01/22 08:44:12 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: gs://entsoe_analytics_1009//opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010000.json.
[2023-01-22T08:44:12.852+0000] {spark_submit.py:495} INFO - org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "gs"
[2023-01-22T08:44:12.852+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2023-01-22T08:44:12.852+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T08:44:12.852+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T08:44:12.852+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T08:44:12.853+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T08:44:12.853+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T08:44:12.853+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T08:44:12.853+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)
[2023-01-22T08:44:12.853+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:370)
[2023-01-22T08:44:12.853+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T08:44:12.854+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T08:44:12.854+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T08:44:12.854+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T08:44:12.854+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T08:44:12.854+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T08:44:12.854+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T08:44:12.854+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T08:44:12.855+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T08:44:12.855+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T08:44:12.855+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T08:44:12.855+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T08:44:12.855+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T08:44:12.856+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T08:44:12.856+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T08:44:12.856+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T08:44:12.856+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T08:44:12.903+0000] {spark_submit.py:495} INFO - Error, ada An error occurred while calling o37.load.
[2023-01-22T08:44:12.903+0000] {spark_submit.py:495} INFO - : org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "gs"
[2023-01-22T08:44:12.904+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2023-01-22T08:44:12.904+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T08:44:12.904+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T08:44:12.904+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T08:44:12.905+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T08:44:12.905+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T08:44:12.905+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T08:44:12.905+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:752)
[2023-01-22T08:44:12.905+0000] {spark_submit.py:495} INFO - at scala.collection.immutable.List.map(List.scala:246)
[2023-01-22T08:44:12.906+0000] {spark_submit.py:495} INFO - at scala.collection.immutable.List.map(List.scala:79)
[2023-01-22T08:44:12.906+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:750)
[2023-01-22T08:44:12.906+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:579)
[2023-01-22T08:44:12.906+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:408)
[2023-01-22T08:44:12.906+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T08:44:12.907+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T08:44:12.907+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T08:44:12.907+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T08:44:12.907+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T08:44:12.908+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T08:44:12.908+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T08:44:12.908+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T08:44:12.908+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T08:44:12.908+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T08:44:12.909+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T08:44:12.909+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T08:44:12.909+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T08:44:12.909+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T08:44:12.909+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T08:44:12.910+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T08:44:12.910+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T08:44:12.910+0000] {spark_submit.py:495} INFO - 
[2023-01-22T08:44:12.910+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T08:44:12.911+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 68, in <module>
[2023-01-22T08:44:12.911+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T08:44:12.911+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 55, in main
[2023-01-22T08:44:12.911+0000] {spark_submit.py:495} INFO - df_spark.write.format("bigquery").option("project", "rafzul-analytics-1009") \
[2023-01-22T08:44:12.911+0000] {spark_submit.py:495} INFO - UnboundLocalError: local variable 'df_spark' referenced before assignment
[2023-01-22T08:44:12.979+0000] {spark_submit.py:495} INFO - 23/01/22 08:44:12 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-22T08:44:12.995+0000] {spark_submit.py:495} INFO - 23/01/22 08:44:12 INFO SparkUI: Stopped Spark web UI at http://9d8db2d97423:4041
[2023-01-22T08:44:13.014+0000] {spark_submit.py:495} INFO - 23/01/22 08:44:13 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-22T08:44:13.031+0000] {spark_submit.py:495} INFO - 23/01/22 08:44:13 INFO MemoryStore: MemoryStore cleared
[2023-01-22T08:44:13.032+0000] {spark_submit.py:495} INFO - 23/01/22 08:44:13 INFO BlockManager: BlockManager stopped
[2023-01-22T08:44:13.044+0000] {spark_submit.py:495} INFO - 23/01/22 08:44:13 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-22T08:44:13.049+0000] {spark_submit.py:495} INFO - 23/01/22 08:44:13 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-22T08:44:13.058+0000] {spark_submit.py:495} INFO - 23/01/22 08:44:13 INFO SparkContext: Successfully stopped SparkContext
[2023-01-22T08:44:13.059+0000] {spark_submit.py:495} INFO - 23/01/22 08:44:13 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T08:44:13.060+0000] {spark_submit.py:495} INFO - 23/01/22 08:44:13 INFO ShutdownHookManager: Deleting directory /tmp/spark-7259405d-8ac4-4f56-8878-2880539c09a1
[2023-01-22T08:44:13.067+0000] {spark_submit.py:495} INFO - 23/01/22 08:44:13 INFO ShutdownHookManager: Deleting directory /tmp/spark-cb023a62-fb06-459b-a351-b4abab84acb4
[2023-01-22T08:44:13.074+0000] {spark_submit.py:495} INFO - 23/01/22 08:44:13 INFO ShutdownHookManager: Deleting directory /tmp/spark-cb023a62-fb06-459b-a351-b4abab84acb4/pyspark-3955c9f4-f304-49b9-9888-f8d0b7c3a240
[2023-01-22T08:44:13.216+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-22T08:44:13.220+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T000000, start_date=20230122T084354, end_date=20230122T084413
[2023-01-22T08:44:13.236+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 376 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 923)
[2023-01-22T08:44:13.259+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T08:44:13.279+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
