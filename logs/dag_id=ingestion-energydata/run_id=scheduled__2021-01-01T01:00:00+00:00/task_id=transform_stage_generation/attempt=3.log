[2023-01-20T16:41:26.183+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-20T16:41:26.211+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-20T16:41:26.212+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T16:41:26.212+0000] {taskinstance.py:1284} INFO - Starting attempt 3 of 4
[2023-01-20T16:41:26.213+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T16:41:26.241+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-20T16:41:26.261+0000] {standard_task_runner.py:55} INFO - Started process 1683 to run task
[2023-01-20T16:41:26.285+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '78', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmplhqcd25c']
[2023-01-20T16:41:26.301+0000] {standard_task_runner.py:83} INFO - Job 78: Subtask transform_stage_generation
[2023-01-20T16:41:26.831+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T16:41:27.211+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=3
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-20T16:41:27.250+0000] {spark_submit.py:222} INFO - Could not load connection string spark_default, defaulting to yarn
[2023-01-20T16:41:27.267+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master yarn --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-20T16:41:27.295+0000] {spark_submit.py:495} INFO - /usr/local/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit: line 27: /opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-class: No such file or directory
[2023-01-20T16:41:27.355+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master yarn --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 127.
[2023-01-20T16:41:27.377+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230120T164126, end_date=20230120T164127
[2023-01-20T16:41:27.429+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 78 for task transform_stage_generation (Cannot execute: spark-submit --master yarn --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 127.; 1683)
[2023-01-20T16:41:27.502+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T16:41:27.618+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T18:28:59.371+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-20T18:28:59.379+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-20T18:28:59.380+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T18:28:59.380+0000] {taskinstance.py:1284} INFO - Starting attempt 3 of 4
[2023-01-20T18:28:59.380+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T18:28:59.391+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-20T18:28:59.398+0000] {standard_task_runner.py:55} INFO - Started process 8391 to run task
[2023-01-20T18:28:59.403+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '117', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmppbbr2m7v']
[2023-01-20T18:28:59.406+0000] {standard_task_runner.py:83} INFO - Job 117: Subtask transform_stage_generation
[2023-01-20T18:28:59.469+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T18:28:59.545+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=3
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-20T18:28:59.546+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 156, in execute
    self._hook = self._get_hook()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 165, in _get_hook
    return SparkSubmitHook(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 155, in __init__
    raise RuntimeError(
RuntimeError: The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH
[2023-01-20T18:28:59.559+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230120T182859, end_date=20230120T182859
[2023-01-20T18:28:59.571+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 117 for task transform_stage_generation (The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH; 8391)
[2023-01-20T18:28:59.614+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T18:28:59.630+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T19:26:00.398+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-20T19:26:00.415+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-20T19:26:00.416+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T19:26:00.416+0000] {taskinstance.py:1284} INFO - Starting attempt 3 of 4
[2023-01-20T19:26:00.416+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T19:26:00.436+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-20T19:26:00.447+0000] {standard_task_runner.py:55} INFO - Started process 961 to run task
[2023-01-20T19:26:00.452+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '132', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpghhejbq1']
[2023-01-20T19:26:00.457+0000] {standard_task_runner.py:83} INFO - Job 132: Subtask transform_stage_generation
[2023-01-20T19:26:00.543+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 984f5901aea4
[2023-01-20T19:26:00.648+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=3
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-20T19:26:00.649+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 156, in execute
    self._hook = self._get_hook()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 165, in _get_hook
    return SparkSubmitHook(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 155, in __init__
    raise RuntimeError(
RuntimeError: The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/usr/local/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH
[2023-01-20T19:26:00.664+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230120T192600, end_date=20230120T192600
[2023-01-20T19:26:00.679+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 132 for task transform_stage_generation (The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/usr/local/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH; 961)
[2023-01-20T19:26:00.704+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T19:26:00.733+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T07:31:08.347+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T07:31:08.366+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T07:31:08.367+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T07:31:08.367+0000] {taskinstance.py:1284} INFO - Starting attempt 3 of 4
[2023-01-21T07:31:08.367+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T07:31:08.392+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-21T07:31:08.409+0000] {standard_task_runner.py:55} INFO - Started process 66 to run task
[2023-01-21T07:31:08.416+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '147', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpe0cesyv4']
[2023-01-21T07:31:08.421+0000] {standard_task_runner.py:83} INFO - Job 147: Subtask transform_stage_generation
[2023-01-21T07:31:08.550+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 2b3134e2b02d
[2023-01-21T07:31:08.733+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=3
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-21T07:31:08.735+0000] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-01-21T07:31:08.738+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master  --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-21T07:31:08.759+0000] {spark_submit.py:495} INFO - /usr/local/spark/bin/load-spark-env.sh: line 68: ps: command not found
[2023-01-21T07:31:14.845+0000] {spark_submit.py:495} INFO - Exception in thread "main" org.apache.spark.SparkException: Master must either be yarn or start with spark, mesos, k8s, or local
[2023-01-21T07:31:14.846+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.error(SparkSubmit.scala:975)
[2023-01-21T07:31:14.846+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:238)
[2023-01-21T07:31:14.846+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:901)
[2023-01-21T07:31:14.846+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
[2023-01-21T07:31:14.847+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
[2023-01-21T07:31:14.847+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
[2023-01-21T07:31:14.847+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)
[2023-01-21T07:31:14.847+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)
[2023-01-21T07:31:14.847+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
[2023-01-21T07:31:14.912+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master  --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-21T07:31:14.918+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230121T073108, end_date=20230121T073114
[2023-01-21T07:31:14.941+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 147 for task transform_stage_generation (Cannot execute: spark-submit --master  --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 66)
[2023-01-21T07:31:14.973+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T07:31:15.022+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T07:57:54.477+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T07:57:54.489+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T07:57:54.489+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T07:57:54.489+0000] {taskinstance.py:1284} INFO - Starting attempt 3 of 4
[2023-01-21T07:57:54.489+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T07:57:54.502+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-21T07:57:54.510+0000] {standard_task_runner.py:55} INFO - Started process 896 to run task
[2023-01-21T07:57:54.515+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '166', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpky74nobv']
[2023-01-21T07:57:54.518+0000] {standard_task_runner.py:83} INFO - Job 166: Subtask transform_stage_generation
[2023-01-21T07:57:54.589+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 4d52b49e46b4
[2023-01-21T07:57:54.670+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=3
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-21T07:57:54.672+0000] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-01-21T07:57:54.674+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master  --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-21T07:57:54.687+0000] {spark_submit.py:495} INFO - /usr/local/spark/bin/load-spark-env.sh: line 68: ps: command not found
[2023-01-21T07:57:57.223+0000] {spark_submit.py:495} INFO - Exception in thread "main" org.apache.spark.SparkException: Master must either be yarn or start with spark, mesos, k8s, or local
[2023-01-21T07:57:57.224+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.error(SparkSubmit.scala:975)
[2023-01-21T07:57:57.224+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:238)
[2023-01-21T07:57:57.224+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:901)
[2023-01-21T07:57:57.224+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
[2023-01-21T07:57:57.224+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
[2023-01-21T07:57:57.225+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
[2023-01-21T07:57:57.225+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)
[2023-01-21T07:57:57.225+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)
[2023-01-21T07:57:57.225+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
[2023-01-21T07:57:57.259+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master  --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-21T07:57:57.262+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230121T075754, end_date=20230121T075757
[2023-01-21T07:57:57.274+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 166 for task transform_stage_generation (Cannot execute: spark-submit --master  --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 896)
[2023-01-21T07:57:57.315+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T07:57:57.338+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T08:34:05.783+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T08:34:05.795+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T08:34:05.796+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T08:34:05.796+0000] {taskinstance.py:1284} INFO - Starting attempt 3 of 4
[2023-01-21T08:34:05.796+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T08:34:05.810+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-21T08:34:05.820+0000] {standard_task_runner.py:55} INFO - Started process 1188 to run task
[2023-01-21T08:34:05.824+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '185', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpjdumibag']
[2023-01-21T08:34:05.827+0000] {standard_task_runner.py:83} INFO - Job 185: Subtask transform_stage_generation
[2023-01-21T08:34:05.891+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host d00464a30031
[2023-01-21T08:34:05.974+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=3
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-21T08:34:05.987+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-21T08:34:05.992+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-21T08:34:06.024+0000] {spark_submit.py:495} INFO - /usr/local/spark/bin/load-spark-env.sh: line 68: ps: command not found
[2023-01-21T08:34:08.422+0000] {spark_submit.py:495} INFO - 23/01/21 08:34:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-21T08:34:08.668+0000] {spark_submit.py:495} INFO - 23/01/21 08:34:08 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-21T08:34:08.779+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-21T08:34:08.779+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 2, in <module>
[2023-01-21T08:34:08.780+0000] {spark_submit.py:495} INFO - from dotenv import load_dotenv
[2023-01-21T08:34:08.780+0000] {spark_submit.py:495} INFO - ModuleNotFoundError: No module named 'dotenv'
[2023-01-21T08:34:08.788+0000] {spark_submit.py:495} INFO - 23/01/21 08:34:08 INFO ShutdownHookManager: Shutdown hook called
[2023-01-21T08:34:08.789+0000] {spark_submit.py:495} INFO - 23/01/21 08:34:08 INFO ShutdownHookManager: Deleting directory /tmp/spark-4ddd3637-f484-4dab-89a2-4cd6021900ae
[2023-01-21T08:34:08.839+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-21T08:34:08.842+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230121T083405, end_date=20230121T083408
[2023-01-21T08:34:08.851+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 185 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 1188)
[2023-01-21T08:34:08.882+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T08:34:08.897+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T10:28:33.757+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T10:28:33.768+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T10:28:33.768+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T10:28:33.768+0000] {taskinstance.py:1284} INFO - Starting attempt 3 of 4
[2023-01-21T10:28:33.768+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T10:28:33.778+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-21T10:28:33.785+0000] {standard_task_runner.py:55} INFO - Started process 3085 to run task
[2023-01-21T10:28:33.789+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '197', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpfz39oko0']
[2023-01-21T10:28:33.792+0000] {standard_task_runner.py:83} INFO - Job 197: Subtask transform_stage_generation
[2023-01-21T10:28:33.848+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host b3a2c4a96689
[2023-01-21T10:28:33.904+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=3
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-21T10:28:33.913+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-21T10:28:33.914+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-21T10:28:36.723+0000] {spark_submit.py:495} INFO - 23/01/21 10:28:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-21T10:28:36.970+0000] {spark_submit.py:495} INFO - 23/01/21 10:28:36 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-21T10:28:37.011+0000] {spark_submit.py:495} INFO - Exception in thread "main" java.io.IOException: Cannot run program "/usr/bin/python3.10": error=2, No such file or directory
[2023-01-21T10:28:37.012+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1128)
[2023-01-21T10:28:37.012+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1071)
[2023-01-21T10:28:37.012+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.PythonRunner$.main(PythonRunner.scala:97)
[2023-01-21T10:28:37.012+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.PythonRunner.main(PythonRunner.scala)
[2023-01-21T10:28:37.013+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-21T10:28:37.013+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-21T10:28:37.013+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-21T10:28:37.013+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-21T10:28:37.013+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
[2023-01-21T10:28:37.014+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:958)
[2023-01-21T10:28:37.014+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
[2023-01-21T10:28:37.014+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
[2023-01-21T10:28:37.014+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
[2023-01-21T10:28:37.014+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)
[2023-01-21T10:28:37.014+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)
[2023-01-21T10:28:37.015+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
[2023-01-21T10:28:37.015+0000] {spark_submit.py:495} INFO - Caused by: java.io.IOException: error=2, No such file or directory
[2023-01-21T10:28:37.015+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessImpl.forkAndExec(Native Method)
[2023-01-21T10:28:37.015+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:340)
[2023-01-21T10:28:37.015+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessImpl.start(ProcessImpl.java:271)
[2023-01-21T10:28:37.015+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1107)
[2023-01-21T10:28:37.015+0000] {spark_submit.py:495} INFO - ... 15 more
[2023-01-21T10:28:37.018+0000] {spark_submit.py:495} INFO - 23/01/21 10:28:37 INFO ShutdownHookManager: Shutdown hook called
[2023-01-21T10:28:37.019+0000] {spark_submit.py:495} INFO - 23/01/21 10:28:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-16be6cf8-d3f8-4d95-86e7-61620a4fbb60
[2023-01-21T10:28:37.087+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-21T10:28:37.091+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230121T102833, end_date=20230121T102837
[2023-01-21T10:28:37.104+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 197 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 3085)
[2023-01-21T10:28:37.147+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T10:28:37.166+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T11:15:59.532+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T11:15:59.560+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T11:15:59.561+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T11:15:59.561+0000] {taskinstance.py:1284} INFO - Starting attempt 3 of 4
[2023-01-21T11:15:59.561+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T11:15:59.598+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-21T11:15:59.610+0000] {standard_task_runner.py:55} INFO - Started process 82 to run task
[2023-01-21T11:15:59.619+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '213', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp3j8ezd1u']
[2023-01-21T11:15:59.625+0000] {standard_task_runner.py:83} INFO - Job 213: Subtask transform_stage_generation
[2023-01-21T11:15:59.749+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host b7c4de7fae0b
[2023-01-21T11:15:59.906+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=3
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-21T11:15:59.927+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-21T11:15:59.930+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-21T11:16:10.803+0000] {spark_submit.py:495} INFO - 23/01/21 11:16:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-21T11:16:13.504+0000] {spark_submit.py:495} INFO - 23/01/21 11:16:13 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-21T11:16:14.695+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-21T11:16:14.696+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 2, in <module>
[2023-01-21T11:16:14.697+0000] {spark_submit.py:495} INFO - from dotenv import load_dotenv
[2023-01-21T11:16:14.697+0000] {spark_submit.py:495} INFO - ModuleNotFoundError: No module named 'dotenv'
[2023-01-21T11:16:14.804+0000] {spark_submit.py:495} INFO - 23/01/21 11:16:14 INFO ShutdownHookManager: Shutdown hook called
[2023-01-21T11:16:14.825+0000] {spark_submit.py:495} INFO - 23/01/21 11:16:14 INFO ShutdownHookManager: Deleting directory /tmp/spark-e01fb4d7-5c5c-49fa-822e-c8a819337fc9
[2023-01-21T11:16:15.904+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-21T11:16:15.964+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230121T111559, end_date=20230121T111615
[2023-01-21T11:16:16.387+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 213 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 82)
[2023-01-21T11:16:16.531+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T11:16:16.642+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T11:41:35.754+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T11:41:35.774+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T11:41:35.774+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T11:41:35.775+0000] {taskinstance.py:1284} INFO - Starting attempt 3 of 4
[2023-01-21T11:41:35.775+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T11:41:35.799+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-21T11:41:35.812+0000] {standard_task_runner.py:55} INFO - Started process 2281 to run task
[2023-01-21T11:41:35.826+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '227', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmppl6tq__o']
[2023-01-21T11:41:35.831+0000] {standard_task_runner.py:83} INFO - Job 227: Subtask transform_stage_generation
[2023-01-21T11:41:35.964+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host b7c4de7fae0b
[2023-01-21T11:41:36.053+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=3
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-21T11:41:36.066+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-21T11:41:36.070+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-21T11:41:40.426+0000] {spark_submit.py:495} INFO - 23/01/21 11:41:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-21T11:41:41.134+0000] {spark_submit.py:495} INFO - 23/01/21 11:41:41 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-21T11:41:41.399+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-21T11:41:41.400+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 2, in <module>
[2023-01-21T11:41:41.400+0000] {spark_submit.py:495} INFO - from dotenv import load_dotenv, find_dotenv
[2023-01-21T11:41:41.401+0000] {spark_submit.py:495} INFO - ModuleNotFoundError: No module named 'dotenv'
[2023-01-21T11:41:41.437+0000] {spark_submit.py:495} INFO - 23/01/21 11:41:41 INFO ShutdownHookManager: Shutdown hook called
[2023-01-21T11:41:41.439+0000] {spark_submit.py:495} INFO - 23/01/21 11:41:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-500638bf-ef97-442f-9faa-e44269b10e56
[2023-01-21T11:41:41.561+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-21T11:41:41.570+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230121T114135, end_date=20230121T114141
[2023-01-21T11:41:41.604+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 227 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 2281)
[2023-01-21T11:41:41.668+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T11:41:41.699+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T16:03:03.296+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T16:03:03.304+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T16:03:03.304+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T16:03:03.304+0000] {taskinstance.py:1284} INFO - Starting attempt 3 of 4
[2023-01-21T16:03:03.304+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T16:03:03.313+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-21T16:03:03.320+0000] {standard_task_runner.py:55} INFO - Started process 1069 to run task
[2023-01-21T16:03:03.323+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '238', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmphaq0_vwr']
[2023-01-21T16:03:03.326+0000] {standard_task_runner.py:83} INFO - Job 238: Subtask transform_stage_generation
[2023-01-21T16:03:03.394+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 673f7b70a136
[2023-01-21T16:03:03.467+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=3
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-21T16:03:03.476+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-21T16:03:03.478+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-21T16:03:06.411+0000] {spark_submit.py:495} INFO - 23/01/21 16:03:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-21T16:03:06.728+0000] {spark_submit.py:495} INFO - 23/01/21 16:03:06 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-21T16:03:06.815+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-21T16:03:06.816+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 2, in <module>
[2023-01-21T16:03:06.816+0000] {spark_submit.py:495} INFO - from dotenv import load_dotenv, find_dotenv
[2023-01-21T16:03:06.816+0000] {spark_submit.py:495} INFO - ModuleNotFoundError: No module named 'dotenv'
[2023-01-21T16:03:06.830+0000] {spark_submit.py:495} INFO - 23/01/21 16:03:06 INFO ShutdownHookManager: Shutdown hook called
[2023-01-21T16:03:06.832+0000] {spark_submit.py:495} INFO - 23/01/21 16:03:06 INFO ShutdownHookManager: Deleting directory /tmp/spark-93e1b859-6ab3-44e1-aa54-34fe231536aa
[2023-01-21T16:03:06.901+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-21T16:03:06.904+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230121T160303, end_date=20230121T160306
[2023-01-21T16:03:06.918+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 238 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 1069)
[2023-01-21T16:03:06.958+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T16:03:06.976+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T22:18:37.011+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T22:18:37.043+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T22:18:37.045+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T22:18:37.046+0000] {taskinstance.py:1284} INFO - Starting attempt 3 of 4
[2023-01-21T22:18:37.047+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T22:18:37.074+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-21T22:18:37.092+0000] {standard_task_runner.py:55} INFO - Started process 1053 to run task
[2023-01-21T22:18:37.097+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '251', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpdienfosj']
[2023-01-21T22:18:37.101+0000] {standard_task_runner.py:83} INFO - Job 251: Subtask transform_stage_generation
[2023-01-21T22:18:37.178+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 4260804338b2
[2023-01-21T22:18:37.227+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=3
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-21T22:18:37.235+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-21T22:18:37.242+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-21T22:18:40.615+0000] {spark_submit.py:495} INFO - 23/01/21 22:18:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-21T22:18:40.981+0000] {spark_submit.py:495} INFO - 23/01/21 22:18:40 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-21T22:18:41.137+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-21T22:18:41.138+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 2, in <module>
[2023-01-21T22:18:41.138+0000] {spark_submit.py:495} INFO - from dotenv import load_dotenv, find_dotenv
[2023-01-21T22:18:41.138+0000] {spark_submit.py:495} INFO - ModuleNotFoundError: No module named 'dotenv'
[2023-01-21T22:18:41.153+0000] {spark_submit.py:495} INFO - 23/01/21 22:18:41 INFO ShutdownHookManager: Shutdown hook called
[2023-01-21T22:18:41.154+0000] {spark_submit.py:495} INFO - 23/01/21 22:18:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-aa9a8dc8-f1b2-43fe-951f-29affa980bca
[2023-01-21T22:18:41.283+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-21T22:18:41.288+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230121T221837, end_date=20230121T221841
[2023-01-21T22:18:41.304+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 251 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 1053)
[2023-01-21T22:18:41.335+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T22:18:41.348+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T23:17:27.491+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T23:17:27.521+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T23:17:27.521+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T23:17:27.522+0000] {taskinstance.py:1284} INFO - Starting attempt 3 of 4
[2023-01-21T23:17:27.524+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T23:17:27.582+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-21T23:17:27.611+0000] {standard_task_runner.py:55} INFO - Started process 5155 to run task
[2023-01-21T23:17:27.621+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '261', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp411yhnlh']
[2023-01-21T23:17:27.628+0000] {standard_task_runner.py:83} INFO - Job 261: Subtask transform_stage_generation
[2023-01-21T23:17:27.880+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 4260804338b2
[2023-01-21T23:17:28.076+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=3
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-21T23:17:28.099+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-21T23:17:28.103+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-21T23:17:36.314+0000] {spark_submit.py:495} INFO - 23/01/21 23:17:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-21T23:17:36.715+0000] {spark_submit.py:495} INFO - 23/01/21 23:17:36 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-21T23:17:36.873+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-21T23:17:36.874+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 2, in <module>
[2023-01-21T23:17:36.874+0000] {spark_submit.py:495} INFO - from dotenv import load_dotenv, find_dotenv
[2023-01-21T23:17:36.875+0000] {spark_submit.py:495} INFO - ModuleNotFoundError: No module named 'dotenv'
[2023-01-21T23:17:36.894+0000] {spark_submit.py:495} INFO - 23/01/21 23:17:36 INFO ShutdownHookManager: Shutdown hook called
[2023-01-21T23:17:36.896+0000] {spark_submit.py:495} INFO - 23/01/21 23:17:36 INFO ShutdownHookManager: Deleting directory /tmp/spark-98764347-96ca-4e1f-96e9-3b3603e15558
[2023-01-21T23:17:36.983+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-21T23:17:36.990+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230121T231727, end_date=20230121T231736
[2023-01-21T23:17:37.010+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 261 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 5155)
[2023-01-21T23:17:37.034+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T23:17:37.061+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T00:21:47.680+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T00:21:47.692+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T00:21:47.692+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T00:21:47.693+0000] {taskinstance.py:1284} INFO - Starting attempt 3 of 4
[2023-01-22T00:21:47.693+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T00:21:47.711+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-22T00:21:47.721+0000] {standard_task_runner.py:55} INFO - Started process 1292 to run task
[2023-01-22T00:21:47.725+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '273', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp_m29jo0f']
[2023-01-22T00:21:47.729+0000] {standard_task_runner.py:83} INFO - Job 273: Subtask transform_stage_generation
[2023-01-22T00:21:47.801+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 669043c9475c
[2023-01-22T00:21:47.880+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=3
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-22T00:21:47.891+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T00:21:47.893+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-22T00:21:50.689+0000] {spark_submit.py:495} INFO - 23/01/22 00:21:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T00:21:50.949+0000] {spark_submit.py:495} INFO - 23/01/22 00:21:50 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-22T00:21:51.279+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T00:21:51.279+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 64, in <module>
[2023-01-22T00:21:51.279+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T00:21:51.279+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 16, in main
[2023-01-22T00:21:51.279+0000] {spark_submit.py:495} INFO - start_label = start.strftime("%Y%m%d%H%M")
[2023-01-22T00:21:51.279+0000] {spark_submit.py:495} INFO - AttributeError: 'str' object has no attribute 'strftime'
[2023-01-22T00:21:51.292+0000] {spark_submit.py:495} INFO - 23/01/22 00:21:51 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T00:21:51.292+0000] {spark_submit.py:495} INFO - 23/01/22 00:21:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-6127efd8-48bd-40b3-a459-84b160638cc0
[2023-01-22T00:21:51.357+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-22T00:21:51.360+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230122T002147, end_date=20230122T002151
[2023-01-22T00:21:51.370+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 273 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 1292)
[2023-01-22T00:21:51.405+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T00:21:51.418+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T04:03:09.406+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T04:03:09.421+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T04:03:09.421+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T04:03:09.421+0000] {taskinstance.py:1284} INFO - Starting attempt 3 of 4
[2023-01-22T04:03:09.422+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T04:03:09.434+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-22T04:03:09.441+0000] {standard_task_runner.py:55} INFO - Started process 1712 to run task
[2023-01-22T04:03:09.446+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '284', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmport9fvvf']
[2023-01-22T04:03:09.449+0000] {standard_task_runner.py:83} INFO - Job 284: Subtask transform_stage_generation
[2023-01-22T04:03:09.507+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host e3ae05c9a97e
[2023-01-22T04:03:09.580+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=3
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-22T04:03:09.590+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T04:03:09.592+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /opt/***/spark/resources/gcs-connector-hadoop3-latest.jar, /opt/***/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-22T04:03:12.276+0000] {spark_submit.py:495} INFO - 23/01/22 04:03:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T04:03:12.534+0000] {spark_submit.py:495} INFO - 23/01/22 04:03:12 WARN DependencyUtils: Local jar /opt/***/ /opt/***/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-22T04:03:12.853+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T04:03:12.853+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 64, in <module>
[2023-01-22T04:03:12.853+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T04:03:12.853+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 16, in main
[2023-01-22T04:03:12.853+0000] {spark_submit.py:495} INFO - start_label = start.strftime("%Y%m%d%H%M")
[2023-01-22T04:03:12.853+0000] {spark_submit.py:495} INFO - AttributeError: 'str' object has no attribute 'strftime'
[2023-01-22T04:03:12.869+0000] {spark_submit.py:495} INFO - 23/01/22 04:03:12 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T04:03:12.870+0000] {spark_submit.py:495} INFO - 23/01/22 04:03:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-addb704e-997e-41e4-8e18-f4790d1954ef
[2023-01-22T04:03:12.930+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /opt/***/spark/resources/gcs-connector-hadoop3-latest.jar, /opt/***/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-22T04:03:12.933+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230122T040309, end_date=20230122T040312
[2023-01-22T04:03:12.945+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 284 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /opt/***/spark/resources/gcs-connector-hadoop3-latest.jar, /opt/***/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 1712)
[2023-01-22T04:03:12.960+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T04:03:12.977+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T04:33:32.639+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T04:33:32.662+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T04:33:32.662+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T04:33:32.663+0000] {taskinstance.py:1284} INFO - Starting attempt 3 of 4
[2023-01-22T04:33:32.663+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T04:33:32.691+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-22T04:33:32.708+0000] {standard_task_runner.py:55} INFO - Started process 108 to run task
[2023-01-22T04:33:32.716+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '296', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpwbxwt5t5']
[2023-01-22T04:33:32.722+0000] {standard_task_runner.py:83} INFO - Job 296: Subtask transform_stage_generation
[2023-01-22T04:33:32.850+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 801017642f38
[2023-01-22T04:33:32.997+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=3
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-22T04:33:33.019+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T04:33:33.022+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar, spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-22T04:33:37.682+0000] {spark_submit.py:495} INFO - 23/01/22 04:33:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T04:33:38.094+0000] {spark_submit.py:495} INFO - 23/01/22 04:33:38 WARN DependencyUtils: Local jar /opt/***/gcs-connector-hadoop3-latest.jar does not exist, skipping.
[2023-01-22T04:33:38.097+0000] {spark_submit.py:495} INFO - 23/01/22 04:33:38 WARN DependencyUtils: Local jar /opt/***/ spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-22T04:33:38.550+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T04:33:38.550+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 64, in <module>
[2023-01-22T04:33:38.550+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T04:33:38.550+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 16, in main
[2023-01-22T04:33:38.551+0000] {spark_submit.py:495} INFO - start_label = start.strftime("%Y%m%d%H%M")
[2023-01-22T04:33:38.551+0000] {spark_submit.py:495} INFO - AttributeError: 'str' object has no attribute 'strftime'
[2023-01-22T04:33:38.574+0000] {spark_submit.py:495} INFO - 23/01/22 04:33:38 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T04:33:38.575+0000] {spark_submit.py:495} INFO - 23/01/22 04:33:38 INFO ShutdownHookManager: Deleting directory /tmp/spark-a2f4ffbd-95f7-4959-b18e-786100f70377
[2023-01-22T04:33:38.645+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar, spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-22T04:33:38.651+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230122T043332, end_date=20230122T043338
[2023-01-22T04:33:38.668+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 296 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar, spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 108)
[2023-01-22T04:33:38.686+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T04:33:38.704+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T06:18:56.631+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T06:18:56.665+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T06:18:56.666+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T06:18:56.666+0000] {taskinstance.py:1284} INFO - Starting attempt 3 of 4
[2023-01-22T06:18:56.667+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T06:18:56.713+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-22T06:18:56.742+0000] {standard_task_runner.py:55} INFO - Started process 807 to run task
[2023-01-22T06:18:56.757+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '308', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpp9v9byda']
[2023-01-22T06:18:56.767+0000] {standard_task_runner.py:83} INFO - Job 308: Subtask transform_stage_generation
[2023-01-22T06:18:57.005+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host d68405340607
[2023-01-22T06:18:57.234+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=3
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-22T06:18:57.269+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T06:18:57.278+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-22T06:18:57.300+0000] {spark_submit.py:495} INFO - /home/***/.local/bin/spark-submit: line 27: /opt/spark/bin/spark-class: No such file or directory
[2023-01-22T06:18:57.326+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 127.
[2023-01-22T06:18:57.339+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230122T061856, end_date=20230122T061857
[2023-01-22T06:18:57.384+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 308 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 127.; 807)
[2023-01-22T06:18:57.451+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T06:18:57.641+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T08:08:59.458+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T08:08:59.486+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T08:08:59.487+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T08:08:59.487+0000] {taskinstance.py:1284} INFO - Starting attempt 3 of 4
[2023-01-22T08:08:59.488+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T08:08:59.515+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-22T08:08:59.532+0000] {standard_task_runner.py:55} INFO - Started process 3206 to run task
[2023-01-22T08:08:59.542+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '335', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmptwo2po43']
[2023-01-22T08:08:59.549+0000] {standard_task_runner.py:83} INFO - Job 335: Subtask transform_stage_generation
[2023-01-22T08:08:59.696+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 16233a798013
[2023-01-22T08:08:59.876+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=3
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-22T08:08:59.898+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T08:08:59.900+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-22T08:09:08.315+0000] {spark_submit.py:495} INFO - 23/01/22 08:09:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T08:09:09.348+0000] {spark_submit.py:495} INFO - 23/01/22 08:09:09 WARN DependencyUtils: Local jar /opt/***/gcs-connector-hadoop3-latest.jar does not exist, skipping.
[2023-01-22T08:09:09.351+0000] {spark_submit.py:495} INFO - 23/01/22 08:09:09 WARN DependencyUtils: Local jar /opt/***/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-22T08:09:12.517+0000] {spark_submit.py:495} INFO - /opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010100.json
[2023-01-22T08:09:12.848+0000] {spark_submit.py:495} INFO - 23/01/22 08:09:12 INFO SparkContext: Running Spark version 3.3.1
[2023-01-22T08:09:12.950+0000] {spark_submit.py:495} INFO - 23/01/22 08:09:12 INFO ResourceUtils: ==============================================================
[2023-01-22T08:09:12.952+0000] {spark_submit.py:495} INFO - 23/01/22 08:09:12 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-22T08:09:12.953+0000] {spark_submit.py:495} INFO - 23/01/22 08:09:12 INFO ResourceUtils: ==============================================================
[2023-01-22T08:09:12.955+0000] {spark_submit.py:495} INFO - 23/01/22 08:09:12 INFO SparkContext: Submitted application: gcp_playground
[2023-01-22T08:09:13.027+0000] {spark_submit.py:495} INFO - 23/01/22 08:09:13 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-22T08:09:13.041+0000] {spark_submit.py:495} INFO - 23/01/22 08:09:13 INFO ResourceProfile: Limiting resource is cpu
[2023-01-22T08:09:13.044+0000] {spark_submit.py:495} INFO - 23/01/22 08:09:13 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-22T08:09:13.194+0000] {spark_submit.py:495} INFO - 23/01/22 08:09:13 INFO SecurityManager: Changing view acls to: ***
[2023-01-22T08:09:13.195+0000] {spark_submit.py:495} INFO - 23/01/22 08:09:13 INFO SecurityManager: Changing modify acls to: ***
[2023-01-22T08:09:13.196+0000] {spark_submit.py:495} INFO - 23/01/22 08:09:13 INFO SecurityManager: Changing view acls groups to:
[2023-01-22T08:09:13.198+0000] {spark_submit.py:495} INFO - 23/01/22 08:09:13 INFO SecurityManager: Changing modify acls groups to:
[2023-01-22T08:09:13.201+0000] {spark_submit.py:495} INFO - 23/01/22 08:09:13 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-22T08:09:13.943+0000] {spark_submit.py:495} INFO - 23/01/22 08:09:13 INFO Utils: Successfully started service 'sparkDriver' on port 41311.
[2023-01-22T08:09:14.075+0000] {spark_submit.py:495} INFO - 23/01/22 08:09:14 INFO SparkEnv: Registering MapOutputTracker
[2023-01-22T08:09:14.162+0000] {spark_submit.py:495} INFO - 23/01/22 08:09:14 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-22T08:09:14.235+0000] {spark_submit.py:495} INFO - 23/01/22 08:09:14 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-22T08:09:14.237+0000] {spark_submit.py:495} INFO - 23/01/22 08:09:14 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-22T08:09:14.253+0000] {spark_submit.py:495} INFO - 23/01/22 08:09:14 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-22T08:09:14.302+0000] {spark_submit.py:495} INFO - 23/01/22 08:09:14 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-14a6bc16-0613-4d79-97e7-0de669cccfaf
[2023-01-22T08:09:14.339+0000] {spark_submit.py:495} INFO - 23/01/22 08:09:14 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-22T08:09:14.376+0000] {spark_submit.py:495} INFO - 23/01/22 08:09:14 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-22T08:09:14.882+0000] {spark_submit.py:495} INFO - 23/01/22 08:09:14 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-22T08:09:14.898+0000] {spark_submit.py:495} INFO - 23/01/22 08:09:14 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2023-01-22T08:09:14.963+0000] {spark_submit.py:495} INFO - 23/01/22 08:09:14 INFO SparkContext: Added JAR /opt/spark/resources/gcs-connector-hadoop3-latest.jar at spark://16233a798013:41311/jars/gcs-connector-hadoop3-latest.jar with timestamp 1674374952773
[2023-01-22T08:09:14.963+0000] {spark_submit.py:495} INFO - 23/01/22 08:09:14 INFO SparkContext: Added JAR /opt/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://16233a798013:41311/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1674374952773
[2023-01-22T08:09:15.088+0000] {spark_submit.py:495} INFO - 23/01/22 08:09:15 INFO Executor: Starting executor ID driver on host 16233a798013
[2023-01-22T08:09:15.102+0000] {spark_submit.py:495} INFO - 23/01/22 08:09:15 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-22T08:09:15.123+0000] {spark_submit.py:495} INFO - 23/01/22 08:09:15 INFO Executor: Fetching spark://16233a798013:41311/jars/gcs-connector-hadoop3-latest.jar with timestamp 1674374952773
[2023-01-22T08:09:15.198+0000] {spark_submit.py:495} INFO - 23/01/22 08:09:15 INFO TransportClientFactory: Successfully created connection to 16233a798013/192.168.32.7:41311 after 44 ms (0 ms spent in bootstraps)
[2023-01-22T08:09:15.210+0000] {spark_submit.py:495} INFO - 23/01/22 08:09:15 INFO Utils: Fetching spark://16233a798013:41311/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-c44c1275-6924-4a99-9837-16b12edf74bb/userFiles-dbefff2e-41c9-4dc9-8705-be6c06aac65d/fetchFileTemp8427201628780032229.tmp
[2023-01-22T08:09:15.606+0000] {spark_submit.py:495} INFO - 23/01/22 08:09:15 INFO Executor: Adding file:/tmp/spark-c44c1275-6924-4a99-9837-16b12edf74bb/userFiles-dbefff2e-41c9-4dc9-8705-be6c06aac65d/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-22T08:09:15.607+0000] {spark_submit.py:495} INFO - 23/01/22 08:09:15 INFO Executor: Fetching spark://16233a798013:41311/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1674374952773
[2023-01-22T08:09:15.614+0000] {spark_submit.py:495} INFO - 23/01/22 08:09:15 INFO Utils: Fetching spark://16233a798013:41311/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-c44c1275-6924-4a99-9837-16b12edf74bb/userFiles-dbefff2e-41c9-4dc9-8705-be6c06aac65d/fetchFileTemp1556816872998684777.tmp
[2023-01-22T08:09:15.987+0000] {spark_submit.py:495} INFO - 23/01/22 08:09:15 INFO Executor: Adding file:/tmp/spark-c44c1275-6924-4a99-9837-16b12edf74bb/userFiles-dbefff2e-41c9-4dc9-8705-be6c06aac65d/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-22T08:09:16.003+0000] {spark_submit.py:495} INFO - 23/01/22 08:09:16 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34867.
[2023-01-22T08:09:16.004+0000] {spark_submit.py:495} INFO - 23/01/22 08:09:16 INFO NettyBlockTransferService: Server created on 16233a798013:34867
[2023-01-22T08:09:16.007+0000] {spark_submit.py:495} INFO - 23/01/22 08:09:16 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-22T08:09:16.030+0000] {spark_submit.py:495} INFO - 23/01/22 08:09:16 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 16233a798013, 34867, None)
[2023-01-22T08:09:16.037+0000] {spark_submit.py:495} INFO - 23/01/22 08:09:16 INFO BlockManagerMasterEndpoint: Registering block manager 16233a798013:34867 with 434.4 MiB RAM, BlockManagerId(driver, 16233a798013, 34867, None)
[2023-01-22T08:09:16.042+0000] {spark_submit.py:495} INFO - 23/01/22 08:09:16 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 16233a798013, 34867, None)
[2023-01-22T08:09:16.044+0000] {spark_submit.py:495} INFO - 23/01/22 08:09:16 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 16233a798013, 34867, None)
[2023-01-22T08:09:17.426+0000] {spark_submit.py:495} INFO - 23/01/22 08:09:17 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-22T08:09:17.434+0000] {spark_submit.py:495} INFO - 23/01/22 08:09:17 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-22T08:09:20.080+0000] {spark_submit.py:495} INFO - 23/01/22 08:09:20 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: gs://entsoe_analytics_1009//opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010100.json.
[2023-01-22T08:09:20.080+0000] {spark_submit.py:495} INFO - org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "gs"
[2023-01-22T08:09:20.080+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2023-01-22T08:09:20.081+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T08:09:20.081+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T08:09:20.081+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T08:09:20.082+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T08:09:20.082+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T08:09:20.082+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T08:09:20.084+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)
[2023-01-22T08:09:20.084+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:370)
[2023-01-22T08:09:20.085+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T08:09:20.085+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T08:09:20.085+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T08:09:20.086+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T08:09:20.086+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T08:09:20.087+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T08:09:20.087+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T08:09:20.087+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T08:09:20.088+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T08:09:20.088+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T08:09:20.092+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T08:09:20.094+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T08:09:20.099+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T08:09:20.100+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T08:09:20.100+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T08:09:20.100+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T08:09:20.101+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T08:09:20.146+0000] {spark_submit.py:495} INFO - Error, ada An error occurred while calling o38.load.
[2023-01-22T08:09:20.146+0000] {spark_submit.py:495} INFO - : org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "gs"
[2023-01-22T08:09:20.147+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2023-01-22T08:09:20.147+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T08:09:20.148+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T08:09:20.148+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T08:09:20.149+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T08:09:20.149+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T08:09:20.150+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T08:09:20.150+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:752)
[2023-01-22T08:09:20.151+0000] {spark_submit.py:495} INFO - at scala.collection.immutable.List.map(List.scala:246)
[2023-01-22T08:09:20.151+0000] {spark_submit.py:495} INFO - at scala.collection.immutable.List.map(List.scala:79)
[2023-01-22T08:09:20.152+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:750)
[2023-01-22T08:09:20.152+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:579)
[2023-01-22T08:09:20.152+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:408)
[2023-01-22T08:09:20.153+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T08:09:20.154+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T08:09:20.154+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T08:09:20.155+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T08:09:20.155+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T08:09:20.156+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T08:09:20.156+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T08:09:20.156+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T08:09:20.156+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T08:09:20.157+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T08:09:20.157+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T08:09:20.157+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T08:09:20.158+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T08:09:20.158+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T08:09:20.159+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T08:09:20.159+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T08:09:20.159+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T08:09:20.160+0000] {spark_submit.py:495} INFO - 
[2023-01-22T08:09:20.160+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T08:09:20.160+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 69, in <module>
[2023-01-22T08:09:20.161+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T08:09:20.161+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 56, in main
[2023-01-22T08:09:20.162+0000] {spark_submit.py:495} INFO - df_spark.write.format("bigquery").option("project", "rafzul-analytics-1009") \
[2023-01-22T08:09:20.162+0000] {spark_submit.py:495} INFO - UnboundLocalError: local variable 'df_spark' referenced before assignment
[2023-01-22T08:09:20.256+0000] {spark_submit.py:495} INFO - 23/01/22 08:09:20 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-22T08:09:20.282+0000] {spark_submit.py:495} INFO - 23/01/22 08:09:20 INFO SparkUI: Stopped Spark web UI at http://16233a798013:4041
[2023-01-22T08:09:20.316+0000] {spark_submit.py:495} INFO - 23/01/22 08:09:20 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-22T08:09:20.350+0000] {spark_submit.py:495} INFO - 23/01/22 08:09:20 INFO MemoryStore: MemoryStore cleared
[2023-01-22T08:09:20.351+0000] {spark_submit.py:495} INFO - 23/01/22 08:09:20 INFO BlockManager: BlockManager stopped
[2023-01-22T08:09:20.376+0000] {spark_submit.py:495} INFO - 23/01/22 08:09:20 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-22T08:09:20.385+0000] {spark_submit.py:495} INFO - 23/01/22 08:09:20 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-22T08:09:20.428+0000] {spark_submit.py:495} INFO - 23/01/22 08:09:20 INFO SparkContext: Successfully stopped SparkContext
[2023-01-22T08:09:20.429+0000] {spark_submit.py:495} INFO - 23/01/22 08:09:20 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T08:09:20.431+0000] {spark_submit.py:495} INFO - 23/01/22 08:09:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-c44c1275-6924-4a99-9837-16b12edf74bb/pyspark-ea71ce56-3c4f-4ae9-9967-d26f55a68611
[2023-01-22T08:09:20.441+0000] {spark_submit.py:495} INFO - 23/01/22 08:09:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-c44c1275-6924-4a99-9837-16b12edf74bb
[2023-01-22T08:09:20.455+0000] {spark_submit.py:495} INFO - 23/01/22 08:09:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-486ca48a-14a5-4ffc-b869-ebd9afdc40b3
[2023-01-22T08:09:20.681+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-22T08:09:20.690+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230122T080859, end_date=20230122T080920
[2023-01-22T08:09:20.716+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 335 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 3206)
[2023-01-22T08:09:20.765+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T08:09:20.798+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T09:15:45.563+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T09:15:45.589+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T09:15:45.589+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T09:15:45.589+0000] {taskinstance.py:1284} INFO - Starting attempt 3 of 4
[2023-01-22T09:15:45.589+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T09:15:45.611+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-22T09:15:45.623+0000] {standard_task_runner.py:55} INFO - Started process 629 to run task
[2023-01-22T09:15:45.632+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '386', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp0xtguy6l']
[2023-01-22T09:15:45.636+0000] {standard_task_runner.py:83} INFO - Job 386: Subtask transform_stage_generation
[2023-01-22T09:15:45.773+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 266f60b86faa
[2023-01-22T09:15:45.915+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=3
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-22T09:15:45.931+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T09:15:45.934+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-22T09:15:51.014+0000] {spark_submit.py:495} INFO - 23/01/22 09:15:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T09:15:51.561+0000] {spark_submit.py:495} INFO - 23/01/22 09:15:51 WARN DependencyUtils: Local jar /opt/***/gcs-connector-hadoop3-latest.jar does not exist, skipping.
[2023-01-22T09:15:51.562+0000] {spark_submit.py:495} INFO - 23/01/22 09:15:51 WARN DependencyUtils: Local jar /opt/***/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-22T09:15:54.385+0000] {spark_submit.py:495} INFO - /opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010100.json
[2023-01-22T09:15:54.535+0000] {spark_submit.py:495} INFO - 23/01/22 09:15:54 INFO SparkContext: Running Spark version 3.3.1
[2023-01-22T09:15:54.593+0000] {spark_submit.py:495} INFO - 23/01/22 09:15:54 INFO ResourceUtils: ==============================================================
[2023-01-22T09:15:54.594+0000] {spark_submit.py:495} INFO - 23/01/22 09:15:54 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-22T09:15:54.595+0000] {spark_submit.py:495} INFO - 23/01/22 09:15:54 INFO ResourceUtils: ==============================================================
[2023-01-22T09:15:54.596+0000] {spark_submit.py:495} INFO - 23/01/22 09:15:54 INFO SparkContext: Submitted application: gcp_playground
[2023-01-22T09:15:54.650+0000] {spark_submit.py:495} INFO - 23/01/22 09:15:54 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-22T09:15:54.662+0000] {spark_submit.py:495} INFO - 23/01/22 09:15:54 INFO ResourceProfile: Limiting resource is cpu
[2023-01-22T09:15:54.665+0000] {spark_submit.py:495} INFO - 23/01/22 09:15:54 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-22T09:15:54.802+0000] {spark_submit.py:495} INFO - 23/01/22 09:15:54 INFO SecurityManager: Changing view acls to: ***
[2023-01-22T09:15:54.803+0000] {spark_submit.py:495} INFO - 23/01/22 09:15:54 INFO SecurityManager: Changing modify acls to: ***
[2023-01-22T09:15:54.804+0000] {spark_submit.py:495} INFO - 23/01/22 09:15:54 INFO SecurityManager: Changing view acls groups to:
[2023-01-22T09:15:54.804+0000] {spark_submit.py:495} INFO - 23/01/22 09:15:54 INFO SecurityManager: Changing modify acls groups to:
[2023-01-22T09:15:54.804+0000] {spark_submit.py:495} INFO - 23/01/22 09:15:54 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-22T09:15:55.667+0000] {spark_submit.py:495} INFO - 23/01/22 09:15:55 INFO Utils: Successfully started service 'sparkDriver' on port 45587.
[2023-01-22T09:15:55.765+0000] {spark_submit.py:495} INFO - 23/01/22 09:15:55 INFO SparkEnv: Registering MapOutputTracker
[2023-01-22T09:15:55.889+0000] {spark_submit.py:495} INFO - 23/01/22 09:15:55 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-22T09:15:56.006+0000] {spark_submit.py:495} INFO - 23/01/22 09:15:56 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-22T09:15:56.009+0000] {spark_submit.py:495} INFO - 23/01/22 09:15:56 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-22T09:15:56.020+0000] {spark_submit.py:495} INFO - 23/01/22 09:15:56 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-22T09:15:56.110+0000] {spark_submit.py:495} INFO - 23/01/22 09:15:56 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-a1e08221-36df-47bb-a53a-228379180549
[2023-01-22T09:15:56.190+0000] {spark_submit.py:495} INFO - 23/01/22 09:15:56 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-22T09:15:56.291+0000] {spark_submit.py:495} INFO - 23/01/22 09:15:56 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-22T09:15:57.298+0000] {spark_submit.py:495} INFO - 23/01/22 09:15:57 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-22T09:15:57.345+0000] {spark_submit.py:495} INFO - 23/01/22 09:15:57 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2023-01-22T09:15:57.542+0000] {spark_submit.py:495} INFO - 23/01/22 09:15:57 INFO SparkContext: Added JAR /opt/spark/gcs-connector-hadoop3-latest.jar at spark://266f60b86faa:45587/jars/gcs-connector-hadoop3-latest.jar with timestamp 1674378954523
[2023-01-22T09:15:57.916+0000] {spark_submit.py:495} INFO - 23/01/22 09:15:57 INFO Executor: Starting executor ID driver on host 266f60b86faa
[2023-01-22T09:15:57.954+0000] {spark_submit.py:495} INFO - 23/01/22 09:15:57 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-22T09:15:58.030+0000] {spark_submit.py:495} INFO - 23/01/22 09:15:58 INFO Executor: Fetching spark://266f60b86faa:45587/jars/gcs-connector-hadoop3-latest.jar with timestamp 1674378954523
[2023-01-22T09:15:58.330+0000] {spark_submit.py:495} INFO - 23/01/22 09:15:58 INFO TransportClientFactory: Successfully created connection to 266f60b86faa/192.168.64.4:45587 after 194 ms (0 ms spent in bootstraps)
[2023-01-22T09:15:58.456+0000] {spark_submit.py:495} INFO - 23/01/22 09:15:58 INFO Utils: Fetching spark://266f60b86faa:45587/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-138d3c7b-3b2b-49b8-8275-bfbeaef5a847/userFiles-a9aba28c-ddb2-498c-a8f7-b84d124d01d7/fetchFileTemp2183060253725526959.tmp
[2023-01-22T09:16:00.349+0000] {spark_submit.py:495} INFO - 23/01/22 09:16:00 INFO Executor: Adding file:/tmp/spark-138d3c7b-3b2b-49b8-8275-bfbeaef5a847/userFiles-a9aba28c-ddb2-498c-a8f7-b84d124d01d7/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-22T09:16:00.400+0000] {spark_submit.py:495} INFO - 23/01/22 09:16:00 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33401.
[2023-01-22T09:16:00.401+0000] {spark_submit.py:495} INFO - 23/01/22 09:16:00 INFO NettyBlockTransferService: Server created on 266f60b86faa:33401
[2023-01-22T09:16:00.430+0000] {spark_submit.py:495} INFO - 23/01/22 09:16:00 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-22T09:16:00.488+0000] {spark_submit.py:495} INFO - 23/01/22 09:16:00 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 266f60b86faa, 33401, None)
[2023-01-22T09:16:00.522+0000] {spark_submit.py:495} INFO - 23/01/22 09:16:00 INFO BlockManagerMasterEndpoint: Registering block manager 266f60b86faa:33401 with 434.4 MiB RAM, BlockManagerId(driver, 266f60b86faa, 33401, None)
[2023-01-22T09:16:00.552+0000] {spark_submit.py:495} INFO - 23/01/22 09:16:00 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 266f60b86faa, 33401, None)
[2023-01-22T09:16:00.556+0000] {spark_submit.py:495} INFO - 23/01/22 09:16:00 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 266f60b86faa, 33401, None)
[2023-01-22T09:16:03.809+0000] {spark_submit.py:495} INFO - 23/01/22 09:16:03 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-22T09:16:03.817+0000] {spark_submit.py:495} INFO - 23/01/22 09:16:03 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-22T09:16:07.558+0000] {spark_submit.py:495} INFO - 23/01/22 09:16:07 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: gs://entsoe_analytics_1009//opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010100.json.
[2023-01-22T09:16:07.562+0000] {spark_submit.py:495} INFO - org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "gs"
[2023-01-22T09:16:07.568+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2023-01-22T09:16:07.569+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T09:16:07.573+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T09:16:07.579+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T09:16:07.580+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T09:16:07.581+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T09:16:07.586+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T09:16:07.594+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)
[2023-01-22T09:16:07.595+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:370)
[2023-01-22T09:16:07.596+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T09:16:07.596+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T09:16:07.597+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T09:16:07.598+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T09:16:07.598+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T09:16:07.599+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T09:16:07.600+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T09:16:07.600+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T09:16:07.601+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T09:16:07.601+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T09:16:07.602+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T09:16:07.602+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T09:16:07.603+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T09:16:07.603+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T09:16:07.604+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T09:16:07.604+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T09:16:07.605+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T09:16:07.660+0000] {spark_submit.py:495} INFO - Error, ada An error occurred while calling o38.load.
[2023-01-22T09:16:07.660+0000] {spark_submit.py:495} INFO - : org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "gs"
[2023-01-22T09:16:07.661+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2023-01-22T09:16:07.661+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T09:16:07.661+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T09:16:07.662+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T09:16:07.662+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T09:16:07.662+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T09:16:07.663+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T09:16:07.663+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:752)
[2023-01-22T09:16:07.663+0000] {spark_submit.py:495} INFO - at scala.collection.immutable.List.map(List.scala:246)
[2023-01-22T09:16:07.664+0000] {spark_submit.py:495} INFO - at scala.collection.immutable.List.map(List.scala:79)
[2023-01-22T09:16:07.664+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:750)
[2023-01-22T09:16:07.665+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:579)
[2023-01-22T09:16:07.665+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:408)
[2023-01-22T09:16:07.665+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T09:16:07.666+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T09:16:07.666+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T09:16:07.666+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T09:16:07.667+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T09:16:07.667+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T09:16:07.667+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T09:16:07.668+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T09:16:07.668+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T09:16:07.668+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T09:16:07.669+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T09:16:07.669+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T09:16:07.669+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T09:16:07.670+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T09:16:07.670+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T09:16:07.670+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T09:16:07.671+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T09:16:07.671+0000] {spark_submit.py:495} INFO - 
[2023-01-22T09:16:07.672+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T09:16:07.672+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 68, in <module>
[2023-01-22T09:16:07.672+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T09:16:07.673+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 55, in main
[2023-01-22T09:16:07.674+0000] {spark_submit.py:495} INFO - df_spark.write.format("bigquery").option("project", "rafzul-analytics-1009") \
[2023-01-22T09:16:07.674+0000] {spark_submit.py:495} INFO - UnboundLocalError: local variable 'df_spark' referenced before assignment
[2023-01-22T09:16:07.829+0000] {spark_submit.py:495} INFO - 23/01/22 09:16:07 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-22T09:16:07.894+0000] {spark_submit.py:495} INFO - 23/01/22 09:16:07 INFO SparkUI: Stopped Spark web UI at http://266f60b86faa:4041
[2023-01-22T09:16:07.999+0000] {spark_submit.py:495} INFO - 23/01/22 09:16:07 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-22T09:16:08.064+0000] {spark_submit.py:495} INFO - 23/01/22 09:16:08 INFO MemoryStore: MemoryStore cleared
[2023-01-22T09:16:08.065+0000] {spark_submit.py:495} INFO - 23/01/22 09:16:08 INFO BlockManager: BlockManager stopped
[2023-01-22T09:16:08.138+0000] {spark_submit.py:495} INFO - 23/01/22 09:16:08 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-22T09:16:08.158+0000] {spark_submit.py:495} INFO - 23/01/22 09:16:08 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-22T09:16:08.246+0000] {spark_submit.py:495} INFO - 23/01/22 09:16:08 INFO SparkContext: Successfully stopped SparkContext
[2023-01-22T09:16:08.263+0000] {spark_submit.py:495} INFO - 23/01/22 09:16:08 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T09:16:08.263+0000] {spark_submit.py:495} INFO - 23/01/22 09:16:08 INFO ShutdownHookManager: Deleting directory /tmp/spark-138d3c7b-3b2b-49b8-8275-bfbeaef5a847
[2023-01-22T09:16:08.287+0000] {spark_submit.py:495} INFO - 23/01/22 09:16:08 INFO ShutdownHookManager: Deleting directory /tmp/spark-138d3c7b-3b2b-49b8-8275-bfbeaef5a847/pyspark-559b556a-5994-4c7a-9855-b6a734f169b2
[2023-01-22T09:16:08.314+0000] {spark_submit.py:495} INFO - 23/01/22 09:16:08 INFO ShutdownHookManager: Deleting directory /tmp/spark-1d29b258-7849-4851-a8cf-0427b73152c2
[2023-01-22T09:16:08.654+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-22T09:16:08.663+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230122T091545, end_date=20230122T091608
[2023-01-22T09:16:08.721+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 386 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 629)
[2023-01-22T09:16:08.779+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T09:16:08.916+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T09:28:59.932+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T09:28:59.963+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T09:28:59.963+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T09:28:59.963+0000] {taskinstance.py:1284} INFO - Starting attempt 3 of 4
[2023-01-22T09:28:59.964+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T09:28:59.993+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-22T09:29:00.020+0000] {standard_task_runner.py:55} INFO - Started process 2200 to run task
[2023-01-22T09:29:00.024+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '394', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpr2rc_igb']
[2023-01-22T09:29:00.028+0000] {standard_task_runner.py:83} INFO - Job 394: Subtask transform_stage_generation
[2023-01-22T09:29:00.186+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 266f60b86faa
[2023-01-22T09:29:00.351+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=3
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-22T09:29:00.372+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T09:29:00.374+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-22T09:29:15.067+0000] {spark_submit.py:495} INFO - /opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010100.json
[2023-01-22T09:29:15.698+0000] {spark_submit.py:495} INFO - 23/01/22 09:29:15 INFO SparkContext: Running Spark version 3.3.1
[2023-01-22T09:29:16.226+0000] {spark_submit.py:495} INFO - 23/01/22 09:29:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T09:29:16.997+0000] {spark_submit.py:495} INFO - 23/01/22 09:29:16 INFO ResourceUtils: ==============================================================
[2023-01-22T09:29:17.001+0000] {spark_submit.py:495} INFO - 23/01/22 09:29:16 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-22T09:29:17.004+0000] {spark_submit.py:495} INFO - 23/01/22 09:29:17 INFO ResourceUtils: ==============================================================
[2023-01-22T09:29:17.010+0000] {spark_submit.py:495} INFO - 23/01/22 09:29:17 INFO SparkContext: Submitted application: gcp_playground
[2023-01-22T09:29:17.174+0000] {spark_submit.py:495} INFO - 23/01/22 09:29:17 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-22T09:29:17.212+0000] {spark_submit.py:495} INFO - 23/01/22 09:29:17 INFO ResourceProfile: Limiting resource is cpu
[2023-01-22T09:29:17.222+0000] {spark_submit.py:495} INFO - 23/01/22 09:29:17 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-22T09:29:17.560+0000] {spark_submit.py:495} INFO - 23/01/22 09:29:17 INFO SecurityManager: Changing view acls to: ***
[2023-01-22T09:29:17.564+0000] {spark_submit.py:495} INFO - 23/01/22 09:29:17 INFO SecurityManager: Changing modify acls to: ***
[2023-01-22T09:29:17.576+0000] {spark_submit.py:495} INFO - 23/01/22 09:29:17 INFO SecurityManager: Changing view acls groups to:
[2023-01-22T09:29:17.577+0000] {spark_submit.py:495} INFO - 23/01/22 09:29:17 INFO SecurityManager: Changing modify acls groups to:
[2023-01-22T09:29:17.578+0000] {spark_submit.py:495} INFO - 23/01/22 09:29:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-22T09:29:18.876+0000] {spark_submit.py:495} INFO - 23/01/22 09:29:18 INFO Utils: Successfully started service 'sparkDriver' on port 42157.
[2023-01-22T09:29:19.022+0000] {spark_submit.py:495} INFO - 23/01/22 09:29:19 INFO SparkEnv: Registering MapOutputTracker
[2023-01-22T09:29:19.202+0000] {spark_submit.py:495} INFO - 23/01/22 09:29:19 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-22T09:29:19.322+0000] {spark_submit.py:495} INFO - 23/01/22 09:29:19 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-22T09:29:19.323+0000] {spark_submit.py:495} INFO - 23/01/22 09:29:19 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-22T09:29:19.338+0000] {spark_submit.py:495} INFO - 23/01/22 09:29:19 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-22T09:29:19.504+0000] {spark_submit.py:495} INFO - 23/01/22 09:29:19 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-36186856-796e-4315-ab7a-63d2c93f23d0
[2023-01-22T09:29:19.634+0000] {spark_submit.py:495} INFO - 23/01/22 09:29:19 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-22T09:29:19.786+0000] {spark_submit.py:495} INFO - 23/01/22 09:29:19 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-22T09:29:20.675+0000] {spark_submit.py:495} INFO - 23/01/22 09:29:20 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-22T09:29:20.701+0000] {spark_submit.py:495} INFO - 23/01/22 09:29:20 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2023-01-22T09:29:20.776+0000] {spark_submit.py:495} INFO - 23/01/22 09:29:20 INFO SparkContext: Added JAR /opt/spark/gcs-connector-hadoop3-latest.jar at spark://266f60b86faa:42157/jars/gcs-connector-hadoop3-latest.jar with timestamp 1674379755633
[2023-01-22T09:29:20.777+0000] {spark_submit.py:495} INFO - 23/01/22 09:29:20 INFO SparkContext: Added JAR /opt/spark/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://266f60b86faa:42157/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1674379755633
[2023-01-22T09:29:21.101+0000] {spark_submit.py:495} INFO - 23/01/22 09:29:21 INFO Executor: Starting executor ID driver on host 266f60b86faa
[2023-01-22T09:29:21.170+0000] {spark_submit.py:495} INFO - 23/01/22 09:29:21 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-22T09:29:21.275+0000] {spark_submit.py:495} INFO - 23/01/22 09:29:21 INFO Executor: Fetching spark://266f60b86faa:42157/jars/gcs-connector-hadoop3-latest.jar with timestamp 1674379755633
[2023-01-22T09:29:21.678+0000] {spark_submit.py:495} INFO - 23/01/22 09:29:21 INFO TransportClientFactory: Successfully created connection to 266f60b86faa/192.168.64.4:42157 after 227 ms (0 ms spent in bootstraps)
[2023-01-22T09:29:21.795+0000] {spark_submit.py:495} INFO - 23/01/22 09:29:21 INFO Utils: Fetching spark://266f60b86faa:42157/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-9e3dc6aa-644c-40b8-b9fb-40fc1613ca4b/userFiles-8826a082-24f8-43ec-8f58-6815061937a9/fetchFileTemp4279742580017394494.tmp
[2023-01-22T09:29:22.458+0000] {spark_submit.py:495} INFO - 23/01/22 09:29:22 INFO Executor: Adding file:/tmp/spark-9e3dc6aa-644c-40b8-b9fb-40fc1613ca4b/userFiles-8826a082-24f8-43ec-8f58-6815061937a9/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-22T09:29:22.458+0000] {spark_submit.py:495} INFO - 23/01/22 09:29:22 INFO Executor: Fetching spark://266f60b86faa:42157/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1674379755633
[2023-01-22T09:29:22.478+0000] {spark_submit.py:495} INFO - 23/01/22 09:29:22 INFO Utils: Fetching spark://266f60b86faa:42157/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-9e3dc6aa-644c-40b8-b9fb-40fc1613ca4b/userFiles-8826a082-24f8-43ec-8f58-6815061937a9/fetchFileTemp11330550569166836759.tmp
[2023-01-22T09:29:23.316+0000] {spark_submit.py:495} INFO - 23/01/22 09:29:23 INFO Executor: Adding file:/tmp/spark-9e3dc6aa-644c-40b8-b9fb-40fc1613ca4b/userFiles-8826a082-24f8-43ec-8f58-6815061937a9/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-22T09:29:23.327+0000] {spark_submit.py:495} INFO - 23/01/22 09:29:23 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36491.
[2023-01-22T09:29:23.327+0000] {spark_submit.py:495} INFO - 23/01/22 09:29:23 INFO NettyBlockTransferService: Server created on 266f60b86faa:36491
[2023-01-22T09:29:23.331+0000] {spark_submit.py:495} INFO - 23/01/22 09:29:23 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-22T09:29:23.344+0000] {spark_submit.py:495} INFO - 23/01/22 09:29:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 266f60b86faa, 36491, None)
[2023-01-22T09:29:23.359+0000] {spark_submit.py:495} INFO - 23/01/22 09:29:23 INFO BlockManagerMasterEndpoint: Registering block manager 266f60b86faa:36491 with 434.4 MiB RAM, BlockManagerId(driver, 266f60b86faa, 36491, None)
[2023-01-22T09:29:23.365+0000] {spark_submit.py:495} INFO - 23/01/22 09:29:23 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 266f60b86faa, 36491, None)
[2023-01-22T09:29:23.367+0000] {spark_submit.py:495} INFO - 23/01/22 09:29:23 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 266f60b86faa, 36491, None)
[2023-01-22T09:29:25.874+0000] {spark_submit.py:495} INFO - 23/01/22 09:29:25 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-22T09:29:25.925+0000] {spark_submit.py:495} INFO - 23/01/22 09:29:25 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-22T09:29:29.914+0000] {spark_submit.py:495} INFO - 23/01/22 09:29:29 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: gs://entsoe_analytics_1009//opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010100.json.
[2023-01-22T09:29:29.916+0000] {spark_submit.py:495} INFO - org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "gs"
[2023-01-22T09:29:29.917+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2023-01-22T09:29:29.920+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T09:29:29.923+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T09:29:29.923+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T09:29:29.923+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T09:29:29.926+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T09:29:29.943+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T09:29:29.945+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)
[2023-01-22T09:29:29.945+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:370)
[2023-01-22T09:29:29.946+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T09:29:29.948+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T09:29:29.952+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T09:29:29.956+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T09:29:29.956+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T09:29:29.956+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T09:29:29.960+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T09:29:29.962+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T09:29:29.964+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T09:29:29.965+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T09:29:29.966+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T09:29:29.967+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T09:29:29.967+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T09:29:29.967+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T09:29:29.969+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T09:29:29.971+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T09:29:29.972+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T09:29:30.163+0000] {spark_submit.py:495} INFO - Error, ada An error occurred while calling o37.load.
[2023-01-22T09:29:30.164+0000] {spark_submit.py:495} INFO - : org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "gs"
[2023-01-22T09:29:30.164+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2023-01-22T09:29:30.165+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T09:29:30.165+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T09:29:30.165+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T09:29:30.166+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T09:29:30.167+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T09:29:30.168+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T09:29:30.169+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:752)
[2023-01-22T09:29:30.170+0000] {spark_submit.py:495} INFO - at scala.collection.immutable.List.map(List.scala:246)
[2023-01-22T09:29:30.183+0000] {spark_submit.py:495} INFO - at scala.collection.immutable.List.map(List.scala:79)
[2023-01-22T09:29:30.184+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:750)
[2023-01-22T09:29:30.185+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:579)
[2023-01-22T09:29:30.186+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:408)
[2023-01-22T09:29:30.186+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T09:29:30.186+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T09:29:30.186+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T09:29:30.187+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T09:29:30.188+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T09:29:30.189+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T09:29:30.190+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T09:29:30.204+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T09:29:30.205+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T09:29:30.206+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T09:29:30.211+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T09:29:30.212+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T09:29:30.213+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T09:29:30.224+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T09:29:30.225+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T09:29:30.227+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T09:29:30.228+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T09:29:30.229+0000] {spark_submit.py:495} INFO - 
[2023-01-22T09:29:30.252+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T09:29:30.275+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 68, in <module>
[2023-01-22T09:29:30.283+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T09:29:30.284+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 55, in main
[2023-01-22T09:29:30.285+0000] {spark_submit.py:495} INFO - df_spark.write.format("bigquery").option("project", "rafzul-analytics-1009") \
[2023-01-22T09:29:30.309+0000] {spark_submit.py:495} INFO - UnboundLocalError: local variable 'df_spark' referenced before assignment
[2023-01-22T09:29:30.646+0000] {spark_submit.py:495} INFO - 23/01/22 09:29:30 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-22T09:29:30.754+0000] {spark_submit.py:495} INFO - 23/01/22 09:29:30 INFO SparkUI: Stopped Spark web UI at http://266f60b86faa:4041
[2023-01-22T09:29:30.864+0000] {spark_submit.py:495} INFO - 23/01/22 09:29:30 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-22T09:29:31.006+0000] {spark_submit.py:495} INFO - 23/01/22 09:29:31 INFO MemoryStore: MemoryStore cleared
[2023-01-22T09:29:31.022+0000] {spark_submit.py:495} INFO - 23/01/22 09:29:31 INFO BlockManager: BlockManager stopped
[2023-01-22T09:29:31.106+0000] {spark_submit.py:495} INFO - 23/01/22 09:29:31 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-22T09:29:31.130+0000] {spark_submit.py:495} INFO - 23/01/22 09:29:31 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-22T09:29:31.206+0000] {spark_submit.py:495} INFO - 23/01/22 09:29:31 INFO SparkContext: Successfully stopped SparkContext
[2023-01-22T09:29:31.213+0000] {spark_submit.py:495} INFO - 23/01/22 09:29:31 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T09:29:31.217+0000] {spark_submit.py:495} INFO - 23/01/22 09:29:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-d44bc9cc-c748-43a9-9bf7-62da82a36ee4
[2023-01-22T09:29:31.257+0000] {spark_submit.py:495} INFO - 23/01/22 09:29:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-9e3dc6aa-644c-40b8-b9fb-40fc1613ca4b
[2023-01-22T09:29:31.278+0000] {spark_submit.py:495} INFO - 23/01/22 09:29:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-9e3dc6aa-644c-40b8-b9fb-40fc1613ca4b/pyspark-f417c0c2-c5a7-4d88-b020-54b8f697d969
[2023-01-22T09:29:31.520+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-22T09:29:31.525+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230122T092859, end_date=20230122T092931
[2023-01-22T09:29:31.568+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 394 for task transform_stage_generation (Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 2200)
[2023-01-22T09:29:31.629+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T09:29:31.714+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T10:14:15.055+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T10:14:15.066+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T10:14:15.067+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T10:14:15.067+0000] {taskinstance.py:1284} INFO - Starting attempt 3 of 4
[2023-01-22T10:14:15.067+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T10:14:15.082+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-22T10:14:15.091+0000] {standard_task_runner.py:55} INFO - Started process 6352 to run task
[2023-01-22T10:14:15.095+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '411', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmps6vvjft1']
[2023-01-22T10:14:15.097+0000] {standard_task_runner.py:83} INFO - Job 411: Subtask transform_stage_generation
[2023-01-22T10:14:15.159+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 266f60b86faa
[2023-01-22T10:14:15.227+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=3
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-22T10:14:15.236+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T10:14:15.237+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-22T10:14:19.932+0000] {spark_submit.py:495} INFO - /opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010100.json
[2023-01-22T10:14:20.039+0000] {spark_submit.py:495} INFO - 23/01/22 10:14:20 INFO SparkContext: Running Spark version 3.3.1
[2023-01-22T10:14:20.250+0000] {spark_submit.py:495} INFO - 23/01/22 10:14:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T10:14:20.692+0000] {spark_submit.py:495} INFO - 23/01/22 10:14:20 INFO ResourceUtils: ==============================================================
[2023-01-22T10:14:20.694+0000] {spark_submit.py:495} INFO - 23/01/22 10:14:20 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-22T10:14:20.695+0000] {spark_submit.py:495} INFO - 23/01/22 10:14:20 INFO ResourceUtils: ==============================================================
[2023-01-22T10:14:20.697+0000] {spark_submit.py:495} INFO - 23/01/22 10:14:20 INFO SparkContext: Submitted application: gcp_playground
[2023-01-22T10:14:20.749+0000] {spark_submit.py:495} INFO - 23/01/22 10:14:20 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-22T10:14:20.761+0000] {spark_submit.py:495} INFO - 23/01/22 10:14:20 INFO ResourceProfile: Limiting resource is cpu
[2023-01-22T10:14:20.763+0000] {spark_submit.py:495} INFO - 23/01/22 10:14:20 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-22T10:14:20.886+0000] {spark_submit.py:495} INFO - 23/01/22 10:14:20 INFO SecurityManager: Changing view acls to: ***
[2023-01-22T10:14:20.888+0000] {spark_submit.py:495} INFO - 23/01/22 10:14:20 INFO SecurityManager: Changing modify acls to: ***
[2023-01-22T10:14:20.889+0000] {spark_submit.py:495} INFO - 23/01/22 10:14:20 INFO SecurityManager: Changing view acls groups to:
[2023-01-22T10:14:20.893+0000] {spark_submit.py:495} INFO - 23/01/22 10:14:20 INFO SecurityManager: Changing modify acls groups to:
[2023-01-22T10:14:20.895+0000] {spark_submit.py:495} INFO - 23/01/22 10:14:20 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-22T10:14:21.693+0000] {spark_submit.py:495} INFO - 23/01/22 10:14:21 INFO Utils: Successfully started service 'sparkDriver' on port 41931.
[2023-01-22T10:14:21.797+0000] {spark_submit.py:495} INFO - 23/01/22 10:14:21 INFO SparkEnv: Registering MapOutputTracker
[2023-01-22T10:14:21.927+0000] {spark_submit.py:495} INFO - 23/01/22 10:14:21 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-22T10:14:21.979+0000] {spark_submit.py:495} INFO - 23/01/22 10:14:21 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-22T10:14:21.981+0000] {spark_submit.py:495} INFO - 23/01/22 10:14:21 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-22T10:14:21.993+0000] {spark_submit.py:495} INFO - 23/01/22 10:14:21 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-22T10:14:22.041+0000] {spark_submit.py:495} INFO - 23/01/22 10:14:22 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d020850c-77db-487d-9086-f285d66a4226
[2023-01-22T10:14:22.075+0000] {spark_submit.py:495} INFO - 23/01/22 10:14:22 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-22T10:14:22.119+0000] {spark_submit.py:495} INFO - 23/01/22 10:14:22 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-22T10:14:22.487+0000] {spark_submit.py:495} INFO - 23/01/22 10:14:22 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-01-22T10:14:22.560+0000] {spark_submit.py:495} INFO - 23/01/22 10:14:22 INFO SparkContext: Added JAR /opt/spark/gcs-connector-hadoop3-latest.jar at spark://266f60b86faa:41931/jars/gcs-connector-hadoop3-latest.jar with timestamp 1674382460027
[2023-01-22T10:14:22.560+0000] {spark_submit.py:495} INFO - 23/01/22 10:14:22 INFO SparkContext: Added JAR /opt/spark/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://266f60b86faa:41931/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1674382460027
[2023-01-22T10:14:22.773+0000] {spark_submit.py:495} INFO - 23/01/22 10:14:22 INFO Executor: Starting executor ID driver on host 266f60b86faa
[2023-01-22T10:14:22.789+0000] {spark_submit.py:495} INFO - 23/01/22 10:14:22 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-22T10:14:22.833+0000] {spark_submit.py:495} INFO - 23/01/22 10:14:22 INFO Executor: Fetching spark://266f60b86faa:41931/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1674382460027
[2023-01-22T10:14:22.945+0000] {spark_submit.py:495} INFO - 23/01/22 10:14:22 INFO TransportClientFactory: Successfully created connection to 266f60b86faa/192.168.64.4:41931 after 66 ms (0 ms spent in bootstraps)
[2023-01-22T10:14:22.962+0000] {spark_submit.py:495} INFO - 23/01/22 10:14:22 INFO Utils: Fetching spark://266f60b86faa:41931/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-7c95a7ba-403d-43fb-9811-7259d3affb14/userFiles-c4fc30da-337f-4dcd-b830-85370d01a40d/fetchFileTemp16151625244225777055.tmp
[2023-01-22T10:14:23.239+0000] {spark_submit.py:495} INFO - 23/01/22 10:14:23 INFO Executor: Adding file:/tmp/spark-7c95a7ba-403d-43fb-9811-7259d3affb14/userFiles-c4fc30da-337f-4dcd-b830-85370d01a40d/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-22T10:14:23.239+0000] {spark_submit.py:495} INFO - 23/01/22 10:14:23 INFO Executor: Fetching spark://266f60b86faa:41931/jars/gcs-connector-hadoop3-latest.jar with timestamp 1674382460027
[2023-01-22T10:14:23.240+0000] {spark_submit.py:495} INFO - 23/01/22 10:14:23 INFO Utils: Fetching spark://266f60b86faa:41931/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-7c95a7ba-403d-43fb-9811-7259d3affb14/userFiles-c4fc30da-337f-4dcd-b830-85370d01a40d/fetchFileTemp10430380978470196920.tmp
[2023-01-22T10:14:23.395+0000] {spark_submit.py:495} INFO - 23/01/22 10:14:23 INFO Executor: Adding file:/tmp/spark-7c95a7ba-403d-43fb-9811-7259d3affb14/userFiles-c4fc30da-337f-4dcd-b830-85370d01a40d/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-22T10:14:23.404+0000] {spark_submit.py:495} INFO - 23/01/22 10:14:23 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42003.
[2023-01-22T10:14:23.405+0000] {spark_submit.py:495} INFO - 23/01/22 10:14:23 INFO NettyBlockTransferService: Server created on 266f60b86faa:42003
[2023-01-22T10:14:23.407+0000] {spark_submit.py:495} INFO - 23/01/22 10:14:23 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-22T10:14:23.421+0000] {spark_submit.py:495} INFO - 23/01/22 10:14:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 266f60b86faa, 42003, None)
[2023-01-22T10:14:23.427+0000] {spark_submit.py:495} INFO - 23/01/22 10:14:23 INFO BlockManagerMasterEndpoint: Registering block manager 266f60b86faa:42003 with 434.4 MiB RAM, BlockManagerId(driver, 266f60b86faa, 42003, None)
[2023-01-22T10:14:23.432+0000] {spark_submit.py:495} INFO - 23/01/22 10:14:23 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 266f60b86faa, 42003, None)
[2023-01-22T10:14:23.434+0000] {spark_submit.py:495} INFO - 23/01/22 10:14:23 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 266f60b86faa, 42003, None)
[2023-01-22T10:14:24.297+0000] {spark_submit.py:495} INFO - 23/01/22 10:14:24 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-22T10:14:24.308+0000] {spark_submit.py:495} INFO - 23/01/22 10:14:24 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-22T10:14:25.377+0000] {spark_submit.py:495} INFO - 23/01/22 10:14:25 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: gs://entsoe_analytics_1009//opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010100.json.
[2023-01-22T10:14:25.378+0000] {spark_submit.py:495} INFO - org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "gs"
[2023-01-22T10:14:25.378+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2023-01-22T10:14:25.378+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T10:14:25.378+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T10:14:25.378+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T10:14:25.379+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T10:14:25.379+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T10:14:25.379+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T10:14:25.379+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)
[2023-01-22T10:14:25.379+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:370)
[2023-01-22T10:14:25.379+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T10:14:25.379+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T10:14:25.380+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T10:14:25.380+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T10:14:25.380+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T10:14:25.380+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T10:14:25.380+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T10:14:25.380+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T10:14:25.380+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T10:14:25.381+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T10:14:25.381+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T10:14:25.381+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T10:14:25.381+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T10:14:25.381+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T10:14:25.381+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T10:14:25.382+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T10:14:25.382+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T10:14:25.417+0000] {spark_submit.py:495} INFO - Error, ada An error occurred while calling o37.load.
[2023-01-22T10:14:25.418+0000] {spark_submit.py:495} INFO - : org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "gs"
[2023-01-22T10:14:25.418+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2023-01-22T10:14:25.418+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T10:14:25.418+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T10:14:25.419+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T10:14:25.419+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T10:14:25.419+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T10:14:25.419+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T10:14:25.419+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:752)
[2023-01-22T10:14:25.419+0000] {spark_submit.py:495} INFO - at scala.collection.immutable.List.map(List.scala:246)
[2023-01-22T10:14:25.420+0000] {spark_submit.py:495} INFO - at scala.collection.immutable.List.map(List.scala:79)
[2023-01-22T10:14:25.420+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:750)
[2023-01-22T10:14:25.420+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:579)
[2023-01-22T10:14:25.420+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:408)
[2023-01-22T10:14:25.420+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T10:14:25.420+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T10:14:25.420+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T10:14:25.421+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T10:14:25.421+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T10:14:25.421+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T10:14:25.421+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T10:14:25.421+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T10:14:25.422+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T10:14:25.422+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T10:14:25.422+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T10:14:25.422+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T10:14:25.423+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T10:14:25.423+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T10:14:25.423+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T10:14:25.423+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T10:14:25.423+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T10:14:25.423+0000] {spark_submit.py:495} INFO - 
[2023-01-22T10:14:25.423+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T10:14:25.424+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 68, in <module>
[2023-01-22T10:14:25.424+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T10:14:25.424+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 55, in main
[2023-01-22T10:14:25.424+0000] {spark_submit.py:495} INFO - df_spark.write.format("bigquery").option("project", "rafzul-analytics-1009") \
[2023-01-22T10:14:25.424+0000] {spark_submit.py:495} INFO - UnboundLocalError: local variable 'df_spark' referenced before assignment
[2023-01-22T10:14:25.480+0000] {spark_submit.py:495} INFO - 23/01/22 10:14:25 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-22T10:14:25.495+0000] {spark_submit.py:495} INFO - 23/01/22 10:14:25 INFO SparkUI: Stopped Spark web UI at http://266f60b86faa:4040
[2023-01-22T10:14:25.515+0000] {spark_submit.py:495} INFO - 23/01/22 10:14:25 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-22T10:14:25.531+0000] {spark_submit.py:495} INFO - 23/01/22 10:14:25 INFO MemoryStore: MemoryStore cleared
[2023-01-22T10:14:25.534+0000] {spark_submit.py:495} INFO - 23/01/22 10:14:25 INFO BlockManager: BlockManager stopped
[2023-01-22T10:14:25.547+0000] {spark_submit.py:495} INFO - 23/01/22 10:14:25 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-22T10:14:25.550+0000] {spark_submit.py:495} INFO - 23/01/22 10:14:25 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-22T10:14:25.574+0000] {spark_submit.py:495} INFO - 23/01/22 10:14:25 INFO SparkContext: Successfully stopped SparkContext
[2023-01-22T10:14:25.574+0000] {spark_submit.py:495} INFO - 23/01/22 10:14:25 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T10:14:25.575+0000] {spark_submit.py:495} INFO - 23/01/22 10:14:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-cc5efb8b-eefd-407c-87d5-0af3523c199e
[2023-01-22T10:14:25.580+0000] {spark_submit.py:495} INFO - 23/01/22 10:14:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-7c95a7ba-403d-43fb-9811-7259d3affb14/pyspark-f3afe5ec-3689-4ed3-b2b1-89e46c0edc20
[2023-01-22T10:14:25.586+0000] {spark_submit.py:495} INFO - 23/01/22 10:14:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-7c95a7ba-403d-43fb-9811-7259d3affb14
[2023-01-22T10:14:25.700+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-22T10:14:25.705+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230122T101415, end_date=20230122T101425
[2023-01-22T10:14:25.722+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 411 for task transform_stage_generation (Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 6352)
[2023-01-22T10:14:25.738+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T10:14:25.757+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T10:58:13.705+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T10:58:13.711+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T10:58:13.711+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T10:58:13.711+0000] {taskinstance.py:1284} INFO - Starting attempt 3 of 4
[2023-01-22T10:58:13.712+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T10:58:13.720+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-22T10:58:13.726+0000] {standard_task_runner.py:55} INFO - Started process 3388 to run task
[2023-01-22T10:58:13.729+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '470', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpd00o4r9c']
[2023-01-22T10:58:13.731+0000] {standard_task_runner.py:83} INFO - Job 470: Subtask transform_stage_generation
[2023-01-22T10:58:13.775+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host ce1344c6e6d4
[2023-01-22T10:58:13.823+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=3
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-22T10:58:13.830+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T10:58:13.831+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-22T10:58:17.309+0000] {spark_submit.py:495} INFO - total_generation__DE_TENNET__202101010100__202101010200.json
[2023-01-22T10:58:17.422+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:17 INFO SparkContext: Running Spark version 3.3.1
[2023-01-22T10:58:17.533+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T10:58:17.702+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:17 INFO ResourceUtils: ==============================================================
[2023-01-22T10:58:17.703+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:17 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-22T10:58:17.703+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:17 INFO ResourceUtils: ==============================================================
[2023-01-22T10:58:17.704+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:17 INFO SparkContext: Submitted application: gcp_playground
[2023-01-22T10:58:17.737+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:17 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-22T10:58:17.744+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:17 INFO ResourceProfile: Limiting resource is cpu
[2023-01-22T10:58:17.746+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:17 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-22T10:58:17.830+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:17 INFO SecurityManager: Changing view acls to: ***
[2023-01-22T10:58:17.831+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:17 INFO SecurityManager: Changing modify acls to: ***
[2023-01-22T10:58:17.831+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:17 INFO SecurityManager: Changing view acls groups to:
[2023-01-22T10:58:17.832+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:17 INFO SecurityManager: Changing modify acls groups to:
[2023-01-22T10:58:17.832+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-22T10:58:18.387+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:18 INFO Utils: Successfully started service 'sparkDriver' on port 35073.
[2023-01-22T10:58:18.435+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:18 INFO SparkEnv: Registering MapOutputTracker
[2023-01-22T10:58:18.490+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:18 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-22T10:58:18.533+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:18 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-22T10:58:18.534+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:18 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-22T10:58:18.540+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:18 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-22T10:58:18.575+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:18 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b4b27142-e0b3-49f4-abc4-72b62cf1036f
[2023-01-22T10:58:18.601+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:18 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-22T10:58:18.634+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:18 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-22T10:58:18.930+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:18 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-01-22T10:58:18.994+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:18 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://ce1344c6e6d4:35073/jars/gcs-connector-hadoop3-latest.jar with timestamp 1674385097410
[2023-01-22T10:58:18.995+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:18 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://ce1344c6e6d4:35073/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1674385097410
[2023-01-22T10:58:19.112+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:19 INFO Executor: Starting executor ID driver on host ce1344c6e6d4
[2023-01-22T10:58:19.120+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:19 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-22T10:58:19.150+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:19 INFO Executor: Fetching spark://ce1344c6e6d4:35073/jars/gcs-connector-hadoop3-latest.jar with timestamp 1674385097410
[2023-01-22T10:58:19.246+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:19 INFO TransportClientFactory: Successfully created connection to ce1344c6e6d4/192.168.80.8:35073 after 70 ms (0 ms spent in bootstraps)
[2023-01-22T10:58:19.258+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:19 INFO Utils: Fetching spark://ce1344c6e6d4:35073/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-4ed266bf-6b3f-4395-9fa2-cfc72a7c31ca/userFiles-90d31b7e-391a-40a6-a604-eb86edac09b7/fetchFileTemp12298095566701353879.tmp
[2023-01-22T10:58:19.507+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:19 INFO Executor: Adding file:/tmp/spark-4ed266bf-6b3f-4395-9fa2-cfc72a7c31ca/userFiles-90d31b7e-391a-40a6-a604-eb86edac09b7/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-22T10:58:19.509+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:19 INFO Executor: Fetching spark://ce1344c6e6d4:35073/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1674385097410
[2023-01-22T10:58:19.514+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:19 INFO Utils: Fetching spark://ce1344c6e6d4:35073/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-4ed266bf-6b3f-4395-9fa2-cfc72a7c31ca/userFiles-90d31b7e-391a-40a6-a604-eb86edac09b7/fetchFileTemp13333333579475137572.tmp
[2023-01-22T10:58:19.710+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:19 INFO Executor: Adding file:/tmp/spark-4ed266bf-6b3f-4395-9fa2-cfc72a7c31ca/userFiles-90d31b7e-391a-40a6-a604-eb86edac09b7/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-22T10:58:19.724+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:19 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39763.
[2023-01-22T10:58:19.724+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:19 INFO NettyBlockTransferService: Server created on ce1344c6e6d4:39763
[2023-01-22T10:58:19.727+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:19 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-22T10:58:19.744+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:19 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ce1344c6e6d4, 39763, None)
[2023-01-22T10:58:19.748+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:19 INFO BlockManagerMasterEndpoint: Registering block manager ce1344c6e6d4:39763 with 434.4 MiB RAM, BlockManagerId(driver, ce1344c6e6d4, 39763, None)
[2023-01-22T10:58:19.752+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:19 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ce1344c6e6d4, 39763, None)
[2023-01-22T10:58:19.754+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:19 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ce1344c6e6d4, 39763, None)
[2023-01-22T10:58:20.935+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:20 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-22T10:58:20.944+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:20 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-22T10:58:23.922+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:23 INFO InMemoryFileIndex: It took 170 ms to list leaf files for 1 paths.
[2023-01-22T10:58:24.220+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:24 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.8 KiB, free 434.2 MiB)
[2023-01-22T10:58:24.329+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:24 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-22T10:58:24.334+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:24 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ce1344c6e6d4:39763 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-22T10:58:24.340+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:24 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-22T10:58:24.879+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:24 INFO FileInputFormat: Total input files to process : 1
[2023-01-22T10:58:24.920+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:24 INFO FileInputFormat: Total input files to process : 1
[2023-01-22T10:58:24.941+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:24 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-22T10:58:24.963+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:24 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-22T10:58:24.964+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:24 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-22T10:58:24.964+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:24 INFO DAGScheduler: Parents of final stage: List()
[2023-01-22T10:58:24.968+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:24 INFO DAGScheduler: Missing parents: List()
[2023-01-22T10:58:24.974+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:24 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-22T10:58:25.059+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:25 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-22T10:58:25.065+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:25 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.2 MiB)
[2023-01-22T10:58:25.066+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:25 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on ce1344c6e6d4:39763 (size: 4.5 KiB, free: 434.4 MiB)
[2023-01-22T10:58:25.067+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:25 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-22T10:58:25.093+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-22T10:58:25.095+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:25 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-22T10:58:25.171+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:25 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (ce1344c6e6d4, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-22T10:58:25.189+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:25 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-22T10:58:25.312+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:25 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010100__202101010200.json:0+9369
[2023-01-22T10:58:25.633+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:25 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3437 bytes result sent to driver
[2023-01-22T10:58:25.655+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:25 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 502 ms on ce1344c6e6d4 (executor driver) (1/1)
[2023-01-22T10:58:25.664+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:25 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-22T10:58:25.674+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:25 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 0.675 s
[2023-01-22T10:58:25.682+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:25 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-22T10:58:25.683+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-22T10:58:25.686+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:25 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 0.743987 s
[2023-01-22T10:58:26.331+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:26 INFO BlockManagerInfo: Removed broadcast_1_piece0 on ce1344c6e6d4:39763 in memory (size: 4.5 KiB, free: 434.4 MiB)
[2023-01-22T10:58:26.337+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:26 INFO BlockManagerInfo: Removed broadcast_0_piece0 on ce1344c6e6d4:39763 in memory (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-22T10:58:30.512+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:30 INFO FileSourceStrategy: Pushed Filters:
[2023-01-22T10:58:30.515+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:30 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-22T10:58:30.519+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:30 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-22T10:58:30.673+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:30 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-22T10:58:30.735+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-22T10:58:30.735+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-22T10:58:30.740+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-22T10:58:30.740+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-22T10:58:30.741+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-22T10:58:30.745+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-22T10:58:31.034+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:31 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.2 KiB, free 434.2 MiB)
[2023-01-22T10:58:31.054+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:31 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-22T10:58:31.055+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:31 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on ce1344c6e6d4:39763 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-22T10:58:31.060+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:31 INFO SparkContext: Created broadcast 2 from save at BigQueryWriteHelper.java:105
[2023-01-22T10:58:31.107+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:31 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-22T10:58:31.291+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:31 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-22T10:58:31.292+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:31 INFO DAGScheduler: Got job 1 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-22T10:58:31.293+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:31 INFO DAGScheduler: Final stage: ResultStage 1 (save at BigQueryWriteHelper.java:105)
[2023-01-22T10:58:31.293+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:31 INFO DAGScheduler: Parents of final stage: List()
[2023-01-22T10:58:31.293+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:31 INFO DAGScheduler: Missing parents: List()
[2023-01-22T10:58:31.294+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:31 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-22T10:58:31.374+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:31 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 215.9 KiB, free 434.0 MiB)
[2023-01-22T10:58:31.384+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:31 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 77.2 KiB, free 433.9 MiB)
[2023-01-22T10:58:31.385+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:31 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on ce1344c6e6d4:39763 (size: 77.2 KiB, free: 434.3 MiB)
[2023-01-22T10:58:31.386+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:31 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-22T10:58:31.388+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-22T10:58:31.388+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:31 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-22T10:58:31.396+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:31 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (ce1344c6e6d4, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-22T10:58:31.398+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:31 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-22T10:58:31.520+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-22T10:58:31.521+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-22T10:58:31.521+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:31 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-22T10:58:31.522+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-22T10:58:31.522+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-22T10:58:31.522+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-22T10:58:31.528+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:31 INFO CodecConfig: Compression: SNAPPY
[2023-01-22T10:58:31.533+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:31 INFO CodecConfig: Compression: SNAPPY
[2023-01-22T10:58:31.566+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:31 INFO ParquetOutputFormat: Parquet block size to 134217728
[2023-01-22T10:58:31.567+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:31 INFO ParquetOutputFormat: Validation is off
[2023-01-22T10:58:31.567+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:31 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-01-22T10:58:31.567+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:31 INFO ParquetOutputFormat: Parquet properties are:
[2023-01-22T10:58:31.567+0000] {spark_submit.py:495} INFO - Parquet page size to 1048576
[2023-01-22T10:58:31.568+0000] {spark_submit.py:495} INFO - Parquet dictionary page size to 1048576
[2023-01-22T10:58:31.568+0000] {spark_submit.py:495} INFO - Dictionary is true
[2023-01-22T10:58:31.568+0000] {spark_submit.py:495} INFO - Writer version is: PARQUET_1_0
[2023-01-22T10:58:31.568+0000] {spark_submit.py:495} INFO - Page size checking is: estimated
[2023-01-22T10:58:31.568+0000] {spark_submit.py:495} INFO - Min row count for page size check is: 100
[2023-01-22T10:58:31.569+0000] {spark_submit.py:495} INFO - Max row count for page size check is: 10000
[2023-01-22T10:58:31.569+0000] {spark_submit.py:495} INFO - Truncate length for column indexes is: 64
[2023-01-22T10:58:31.569+0000] {spark_submit.py:495} INFO - Truncate length for statistics min/max  is: 2147483647
[2023-01-22T10:58:31.569+0000] {spark_submit.py:495} INFO - Bloom filter enabled: false
[2023-01-22T10:58:31.569+0000] {spark_submit.py:495} INFO - Max Bloom filter size for a column is 1048576
[2023-01-22T10:58:31.569+0000] {spark_submit.py:495} INFO - Bloom filter expected number of distinct values are: null
[2023-01-22T10:58:31.570+0000] {spark_submit.py:495} INFO - Page row count limit to 20000
[2023-01-22T10:58:31.570+0000] {spark_submit.py:495} INFO - Writing page checksums is: on
[2023-01-22T10:58:31.641+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:31 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-01-22T10:58:31.642+0000] {spark_submit.py:495} INFO - {
[2023-01-22T10:58:31.642+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-22T10:58:31.642+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-22T10:58:31.642+0000] {spark_submit.py:495} INFO - "name" : "GL_MarketDocument",
[2023-01-22T10:58:31.643+0000] {spark_submit.py:495} INFO - "type" : {
[2023-01-22T10:58:31.643+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-22T10:58:31.644+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-22T10:58:31.644+0000] {spark_submit.py:495} INFO - "name" : "@xmlns",
[2023-01-22T10:58:31.644+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:58:31.644+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:58:31.645+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:58:31.645+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:58:31.645+0000] {spark_submit.py:495} INFO - "name" : "TimeSeries",
[2023-01-22T10:58:31.645+0000] {spark_submit.py:495} INFO - "type" : {
[2023-01-22T10:58:31.646+0000] {spark_submit.py:495} INFO - "type" : "array",
[2023-01-22T10:58:31.646+0000] {spark_submit.py:495} INFO - "elementType" : {
[2023-01-22T10:58:31.646+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-22T10:58:31.646+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-22T10:58:31.646+0000] {spark_submit.py:495} INFO - "name" : "MktPSRType",
[2023-01-22T10:58:31.646+0000] {spark_submit.py:495} INFO - "type" : {
[2023-01-22T10:58:31.647+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-22T10:58:31.647+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-22T10:58:31.647+0000] {spark_submit.py:495} INFO - "name" : "psrType",
[2023-01-22T10:58:31.647+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:58:31.647+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:58:31.648+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:58:31.650+0000] {spark_submit.py:495} INFO - } ]
[2023-01-22T10:58:31.650+0000] {spark_submit.py:495} INFO - },
[2023-01-22T10:58:31.651+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:58:31.651+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:58:31.651+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:58:31.651+0000] {spark_submit.py:495} INFO - "name" : "Period",
[2023-01-22T10:58:31.651+0000] {spark_submit.py:495} INFO - "type" : {
[2023-01-22T10:58:31.651+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-22T10:58:31.651+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-22T10:58:31.652+0000] {spark_submit.py:495} INFO - "name" : "Point",
[2023-01-22T10:58:31.652+0000] {spark_submit.py:495} INFO - "type" : {
[2023-01-22T10:58:31.652+0000] {spark_submit.py:495} INFO - "type" : "array",
[2023-01-22T10:58:31.652+0000] {spark_submit.py:495} INFO - "elementType" : {
[2023-01-22T10:58:31.652+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-22T10:58:31.652+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-22T10:58:31.653+0000] {spark_submit.py:495} INFO - "name" : "position",
[2023-01-22T10:58:31.653+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:58:31.653+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:58:31.653+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:58:31.653+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:58:31.653+0000] {spark_submit.py:495} INFO - "name" : "quantity",
[2023-01-22T10:58:31.654+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:58:31.654+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:58:31.654+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:58:31.654+0000] {spark_submit.py:495} INFO - } ]
[2023-01-22T10:58:31.658+0000] {spark_submit.py:495} INFO - },
[2023-01-22T10:58:31.658+0000] {spark_submit.py:495} INFO - "containsNull" : true
[2023-01-22T10:58:31.658+0000] {spark_submit.py:495} INFO - },
[2023-01-22T10:58:31.658+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:58:31.658+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:58:31.658+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:58:31.659+0000] {spark_submit.py:495} INFO - "name" : "resolution",
[2023-01-22T10:58:31.659+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:58:31.659+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:58:31.659+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:58:31.659+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:58:31.659+0000] {spark_submit.py:495} INFO - "name" : "timeInterval",
[2023-01-22T10:58:31.660+0000] {spark_submit.py:495} INFO - "type" : {
[2023-01-22T10:58:31.660+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-22T10:58:31.660+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-22T10:58:31.660+0000] {spark_submit.py:495} INFO - "name" : "end",
[2023-01-22T10:58:31.660+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:58:31.660+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:58:31.660+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:58:31.661+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:58:31.661+0000] {spark_submit.py:495} INFO - "name" : "start",
[2023-01-22T10:58:31.661+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:58:31.661+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:58:31.661+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:58:31.661+0000] {spark_submit.py:495} INFO - } ]
[2023-01-22T10:58:31.662+0000] {spark_submit.py:495} INFO - },
[2023-01-22T10:58:31.662+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:58:31.662+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:58:31.662+0000] {spark_submit.py:495} INFO - } ]
[2023-01-22T10:58:31.662+0000] {spark_submit.py:495} INFO - },
[2023-01-22T10:58:31.662+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:58:31.672+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:58:31.672+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:58:31.672+0000] {spark_submit.py:495} INFO - "name" : "businessType",
[2023-01-22T10:58:31.673+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:58:31.673+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:58:31.673+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:58:31.673+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:58:31.673+0000] {spark_submit.py:495} INFO - "name" : "curveType",
[2023-01-22T10:58:31.673+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:58:31.674+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:58:31.674+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:58:31.674+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:58:31.674+0000] {spark_submit.py:495} INFO - "name" : "inBiddingZone_Domain.mRID",
[2023-01-22T10:58:31.674+0000] {spark_submit.py:495} INFO - "type" : {
[2023-01-22T10:58:31.674+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-22T10:58:31.675+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-22T10:58:31.675+0000] {spark_submit.py:495} INFO - "name" : "#text",
[2023-01-22T10:58:31.675+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:58:31.675+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:58:31.675+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:58:31.675+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:58:31.675+0000] {spark_submit.py:495} INFO - "name" : "@codingScheme",
[2023-01-22T10:58:31.676+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:58:31.676+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:58:31.676+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:58:31.676+0000] {spark_submit.py:495} INFO - } ]
[2023-01-22T10:58:31.676+0000] {spark_submit.py:495} INFO - },
[2023-01-22T10:58:31.676+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:58:31.677+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:58:31.677+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:58:31.677+0000] {spark_submit.py:495} INFO - "name" : "mRID",
[2023-01-22T10:58:31.677+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:58:31.677+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:58:31.677+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:58:31.678+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:58:31.678+0000] {spark_submit.py:495} INFO - "name" : "objectAggregation",
[2023-01-22T10:58:31.678+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:58:31.678+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:58:31.678+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:58:31.678+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:58:31.679+0000] {spark_submit.py:495} INFO - "name" : "outBiddingZone_Domain.mRID",
[2023-01-22T10:58:31.679+0000] {spark_submit.py:495} INFO - "type" : {
[2023-01-22T10:58:31.679+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-22T10:58:31.679+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-22T10:58:31.679+0000] {spark_submit.py:495} INFO - "name" : "#text",
[2023-01-22T10:58:31.680+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:58:31.680+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:58:31.680+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:58:31.680+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:58:31.680+0000] {spark_submit.py:495} INFO - "name" : "@codingScheme",
[2023-01-22T10:58:31.680+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:58:31.681+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:58:31.681+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:58:31.681+0000] {spark_submit.py:495} INFO - } ]
[2023-01-22T10:58:31.681+0000] {spark_submit.py:495} INFO - },
[2023-01-22T10:58:31.681+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:58:31.681+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:58:31.682+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:58:31.682+0000] {spark_submit.py:495} INFO - "name" : "quantity_Measure_Unit.name",
[2023-01-22T10:58:31.682+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:58:31.682+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:58:31.682+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:58:31.683+0000] {spark_submit.py:495} INFO - } ]
[2023-01-22T10:58:31.683+0000] {spark_submit.py:495} INFO - },
[2023-01-22T10:58:31.683+0000] {spark_submit.py:495} INFO - "containsNull" : true
[2023-01-22T10:58:31.683+0000] {spark_submit.py:495} INFO - },
[2023-01-22T10:58:31.684+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:58:31.684+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:58:31.684+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:58:31.684+0000] {spark_submit.py:495} INFO - "name" : "createdDateTime",
[2023-01-22T10:58:31.685+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:58:31.685+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:58:31.685+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:58:31.685+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:58:31.685+0000] {spark_submit.py:495} INFO - "name" : "mRID",
[2023-01-22T10:58:31.686+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:58:31.686+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:58:31.686+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:58:31.686+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:58:31.686+0000] {spark_submit.py:495} INFO - "name" : "process.processType",
[2023-01-22T10:58:31.687+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:58:31.687+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:58:31.687+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:58:31.687+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:58:31.688+0000] {spark_submit.py:495} INFO - "name" : "receiver_MarketParticipant.mRID",
[2023-01-22T10:58:31.688+0000] {spark_submit.py:495} INFO - "type" : {
[2023-01-22T10:58:31.688+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-22T10:58:31.689+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-22T10:58:31.689+0000] {spark_submit.py:495} INFO - "name" : "#text",
[2023-01-22T10:58:31.689+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:58:31.689+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:58:31.689+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:58:31.689+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:58:31.690+0000] {spark_submit.py:495} INFO - "name" : "@codingScheme",
[2023-01-22T10:58:31.690+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:58:31.690+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:58:31.690+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:58:31.690+0000] {spark_submit.py:495} INFO - } ]
[2023-01-22T10:58:31.691+0000] {spark_submit.py:495} INFO - },
[2023-01-22T10:58:31.694+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:58:31.694+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:58:31.694+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:58:31.695+0000] {spark_submit.py:495} INFO - "name" : "receiver_MarketParticipant.marketRole.type",
[2023-01-22T10:58:31.695+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:58:31.695+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:58:31.695+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:58:31.695+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:58:31.695+0000] {spark_submit.py:495} INFO - "name" : "revisionNumber",
[2023-01-22T10:58:31.696+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:58:31.696+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:58:31.696+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:58:31.696+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:58:31.696+0000] {spark_submit.py:495} INFO - "name" : "sender_MarketParticipant.mRID",
[2023-01-22T10:58:31.696+0000] {spark_submit.py:495} INFO - "type" : {
[2023-01-22T10:58:31.696+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-22T10:58:31.697+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-22T10:58:31.697+0000] {spark_submit.py:495} INFO - "name" : "#text",
[2023-01-22T10:58:31.697+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:58:31.697+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:58:31.697+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:58:31.697+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:58:31.698+0000] {spark_submit.py:495} INFO - "name" : "@codingScheme",
[2023-01-22T10:58:31.698+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:58:31.698+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:58:31.698+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:58:31.698+0000] {spark_submit.py:495} INFO - } ]
[2023-01-22T10:58:31.698+0000] {spark_submit.py:495} INFO - },
[2023-01-22T10:58:31.699+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:58:31.699+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:58:31.699+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:58:31.699+0000] {spark_submit.py:495} INFO - "name" : "sender_MarketParticipant.marketRole.type",
[2023-01-22T10:58:31.699+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:58:31.699+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:58:31.699+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:58:31.700+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:58:31.700+0000] {spark_submit.py:495} INFO - "name" : "time_Period.timeInterval",
[2023-01-22T10:58:31.700+0000] {spark_submit.py:495} INFO - "type" : {
[2023-01-22T10:58:31.700+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-22T10:58:31.700+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-22T10:58:31.700+0000] {spark_submit.py:495} INFO - "name" : "end",
[2023-01-22T10:58:31.700+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:58:31.701+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:58:31.701+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:58:31.701+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:58:31.701+0000] {spark_submit.py:495} INFO - "name" : "start",
[2023-01-22T10:58:31.701+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:58:31.701+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:58:31.702+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:58:31.702+0000] {spark_submit.py:495} INFO - } ]
[2023-01-22T10:58:31.702+0000] {spark_submit.py:495} INFO - },
[2023-01-22T10:58:31.702+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:58:31.702+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:58:31.702+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:58:31.702+0000] {spark_submit.py:495} INFO - "name" : "type",
[2023-01-22T10:58:31.703+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:58:31.703+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:58:31.703+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:58:31.703+0000] {spark_submit.py:495} INFO - } ]
[2023-01-22T10:58:31.703+0000] {spark_submit.py:495} INFO - },
[2023-01-22T10:58:31.703+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:58:31.703+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:58:31.704+0000] {spark_submit.py:495} INFO - } ]
[2023-01-22T10:58:31.704+0000] {spark_submit.py:495} INFO - }
[2023-01-22T10:58:31.704+0000] {spark_submit.py:495} INFO - and corresponding Parquet message type:
[2023-01-22T10:58:31.704+0000] {spark_submit.py:495} INFO - message spark_schema {
[2023-01-22T10:58:31.704+0000] {spark_submit.py:495} INFO - optional group GL_MarketDocument {
[2023-01-22T10:58:31.704+0000] {spark_submit.py:495} INFO - optional binary @xmlns (STRING);
[2023-01-22T10:58:31.705+0000] {spark_submit.py:495} INFO - optional group TimeSeries (LIST) {
[2023-01-22T10:58:31.705+0000] {spark_submit.py:495} INFO - repeated group list {
[2023-01-22T10:58:31.705+0000] {spark_submit.py:495} INFO - optional group element {
[2023-01-22T10:58:31.705+0000] {spark_submit.py:495} INFO - optional group MktPSRType {
[2023-01-22T10:58:31.705+0000] {spark_submit.py:495} INFO - optional binary psrType (STRING);
[2023-01-22T10:58:31.705+0000] {spark_submit.py:495} INFO - }
[2023-01-22T10:58:31.705+0000] {spark_submit.py:495} INFO - optional group Period {
[2023-01-22T10:58:31.706+0000] {spark_submit.py:495} INFO - optional group Point (LIST) {
[2023-01-22T10:58:31.706+0000] {spark_submit.py:495} INFO - repeated group list {
[2023-01-22T10:58:31.706+0000] {spark_submit.py:495} INFO - optional group element {
[2023-01-22T10:58:31.706+0000] {spark_submit.py:495} INFO - optional binary position (STRING);
[2023-01-22T10:58:31.706+0000] {spark_submit.py:495} INFO - optional binary quantity (STRING);
[2023-01-22T10:58:31.707+0000] {spark_submit.py:495} INFO - }
[2023-01-22T10:58:31.707+0000] {spark_submit.py:495} INFO - }
[2023-01-22T10:58:31.707+0000] {spark_submit.py:495} INFO - }
[2023-01-22T10:58:31.707+0000] {spark_submit.py:495} INFO - optional binary resolution (STRING);
[2023-01-22T10:58:31.707+0000] {spark_submit.py:495} INFO - optional group timeInterval {
[2023-01-22T10:58:31.707+0000] {spark_submit.py:495} INFO - optional binary end (STRING);
[2023-01-22T10:58:31.707+0000] {spark_submit.py:495} INFO - optional binary start (STRING);
[2023-01-22T10:58:31.708+0000] {spark_submit.py:495} INFO - }
[2023-01-22T10:58:31.708+0000] {spark_submit.py:495} INFO - }
[2023-01-22T10:58:31.708+0000] {spark_submit.py:495} INFO - optional binary businessType (STRING);
[2023-01-22T10:58:31.708+0000] {spark_submit.py:495} INFO - optional binary curveType (STRING);
[2023-01-22T10:58:31.708+0000] {spark_submit.py:495} INFO - optional group inBiddingZone_Domain.mRID {
[2023-01-22T10:58:31.708+0000] {spark_submit.py:495} INFO - optional binary #text (STRING);
[2023-01-22T10:58:31.709+0000] {spark_submit.py:495} INFO - optional binary @codingScheme (STRING);
[2023-01-22T10:58:31.709+0000] {spark_submit.py:495} INFO - }
[2023-01-22T10:58:31.709+0000] {spark_submit.py:495} INFO - optional binary mRID (STRING);
[2023-01-22T10:58:31.709+0000] {spark_submit.py:495} INFO - optional binary objectAggregation (STRING);
[2023-01-22T10:58:31.709+0000] {spark_submit.py:495} INFO - optional group outBiddingZone_Domain.mRID {
[2023-01-22T10:58:31.709+0000] {spark_submit.py:495} INFO - optional binary #text (STRING);
[2023-01-22T10:58:31.709+0000] {spark_submit.py:495} INFO - optional binary @codingScheme (STRING);
[2023-01-22T10:58:31.710+0000] {spark_submit.py:495} INFO - }
[2023-01-22T10:58:31.710+0000] {spark_submit.py:495} INFO - optional binary quantity_Measure_Unit.name (STRING);
[2023-01-22T10:58:31.710+0000] {spark_submit.py:495} INFO - }
[2023-01-22T10:58:31.710+0000] {spark_submit.py:495} INFO - }
[2023-01-22T10:58:31.710+0000] {spark_submit.py:495} INFO - }
[2023-01-22T10:58:31.710+0000] {spark_submit.py:495} INFO - optional binary createdDateTime (STRING);
[2023-01-22T10:58:31.711+0000] {spark_submit.py:495} INFO - optional binary mRID (STRING);
[2023-01-22T10:58:31.711+0000] {spark_submit.py:495} INFO - optional binary process.processType (STRING);
[2023-01-22T10:58:31.711+0000] {spark_submit.py:495} INFO - optional group receiver_MarketParticipant.mRID {
[2023-01-22T10:58:31.711+0000] {spark_submit.py:495} INFO - optional binary #text (STRING);
[2023-01-22T10:58:31.711+0000] {spark_submit.py:495} INFO - optional binary @codingScheme (STRING);
[2023-01-22T10:58:31.711+0000] {spark_submit.py:495} INFO - }
[2023-01-22T10:58:31.711+0000] {spark_submit.py:495} INFO - optional binary receiver_MarketParticipant.marketRole.type (STRING);
[2023-01-22T10:58:31.712+0000] {spark_submit.py:495} INFO - optional binary revisionNumber (STRING);
[2023-01-22T10:58:31.712+0000] {spark_submit.py:495} INFO - optional group sender_MarketParticipant.mRID {
[2023-01-22T10:58:31.712+0000] {spark_submit.py:495} INFO - optional binary #text (STRING);
[2023-01-22T10:58:31.712+0000] {spark_submit.py:495} INFO - optional binary @codingScheme (STRING);
[2023-01-22T10:58:31.712+0000] {spark_submit.py:495} INFO - }
[2023-01-22T10:58:31.712+0000] {spark_submit.py:495} INFO - optional binary sender_MarketParticipant.marketRole.type (STRING);
[2023-01-22T10:58:31.712+0000] {spark_submit.py:495} INFO - optional group time_Period.timeInterval {
[2023-01-22T10:58:31.713+0000] {spark_submit.py:495} INFO - optional binary end (STRING);
[2023-01-22T10:58:31.713+0000] {spark_submit.py:495} INFO - optional binary start (STRING);
[2023-01-22T10:58:31.713+0000] {spark_submit.py:495} INFO - }
[2023-01-22T10:58:31.713+0000] {spark_submit.py:495} INFO - optional binary type (STRING);
[2023-01-22T10:58:31.713+0000] {spark_submit.py:495} INFO - }
[2023-01-22T10:58:31.713+0000] {spark_submit.py:495} INFO - }
[2023-01-22T10:58:31.713+0000] {spark_submit.py:495} INFO - 
[2023-01-22T10:58:31.714+0000] {spark_submit.py:495} INFO - 
[2023-01-22T10:58:31.965+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:31 INFO CodecPool: Got brand-new compressor [.snappy]
[2023-01-22T10:58:32.245+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:32 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-22T10:58:32.833+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:32 INFO CodeGenerator: Code generated in 440.56555 ms
[2023-01-22T10:58:34.575+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:34 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1674385099048-3860ab26-f17c-445b-81e0-f65a846f6f89/_temporary/0/_temporary/' directory.
[2023-01-22T10:58:34.576+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:34 INFO FileOutputCommitter: Saved output of task 'attempt_202301221058317831842208118676161_0001_m_000000_1' to gs://entsoe_temp_1009/.spark-bigquery-local-1674385099048-3860ab26-f17c-445b-81e0-f65a846f6f89/_temporary/0/task_202301221058317831842208118676161_0001_m_000000
[2023-01-22T10:58:34.576+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:34 INFO SparkHadoopMapRedUtil: attempt_202301221058317831842208118676161_0001_m_000000_1: Committed. Elapsed time: 542 ms.
[2023-01-22T10:58:34.586+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:34 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2799 bytes result sent to driver
[2023-01-22T10:58:34.589+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:34 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 3199 ms on ce1344c6e6d4 (executor driver) (1/1)
[2023-01-22T10:58:34.589+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:34 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-22T10:58:34.591+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:34 INFO DAGScheduler: ResultStage 1 (save at BigQueryWriteHelper.java:105) finished in 3.288 s
[2023-01-22T10:58:34.592+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:34 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-22T10:58:34.592+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-22T10:58:34.592+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:34 INFO DAGScheduler: Job 1 finished: save at BigQueryWriteHelper.java:105, took 3.300287 s
[2023-01-22T10:58:34.595+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:34 INFO FileFormatWriter: Start to commit write Job 953994d3-39fe-4efc-ac69-caf6ea1fb176.
[2023-01-22T10:58:35.129+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:35 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1674385099048-3860ab26-f17c-445b-81e0-f65a846f6f89/_temporary/0/task_202301221058317831842208118676161_0001_m_000000/' directory.
[2023-01-22T10:58:35.371+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:35 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1674385099048-3860ab26-f17c-445b-81e0-f65a846f6f89/' directory.
[2023-01-22T10:58:35.505+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:35 INFO BlockManagerInfo: Removed broadcast_3_piece0 on ce1344c6e6d4:39763 in memory (size: 77.2 KiB, free: 434.4 MiB)
[2023-01-22T10:58:36.058+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:36 INFO FileFormatWriter: Write Job 953994d3-39fe-4efc-ac69-caf6ea1fb176 committed. Elapsed time: 1463 ms.
[2023-01-22T10:58:36.063+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:36 INFO FileFormatWriter: Finished processing stats for write job 953994d3-39fe-4efc-ac69-caf6ea1fb176.
[2023-01-22T10:58:36.642+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:36 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=entsoe_playground, projectId=rafzul-analytics-1009, tableId=TEST_total_generation_staging}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=null, ignoreUnknownValue=null, sourceUris=[gs://entsoe_temp_1009/.spark-bigquery-local-1674385099048-3860ab26-f17c-445b-81e0-f65a846f6f89/part-00000-d8a45cac-1d88-497d-b002-72abce8d0d74-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=true, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=rafzul-analytics-1009, job=d6f7ce95-d9ba-40fb-aded-19013f42b841, location=asia-southeast1}
[2023-01-22T10:58:37.806+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:37 ERROR BigQueryClient: Unable to create the job to load to rafzul-analytics-1009.entsoe_playground.TEST_total_generation_staging
[2023-01-22T10:58:38.115+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T10:58:38.115+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 68, in <module>
[2023-01-22T10:58:38.116+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T10:58:38.116+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 58, in main
[2023-01-22T10:58:38.116+0000] {spark_submit.py:495} INFO - .save("rafzul-analytics-1009.entsoe_playground.TEST_total_generation_staging")
[2023-01-22T10:58:38.116+0000] {spark_submit.py:495} INFO - File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 968, in save
[2023-01-22T10:58:38.116+0000] {spark_submit.py:495} INFO - File "/opt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py", line 1321, in __call__
[2023-01-22T10:58:38.116+0000] {spark_submit.py:495} INFO - File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 190, in deco
[2023-01-22T10:58:38.117+0000] {spark_submit.py:495} INFO - File "/opt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py", line 326, in get_return_value
[2023-01-22T10:58:38.117+0000] {spark_submit.py:495} INFO - py4j.protocol.Py4JJavaError: An error occurred while calling o43.save.
[2023-01-22T10:58:38.117+0000] {spark_submit.py:495} INFO - : com.google.cloud.bigquery.connector.common.BigQueryConnectorException: Failed to write to BigQuery
[2023-01-22T10:58:38.117+0000] {spark_submit.py:495} INFO - at com.google.cloud.spark.bigquery.write.BigQueryWriteHelper.writeDataFrameToBigQuery(BigQueryWriteHelper.java:110)
[2023-01-22T10:58:38.117+0000] {spark_submit.py:495} INFO - at com.google.cloud.spark.bigquery.write.BigQueryDeprecatedIndirectInsertableRelation.insert(BigQueryDeprecatedIndirectInsertableRelation.java:43)
[2023-01-22T10:58:38.117+0000] {spark_submit.py:495} INFO - at com.google.cloud.spark.bigquery.write.CreatableRelationProviderHelper.createRelation(CreatableRelationProviderHelper.java:51)
[2023-01-22T10:58:38.118+0000] {spark_submit.py:495} INFO - at com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelation(BigQueryRelationProvider.scala:106)
[2023-01-22T10:58:38.118+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
[2023-01-22T10:58:38.118+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
[2023-01-22T10:58:38.118+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
[2023-01-22T10:58:38.118+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
[2023-01-22T10:58:38.118+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
[2023-01-22T10:58:38.119+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-22T10:58:38.119+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-22T10:58:38.119+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-22T10:58:38.119+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-22T10:58:38.119+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-22T10:58:38.119+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
[2023-01-22T10:58:38.119+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
[2023-01-22T10:58:38.119+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
[2023-01-22T10:58:38.120+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
[2023-01-22T10:58:38.120+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
[2023-01-22T10:58:38.120+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-22T10:58:38.120+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2023-01-22T10:58:38.120+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2023-01-22T10:58:38.120+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-22T10:58:38.120+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-22T10:58:38.121+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
[2023-01-22T10:58:38.121+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
[2023-01-22T10:58:38.121+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
[2023-01-22T10:58:38.121+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
[2023-01-22T10:58:38.121+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)
[2023-01-22T10:58:38.121+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)
[2023-01-22T10:58:38.121+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)
[2023-01-22T10:58:38.121+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)
[2023-01-22T10:58:38.122+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
[2023-01-22T10:58:38.122+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T10:58:38.122+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T10:58:38.122+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T10:58:38.122+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T10:58:38.122+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T10:58:38.122+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T10:58:38.123+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T10:58:38.123+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T10:58:38.123+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T10:58:38.123+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T10:58:38.123+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T10:58:38.123+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T10:58:38.123+0000] {spark_submit.py:495} INFO - Caused by: com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryException: Character '.' found in field name: inBiddingZone_Domain.mRID, parquet file: /bigstore/entsoe_temp_1009/.spark-bigquery-local-1674385099048-3860ab26-f17c-445b-81e0-f65a846f6f89/part-00000-d8a45cac-1d88-497d-b002-72abce8d0d74-c000.snappy.parquet.Reading such fields is not yet supported.
[2023-01-22T10:58:38.124+0000] {spark_submit.py:495} INFO - at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.Job.reload(Job.java:419)
[2023-01-22T10:58:38.124+0000] {spark_submit.py:495} INFO - at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.Job.waitFor(Job.java:252)
[2023-01-22T10:58:38.124+0000] {spark_submit.py:495} INFO - at com.google.cloud.bigquery.connector.common.BigQueryClient.createAndWaitFor(BigQueryClient.java:333)
[2023-01-22T10:58:38.124+0000] {spark_submit.py:495} INFO - at com.google.cloud.bigquery.connector.common.BigQueryClient.createAndWaitFor(BigQueryClient.java:323)
[2023-01-22T10:58:38.124+0000] {spark_submit.py:495} INFO - at com.google.cloud.bigquery.connector.common.BigQueryClient.loadDataIntoTable(BigQueryClient.java:553)
[2023-01-22T10:58:38.124+0000] {spark_submit.py:495} INFO - at com.google.cloud.spark.bigquery.write.BigQueryWriteHelper.loadDataToBigQuery(BigQueryWriteHelper.java:130)
[2023-01-22T10:58:38.124+0000] {spark_submit.py:495} INFO - at com.google.cloud.spark.bigquery.write.BigQueryWriteHelper.writeDataFrameToBigQuery(BigQueryWriteHelper.java:107)
[2023-01-22T10:58:38.124+0000] {spark_submit.py:495} INFO - ... 44 more
[2023-01-22T10:58:38.125+0000] {spark_submit.py:495} INFO - 
[2023-01-22T10:58:38.182+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:38 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-22T10:58:38.196+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:38 INFO SparkUI: Stopped Spark web UI at http://ce1344c6e6d4:4040
[2023-01-22T10:58:38.211+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:38 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-22T10:58:38.223+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:38 INFO MemoryStore: MemoryStore cleared
[2023-01-22T10:58:38.224+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:38 INFO BlockManager: BlockManager stopped
[2023-01-22T10:58:38.227+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:38 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-22T10:58:38.231+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:38 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-22T10:58:38.238+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:38 INFO SparkContext: Successfully stopped SparkContext
[2023-01-22T10:58:38.239+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:38 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T10:58:38.240+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:38 INFO ShutdownHookManager: Deleting directory /tmp/spark-4ed266bf-6b3f-4395-9fa2-cfc72a7c31ca/pyspark-13504043-878c-45a2-936d-31c53e437224
[2023-01-22T10:58:38.245+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:38 INFO ShutdownHookManager: Deleting directory /tmp/spark-bd596f71-b790-4292-955c-1700d60bd6f1
[2023-01-22T10:58:38.250+0000] {spark_submit.py:495} INFO - 23/01/22 10:58:38 INFO ShutdownHookManager: Deleting directory /tmp/spark-4ed266bf-6b3f-4395-9fa2-cfc72a7c31ca
[2023-01-22T10:58:38.364+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-22T10:58:38.368+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230122T105813, end_date=20230122T105838
[2023-01-22T10:58:38.381+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 470 for task transform_stage_generation (Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 3388)
[2023-01-22T10:58:38.415+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T10:58:38.436+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
