[2023-01-20T16:16:53.825+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-20T16:16:53.839+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-20T16:16:53.839+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T16:16:53.839+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-20T16:16:53.839+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T16:16:53.859+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-20T16:16:53.881+0000] {standard_task_runner.py:55} INFO - Started process 196 to run task
[2023-01-20T16:16:53.889+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '60', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpx72301rs']
[2023-01-20T16:16:53.895+0000] {standard_task_runner.py:83} INFO - Job 60: Subtask transform_stage_generation
[2023-01-20T16:16:53.975+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T16:16:54.063+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-20T16:16:54.074+0000] {spark_submit.py:222} INFO - Could not load connection string spark_default, defaulting to yarn
[2023-01-20T16:16:54.075+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master yarn --name arrow-spark /opt/***/plugins/scripts/transform_stage.py total_generation 202101010100 202101010200 Europe/Berlin DE_TENNET
[2023-01-20T16:16:54.084+0000] {spark_submit.py:495} INFO - /usr/local/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit: line 27: /opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-class: No such file or directory
[2023-01-20T16:16:54.095+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master yarn --name arrow-spark /opt/***/plugins/scripts/transform_stage.py total_generation 202101010100 202101010200 Europe/Berlin DE_TENNET. Error code is: 127.
[2023-01-20T16:16:54.100+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230120T161653, end_date=20230120T161654
[2023-01-20T16:16:54.115+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 60 for task transform_stage_generation (Cannot execute: spark-submit --master yarn --name arrow-spark /opt/***/plugins/scripts/transform_stage.py total_generation 202101010100 202101010200 Europe/Berlin DE_TENNET. Error code is: 127.; 196)
[2023-01-20T16:16:54.139+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T16:16:54.158+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T16:19:06.350+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-20T16:19:06.364+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-20T16:19:06.364+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T16:19:06.364+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-20T16:19:06.364+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T16:19:06.381+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-20T16:19:06.392+0000] {standard_task_runner.py:55} INFO - Started process 336 to run task
[2023-01-20T16:19:06.397+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '64', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpi7u67u0o']
[2023-01-20T16:19:06.400+0000] {standard_task_runner.py:83} INFO - Job 64: Subtask transform_stage_generation
[2023-01-20T16:19:06.474+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T16:19:06.546+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-20T16:19:06.556+0000] {spark_submit.py:222} INFO - Could not load connection string spark_default, defaulting to yarn
[2023-01-20T16:19:06.557+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master yarn --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 Europe/Berlin DE_TENNET
[2023-01-20T16:19:06.563+0000] {spark_submit.py:495} INFO - /usr/local/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit: line 27: /opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-class: No such file or directory
[2023-01-20T16:19:06.573+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master yarn --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 Europe/Berlin DE_TENNET. Error code is: 127.
[2023-01-20T16:19:06.576+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230120T161906, end_date=20230120T161906
[2023-01-20T16:19:06.588+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 64 for task transform_stage_generation (Cannot execute: spark-submit --master yarn --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 Europe/Berlin DE_TENNET. Error code is: 127.; 336)
[2023-01-20T16:19:06.608+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T16:19:06.629+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T16:22:27.155+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-20T16:22:27.203+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-20T16:22:27.203+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T16:22:27.204+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-20T16:22:27.204+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T16:22:27.254+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-20T16:22:27.277+0000] {standard_task_runner.py:55} INFO - Started process 553 to run task
[2023-01-20T16:22:27.289+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '69', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpcv_xr7ip']
[2023-01-20T16:22:27.296+0000] {standard_task_runner.py:83} INFO - Job 69: Subtask transform_stage_generation
[2023-01-20T16:22:27.500+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T16:22:27.689+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-20T16:22:27.717+0000] {spark_submit.py:222} INFO - Could not load connection string spark_default, defaulting to yarn
[2023-01-20T16:22:27.720+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master yarn --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-20T16:22:27.737+0000] {spark_submit.py:495} INFO - /usr/local/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit: line 27: /opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-class: No such file or directory
[2023-01-20T16:22:27.779+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master yarn --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 127.
[2023-01-20T16:22:27.790+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230120T162227, end_date=20230120T162227
[2023-01-20T16:22:27.831+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 69 for task transform_stage_generation (Cannot execute: spark-submit --master yarn --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 127.; 553)
[2023-01-20T16:22:27.904+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T16:22:27.980+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T16:31:16.656+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-20T16:31:16.667+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-20T16:31:16.667+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T16:31:16.668+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-20T16:31:16.668+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T16:31:16.680+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-20T16:31:16.688+0000] {standard_task_runner.py:55} INFO - Started process 1110 to run task
[2023-01-20T16:31:16.691+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '74', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpxdv41lm5']
[2023-01-20T16:31:16.694+0000] {standard_task_runner.py:83} INFO - Job 74: Subtask transform_stage_generation
[2023-01-20T16:31:16.753+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T16:31:16.843+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-20T16:31:16.852+0000] {spark_submit.py:222} INFO - Could not load connection string spark_default, defaulting to yarn
[2023-01-20T16:31:16.855+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master yarn --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-20T16:31:16.866+0000] {spark_submit.py:495} INFO - /usr/local/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit: line 27: /opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-class: No such file or directory
[2023-01-20T16:31:16.879+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master yarn --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 127.
[2023-01-20T16:31:16.883+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230120T163116, end_date=20230120T163116
[2023-01-20T16:31:16.899+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 74 for task transform_stage_generation (Cannot execute: spark-submit --master yarn --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 127.; 1110)
[2023-01-20T16:31:16.943+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T16:31:16.965+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T17:55:57.553+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-20T17:55:57.564+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-20T17:55:57.565+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T17:55:57.565+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-20T17:55:57.565+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T17:55:57.576+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-20T17:55:57.588+0000] {standard_task_runner.py:55} INFO - Started process 6214 to run task
[2023-01-20T17:55:57.592+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '85', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpq3zofpe4']
[2023-01-20T17:55:57.596+0000] {standard_task_runner.py:83} INFO - Job 85: Subtask transform_stage_generation
[2023-01-20T17:55:57.675+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T17:55:57.751+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-20T17:55:57.752+0000] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-01-20T17:55:57.753+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master  --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-20T17:55:57.760+0000] {spark_submit.py:495} INFO - /usr/local/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit: line 27: /opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-class: No such file or directory
[2023-01-20T17:55:57.771+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master  --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 127.
[2023-01-20T17:55:57.774+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230120T175557, end_date=20230120T175557
[2023-01-20T17:55:57.787+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 85 for task transform_stage_generation (Cannot execute: spark-submit --master  --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 127.; 6214)
[2023-01-20T17:55:57.804+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T17:55:57.825+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T17:59:00.468+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-20T17:59:00.486+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-20T17:59:00.487+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T17:59:00.487+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-20T17:59:00.487+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T17:59:00.511+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-20T17:59:00.530+0000] {standard_task_runner.py:55} INFO - Started process 6425 to run task
[2023-01-20T17:59:00.536+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '88', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp5_c9vvdh']
[2023-01-20T17:59:00.541+0000] {standard_task_runner.py:83} INFO - Job 88: Subtask transform_stage_generation
[2023-01-20T17:59:00.649+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T17:59:00.745+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-20T17:59:00.746+0000] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-01-20T17:59:00.749+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master  --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-20T17:59:00.786+0000] {spark_submit.py:495} INFO - /usr/local/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit: line 27: /opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-class: No such file or directory
[2023-01-20T17:59:00.800+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master  --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 127.
[2023-01-20T17:59:00.806+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230120T175900, end_date=20230120T175900
[2023-01-20T17:59:00.822+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 88 for task transform_stage_generation (Cannot execute: spark-submit --master  --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 127.; 6425)
[2023-01-20T17:59:00.870+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T17:59:00.894+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T18:05:02.154+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-20T18:05:02.171+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-20T18:05:02.171+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T18:05:02.171+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-20T18:05:02.172+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T18:05:02.196+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-20T18:05:02.211+0000] {standard_task_runner.py:55} INFO - Started process 6787 to run task
[2023-01-20T18:05:02.218+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '95', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpda6ut4ct']
[2023-01-20T18:05:02.223+0000] {standard_task_runner.py:83} INFO - Job 95: Subtask transform_stage_generation
[2023-01-20T18:05:02.322+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T18:05:02.443+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-20T18:05:02.445+0000] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-01-20T18:05:02.446+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master  --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-20T18:05:02.456+0000] {spark_submit.py:495} INFO - /usr/local/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit: line 27: /opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-class: No such file or directory
[2023-01-20T18:05:02.469+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master  --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 127.
[2023-01-20T18:05:02.474+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230120T180502, end_date=20230120T180502
[2023-01-20T18:05:02.497+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 95 for task transform_stage_generation (Cannot execute: spark-submit --master  --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 127.; 6787)
[2023-01-20T18:05:02.557+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T18:05:02.578+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T18:11:16.199+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-20T18:11:16.207+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-20T18:11:16.208+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T18:11:16.208+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-20T18:11:16.208+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T18:11:16.219+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-20T18:11:16.227+0000] {standard_task_runner.py:55} INFO - Started process 7213 to run task
[2023-01-20T18:11:16.230+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '101', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpcqjposfm']
[2023-01-20T18:11:16.233+0000] {standard_task_runner.py:83} INFO - Job 101: Subtask transform_stage_generation
[2023-01-20T18:11:16.289+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T18:11:16.348+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-20T18:11:16.348+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 156, in execute
    self._hook = self._get_hook()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 165, in _get_hook
    return SparkSubmitHook(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 155, in __init__
    raise RuntimeError(
RuntimeError: The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/opt/spark-3.3.1-bin-hadoop3-scala2.13bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH
[2023-01-20T18:11:16.356+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230120T181116, end_date=20230120T181116
[2023-01-20T18:11:16.366+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 101 for task transform_stage_generation (The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/opt/spark-3.3.1-bin-hadoop3-scala2.13bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH; 7213)
[2023-01-20T18:11:16.402+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T18:11:16.420+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T18:12:43.187+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-20T18:12:43.211+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-20T18:12:43.212+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T18:12:43.212+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-20T18:12:43.212+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T18:12:43.244+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-20T18:12:43.259+0000] {standard_task_runner.py:55} INFO - Started process 7308 to run task
[2023-01-20T18:12:43.270+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '105', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp8anc4g1g']
[2023-01-20T18:12:43.278+0000] {standard_task_runner.py:83} INFO - Job 105: Subtask transform_stage_generation
[2023-01-20T18:12:43.444+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T18:12:43.597+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-20T18:12:43.598+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 156, in execute
    self._hook = self._get_hook()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 165, in _get_hook
    return SparkSubmitHook(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 155, in __init__
    raise RuntimeError(
RuntimeError: The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH
[2023-01-20T18:12:43.615+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230120T181243, end_date=20230120T181243
[2023-01-20T18:12:43.636+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 105 for task transform_stage_generation (The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH; 7308)
[2023-01-20T18:12:43.681+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T18:12:43.715+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T18:16:15.998+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-20T18:16:16.025+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-20T18:16:16.025+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T18:16:16.025+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-20T18:16:16.025+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T18:16:16.076+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-20T18:16:16.102+0000] {standard_task_runner.py:55} INFO - Started process 7544 to run task
[2023-01-20T18:16:16.110+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '109', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpd5b_z3t7']
[2023-01-20T18:16:16.117+0000] {standard_task_runner.py:83} INFO - Job 109: Subtask transform_stage_generation
[2023-01-20T18:16:16.276+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T18:16:16.476+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-20T18:16:16.477+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 156, in execute
    self._hook = self._get_hook()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 165, in _get_hook
    return SparkSubmitHook(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 155, in __init__
    raise RuntimeError(
RuntimeError: The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH
[2023-01-20T18:16:16.504+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230120T181616, end_date=20230120T181616
[2023-01-20T18:16:16.514+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 109 for task transform_stage_generation (The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH; 7544)
[2023-01-20T18:16:16.560+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T18:16:16.618+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T18:18:55.646+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-20T18:18:55.664+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-20T18:18:55.665+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T18:18:55.665+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-20T18:18:55.666+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T18:18:55.697+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-20T18:18:55.712+0000] {standard_task_runner.py:55} INFO - Started process 7715 to run task
[2023-01-20T18:18:55.719+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '112', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpw4s4bj_2']
[2023-01-20T18:18:55.723+0000] {standard_task_runner.py:83} INFO - Job 112: Subtask transform_stage_generation
[2023-01-20T18:18:55.817+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T18:18:55.947+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-20T18:18:55.948+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 156, in execute
    self._hook = self._get_hook()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 165, in _get_hook
    return SparkSubmitHook(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 155, in __init__
    raise RuntimeError(
RuntimeError: The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH
[2023-01-20T18:18:55.961+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230120T181855, end_date=20230120T181855
[2023-01-20T18:18:55.980+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 112 for task transform_stage_generation (The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH; 7715)
[2023-01-20T18:18:56.017+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T18:18:56.063+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T19:15:54.700+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-20T19:15:54.775+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-20T19:15:54.776+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T19:15:54.777+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-20T19:15:54.779+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T19:15:54.833+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-20T19:15:54.848+0000] {standard_task_runner.py:55} INFO - Started process 361 to run task
[2023-01-20T19:15:54.857+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '129', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpxsm2i8hk']
[2023-01-20T19:15:54.860+0000] {standard_task_runner.py:83} INFO - Job 129: Subtask transform_stage_generation
[2023-01-20T19:15:55.109+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 984f5901aea4
[2023-01-20T19:15:55.329+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-20T19:15:55.330+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 156, in execute
    self._hook = self._get_hook()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 165, in _get_hook
    return SparkSubmitHook(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 155, in __init__
    raise RuntimeError(
RuntimeError: The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/usr/local/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH
[2023-01-20T19:15:55.398+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230120T191554, end_date=20230120T191555
[2023-01-20T19:15:55.457+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 129 for task transform_stage_generation (The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/usr/local/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH; 361)
[2023-01-20T19:15:55.564+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T19:15:55.717+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T03:03:17.789+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T03:03:17.813+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T03:03:17.814+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T03:03:17.814+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-21T03:03:17.814+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T03:03:17.846+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-21T03:03:17.863+0000] {standard_task_runner.py:55} INFO - Started process 325 to run task
[2023-01-21T03:03:17.873+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '141', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmps497ehii']
[2023-01-21T03:03:17.879+0000] {standard_task_runner.py:83} INFO - Job 141: Subtask transform_stage_generation
[2023-01-21T03:03:18.039+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 2b3134e2b02d
[2023-01-21T03:03:18.204+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-21T03:03:18.206+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 156, in execute
    self._hook = self._get_hook()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 165, in _get_hook
    return SparkSubmitHook(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 155, in __init__
    raise RuntimeError(
RuntimeError: The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/usr/local/spark/bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH
[2023-01-21T03:03:18.225+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230121T030317, end_date=20230121T030318
[2023-01-21T03:03:18.249+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 141 for task transform_stage_generation (The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/usr/local/spark/bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH; 325)
[2023-01-21T03:03:18.285+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T03:03:18.327+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T07:47:28.847+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T07:47:28.862+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T07:47:28.863+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T07:47:28.863+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-21T07:47:28.863+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T07:47:28.884+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-21T07:47:28.894+0000] {standard_task_runner.py:55} INFO - Started process 72 to run task
[2023-01-21T07:47:28.900+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '163', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpbll33cg5']
[2023-01-21T07:47:28.906+0000] {standard_task_runner.py:83} INFO - Job 163: Subtask transform_stage_generation
[2023-01-21T07:47:28.998+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 4d52b49e46b4
[2023-01-21T07:47:29.096+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-21T07:47:29.098+0000] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-01-21T07:47:29.099+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master  --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-21T07:47:29.111+0000] {spark_submit.py:495} INFO - /usr/local/spark/bin/load-spark-env.sh: line 68: ps: command not found
[2023-01-21T07:47:32.688+0000] {spark_submit.py:495} INFO - Exception in thread "main" org.apache.spark.SparkException: Master must either be yarn or start with spark, mesos, k8s, or local
[2023-01-21T07:47:32.689+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.error(SparkSubmit.scala:975)
[2023-01-21T07:47:32.690+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:238)
[2023-01-21T07:47:32.690+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:901)
[2023-01-21T07:47:32.690+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
[2023-01-21T07:47:32.690+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
[2023-01-21T07:47:32.691+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
[2023-01-21T07:47:32.692+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)
[2023-01-21T07:47:32.692+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)
[2023-01-21T07:47:32.692+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
[2023-01-21T07:47:32.730+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master  --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-21T07:47:32.737+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230121T074728, end_date=20230121T074732
[2023-01-21T07:47:32.768+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 163 for task transform_stage_generation (Cannot execute: spark-submit --master  --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 72)
[2023-01-21T07:47:32.808+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T07:47:32.836+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T08:16:38.436+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T08:16:38.451+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T08:16:38.451+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T08:16:38.452+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-21T08:16:38.452+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T08:16:38.467+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-21T08:16:38.477+0000] {standard_task_runner.py:55} INFO - Started process 2286 to run task
[2023-01-21T08:16:38.481+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '172', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpjm05xbvl']
[2023-01-21T08:16:38.484+0000] {standard_task_runner.py:83} INFO - Job 172: Subtask transform_stage_generation
[2023-01-21T08:16:38.565+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 4d52b49e46b4
[2023-01-21T08:16:38.664+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-21T08:16:38.677+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-21T08:16:38.681+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-21T08:16:38.693+0000] {spark_submit.py:495} INFO - /usr/local/spark/bin/load-spark-env.sh: line 68: ps: command not found
[2023-01-21T08:16:41.953+0000] {spark_submit.py:495} INFO - 23/01/21 08:16:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-21T08:16:42.256+0000] {spark_submit.py:495} INFO - 23/01/21 08:16:42 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-21T08:16:42.358+0000] {spark_submit.py:495} INFO - Exception in thread "main" java.io.IOException: Cannot run program "/usr/bin/python3.10": error=2, No such file or directory
[2023-01-21T08:16:42.359+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1128)
[2023-01-21T08:16:42.359+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1071)
[2023-01-21T08:16:42.359+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.PythonRunner$.main(PythonRunner.scala:97)
[2023-01-21T08:16:42.359+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.PythonRunner.main(PythonRunner.scala)
[2023-01-21T08:16:42.360+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-21T08:16:42.360+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-21T08:16:42.360+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-21T08:16:42.360+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-21T08:16:42.360+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
[2023-01-21T08:16:42.360+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:958)
[2023-01-21T08:16:42.361+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
[2023-01-21T08:16:42.361+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
[2023-01-21T08:16:42.361+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
[2023-01-21T08:16:42.361+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)
[2023-01-21T08:16:42.361+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)
[2023-01-21T08:16:42.361+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
[2023-01-21T08:16:42.362+0000] {spark_submit.py:495} INFO - Caused by: java.io.IOException: error=2, No such file or directory
[2023-01-21T08:16:42.362+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessImpl.forkAndExec(Native Method)
[2023-01-21T08:16:42.362+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:340)
[2023-01-21T08:16:42.362+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessImpl.start(ProcessImpl.java:271)
[2023-01-21T08:16:42.362+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1107)
[2023-01-21T08:16:42.362+0000] {spark_submit.py:495} INFO - ... 15 more
[2023-01-21T08:16:42.367+0000] {spark_submit.py:495} INFO - 23/01/21 08:16:42 INFO ShutdownHookManager: Shutdown hook called
[2023-01-21T08:16:42.368+0000] {spark_submit.py:495} INFO - 23/01/21 08:16:42 INFO ShutdownHookManager: Deleting directory /tmp/spark-ce0fd7a3-270e-4354-b72c-7cdd21e52b06
[2023-01-21T08:16:42.445+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-21T08:16:42.450+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230121T081638, end_date=20230121T081642
[2023-01-21T08:16:42.464+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 172 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 2286)
[2023-01-21T08:16:42.500+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T08:16:42.520+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T08:23:55.624+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T08:23:55.632+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T08:23:55.632+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T08:23:55.632+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-21T08:23:55.632+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T08:23:55.642+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-21T08:23:55.649+0000] {standard_task_runner.py:55} INFO - Started process 284 to run task
[2023-01-21T08:23:55.653+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '180', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpn8hul7w4']
[2023-01-21T08:23:55.655+0000] {standard_task_runner.py:83} INFO - Job 180: Subtask transform_stage_generation
[2023-01-21T08:23:55.725+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host d00464a30031
[2023-01-21T08:23:55.802+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-21T08:23:55.812+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-21T08:23:55.814+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-21T08:23:55.823+0000] {spark_submit.py:495} INFO - /usr/local/spark/bin/load-spark-env.sh: line 68: ps: command not found
[2023-01-21T08:23:59.427+0000] {spark_submit.py:495} INFO - 23/01/21 08:23:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-21T08:23:59.755+0000] {spark_submit.py:495} INFO - 23/01/21 08:23:59 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-21T08:23:59.895+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-21T08:23:59.896+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 2, in <module>
[2023-01-21T08:23:59.896+0000] {spark_submit.py:495} INFO - from dotenv import load_dotenv
[2023-01-21T08:23:59.896+0000] {spark_submit.py:495} INFO - ModuleNotFoundError: No module named 'dotenv'
[2023-01-21T08:23:59.915+0000] {spark_submit.py:495} INFO - 23/01/21 08:23:59 INFO ShutdownHookManager: Shutdown hook called
[2023-01-21T08:23:59.918+0000] {spark_submit.py:495} INFO - 23/01/21 08:23:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-8e2e4a9e-6256-4343-b46f-678afc03c705
[2023-01-21T08:24:00.014+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-21T08:24:00.022+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230121T082355, end_date=20230121T082400
[2023-01-21T08:24:00.058+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 180 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 284)
[2023-01-21T08:24:00.092+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T08:24:00.129+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T10:18:15.165+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T10:18:15.191+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T10:18:15.192+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T10:18:15.192+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-21T10:18:15.192+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T10:18:15.224+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-21T10:18:15.240+0000] {standard_task_runner.py:55} INFO - Started process 2218 to run task
[2023-01-21T10:18:15.251+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '193', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp1ey56rip']
[2023-01-21T10:18:15.259+0000] {standard_task_runner.py:83} INFO - Job 193: Subtask transform_stage_generation
[2023-01-21T10:18:15.387+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host b3a2c4a96689
[2023-01-21T10:18:15.534+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-21T10:18:15.555+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-21T10:18:15.561+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-21T10:18:25.329+0000] {spark_submit.py:495} INFO - 23/01/21 10:18:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-21T10:18:26.239+0000] {spark_submit.py:495} INFO - 23/01/21 10:18:26 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-21T10:18:26.390+0000] {spark_submit.py:495} INFO - Exception in thread "main" java.io.IOException: Cannot run program "/usr/bin/python3.10": error=2, No such file or directory
[2023-01-21T10:18:26.391+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1128)
[2023-01-21T10:18:26.392+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1071)
[2023-01-21T10:18:26.392+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.PythonRunner$.main(PythonRunner.scala:97)
[2023-01-21T10:18:26.392+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.PythonRunner.main(PythonRunner.scala)
[2023-01-21T10:18:26.393+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-21T10:18:26.393+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-21T10:18:26.393+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-21T10:18:26.393+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-21T10:18:26.394+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
[2023-01-21T10:18:26.394+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:958)
[2023-01-21T10:18:26.394+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
[2023-01-21T10:18:26.394+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
[2023-01-21T10:18:26.395+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
[2023-01-21T10:18:26.395+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)
[2023-01-21T10:18:26.395+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)
[2023-01-21T10:18:26.395+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
[2023-01-21T10:18:26.396+0000] {spark_submit.py:495} INFO - Caused by: java.io.IOException: error=2, No such file or directory
[2023-01-21T10:18:26.396+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessImpl.forkAndExec(Native Method)
[2023-01-21T10:18:26.396+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:340)
[2023-01-21T10:18:26.396+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessImpl.start(ProcessImpl.java:271)
[2023-01-21T10:18:26.396+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1107)
[2023-01-21T10:18:26.397+0000] {spark_submit.py:495} INFO - ... 15 more
[2023-01-21T10:18:26.405+0000] {spark_submit.py:495} INFO - 23/01/21 10:18:26 INFO ShutdownHookManager: Shutdown hook called
[2023-01-21T10:18:26.408+0000] {spark_submit.py:495} INFO - 23/01/21 10:18:26 INFO ShutdownHookManager: Deleting directory /tmp/spark-ac0f44ff-0598-478f-8ed8-a3d0faf96a6d
[2023-01-21T10:18:26.534+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-21T10:18:26.542+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230121T101815, end_date=20230121T101826
[2023-01-21T10:18:26.571+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 193 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 2218)
[2023-01-21T10:18:26.636+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T10:18:26.681+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T10:45:46.034+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T10:45:46.095+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T10:45:46.096+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T10:45:46.096+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-21T10:45:46.096+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T10:45:46.165+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-21T10:45:46.194+0000] {standard_task_runner.py:55} INFO - Started process 156 to run task
[2023-01-21T10:45:46.218+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '204', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpj5bwfys3']
[2023-01-21T10:45:46.230+0000] {standard_task_runner.py:83} INFO - Job 204: Subtask transform_stage_generation
[2023-01-21T10:45:46.435+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host b3a2c4a96689
[2023-01-21T10:45:46.606+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-21T10:45:46.626+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-21T10:45:46.630+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-21T10:45:51.797+0000] {spark_submit.py:495} INFO - 23/01/21 10:45:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-21T10:45:52.344+0000] {spark_submit.py:495} INFO - 23/01/21 10:45:52 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-21T10:45:52.432+0000] {spark_submit.py:495} INFO - Exception in thread "main" java.io.IOException: Cannot run program "/usr/bin/python3.10": error=2, No such file or directory
[2023-01-21T10:45:52.433+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1128)
[2023-01-21T10:45:52.433+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1071)
[2023-01-21T10:45:52.433+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.PythonRunner$.main(PythonRunner.scala:97)
[2023-01-21T10:45:52.433+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.PythonRunner.main(PythonRunner.scala)
[2023-01-21T10:45:52.433+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-21T10:45:52.434+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-21T10:45:52.434+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-21T10:45:52.434+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-21T10:45:52.434+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
[2023-01-21T10:45:52.434+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:958)
[2023-01-21T10:45:52.434+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
[2023-01-21T10:45:52.434+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
[2023-01-21T10:45:52.435+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
[2023-01-21T10:45:52.435+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)
[2023-01-21T10:45:52.435+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)
[2023-01-21T10:45:52.435+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
[2023-01-21T10:45:52.436+0000] {spark_submit.py:495} INFO - Caused by: java.io.IOException: error=2, No such file or directory
[2023-01-21T10:45:52.436+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessImpl.forkAndExec(Native Method)
[2023-01-21T10:45:52.436+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:340)
[2023-01-21T10:45:52.437+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessImpl.start(ProcessImpl.java:271)
[2023-01-21T10:45:52.437+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1107)
[2023-01-21T10:45:52.437+0000] {spark_submit.py:495} INFO - ... 15 more
[2023-01-21T10:45:52.450+0000] {spark_submit.py:495} INFO - 23/01/21 10:45:52 INFO ShutdownHookManager: Shutdown hook called
[2023-01-21T10:45:52.452+0000] {spark_submit.py:495} INFO - 23/01/21 10:45:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-eb8c676b-7aa4-4365-b4ca-79b3b5b989ca
[2023-01-21T10:45:52.615+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-21T10:45:52.626+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230121T104546, end_date=20230121T104552
[2023-01-21T10:45:52.660+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 204 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 156)
[2023-01-21T10:45:52.699+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T10:45:52.767+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T11:29:48.663+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T11:29:48.679+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T11:29:48.679+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T11:29:48.680+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-21T11:29:48.680+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T11:29:48.695+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-21T11:29:48.704+0000] {standard_task_runner.py:55} INFO - Started process 1125 to run task
[2023-01-21T11:29:48.710+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '219', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpnfub2qvq']
[2023-01-21T11:29:48.715+0000] {standard_task_runner.py:83} INFO - Job 219: Subtask transform_stage_generation
[2023-01-21T11:29:48.806+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host b7c4de7fae0b
[2023-01-21T11:29:48.915+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-21T11:29:48.931+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-21T11:29:48.935+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-21T11:29:57.587+0000] {spark_submit.py:495} INFO - 23/01/21 11:29:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-21T11:29:58.339+0000] {spark_submit.py:495} INFO - 23/01/21 11:29:58 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-21T11:29:58.672+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-21T11:29:58.673+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 2, in <module>
[2023-01-21T11:29:58.674+0000] {spark_submit.py:495} INFO - from dotenv import load_dotenv, find_dotenv
[2023-01-21T11:29:58.675+0000] {spark_submit.py:495} INFO - ModuleNotFoundError: No module named 'dotenv'
[2023-01-21T11:29:58.715+0000] {spark_submit.py:495} INFO - 23/01/21 11:29:58 INFO ShutdownHookManager: Shutdown hook called
[2023-01-21T11:29:58.720+0000] {spark_submit.py:495} INFO - 23/01/21 11:29:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-c0ce408d-3076-49bc-b4c4-ce896e904aa4
[2023-01-21T11:29:59.026+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-21T11:29:59.039+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230121T112948, end_date=20230121T112959
[2023-01-21T11:29:59.082+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 219 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 1125)
[2023-01-21T11:29:59.120+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T11:29:59.201+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T11:31:20.913+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T11:31:20.932+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T11:31:20.932+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T11:31:20.933+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-21T11:31:20.933+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T11:31:20.953+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-21T11:31:20.964+0000] {standard_task_runner.py:55} INFO - Started process 1378 to run task
[2023-01-21T11:31:20.971+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '223', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpno4vb9yz']
[2023-01-21T11:31:20.974+0000] {standard_task_runner.py:83} INFO - Job 223: Subtask transform_stage_generation
[2023-01-21T11:31:21.099+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host b7c4de7fae0b
[2023-01-21T11:31:21.296+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-21T11:31:21.319+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-21T11:31:21.321+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-21T11:31:26.787+0000] {spark_submit.py:495} INFO - 23/01/21 11:31:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-21T11:31:27.126+0000] {spark_submit.py:495} INFO - 23/01/21 11:31:27 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-21T11:31:27.243+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 2
[2023-01-21T11:31:27.244+0000] {spark_submit.py:495} INFO - from python-dotenv import load_dotenv, find_dotenv
[2023-01-21T11:31:27.244+0000] {spark_submit.py:495} INFO - ^
[2023-01-21T11:31:27.244+0000] {spark_submit.py:495} INFO - SyntaxError: invalid syntax
[2023-01-21T11:31:27.256+0000] {spark_submit.py:495} INFO - 23/01/21 11:31:27 INFO ShutdownHookManager: Shutdown hook called
[2023-01-21T11:31:27.258+0000] {spark_submit.py:495} INFO - 23/01/21 11:31:27 INFO ShutdownHookManager: Deleting directory /tmp/spark-e424380a-653b-4be5-8e0b-7e29ff077c38
[2023-01-21T11:31:27.327+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-21T11:31:27.332+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230121T113120, end_date=20230121T113127
[2023-01-21T11:31:27.346+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 223 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 1378)
[2023-01-21T11:31:27.398+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T11:31:27.418+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T15:52:49.562+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T15:52:49.590+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T15:52:49.590+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T15:52:49.591+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-21T15:52:49.591+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T15:52:49.616+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-21T15:52:49.629+0000] {standard_task_runner.py:55} INFO - Started process 169 to run task
[2023-01-21T15:52:49.636+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '234', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp7mibqyvf']
[2023-01-21T15:52:49.639+0000] {standard_task_runner.py:83} INFO - Job 234: Subtask transform_stage_generation
[2023-01-21T15:52:49.725+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 673f7b70a136
[2023-01-21T15:52:49.825+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-21T15:52:49.839+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-21T15:52:49.841+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-21T15:52:56.212+0000] {spark_submit.py:495} INFO - 23/01/21 15:52:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-21T15:52:56.802+0000] {spark_submit.py:495} INFO - 23/01/21 15:52:56 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-21T15:52:56.984+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-21T15:52:56.984+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 2, in <module>
[2023-01-21T15:52:56.985+0000] {spark_submit.py:495} INFO - from dotenv import load_dotenv, find_dotenv
[2023-01-21T15:52:56.985+0000] {spark_submit.py:495} INFO - ModuleNotFoundError: No module named 'dotenv'
[2023-01-21T15:52:57.008+0000] {spark_submit.py:495} INFO - 23/01/21 15:52:57 INFO ShutdownHookManager: Shutdown hook called
[2023-01-21T15:52:57.010+0000] {spark_submit.py:495} INFO - 23/01/21 15:52:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-b75af86d-4324-40af-bfe3-ed74bd6e8692
[2023-01-21T15:52:57.105+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-21T15:52:57.111+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230121T155249, end_date=20230121T155257
[2023-01-21T15:52:57.129+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 234 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 169)
[2023-01-21T15:52:57.151+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T15:52:57.190+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T16:54:29.911+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T16:54:29.921+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T16:54:29.922+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T16:54:29.922+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-21T16:54:29.922+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T16:54:29.935+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-21T16:54:29.944+0000] {standard_task_runner.py:55} INFO - Started process 357 to run task
[2023-01-21T16:54:29.949+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '246', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpv8f86g3_']
[2023-01-21T16:54:29.953+0000] {standard_task_runner.py:83} INFO - Job 246: Subtask transform_stage_generation
[2023-01-21T16:54:30.041+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 4260804338b2
[2023-01-21T16:54:30.145+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-21T16:54:30.156+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-21T16:54:30.159+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-21T16:54:33.434+0000] {spark_submit.py:495} INFO - 23/01/21 16:54:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-21T16:54:33.800+0000] {spark_submit.py:495} INFO - 23/01/21 16:54:33 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-21T16:54:33.942+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-21T16:54:33.942+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 2, in <module>
[2023-01-21T16:54:33.943+0000] {spark_submit.py:495} INFO - from dotenv import load_dotenv, find_dotenv
[2023-01-21T16:54:33.943+0000] {spark_submit.py:495} INFO - ModuleNotFoundError: No module named 'dotenv'
[2023-01-21T16:54:33.984+0000] {spark_submit.py:495} INFO - 23/01/21 16:54:33 INFO ShutdownHookManager: Shutdown hook called
[2023-01-21T16:54:33.987+0000] {spark_submit.py:495} INFO - 23/01/21 16:54:33 INFO ShutdownHookManager: Deleting directory /tmp/spark-e663c61f-9a92-43e2-9694-6d403df9379c
[2023-01-21T16:54:34.076+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-21T16:54:34.087+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230121T165429, end_date=20230121T165434
[2023-01-21T16:54:34.110+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 246 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 357)
[2023-01-21T16:54:34.153+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T16:54:34.187+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T23:07:13.842+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T23:07:13.856+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T23:07:13.857+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T23:07:13.857+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-21T23:07:13.857+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T23:07:13.879+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-21T23:07:13.896+0000] {standard_task_runner.py:55} INFO - Started process 4272 to run task
[2023-01-21T23:07:13.903+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '256', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpodos5nw3']
[2023-01-21T23:07:13.909+0000] {standard_task_runner.py:83} INFO - Job 256: Subtask transform_stage_generation
[2023-01-21T23:07:14.008+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 4260804338b2
[2023-01-21T23:07:14.107+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-21T23:07:14.121+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-21T23:07:14.124+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-21T23:07:18.756+0000] {spark_submit.py:495} INFO - 23/01/21 23:07:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-21T23:07:19.234+0000] {spark_submit.py:495} INFO - 23/01/21 23:07:19 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-21T23:07:19.447+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-21T23:07:19.448+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 2, in <module>
[2023-01-21T23:07:19.448+0000] {spark_submit.py:495} INFO - from dotenv import load_dotenv, find_dotenv
[2023-01-21T23:07:19.448+0000] {spark_submit.py:495} INFO - ModuleNotFoundError: No module named 'dotenv'
[2023-01-21T23:07:19.464+0000] {spark_submit.py:495} INFO - 23/01/21 23:07:19 INFO ShutdownHookManager: Shutdown hook called
[2023-01-21T23:07:19.465+0000] {spark_submit.py:495} INFO - 23/01/21 23:07:19 INFO ShutdownHookManager: Deleting directory /tmp/spark-8a33cd29-e460-4c3e-ac17-21a38ada8033
[2023-01-21T23:07:19.536+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-21T23:07:19.542+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230121T230713, end_date=20230121T230719
[2023-01-21T23:07:19.561+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 256 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 4272)
[2023-01-21T23:07:19.596+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T23:07:19.624+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T00:11:33.601+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T00:11:33.622+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T00:11:33.623+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T00:11:33.623+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-22T00:11:33.623+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T00:11:33.648+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-22T00:11:33.659+0000] {standard_task_runner.py:55} INFO - Started process 383 to run task
[2023-01-22T00:11:33.665+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '269', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpgmwdt5et']
[2023-01-22T00:11:33.668+0000] {standard_task_runner.py:83} INFO - Job 269: Subtask transform_stage_generation
[2023-01-22T00:11:33.751+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 669043c9475c
[2023-01-22T00:11:33.833+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-22T00:11:33.843+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T00:11:33.845+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-22T00:11:37.373+0000] {spark_submit.py:495} INFO - 23/01/22 00:11:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T00:11:37.788+0000] {spark_submit.py:495} INFO - 23/01/22 00:11:37 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-22T00:11:38.455+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T00:11:38.456+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 64, in <module>
[2023-01-22T00:11:38.456+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T00:11:38.456+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 16, in main
[2023-01-22T00:11:38.456+0000] {spark_submit.py:495} INFO - start_label = start.strftime("%Y%m%d%H%M")
[2023-01-22T00:11:38.456+0000] {spark_submit.py:495} INFO - AttributeError: 'str' object has no attribute 'strftime'
[2023-01-22T00:11:38.485+0000] {spark_submit.py:495} INFO - 23/01/22 00:11:38 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T00:11:38.486+0000] {spark_submit.py:495} INFO - 23/01/22 00:11:38 INFO ShutdownHookManager: Deleting directory /tmp/spark-0070f739-c2c4-4a88-ba2e-638a0f0a77c0
[2023-01-22T00:11:38.568+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-22T00:11:38.575+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230122T001133, end_date=20230122T001138
[2023-01-22T00:11:38.593+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 269 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 383)
[2023-01-22T00:11:38.656+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T00:11:38.682+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T03:52:53.317+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T03:52:53.331+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T03:52:53.331+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T03:52:53.332+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-22T03:52:53.332+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T03:52:53.350+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-22T03:52:53.361+0000] {standard_task_runner.py:55} INFO - Started process 861 to run task
[2023-01-22T03:52:53.368+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '281', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpc4csf0t_']
[2023-01-22T03:52:53.382+0000] {standard_task_runner.py:83} INFO - Job 281: Subtask transform_stage_generation
[2023-01-22T03:52:53.623+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host e3ae05c9a97e
[2023-01-22T03:52:53.799+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-22T03:52:53.831+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T03:52:53.833+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /opt/***/spark/resources/gcs-connector-hadoop3-latest.jar, /opt/***/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-22T03:52:59.453+0000] {spark_submit.py:495} INFO - 23/01/22 03:52:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T03:52:59.894+0000] {spark_submit.py:495} INFO - 23/01/22 03:52:59 WARN DependencyUtils: Local jar /opt/***/ /opt/***/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-22T03:53:00.320+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T03:53:00.321+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 64, in <module>
[2023-01-22T03:53:00.321+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T03:53:00.321+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 16, in main
[2023-01-22T03:53:00.321+0000] {spark_submit.py:495} INFO - start_label = start.strftime("%Y%m%d%H%M")
[2023-01-22T03:53:00.321+0000] {spark_submit.py:495} INFO - AttributeError: 'str' object has no attribute 'strftime'
[2023-01-22T03:53:00.341+0000] {spark_submit.py:495} INFO - 23/01/22 03:53:00 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T03:53:00.341+0000] {spark_submit.py:495} INFO - 23/01/22 03:53:00 INFO ShutdownHookManager: Deleting directory /tmp/spark-26f35389-626b-429e-9d4e-25ba16a2bd19
[2023-01-22T03:53:00.401+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /opt/***/spark/resources/gcs-connector-hadoop3-latest.jar, /opt/***/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-22T03:53:00.404+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230122T035253, end_date=20230122T035300
[2023-01-22T03:53:00.414+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 281 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /opt/***/spark/resources/gcs-connector-hadoop3-latest.jar, /opt/***/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 861)
[2023-01-22T03:53:00.427+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T03:53:00.443+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T04:19:42.407+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T04:19:42.421+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T04:19:42.421+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T04:19:42.422+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-22T04:19:42.422+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T04:19:42.438+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-22T04:19:42.450+0000] {standard_task_runner.py:55} INFO - Started process 2965 to run task
[2023-01-22T04:19:42.457+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '290', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpv7aps0hr']
[2023-01-22T04:19:42.462+0000] {standard_task_runner.py:83} INFO - Job 290: Subtask transform_stage_generation
[2023-01-22T04:19:42.557+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host e3ae05c9a97e
[2023-01-22T04:19:42.658+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-22T04:19:42.673+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T04:19:42.674+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar, spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-22T04:19:48.916+0000] {spark_submit.py:495} INFO - 23/01/22 04:19:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T04:19:49.762+0000] {spark_submit.py:495} INFO - 23/01/22 04:19:49 WARN DependencyUtils: Local jar /opt/***/gcs-connector-hadoop3-latest.jar does not exist, skipping.
[2023-01-22T04:19:49.764+0000] {spark_submit.py:495} INFO - 23/01/22 04:19:49 WARN DependencyUtils: Local jar /opt/***/ spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-22T04:19:50.972+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T04:19:50.973+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 64, in <module>
[2023-01-22T04:19:50.973+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T04:19:50.973+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 16, in main
[2023-01-22T04:19:50.974+0000] {spark_submit.py:495} INFO - start_label = start.strftime("%Y%m%d%H%M")
[2023-01-22T04:19:50.974+0000] {spark_submit.py:495} INFO - AttributeError: 'str' object has no attribute 'strftime'
[2023-01-22T04:19:51.003+0000] {spark_submit.py:495} INFO - 23/01/22 04:19:51 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T04:19:51.005+0000] {spark_submit.py:495} INFO - 23/01/22 04:19:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-61fae46b-d5ab-4dfe-b9f0-fc94a2901fa8
[2023-01-22T04:19:51.123+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar, spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-22T04:19:51.131+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230122T041942, end_date=20230122T041951
[2023-01-22T04:19:51.158+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 290 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar, spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 2965)
[2023-01-22T04:19:51.206+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T04:19:51.259+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T06:08:46.518+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T06:08:46.573+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T06:08:46.574+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T06:08:46.574+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-22T06:08:46.575+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T06:08:46.630+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-22T06:08:46.657+0000] {standard_task_runner.py:55} INFO - Started process 242 to run task
[2023-01-22T06:08:46.676+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '304', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpo63wwj2x']
[2023-01-22T06:08:46.688+0000] {standard_task_runner.py:83} INFO - Job 304: Subtask transform_stage_generation
[2023-01-22T06:08:46.953+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host d68405340607
[2023-01-22T06:08:47.581+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-22T06:08:47.654+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T06:08:47.665+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-22T06:08:47.701+0000] {spark_submit.py:495} INFO - /home/***/.local/bin/spark-submit: line 27: /opt/spark/bin/spark-class: No such file or directory
[2023-01-22T06:08:47.758+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 127.
[2023-01-22T06:08:47.778+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230122T060846, end_date=20230122T060847
[2023-01-22T06:08:47.836+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 304 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 127.; 242)
[2023-01-22T06:08:47.903+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T06:08:47.978+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T07:45:01.962+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T07:45:01.974+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T07:45:01.975+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T07:45:01.975+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-22T07:45:01.975+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T07:45:01.988+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-22T07:45:01.996+0000] {standard_task_runner.py:55} INFO - Started process 450 to run task
[2023-01-22T07:45:02.001+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '316', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp7c2ztny6']
[2023-01-22T07:45:02.004+0000] {standard_task_runner.py:83} INFO - Job 316: Subtask transform_stage_generation
[2023-01-22T07:45:02.090+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 16233a798013
[2023-01-22T07:45:02.192+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-22T07:45:02.220+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T07:45:02.222+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-22T07:45:04.942+0000] {spark_submit.py:495} INFO - 23/01/22 07:45:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T07:45:05.191+0000] {spark_submit.py:495} INFO - 23/01/22 07:45:05 WARN DependencyUtils: Local jar /opt/***/gcs-connector-hadoop3-latest.jar does not exist, skipping.
[2023-01-22T07:45:05.192+0000] {spark_submit.py:495} INFO - 23/01/22 07:45:05 WARN DependencyUtils: Local jar /opt/***/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-22T07:45:05.554+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T07:45:05.554+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 64, in <module>
[2023-01-22T07:45:05.554+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T07:45:05.554+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 16, in main
[2023-01-22T07:45:05.555+0000] {spark_submit.py:495} INFO - start_label = start.strftime("%Y%m%d%H%M")
[2023-01-22T07:45:05.555+0000] {spark_submit.py:495} INFO - AttributeError: 'str' object has no attribute 'strftime'
[2023-01-22T07:45:05.571+0000] {spark_submit.py:495} INFO - 23/01/22 07:45:05 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T07:45:05.572+0000] {spark_submit.py:495} INFO - 23/01/22 07:45:05 INFO ShutdownHookManager: Deleting directory /tmp/spark-e6e05eb9-c4eb-4e67-8707-d33f2524aaad
[2023-01-22T07:45:05.628+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-22T07:45:05.632+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230122T074501, end_date=20230122T074505
[2023-01-22T07:45:05.644+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 316 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 450)
[2023-01-22T07:45:05.676+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T07:45:05.695+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T07:48:18.644+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T07:48:18.656+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T07:48:18.656+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T07:48:18.656+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-22T07:48:18.657+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T07:48:18.673+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-22T07:48:18.685+0000] {standard_task_runner.py:55} INFO - Started process 761 to run task
[2023-01-22T07:48:18.689+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '320', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpoi7yutsd']
[2023-01-22T07:48:18.693+0000] {standard_task_runner.py:83} INFO - Job 320: Subtask transform_stage_generation
[2023-01-22T07:48:18.762+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 16233a798013
[2023-01-22T07:48:18.831+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-22T07:48:18.840+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T07:48:18.841+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-22T07:48:22.178+0000] {spark_submit.py:495} INFO - 23/01/22 07:48:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T07:48:22.441+0000] {spark_submit.py:495} INFO - 23/01/22 07:48:22 WARN DependencyUtils: Local jar /opt/***/gcs-connector-hadoop3-latest.jar does not exist, skipping.
[2023-01-22T07:48:22.442+0000] {spark_submit.py:495} INFO - 23/01/22 07:48:22 WARN DependencyUtils: Local jar /opt/***/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-22T07:48:23.003+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T07:48:23.003+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 64, in <module>
[2023-01-22T07:48:23.003+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T07:48:23.004+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 28, in main
[2023-01-22T07:48:23.004+0000] {spark_submit.py:495} INFO - SparkSession.builder.appName("gcp_playground")
[2023-01-22T07:48:23.004+0000] {spark_submit.py:495} INFO - NameError: name 'SparkSession' is not defined
[2023-01-22T07:48:23.034+0000] {spark_submit.py:495} INFO - 23/01/22 07:48:23 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T07:48:23.036+0000] {spark_submit.py:495} INFO - 23/01/22 07:48:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-7f6f16ce-735f-4fa7-86d4-4a7fec82652d
[2023-01-22T07:48:23.123+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-22T07:48:23.128+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230122T074818, end_date=20230122T074823
[2023-01-22T07:48:23.146+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 320 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 761)
[2023-01-22T07:48:23.205+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T07:48:23.230+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T07:49:50.265+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T07:49:50.280+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T07:49:50.280+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T07:49:50.280+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-22T07:49:50.281+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T07:49:50.296+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-22T07:49:50.308+0000] {standard_task_runner.py:55} INFO - Started process 1025 to run task
[2023-01-22T07:49:50.314+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '325', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmprg310p_g']
[2023-01-22T07:49:50.318+0000] {standard_task_runner.py:83} INFO - Job 325: Subtask transform_stage_generation
[2023-01-22T07:49:50.403+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 16233a798013
[2023-01-22T07:49:50.499+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-22T07:49:50.509+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T07:49:50.510+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-22T07:49:53.181+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T07:49:53.568+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:53 WARN DependencyUtils: Local jar /opt/***/gcs-connector-hadoop3-latest.jar does not exist, skipping.
[2023-01-22T07:49:53.569+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:53 WARN DependencyUtils: Local jar /opt/***/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-22T07:49:55.369+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:55 INFO SparkContext: Running Spark version 3.3.1
[2023-01-22T07:49:55.399+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:55 INFO ResourceUtils: ==============================================================
[2023-01-22T07:49:55.399+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:55 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-22T07:49:55.400+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:55 INFO ResourceUtils: ==============================================================
[2023-01-22T07:49:55.401+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:55 INFO SparkContext: Submitted application: gcp_playground
[2023-01-22T07:49:55.432+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:55 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-22T07:49:55.438+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:55 INFO ResourceProfile: Limiting resource is cpu
[2023-01-22T07:49:55.440+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:55 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-22T07:49:55.518+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:55 INFO SecurityManager: Changing view acls to: ***
[2023-01-22T07:49:55.519+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:55 INFO SecurityManager: Changing modify acls to: ***
[2023-01-22T07:49:55.520+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:55 INFO SecurityManager: Changing view acls groups to:
[2023-01-22T07:49:55.521+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:55 INFO SecurityManager: Changing modify acls groups to:
[2023-01-22T07:49:55.522+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:55 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-22T07:49:55.896+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:55 INFO Utils: Successfully started service 'sparkDriver' on port 32865.
[2023-01-22T07:49:55.945+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:55 INFO SparkEnv: Registering MapOutputTracker
[2023-01-22T07:49:56.004+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:56 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-22T07:49:56.055+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:56 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-22T07:49:56.056+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:56 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-22T07:49:56.062+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:56 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-22T07:49:56.099+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:56 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-40a7e933-cd8e-4e5f-8309-9cb2f16f1a09
[2023-01-22T07:49:56.122+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:56 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-22T07:49:56.143+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:56 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-22T07:49:56.643+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:56 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-01-22T07:49:56.742+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:56 INFO SparkContext: Added JAR /opt/spark/resources/gcs-connector-hadoop3-latest.jar at spark://16233a798013:32865/jars/gcs-connector-hadoop3-latest.jar with timestamp 1674373795363
[2023-01-22T07:49:56.742+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:56 INFO SparkContext: Added JAR /opt/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://16233a798013:32865/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1674373795363
[2023-01-22T07:49:56.927+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:56 INFO Executor: Starting executor ID driver on host 16233a798013
[2023-01-22T07:49:56.946+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:56 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-22T07:49:56.984+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:56 INFO Executor: Fetching spark://16233a798013:32865/jars/gcs-connector-hadoop3-latest.jar with timestamp 1674373795363
[2023-01-22T07:49:57.156+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:57 INFO TransportClientFactory: Successfully created connection to 16233a798013/192.168.32.7:32865 after 94 ms (0 ms spent in bootstraps)
[2023-01-22T07:49:57.181+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:57 INFO Utils: Fetching spark://16233a798013:32865/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-bc3934f5-7c91-4411-9d45-e3c72c49846c/userFiles-97140927-74ec-482f-943e-d7451fb7a6f7/fetchFileTemp17119910698754061970.tmp
[2023-01-22T07:49:57.710+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:57 INFO Executor: Adding file:/tmp/spark-bc3934f5-7c91-4411-9d45-e3c72c49846c/userFiles-97140927-74ec-482f-943e-d7451fb7a6f7/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-22T07:49:57.710+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:57 INFO Executor: Fetching spark://16233a798013:32865/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1674373795363
[2023-01-22T07:49:57.712+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:57 INFO Utils: Fetching spark://16233a798013:32865/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-bc3934f5-7c91-4411-9d45-e3c72c49846c/userFiles-97140927-74ec-482f-943e-d7451fb7a6f7/fetchFileTemp10683315057589947344.tmp
[2023-01-22T07:49:58.225+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:58 INFO Executor: Adding file:/tmp/spark-bc3934f5-7c91-4411-9d45-e3c72c49846c/userFiles-97140927-74ec-482f-943e-d7451fb7a6f7/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-22T07:49:58.274+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:58 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46529.
[2023-01-22T07:49:58.275+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:58 INFO NettyBlockTransferService: Server created on 16233a798013:46529
[2023-01-22T07:49:58.283+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:58 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-22T07:49:58.311+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:58 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 16233a798013, 46529, None)
[2023-01-22T07:49:58.321+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:58 INFO BlockManagerMasterEndpoint: Registering block manager 16233a798013:46529 with 434.4 MiB RAM, BlockManagerId(driver, 16233a798013, 46529, None)
[2023-01-22T07:49:58.329+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:58 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 16233a798013, 46529, None)
[2023-01-22T07:49:58.331+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:58 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 16233a798013, 46529, None)
[2023-01-22T07:49:59.183+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:59 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-22T07:49:59.187+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:59 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-22T07:50:01.809+0000] {spark_submit.py:495} INFO - 23/01/22 07:50:01 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: gs://entsoe_analytics_1009//opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010100.json.
[2023-01-22T07:50:01.809+0000] {spark_submit.py:495} INFO - org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "gs"
[2023-01-22T07:50:01.810+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2023-01-22T07:50:01.810+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T07:50:01.810+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T07:50:01.811+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T07:50:01.811+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T07:50:01.811+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T07:50:01.811+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T07:50:01.812+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)
[2023-01-22T07:50:01.812+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:370)
[2023-01-22T07:50:01.812+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T07:50:01.812+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T07:50:01.813+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T07:50:01.813+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T07:50:01.813+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T07:50:01.813+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T07:50:01.814+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T07:50:01.814+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T07:50:01.814+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T07:50:01.815+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T07:50:01.815+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T07:50:01.815+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T07:50:01.815+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T07:50:01.816+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T07:50:01.816+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T07:50:01.816+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T07:50:01.817+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T07:50:01.861+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T07:50:01.862+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 67, in <module>
[2023-01-22T07:50:01.866+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T07:50:01.867+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 54, in main
[2023-01-22T07:50:01.867+0000] {spark_submit.py:495} INFO - df_spark.write.format("bigquery").option("project", "rafzul-analytics-1009") \
[2023-01-22T07:50:01.867+0000] {spark_submit.py:495} INFO - UnboundLocalError: local variable 'df_spark' referenced before assignment
[2023-01-22T07:50:01.959+0000] {spark_submit.py:495} INFO - 23/01/22 07:50:01 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-22T07:50:01.977+0000] {spark_submit.py:495} INFO - 23/01/22 07:50:01 INFO SparkUI: Stopped Spark web UI at http://16233a798013:4040
[2023-01-22T07:50:01.997+0000] {spark_submit.py:495} INFO - 23/01/22 07:50:01 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-22T07:50:02.017+0000] {spark_submit.py:495} INFO - 23/01/22 07:50:02 INFO MemoryStore: MemoryStore cleared
[2023-01-22T07:50:02.018+0000] {spark_submit.py:495} INFO - 23/01/22 07:50:02 INFO BlockManager: BlockManager stopped
[2023-01-22T07:50:02.034+0000] {spark_submit.py:495} INFO - 23/01/22 07:50:02 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-22T07:50:02.040+0000] {spark_submit.py:495} INFO - 23/01/22 07:50:02 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-22T07:50:02.076+0000] {spark_submit.py:495} INFO - 23/01/22 07:50:02 INFO SparkContext: Successfully stopped SparkContext
[2023-01-22T07:50:02.077+0000] {spark_submit.py:495} INFO - 23/01/22 07:50:02 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T07:50:02.078+0000] {spark_submit.py:495} INFO - 23/01/22 07:50:02 INFO ShutdownHookManager: Deleting directory /tmp/spark-82b9d49a-0514-4886-af64-f7b79b35e4d9
[2023-01-22T07:50:02.086+0000] {spark_submit.py:495} INFO - 23/01/22 07:50:02 INFO ShutdownHookManager: Deleting directory /tmp/spark-bc3934f5-7c91-4411-9d45-e3c72c49846c/pyspark-5fc69fcf-3bd4-4660-9e0b-f62f957ac2f1
[2023-01-22T07:50:02.094+0000] {spark_submit.py:495} INFO - 23/01/22 07:50:02 INFO ShutdownHookManager: Deleting directory /tmp/spark-bc3934f5-7c91-4411-9d45-e3c72c49846c
[2023-01-22T07:50:02.215+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-22T07:50:02.222+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230122T074950, end_date=20230122T075002
[2023-01-22T07:50:02.258+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 325 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 1025)
[2023-01-22T07:50:02.297+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T07:50:02.330+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T07:58:21.546+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T07:58:21.564+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T07:58:21.565+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T07:58:21.565+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-22T07:58:21.565+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T07:58:21.586+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-22T07:58:21.598+0000] {standard_task_runner.py:55} INFO - Started process 2016 to run task
[2023-01-22T07:58:21.607+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '330', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpltk2dxyf']
[2023-01-22T07:58:21.612+0000] {standard_task_runner.py:83} INFO - Job 330: Subtask transform_stage_generation
[2023-01-22T07:58:21.753+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 16233a798013
[2023-01-22T07:58:21.904+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-22T07:58:21.924+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T07:58:21.929+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-22T07:58:29.042+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T07:58:29.529+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:29 WARN DependencyUtils: Local jar /opt/***/gcs-connector-hadoop3-latest.jar does not exist, skipping.
[2023-01-22T07:58:29.530+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:29 WARN DependencyUtils: Local jar /opt/***/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-22T07:58:32.863+0000] {spark_submit.py:495} INFO - /opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010100.json
[2023-01-22T07:58:33.148+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:33 INFO SparkContext: Running Spark version 3.3.1
[2023-01-22T07:58:33.205+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:33 INFO ResourceUtils: ==============================================================
[2023-01-22T07:58:33.206+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:33 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-22T07:58:33.206+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:33 INFO ResourceUtils: ==============================================================
[2023-01-22T07:58:33.207+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:33 INFO SparkContext: Submitted application: gcp_playground
[2023-01-22T07:58:33.262+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:33 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-22T07:58:33.275+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:33 INFO ResourceProfile: Limiting resource is cpu
[2023-01-22T07:58:33.277+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:33 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-22T07:58:33.404+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:33 INFO SecurityManager: Changing view acls to: ***
[2023-01-22T07:58:33.405+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:33 INFO SecurityManager: Changing modify acls to: ***
[2023-01-22T07:58:33.406+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:33 INFO SecurityManager: Changing view acls groups to:
[2023-01-22T07:58:33.407+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:33 INFO SecurityManager: Changing modify acls groups to:
[2023-01-22T07:58:33.408+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:33 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-22T07:58:34.150+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:34 INFO Utils: Successfully started service 'sparkDriver' on port 34383.
[2023-01-22T07:58:34.282+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:34 INFO SparkEnv: Registering MapOutputTracker
[2023-01-22T07:58:34.453+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:34 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-22T07:58:34.556+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:34 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-22T07:58:34.557+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:34 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-22T07:58:34.567+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:34 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-22T07:58:34.636+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:34 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-2b26c3f0-dddc-4008-ad09-a2a3a1fd61ae
[2023-01-22T07:58:34.703+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:34 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-22T07:58:34.781+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:34 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-22T07:58:35.596+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:35 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-22T07:58:35.657+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:35 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2023-01-22T07:58:35.807+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:35 INFO SparkContext: Added JAR /opt/spark/resources/gcs-connector-hadoop3-latest.jar at spark://16233a798013:34383/jars/gcs-connector-hadoop3-latest.jar with timestamp 1674374313137
[2023-01-22T07:58:35.808+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:35 INFO SparkContext: Added JAR /opt/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://16233a798013:34383/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1674374313137
[2023-01-22T07:58:36.135+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:36 INFO Executor: Starting executor ID driver on host 16233a798013
[2023-01-22T07:58:36.158+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:36 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-22T07:58:36.210+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:36 INFO Executor: Fetching spark://16233a798013:34383/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1674374313137
[2023-01-22T07:58:36.354+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:36 INFO TransportClientFactory: Successfully created connection to 16233a798013/192.168.32.7:34383 after 74 ms (0 ms spent in bootstraps)
[2023-01-22T07:58:36.374+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:36 INFO Utils: Fetching spark://16233a798013:34383/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-30bdaed2-2f51-4ddf-94b4-f578f3ad695f/userFiles-2da060fb-9379-46f4-b043-bbdc19232cd0/fetchFileTemp5218386812464031243.tmp
[2023-01-22T07:58:36.802+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:36 INFO Executor: Adding file:/tmp/spark-30bdaed2-2f51-4ddf-94b4-f578f3ad695f/userFiles-2da060fb-9379-46f4-b043-bbdc19232cd0/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-22T07:58:36.804+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:36 INFO Executor: Fetching spark://16233a798013:34383/jars/gcs-connector-hadoop3-latest.jar with timestamp 1674374313137
[2023-01-22T07:58:36.809+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:36 INFO Utils: Fetching spark://16233a798013:34383/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-30bdaed2-2f51-4ddf-94b4-f578f3ad695f/userFiles-2da060fb-9379-46f4-b043-bbdc19232cd0/fetchFileTemp13886735255540351745.tmp
[2023-01-22T07:58:37.095+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:37 INFO Executor: Adding file:/tmp/spark-30bdaed2-2f51-4ddf-94b4-f578f3ad695f/userFiles-2da060fb-9379-46f4-b043-bbdc19232cd0/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-22T07:58:37.104+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:37 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41867.
[2023-01-22T07:58:37.104+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:37 INFO NettyBlockTransferService: Server created on 16233a798013:41867
[2023-01-22T07:58:37.106+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:37 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-22T07:58:37.115+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:37 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 16233a798013, 41867, None)
[2023-01-22T07:58:37.119+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:37 INFO BlockManagerMasterEndpoint: Registering block manager 16233a798013:41867 with 434.4 MiB RAM, BlockManagerId(driver, 16233a798013, 41867, None)
[2023-01-22T07:58:37.124+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:37 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 16233a798013, 41867, None)
[2023-01-22T07:58:37.125+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:37 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 16233a798013, 41867, None)
[2023-01-22T07:58:37.757+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:37 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-22T07:58:37.761+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:37 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-22T07:58:38.870+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:38 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: gs://entsoe_analytics_1009//opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010100.json.
[2023-01-22T07:58:38.871+0000] {spark_submit.py:495} INFO - org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "gs"
[2023-01-22T07:58:38.871+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2023-01-22T07:58:38.871+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T07:58:38.871+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T07:58:38.871+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T07:58:38.871+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T07:58:38.871+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T07:58:38.871+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T07:58:38.872+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)
[2023-01-22T07:58:38.872+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:370)
[2023-01-22T07:58:38.872+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T07:58:38.872+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T07:58:38.872+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T07:58:38.872+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T07:58:38.872+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T07:58:38.873+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T07:58:38.873+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T07:58:38.873+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T07:58:38.873+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T07:58:38.873+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T07:58:38.873+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T07:58:38.873+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T07:58:38.873+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T07:58:38.874+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T07:58:38.874+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T07:58:38.874+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T07:58:38.874+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T07:58:38.894+0000] {spark_submit.py:495} INFO - Error, ada An error occurred while calling o38.load.
[2023-01-22T07:58:38.895+0000] {spark_submit.py:495} INFO - : org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "gs"
[2023-01-22T07:58:38.895+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2023-01-22T07:58:38.895+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T07:58:38.895+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T07:58:38.895+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T07:58:38.896+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T07:58:38.896+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T07:58:38.896+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T07:58:38.896+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:752)
[2023-01-22T07:58:38.896+0000] {spark_submit.py:495} INFO - at scala.collection.immutable.List.map(List.scala:246)
[2023-01-22T07:58:38.896+0000] {spark_submit.py:495} INFO - at scala.collection.immutable.List.map(List.scala:79)
[2023-01-22T07:58:38.896+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:750)
[2023-01-22T07:58:38.897+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:579)
[2023-01-22T07:58:38.897+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:408)
[2023-01-22T07:58:38.897+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T07:58:38.897+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T07:58:38.897+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T07:58:38.897+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T07:58:38.897+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T07:58:38.898+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T07:58:38.898+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T07:58:38.898+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T07:58:38.898+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T07:58:38.898+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T07:58:38.898+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T07:58:38.899+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T07:58:38.899+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T07:58:38.899+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T07:58:38.899+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T07:58:38.899+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T07:58:38.899+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T07:58:38.899+0000] {spark_submit.py:495} INFO - 
[2023-01-22T07:58:38.900+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T07:58:38.900+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 69, in <module>
[2023-01-22T07:58:38.900+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T07:58:38.900+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 56, in main
[2023-01-22T07:58:38.900+0000] {spark_submit.py:495} INFO - df_spark.write.format("bigquery").option("project", "rafzul-analytics-1009") \
[2023-01-22T07:58:38.900+0000] {spark_submit.py:495} INFO - UnboundLocalError: local variable 'df_spark' referenced before assignment
[2023-01-22T07:58:38.973+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:38 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-22T07:58:38.985+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:38 INFO SparkUI: Stopped Spark web UI at http://16233a798013:4041
[2023-01-22T07:58:39.002+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:39 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-22T07:58:39.019+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:39 INFO MemoryStore: MemoryStore cleared
[2023-01-22T07:58:39.019+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:39 INFO BlockManager: BlockManager stopped
[2023-01-22T07:58:39.029+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:39 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-22T07:58:39.033+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:39 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-22T07:58:39.062+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:39 INFO SparkContext: Successfully stopped SparkContext
[2023-01-22T07:58:39.062+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:39 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T07:58:39.063+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-30bdaed2-2f51-4ddf-94b4-f578f3ad695f
[2023-01-22T07:58:39.076+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-30bdaed2-2f51-4ddf-94b4-f578f3ad695f/pyspark-4330ca6b-852a-45ac-b413-780b3059451b
[2023-01-22T07:58:39.082+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-f60bee33-67b7-44bf-a8d3-5200c60b3f12
[2023-01-22T07:58:39.154+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-22T07:58:39.159+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230122T075821, end_date=20230122T075839
[2023-01-22T07:58:39.171+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 330 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 2016)
[2023-01-22T07:58:39.221+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T07:58:39.236+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T08:38:41.403+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T08:38:41.420+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T08:38:41.420+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T08:38:41.421+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-22T08:38:41.421+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T08:38:41.433+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-22T08:38:41.440+0000] {standard_task_runner.py:55} INFO - Started process 359 to run task
[2023-01-22T08:38:41.445+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '373', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp78l3ba9q']
[2023-01-22T08:38:41.448+0000] {standard_task_runner.py:83} INFO - Job 373: Subtask transform_stage_generation
[2023-01-22T08:38:41.513+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 9d8db2d97423
[2023-01-22T08:38:41.590+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-22T08:38:41.600+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T08:38:41.601+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-22T08:38:45.754+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T08:38:46.063+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:46 WARN DependencyUtils: Local jar /opt/***/gcs-connector-hadoop3-latest.jar does not exist, skipping.
[2023-01-22T08:38:46.063+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:46 WARN DependencyUtils: Local jar /opt/***/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-22T08:38:47.296+0000] {spark_submit.py:495} INFO - /opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010100.json
[2023-01-22T08:38:47.392+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:47 INFO SparkContext: Running Spark version 3.3.1
[2023-01-22T08:38:47.429+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:47 INFO ResourceUtils: ==============================================================
[2023-01-22T08:38:47.430+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:47 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-22T08:38:47.431+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:47 INFO ResourceUtils: ==============================================================
[2023-01-22T08:38:47.432+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:47 INFO SparkContext: Submitted application: gcp_playground
[2023-01-22T08:38:47.467+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:47 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-22T08:38:47.475+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:47 INFO ResourceProfile: Limiting resource is cpu
[2023-01-22T08:38:47.476+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:47 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-22T08:38:47.563+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:47 INFO SecurityManager: Changing view acls to: ***
[2023-01-22T08:38:47.564+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:47 INFO SecurityManager: Changing modify acls to: ***
[2023-01-22T08:38:47.564+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:47 INFO SecurityManager: Changing view acls groups to:
[2023-01-22T08:38:47.565+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:47 INFO SecurityManager: Changing modify acls groups to:
[2023-01-22T08:38:47.565+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:47 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-22T08:38:47.968+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:47 INFO Utils: Successfully started service 'sparkDriver' on port 37125.
[2023-01-22T08:38:48.022+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO SparkEnv: Registering MapOutputTracker
[2023-01-22T08:38:48.081+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-22T08:38:48.164+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-22T08:38:48.165+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-22T08:38:48.171+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-22T08:38:48.203+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ac15dfe1-7fef-4917-be0e-6d0f11272ee4
[2023-01-22T08:38:48.226+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-22T08:38:48.247+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-22T08:38:48.602+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-01-22T08:38:48.690+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 ERROR SparkContext: Failed to add file:/opt/***/gcs-connector-hadoop3-latest.jar to Spark environment
[2023-01-22T08:38:48.690+0000] {spark_submit.py:495} INFO - java.io.FileNotFoundException: Jar /opt/***/gcs-connector-hadoop3-latest.jar not found
[2023-01-22T08:38:48.690+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1949)
[2023-01-22T08:38:48.691+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.addJar(SparkContext.scala:2004)
[2023-01-22T08:38:48.691+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:507)
[2023-01-22T08:38:48.691+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:507)
[2023-01-22T08:38:48.691+0000] {spark_submit.py:495} INFO - at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
[2023-01-22T08:38:48.691+0000] {spark_submit.py:495} INFO - at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
[2023-01-22T08:38:48.691+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
[2023-01-22T08:38:48.691+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.<init>(SparkContext.scala:507)
[2023-01-22T08:38:48.691+0000] {spark_submit.py:495} INFO - at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
[2023-01-22T08:38:48.691+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
[2023-01-22T08:38:48.691+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
[2023-01-22T08:38:48.692+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
[2023-01-22T08:38:48.692+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
[2023-01-22T08:38:48.692+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
[2023-01-22T08:38:48.692+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T08:38:48.692+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:238)
[2023-01-22T08:38:48.692+0000] {spark_submit.py:495} INFO - at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
[2023-01-22T08:38:48.692+0000] {spark_submit.py:495} INFO - at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
[2023-01-22T08:38:48.692+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T08:38:48.692+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T08:38:48.692+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T08:38:48.692+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 ERROR SparkContext: Failed to add file:/opt/***/spark-bigquery-with-dependencies_2.13-0.27.1.jar to Spark environment
[2023-01-22T08:38:48.693+0000] {spark_submit.py:495} INFO - java.io.FileNotFoundException: Jar /opt/***/spark-bigquery-with-dependencies_2.13-0.27.1.jar not found
[2023-01-22T08:38:48.693+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1949)
[2023-01-22T08:38:48.693+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.addJar(SparkContext.scala:2004)
[2023-01-22T08:38:48.693+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:507)
[2023-01-22T08:38:48.693+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:507)
[2023-01-22T08:38:48.693+0000] {spark_submit.py:495} INFO - at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
[2023-01-22T08:38:48.693+0000] {spark_submit.py:495} INFO - at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
[2023-01-22T08:38:48.693+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
[2023-01-22T08:38:48.694+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.<init>(SparkContext.scala:507)
[2023-01-22T08:38:48.694+0000] {spark_submit.py:495} INFO - at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
[2023-01-22T08:38:48.694+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
[2023-01-22T08:38:48.694+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
[2023-01-22T08:38:48.694+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
[2023-01-22T08:38:48.694+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
[2023-01-22T08:38:48.694+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
[2023-01-22T08:38:48.695+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T08:38:48.695+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:238)
[2023-01-22T08:38:48.695+0000] {spark_submit.py:495} INFO - at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
[2023-01-22T08:38:48.695+0000] {spark_submit.py:495} INFO - at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
[2023-01-22T08:38:48.695+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T08:38:48.695+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T08:38:48.696+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T08:38:48.813+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO Executor: Starting executor ID driver on host 9d8db2d97423
[2023-01-22T08:38:48.825+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-22T08:38:48.860+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45679.
[2023-01-22T08:38:48.860+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO NettyBlockTransferService: Server created on 9d8db2d97423:45679
[2023-01-22T08:38:48.863+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-22T08:38:48.877+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 9d8db2d97423, 45679, None)
[2023-01-22T08:38:48.883+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO BlockManagerMasterEndpoint: Registering block manager 9d8db2d97423:45679 with 434.4 MiB RAM, BlockManagerId(driver, 9d8db2d97423, 45679, None)
[2023-01-22T08:38:48.890+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 9d8db2d97423, 45679, None)
[2023-01-22T08:38:48.892+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 9d8db2d97423, 45679, None)
[2023-01-22T08:38:49.698+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:49 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-22T08:38:49.707+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:49 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-22T08:38:51.343+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:51 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: gs://entsoe_analytics_1009//opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010100.json.
[2023-01-22T08:38:51.343+0000] {spark_submit.py:495} INFO - org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "gs"
[2023-01-22T08:38:51.343+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2023-01-22T08:38:51.344+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T08:38:51.344+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T08:38:51.344+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T08:38:51.344+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T08:38:51.344+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T08:38:51.344+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T08:38:51.345+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)
[2023-01-22T08:38:51.345+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:370)
[2023-01-22T08:38:51.345+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T08:38:51.345+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T08:38:51.345+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T08:38:51.346+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T08:38:51.346+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T08:38:51.346+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T08:38:51.346+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T08:38:51.346+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T08:38:51.347+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T08:38:51.347+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T08:38:51.347+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T08:38:51.347+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T08:38:51.347+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T08:38:51.348+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T08:38:51.348+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T08:38:51.348+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T08:38:51.348+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T08:38:51.385+0000] {spark_submit.py:495} INFO - Error, ada An error occurred while calling o37.load.
[2023-01-22T08:38:51.386+0000] {spark_submit.py:495} INFO - : org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "gs"
[2023-01-22T08:38:51.386+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2023-01-22T08:38:51.386+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T08:38:51.386+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T08:38:51.387+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T08:38:51.387+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T08:38:51.387+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T08:38:51.387+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T08:38:51.388+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:752)
[2023-01-22T08:38:51.388+0000] {spark_submit.py:495} INFO - at scala.collection.immutable.List.map(List.scala:246)
[2023-01-22T08:38:51.388+0000] {spark_submit.py:495} INFO - at scala.collection.immutable.List.map(List.scala:79)
[2023-01-22T08:38:51.388+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:750)
[2023-01-22T08:38:51.388+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:579)
[2023-01-22T08:38:51.388+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:408)
[2023-01-22T08:38:51.389+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T08:38:51.389+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T08:38:51.389+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T08:38:51.389+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T08:38:51.389+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T08:38:51.389+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T08:38:51.390+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T08:38:51.390+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T08:38:51.390+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T08:38:51.390+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T08:38:51.390+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T08:38:51.391+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T08:38:51.391+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T08:38:51.391+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T08:38:51.391+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T08:38:51.391+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T08:38:51.391+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T08:38:51.392+0000] {spark_submit.py:495} INFO - 
[2023-01-22T08:38:51.392+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T08:38:51.392+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 68, in <module>
[2023-01-22T08:38:51.392+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T08:38:51.392+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 55, in main
[2023-01-22T08:38:51.392+0000] {spark_submit.py:495} INFO - df_spark.write.format("bigquery").option("project", "rafzul-analytics-1009") \
[2023-01-22T08:38:51.393+0000] {spark_submit.py:495} INFO - UnboundLocalError: local variable 'df_spark' referenced before assignment
[2023-01-22T08:38:51.472+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:51 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-22T08:38:51.491+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:51 INFO SparkUI: Stopped Spark web UI at http://9d8db2d97423:4040
[2023-01-22T08:38:51.516+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:51 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-22T08:38:51.561+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:51 INFO MemoryStore: MemoryStore cleared
[2023-01-22T08:38:51.562+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:51 INFO BlockManager: BlockManager stopped
[2023-01-22T08:38:51.598+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:51 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-22T08:38:51.609+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:51 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-22T08:38:51.638+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:51 INFO SparkContext: Successfully stopped SparkContext
[2023-01-22T08:38:51.645+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:51 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T08:38:51.646+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-2eddc059-6165-4f9b-8a32-6723a661bc0f/pyspark-1daf798c-5639-412c-b7c2-fd43024b60b0
[2023-01-22T08:38:51.656+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-f1c93bf4-3833-404e-b926-dbb257354a5a
[2023-01-22T08:38:51.663+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-2eddc059-6165-4f9b-8a32-6723a661bc0f
[2023-01-22T08:38:51.753+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-22T08:38:51.757+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230122T083841, end_date=20230122T083851
[2023-01-22T08:38:51.772+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 373 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 359)
[2023-01-22T08:38:51.813+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T08:38:51.834+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T08:48:47.816+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T08:48:47.825+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T08:48:47.825+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T08:48:47.826+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-22T08:48:47.826+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T08:48:47.837+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-22T08:48:47.844+0000] {standard_task_runner.py:55} INFO - Started process 1493 to run task
[2023-01-22T08:48:47.848+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '379', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpuest1vu4']
[2023-01-22T08:48:47.851+0000] {standard_task_runner.py:83} INFO - Job 379: Subtask transform_stage_generation
[2023-01-22T08:48:47.914+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 9d8db2d97423
[2023-01-22T08:48:47.985+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-22T08:48:47.994+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T08:48:47.995+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-22T08:48:51.058+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T08:48:51.372+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:51 WARN DependencyUtils: Local jar /opt/***/gcs-connector-hadoop3-latest.jar does not exist, skipping.
[2023-01-22T08:48:51.373+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:51 WARN DependencyUtils: Local jar /opt/***/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-22T08:48:52.546+0000] {spark_submit.py:495} INFO - /opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010100.json
[2023-01-22T08:48:52.644+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:52 INFO SparkContext: Running Spark version 3.3.1
[2023-01-22T08:48:52.699+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:52 INFO ResourceUtils: ==============================================================
[2023-01-22T08:48:52.706+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:52 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-22T08:48:52.706+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:52 INFO ResourceUtils: ==============================================================
[2023-01-22T08:48:52.707+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:52 INFO SparkContext: Submitted application: gcp_playground
[2023-01-22T08:48:52.736+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:52 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-22T08:48:52.743+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:52 INFO ResourceProfile: Limiting resource is cpu
[2023-01-22T08:48:52.745+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:52 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-22T08:48:52.817+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:52 INFO SecurityManager: Changing view acls to: ***
[2023-01-22T08:48:52.817+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:52 INFO SecurityManager: Changing modify acls to: ***
[2023-01-22T08:48:52.818+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:52 INFO SecurityManager: Changing view acls groups to:
[2023-01-22T08:48:52.818+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:52 INFO SecurityManager: Changing modify acls groups to:
[2023-01-22T08:48:52.819+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:52 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-22T08:48:53.226+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:53 INFO Utils: Successfully started service 'sparkDriver' on port 37739.
[2023-01-22T08:48:53.296+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:53 INFO SparkEnv: Registering MapOutputTracker
[2023-01-22T08:48:53.365+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:53 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-22T08:48:53.432+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:53 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-22T08:48:53.433+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:53 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-22T08:48:53.441+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:53 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-22T08:48:53.487+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:53 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-81f7c4a0-a9f7-4edd-a8cc-320c7b32abcc
[2023-01-22T08:48:53.517+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:53 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-22T08:48:53.561+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:53 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-22T08:48:54.073+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:54 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-01-22T08:48:54.228+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:54 ERROR SparkContext: Failed to add file:/opt/***/gcs-connector-hadoop3-latest.jar to Spark environment
[2023-01-22T08:48:54.229+0000] {spark_submit.py:495} INFO - java.io.FileNotFoundException: Jar /opt/***/gcs-connector-hadoop3-latest.jar not found
[2023-01-22T08:48:54.229+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1949)
[2023-01-22T08:48:54.229+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.addJar(SparkContext.scala:2004)
[2023-01-22T08:48:54.229+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:507)
[2023-01-22T08:48:54.230+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:507)
[2023-01-22T08:48:54.230+0000] {spark_submit.py:495} INFO - at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
[2023-01-22T08:48:54.230+0000] {spark_submit.py:495} INFO - at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
[2023-01-22T08:48:54.230+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
[2023-01-22T08:48:54.230+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.<init>(SparkContext.scala:507)
[2023-01-22T08:48:54.230+0000] {spark_submit.py:495} INFO - at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
[2023-01-22T08:48:54.231+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
[2023-01-22T08:48:54.231+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
[2023-01-22T08:48:54.231+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
[2023-01-22T08:48:54.231+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
[2023-01-22T08:48:54.231+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
[2023-01-22T08:48:54.231+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T08:48:54.232+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:238)
[2023-01-22T08:48:54.232+0000] {spark_submit.py:495} INFO - at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
[2023-01-22T08:48:54.232+0000] {spark_submit.py:495} INFO - at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
[2023-01-22T08:48:54.232+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T08:48:54.232+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T08:48:54.232+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T08:48:54.236+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:54 ERROR SparkContext: Failed to add file:/opt/***/spark-bigquery-with-dependencies_2.13-0.27.1.jar to Spark environment
[2023-01-22T08:48:54.237+0000] {spark_submit.py:495} INFO - java.io.FileNotFoundException: Jar /opt/***/spark-bigquery-with-dependencies_2.13-0.27.1.jar not found
[2023-01-22T08:48:54.237+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1949)
[2023-01-22T08:48:54.237+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.addJar(SparkContext.scala:2004)
[2023-01-22T08:48:54.237+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:507)
[2023-01-22T08:48:54.238+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:507)
[2023-01-22T08:48:54.238+0000] {spark_submit.py:495} INFO - at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
[2023-01-22T08:48:54.238+0000] {spark_submit.py:495} INFO - at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
[2023-01-22T08:48:54.238+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
[2023-01-22T08:48:54.238+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.<init>(SparkContext.scala:507)
[2023-01-22T08:48:54.238+0000] {spark_submit.py:495} INFO - at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
[2023-01-22T08:48:54.238+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
[2023-01-22T08:48:54.239+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
[2023-01-22T08:48:54.239+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
[2023-01-22T08:48:54.239+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
[2023-01-22T08:48:54.239+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
[2023-01-22T08:48:54.239+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T08:48:54.240+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:238)
[2023-01-22T08:48:54.240+0000] {spark_submit.py:495} INFO - at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
[2023-01-22T08:48:54.240+0000] {spark_submit.py:495} INFO - at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
[2023-01-22T08:48:54.240+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T08:48:54.240+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T08:48:54.240+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T08:48:54.376+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:54 INFO Executor: Starting executor ID driver on host 9d8db2d97423
[2023-01-22T08:48:54.390+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:54 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-22T08:48:54.429+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:54 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33217.
[2023-01-22T08:48:54.430+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:54 INFO NettyBlockTransferService: Server created on 9d8db2d97423:33217
[2023-01-22T08:48:54.434+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:54 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-22T08:48:54.445+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:54 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 9d8db2d97423, 33217, None)
[2023-01-22T08:48:54.455+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:54 INFO BlockManagerMasterEndpoint: Registering block manager 9d8db2d97423:33217 with 434.4 MiB RAM, BlockManagerId(driver, 9d8db2d97423, 33217, None)
[2023-01-22T08:48:54.475+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:54 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 9d8db2d97423, 33217, None)
[2023-01-22T08:48:54.477+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:54 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 9d8db2d97423, 33217, None)
[2023-01-22T08:48:55.194+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:55 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-22T08:48:55.199+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:55 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-22T08:48:56.796+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:56 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: gs://entsoe_analytics_1009//opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010100.json.
[2023-01-22T08:48:56.797+0000] {spark_submit.py:495} INFO - org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "gs"
[2023-01-22T08:48:56.797+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2023-01-22T08:48:56.797+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T08:48:56.797+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T08:48:56.797+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T08:48:56.797+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T08:48:56.798+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T08:48:56.798+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T08:48:56.798+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)
[2023-01-22T08:48:56.798+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:370)
[2023-01-22T08:48:56.798+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T08:48:56.798+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T08:48:56.799+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T08:48:56.799+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T08:48:56.799+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T08:48:56.799+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T08:48:56.799+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T08:48:56.799+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T08:48:56.799+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T08:48:56.800+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T08:48:56.800+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T08:48:56.800+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T08:48:56.800+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T08:48:56.800+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T08:48:56.800+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T08:48:56.800+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T08:48:56.800+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T08:48:56.849+0000] {spark_submit.py:495} INFO - Error, ada An error occurred while calling o37.load.
[2023-01-22T08:48:56.849+0000] {spark_submit.py:495} INFO - : org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "gs"
[2023-01-22T08:48:56.849+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2023-01-22T08:48:56.850+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T08:48:56.850+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T08:48:56.850+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T08:48:56.850+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T08:48:56.850+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T08:48:56.850+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T08:48:56.851+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:752)
[2023-01-22T08:48:56.851+0000] {spark_submit.py:495} INFO - at scala.collection.immutable.List.map(List.scala:246)
[2023-01-22T08:48:56.851+0000] {spark_submit.py:495} INFO - at scala.collection.immutable.List.map(List.scala:79)
[2023-01-22T08:48:56.851+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:750)
[2023-01-22T08:48:56.851+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:579)
[2023-01-22T08:48:56.851+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:408)
[2023-01-22T08:48:56.852+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T08:48:56.852+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T08:48:56.852+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T08:48:56.852+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T08:48:56.852+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T08:48:56.853+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T08:48:56.853+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T08:48:56.854+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T08:48:56.854+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T08:48:56.854+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T08:48:56.854+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T08:48:56.854+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T08:48:56.854+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T08:48:56.855+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T08:48:56.855+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T08:48:56.855+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T08:48:56.855+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T08:48:56.855+0000] {spark_submit.py:495} INFO - 
[2023-01-22T08:48:56.855+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T08:48:56.856+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 67, in <module>
[2023-01-22T08:48:56.856+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T08:48:56.856+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 54, in main
[2023-01-22T08:48:56.856+0000] {spark_submit.py:495} INFO - df_spark.write.format("bigquery").option("project", "rafzul-analytics-1009") \
[2023-01-22T08:48:56.856+0000] {spark_submit.py:495} INFO - UnboundLocalError: local variable 'df_spark' referenced before assignment
[2023-01-22T08:48:56.964+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:56 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-22T08:48:56.992+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:56 INFO SparkUI: Stopped Spark web UI at http://9d8db2d97423:4040
[2023-01-22T08:48:57.023+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:57 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-22T08:48:57.044+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:57 INFO MemoryStore: MemoryStore cleared
[2023-01-22T08:48:57.046+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:57 INFO BlockManager: BlockManager stopped
[2023-01-22T08:48:57.060+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:57 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-22T08:48:57.066+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:57 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-22T08:48:57.080+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:57 INFO SparkContext: Successfully stopped SparkContext
[2023-01-22T08:48:57.080+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:57 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T08:48:57.081+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-4eb560d6-c11a-452f-8d14-b2123ce76aed/pyspark-eb452fe4-eb33-4650-9585-8edc037e780a
[2023-01-22T08:48:57.092+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-4eb560d6-c11a-452f-8d14-b2123ce76aed
[2023-01-22T08:48:57.099+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-2430c496-8884-4f68-97c6-22115c374539
[2023-01-22T08:48:57.277+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-22T08:48:57.284+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230122T084847, end_date=20230122T084857
[2023-01-22T08:48:57.311+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 379 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 1493)
[2023-01-22T08:48:57.353+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T08:48:57.380+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T09:18:09.768+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T09:18:09.783+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T09:18:09.784+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T09:18:09.784+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-22T09:18:09.784+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T09:18:09.813+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-22T09:18:09.836+0000] {standard_task_runner.py:55} INFO - Started process 1037 to run task
[2023-01-22T09:18:09.842+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '390', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmplv19h03t']
[2023-01-22T09:18:09.846+0000] {standard_task_runner.py:83} INFO - Job 390: Subtask transform_stage_generation
[2023-01-22T09:18:09.971+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 266f60b86faa
[2023-01-22T09:18:10.111+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-22T09:18:10.148+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T09:18:10.153+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-22T09:18:23.537+0000] {spark_submit.py:495} INFO - /opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010100.json
[2023-01-22T09:18:23.786+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:23 INFO SparkContext: Running Spark version 3.3.1
[2023-01-22T09:18:24.050+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T09:18:24.484+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:24 INFO ResourceUtils: ==============================================================
[2023-01-22T09:18:24.494+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:24 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-22T09:18:24.494+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:24 INFO ResourceUtils: ==============================================================
[2023-01-22T09:18:24.497+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:24 INFO SparkContext: Submitted application: gcp_playground
[2023-01-22T09:18:24.644+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:24 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-22T09:18:24.670+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:24 INFO ResourceProfile: Limiting resource is cpu
[2023-01-22T09:18:24.675+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:24 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-22T09:18:24.916+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:24 INFO SecurityManager: Changing view acls to: ***
[2023-01-22T09:18:24.918+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:24 INFO SecurityManager: Changing modify acls to: ***
[2023-01-22T09:18:24.920+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:24 INFO SecurityManager: Changing view acls groups to:
[2023-01-22T09:18:24.923+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:24 INFO SecurityManager: Changing modify acls groups to:
[2023-01-22T09:18:24.932+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:24 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-22T09:18:26.133+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:26 INFO Utils: Successfully started service 'sparkDriver' on port 43745.
[2023-01-22T09:18:26.307+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:26 INFO SparkEnv: Registering MapOutputTracker
[2023-01-22T09:18:26.443+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:26 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-22T09:18:26.548+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:26 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-22T09:18:26.549+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:26 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-22T09:18:26.562+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:26 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-22T09:18:26.632+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:26 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-de5ec3dc-3b94-4b17-9283-e7934fb91f6e
[2023-01-22T09:18:26.672+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:26 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-22T09:18:26.718+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:26 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-22T09:18:27.393+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:27 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-22T09:18:27.438+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:27 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2023-01-22T09:18:27.614+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:27 INFO SparkContext: Added JAR /opt/spark/gcs-connector-hadoop3-latest.jar at spark://266f60b86faa:43745/jars/gcs-connector-hadoop3-latest.jar with timestamp 1674379103752
[2023-01-22T09:18:27.615+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:27 INFO SparkContext: Added JAR /opt/spark/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://266f60b86faa:43745/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1674379103752
[2023-01-22T09:18:27.881+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:27 INFO Executor: Starting executor ID driver on host 266f60b86faa
[2023-01-22T09:18:27.899+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:27 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-22T09:18:27.928+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:27 INFO Executor: Fetching spark://266f60b86faa:43745/jars/gcs-connector-hadoop3-latest.jar with timestamp 1674379103752
[2023-01-22T09:18:28.101+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:28 INFO TransportClientFactory: Successfully created connection to 266f60b86faa/192.168.64.4:43745 after 118 ms (0 ms spent in bootstraps)
[2023-01-22T09:18:28.128+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:28 INFO Utils: Fetching spark://266f60b86faa:43745/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-b1170c57-5237-4978-868c-3f74542340b8/userFiles-2fea31b0-018c-4e6e-ae8a-d0583e232458/fetchFileTemp11391983008603601308.tmp
[2023-01-22T09:18:29.055+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:29 INFO Executor: Adding file:/tmp/spark-b1170c57-5237-4978-868c-3f74542340b8/userFiles-2fea31b0-018c-4e6e-ae8a-d0583e232458/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-22T09:18:29.056+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:29 INFO Executor: Fetching spark://266f60b86faa:43745/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1674379103752
[2023-01-22T09:18:29.070+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:29 INFO Utils: Fetching spark://266f60b86faa:43745/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-b1170c57-5237-4978-868c-3f74542340b8/userFiles-2fea31b0-018c-4e6e-ae8a-d0583e232458/fetchFileTemp15336339629784943541.tmp
[2023-01-22T09:18:29.876+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:29 INFO Executor: Adding file:/tmp/spark-b1170c57-5237-4978-868c-3f74542340b8/userFiles-2fea31b0-018c-4e6e-ae8a-d0583e232458/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-22T09:18:29.938+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:29 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36009.
[2023-01-22T09:18:29.939+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:29 INFO NettyBlockTransferService: Server created on 266f60b86faa:36009
[2023-01-22T09:18:29.942+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:29 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-22T09:18:30.005+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:30 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 266f60b86faa, 36009, None)
[2023-01-22T09:18:30.059+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:30 INFO BlockManagerMasterEndpoint: Registering block manager 266f60b86faa:36009 with 434.4 MiB RAM, BlockManagerId(driver, 266f60b86faa, 36009, None)
[2023-01-22T09:18:30.134+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:30 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 266f60b86faa, 36009, None)
[2023-01-22T09:18:30.140+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:30 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 266f60b86faa, 36009, None)
[2023-01-22T09:18:31.938+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:31 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-22T09:18:31.958+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:31 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-22T09:18:36.035+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:36 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: gs://entsoe_analytics_1009//opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010100.json.
[2023-01-22T09:18:36.036+0000] {spark_submit.py:495} INFO - org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "gs"
[2023-01-22T09:18:36.036+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2023-01-22T09:18:36.036+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T09:18:36.037+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T09:18:36.037+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T09:18:36.037+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T09:18:36.038+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T09:18:36.038+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T09:18:36.039+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)
[2023-01-22T09:18:36.039+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:370)
[2023-01-22T09:18:36.040+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T09:18:36.040+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T09:18:36.040+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T09:18:36.041+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T09:18:36.041+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T09:18:36.043+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T09:18:36.043+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T09:18:36.044+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T09:18:36.044+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T09:18:36.044+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T09:18:36.045+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T09:18:36.045+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T09:18:36.045+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T09:18:36.045+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T09:18:36.046+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T09:18:36.046+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T09:18:36.047+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T09:18:36.114+0000] {spark_submit.py:495} INFO - Error, ada An error occurred while calling o37.load.
[2023-01-22T09:18:36.115+0000] {spark_submit.py:495} INFO - : org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "gs"
[2023-01-22T09:18:36.115+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2023-01-22T09:18:36.115+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T09:18:36.116+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T09:18:36.116+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T09:18:36.117+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T09:18:36.117+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T09:18:36.117+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T09:18:36.117+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:752)
[2023-01-22T09:18:36.118+0000] {spark_submit.py:495} INFO - at scala.collection.immutable.List.map(List.scala:246)
[2023-01-22T09:18:36.118+0000] {spark_submit.py:495} INFO - at scala.collection.immutable.List.map(List.scala:79)
[2023-01-22T09:18:36.119+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:750)
[2023-01-22T09:18:36.120+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:579)
[2023-01-22T09:18:36.120+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:408)
[2023-01-22T09:18:36.120+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T09:18:36.120+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T09:18:36.121+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T09:18:36.121+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T09:18:36.121+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T09:18:36.122+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T09:18:36.122+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T09:18:36.123+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T09:18:36.123+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T09:18:36.124+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T09:18:36.124+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T09:18:36.124+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T09:18:36.124+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T09:18:36.125+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T09:18:36.125+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T09:18:36.125+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T09:18:36.126+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T09:18:36.126+0000] {spark_submit.py:495} INFO - 
[2023-01-22T09:18:36.126+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T09:18:36.127+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 68, in <module>
[2023-01-22T09:18:36.127+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T09:18:36.127+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 55, in main
[2023-01-22T09:18:36.128+0000] {spark_submit.py:495} INFO - df_spark.write.format("bigquery").option("project", "rafzul-analytics-1009") \
[2023-01-22T09:18:36.128+0000] {spark_submit.py:495} INFO - UnboundLocalError: local variable 'df_spark' referenced before assignment
[2023-01-22T09:18:36.243+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:36 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-22T09:18:36.267+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:36 INFO SparkUI: Stopped Spark web UI at http://266f60b86faa:4041
[2023-01-22T09:18:36.328+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:36 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-22T09:18:36.386+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:36 INFO MemoryStore: MemoryStore cleared
[2023-01-22T09:18:36.387+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:36 INFO BlockManager: BlockManager stopped
[2023-01-22T09:18:36.410+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:36 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-22T09:18:36.438+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:36 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-22T09:18:36.514+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:36 INFO SparkContext: Successfully stopped SparkContext
[2023-01-22T09:18:36.515+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:36 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T09:18:36.522+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:36 INFO ShutdownHookManager: Deleting directory /tmp/spark-1b2de968-0a2e-4be1-be7f-0de9a6d0fb55
[2023-01-22T09:18:36.558+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:36 INFO ShutdownHookManager: Deleting directory /tmp/spark-b1170c57-5237-4978-868c-3f74542340b8/pyspark-c66f1187-db3a-48be-bf60-bfca8f11bb6e
[2023-01-22T09:18:36.591+0000] {spark_submit.py:495} INFO - 23/01/22 09:18:36 INFO ShutdownHookManager: Deleting directory /tmp/spark-b1170c57-5237-4978-868c-3f74542340b8
[2023-01-22T09:18:36.868+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-22T09:18:36.875+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230122T091809, end_date=20230122T091836
[2023-01-22T09:18:36.916+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 390 for task transform_stage_generation (Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 1037)
[2023-01-22T09:18:36.961+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T09:18:37.013+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T10:00:15.950+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T10:00:15.965+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T10:00:15.965+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T10:00:15.966+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-22T10:00:15.966+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T10:00:15.978+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-22T10:00:15.987+0000] {standard_task_runner.py:55} INFO - Started process 4570 to run task
[2023-01-22T10:00:15.991+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '399', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpc7np2ss5']
[2023-01-22T10:00:15.994+0000] {standard_task_runner.py:83} INFO - Job 399: Subtask transform_stage_generation
[2023-01-22T10:00:16.068+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 266f60b86faa
[2023-01-22T10:00:16.145+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-22T10:00:16.156+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T10:00:16.157+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-22T10:00:19.222+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 36
[2023-01-22T10:00:19.223+0000] {spark_submit.py:495} INFO - .config("spark.hadoop.fs.AbstractFileSystem.gs.impl","com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS",) \
[2023-01-22T10:00:19.223+0000] {spark_submit.py:495} INFO - IndentationError: unexpected indent
[2023-01-22T10:00:19.242+0000] {spark_submit.py:495} INFO - 23/01/22 10:00:19 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T10:00:19.247+0000] {spark_submit.py:495} INFO - 23/01/22 10:00:19 INFO ShutdownHookManager: Deleting directory /tmp/spark-f3d43176-9d99-4546-a9e5-82cf13c2f692
[2023-01-22T10:00:19.306+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-22T10:00:19.312+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230122T100015, end_date=20230122T100019
[2023-01-22T10:00:19.328+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 399 for task transform_stage_generation (Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 4570)
[2023-01-22T10:00:19.345+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T10:00:19.372+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T10:01:43.277+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T10:01:43.285+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T10:01:43.285+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T10:01:43.285+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-22T10:01:43.285+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T10:01:43.297+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-22T10:01:43.304+0000] {standard_task_runner.py:55} INFO - Started process 4780 to run task
[2023-01-22T10:01:43.309+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '403', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpo_el0049']
[2023-01-22T10:01:43.313+0000] {standard_task_runner.py:83} INFO - Job 403: Subtask transform_stage_generation
[2023-01-22T10:01:43.381+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 266f60b86faa
[2023-01-22T10:01:43.472+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-22T10:01:43.483+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T10:01:43.485+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-22T10:01:48.637+0000] {spark_submit.py:495} INFO - /opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010100.json
[2023-01-22T10:01:48.731+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:48 INFO SparkContext: Running Spark version 3.3.1
[2023-01-22T10:01:48.834+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T10:01:48.997+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:48 INFO ResourceUtils: ==============================================================
[2023-01-22T10:01:48.998+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:48 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-22T10:01:48.999+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:48 INFO ResourceUtils: ==============================================================
[2023-01-22T10:01:49.001+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:49 INFO SparkContext: Submitted application: gcp_playground
[2023-01-22T10:01:49.037+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:49 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-22T10:01:49.044+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:49 INFO ResourceProfile: Limiting resource is cpu
[2023-01-22T10:01:49.047+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:49 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-22T10:01:49.126+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:49 INFO SecurityManager: Changing view acls to: ***
[2023-01-22T10:01:49.126+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:49 INFO SecurityManager: Changing modify acls to: ***
[2023-01-22T10:01:49.126+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:49 INFO SecurityManager: Changing view acls groups to:
[2023-01-22T10:01:49.127+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:49 INFO SecurityManager: Changing modify acls groups to:
[2023-01-22T10:01:49.127+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:49 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-22T10:01:49.568+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:49 INFO Utils: Successfully started service 'sparkDriver' on port 45903.
[2023-01-22T10:01:49.619+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:49 INFO SparkEnv: Registering MapOutputTracker
[2023-01-22T10:01:49.692+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:49 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-22T10:01:49.792+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:49 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-22T10:01:49.795+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:49 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-22T10:01:49.805+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:49 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-22T10:01:49.856+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:49 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-641a4979-fb95-4f11-ac75-3d819422c527
[2023-01-22T10:01:49.892+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:49 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-22T10:01:49.951+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:49 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-22T10:01:50.568+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:50 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-01-22T10:01:50.646+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:50 INFO SparkContext: Added JAR /opt/spark/gcs-connector-hadoop3-latest.jar at spark://266f60b86faa:45903/jars/gcs-connector-hadoop3-latest.jar with timestamp 1674381708718
[2023-01-22T10:01:50.647+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:50 INFO SparkContext: Added JAR /opt/spark/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://266f60b86faa:45903/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1674381708718
[2023-01-22T10:01:50.806+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:50 INFO Executor: Starting executor ID driver on host 266f60b86faa
[2023-01-22T10:01:50.819+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:50 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-22T10:01:50.847+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:50 INFO Executor: Fetching spark://266f60b86faa:45903/jars/gcs-connector-hadoop3-latest.jar with timestamp 1674381708718
[2023-01-22T10:01:50.937+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:50 INFO TransportClientFactory: Successfully created connection to 266f60b86faa/192.168.64.4:45903 after 53 ms (0 ms spent in bootstraps)
[2023-01-22T10:01:50.954+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:50 INFO Utils: Fetching spark://266f60b86faa:45903/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-c1a4ce71-6d22-4888-a881-96c19366a8f7/userFiles-459cb55e-5364-435f-af93-1eb4aa6fd453/fetchFileTemp7138460946095225494.tmp
[2023-01-22T10:01:51.267+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:51 INFO Executor: Adding file:/tmp/spark-c1a4ce71-6d22-4888-a881-96c19366a8f7/userFiles-459cb55e-5364-435f-af93-1eb4aa6fd453/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-22T10:01:51.268+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:51 INFO Executor: Fetching spark://266f60b86faa:45903/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1674381708718
[2023-01-22T10:01:51.269+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:51 INFO Utils: Fetching spark://266f60b86faa:45903/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-c1a4ce71-6d22-4888-a881-96c19366a8f7/userFiles-459cb55e-5364-435f-af93-1eb4aa6fd453/fetchFileTemp13533057715101845401.tmp
[2023-01-22T10:01:51.562+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:51 INFO Executor: Adding file:/tmp/spark-c1a4ce71-6d22-4888-a881-96c19366a8f7/userFiles-459cb55e-5364-435f-af93-1eb4aa6fd453/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-22T10:01:51.599+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:51 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39579.
[2023-01-22T10:01:51.599+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:51 INFO NettyBlockTransferService: Server created on 266f60b86faa:39579
[2023-01-22T10:01:51.604+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:51 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-22T10:01:51.621+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:51 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 266f60b86faa, 39579, None)
[2023-01-22T10:01:51.630+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:51 INFO BlockManagerMasterEndpoint: Registering block manager 266f60b86faa:39579 with 434.4 MiB RAM, BlockManagerId(driver, 266f60b86faa, 39579, None)
[2023-01-22T10:01:51.648+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:51 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 266f60b86faa, 39579, None)
[2023-01-22T10:01:51.650+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:51 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 266f60b86faa, 39579, None)
[2023-01-22T10:01:53.031+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:53 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-22T10:01:53.049+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:53 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-22T10:01:55.222+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:55 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: gs://entsoe_analytics_1009//opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010100.json.
[2023-01-22T10:01:55.226+0000] {spark_submit.py:495} INFO - java.lang.RuntimeException: java.lang.ClassNotFoundException: Class com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem not found
[2023-01-22T10:01:55.226+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)
[2023-01-22T10:01:55.226+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)
[2023-01-22T10:01:55.227+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T10:01:55.227+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T10:01:55.227+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T10:01:55.227+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T10:01:55.227+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T10:01:55.228+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T10:01:55.228+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)
[2023-01-22T10:01:55.228+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:370)
[2023-01-22T10:01:55.228+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T10:01:55.228+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T10:01:55.228+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T10:01:55.229+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T10:01:55.229+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T10:01:55.237+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T10:01:55.237+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T10:01:55.237+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T10:01:55.237+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T10:01:55.238+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T10:01:55.238+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T10:01:55.238+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T10:01:55.240+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T10:01:55.240+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T10:01:55.240+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T10:01:55.240+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T10:01:55.241+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T10:01:55.241+0000] {spark_submit.py:495} INFO - Caused by: java.lang.ClassNotFoundException: Class com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem not found
[2023-01-22T10:01:55.241+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)
[2023-01-22T10:01:55.241+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)
[2023-01-22T10:01:55.241+0000] {spark_submit.py:495} INFO - ... 26 more
[2023-01-22T10:01:55.313+0000] {spark_submit.py:495} INFO - Error, ada An error occurred while calling o39.load.
[2023-01-22T10:01:55.313+0000] {spark_submit.py:495} INFO - : java.lang.RuntimeException: java.lang.ClassNotFoundException: Class com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem not found
[2023-01-22T10:01:55.313+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)
[2023-01-22T10:01:55.314+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)
[2023-01-22T10:01:55.314+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T10:01:55.314+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T10:01:55.315+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T10:01:55.316+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T10:01:55.317+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T10:01:55.317+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T10:01:55.317+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:752)
[2023-01-22T10:01:55.317+0000] {spark_submit.py:495} INFO - at scala.collection.immutable.List.map(List.scala:246)
[2023-01-22T10:01:55.317+0000] {spark_submit.py:495} INFO - at scala.collection.immutable.List.map(List.scala:79)
[2023-01-22T10:01:55.318+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:750)
[2023-01-22T10:01:55.318+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:579)
[2023-01-22T10:01:55.318+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:408)
[2023-01-22T10:01:55.318+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T10:01:55.319+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T10:01:55.319+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T10:01:55.319+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T10:01:55.320+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T10:01:55.320+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T10:01:55.320+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T10:01:55.320+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T10:01:55.320+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T10:01:55.321+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T10:01:55.321+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T10:01:55.321+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T10:01:55.321+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T10:01:55.322+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T10:01:55.322+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T10:01:55.322+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T10:01:55.322+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T10:01:55.322+0000] {spark_submit.py:495} INFO - Caused by: java.lang.ClassNotFoundException: Class com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem not found
[2023-01-22T10:01:55.323+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)
[2023-01-22T10:01:55.323+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)
[2023-01-22T10:01:55.323+0000] {spark_submit.py:495} INFO - ... 30 more
[2023-01-22T10:01:55.323+0000] {spark_submit.py:495} INFO - 
[2023-01-22T10:01:55.323+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T10:01:55.323+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 69, in <module>
[2023-01-22T10:01:55.324+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T10:01:55.324+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 56, in main
[2023-01-22T10:01:55.324+0000] {spark_submit.py:495} INFO - df_spark.write.format("bigquery").option("project", "rafzul-analytics-1009") \
[2023-01-22T10:01:55.324+0000] {spark_submit.py:495} INFO - UnboundLocalError: local variable 'df_spark' referenced before assignment
[2023-01-22T10:01:55.424+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:55 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-22T10:01:55.449+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:55 INFO SparkUI: Stopped Spark web UI at http://266f60b86faa:4040
[2023-01-22T10:01:55.504+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:55 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-22T10:01:55.546+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:55 INFO MemoryStore: MemoryStore cleared
[2023-01-22T10:01:55.547+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:55 INFO BlockManager: BlockManager stopped
[2023-01-22T10:01:55.582+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:55 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-22T10:01:55.586+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:55 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-22T10:01:55.607+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:55 INFO SparkContext: Successfully stopped SparkContext
[2023-01-22T10:01:55.609+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:55 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T10:01:55.620+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-c1a4ce71-6d22-4888-a881-96c19366a8f7
[2023-01-22T10:01:55.644+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-f8d0eb23-75d4-4eea-acf0-5726bf5accb8
[2023-01-22T10:01:55.655+0000] {spark_submit.py:495} INFO - 23/01/22 10:01:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-c1a4ce71-6d22-4888-a881-96c19366a8f7/pyspark-8afb91a9-8ded-4789-b89c-33ec9b8ba438
[2023-01-22T10:01:55.893+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-22T10:01:55.901+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230122T100143, end_date=20230122T100155
[2023-01-22T10:01:55.925+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 403 for task transform_stage_generation (Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 4780)
[2023-01-22T10:01:55.949+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T10:01:55.977+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T10:03:45.334+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T10:03:45.357+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T10:03:45.357+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T10:03:45.357+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-22T10:03:45.357+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T10:03:45.378+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-22T10:03:45.394+0000] {standard_task_runner.py:55} INFO - Started process 5172 to run task
[2023-01-22T10:03:45.402+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '407', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp2k3xbm69']
[2023-01-22T10:03:45.409+0000] {standard_task_runner.py:83} INFO - Job 407: Subtask transform_stage_generation
[2023-01-22T10:03:45.526+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 266f60b86faa
[2023-01-22T10:03:45.639+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-22T10:03:45.651+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T10:03:45.654+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-22T10:03:50.989+0000] {spark_submit.py:495} INFO - /opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010100.json
[2023-01-22T10:03:51.093+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:51 INFO SparkContext: Running Spark version 3.3.1
[2023-01-22T10:03:51.230+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T10:03:51.465+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:51 INFO ResourceUtils: ==============================================================
[2023-01-22T10:03:51.467+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:51 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-22T10:03:51.468+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:51 INFO ResourceUtils: ==============================================================
[2023-01-22T10:03:51.470+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:51 INFO SparkContext: Submitted application: gcp_playground
[2023-01-22T10:03:51.520+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:51 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-22T10:03:51.533+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:51 INFO ResourceProfile: Limiting resource is cpu
[2023-01-22T10:03:51.537+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:51 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-22T10:03:51.681+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:51 INFO SecurityManager: Changing view acls to: ***
[2023-01-22T10:03:51.682+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:51 INFO SecurityManager: Changing modify acls to: ***
[2023-01-22T10:03:51.683+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:51 INFO SecurityManager: Changing view acls groups to:
[2023-01-22T10:03:51.684+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:51 INFO SecurityManager: Changing modify acls groups to:
[2023-01-22T10:03:51.685+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:51 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-22T10:03:52.264+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:52 INFO Utils: Successfully started service 'sparkDriver' on port 35349.
[2023-01-22T10:03:52.335+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:52 INFO SparkEnv: Registering MapOutputTracker
[2023-01-22T10:03:52.453+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:52 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-22T10:03:52.581+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:52 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-22T10:03:52.584+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:52 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-22T10:03:52.594+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:52 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-22T10:03:52.644+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:52 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d0af2c48-8297-412f-8f5e-60c6003d8460
[2023-01-22T10:03:52.681+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:52 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-22T10:03:52.722+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:52 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-22T10:03:53.218+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:53 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-22T10:03:53.235+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:53 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2023-01-22T10:03:53.318+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:53 INFO SparkContext: Added JAR /opt/spark/gcs-connector-hadoop3-latest.jar at spark://266f60b86faa:35349/jars/gcs-connector-hadoop3-latest.jar with timestamp 1674381831080
[2023-01-22T10:03:53.320+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:53 INFO SparkContext: Added JAR /opt/spark/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://266f60b86faa:35349/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1674381831080
[2023-01-22T10:03:53.496+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:53 INFO Executor: Starting executor ID driver on host 266f60b86faa
[2023-01-22T10:03:53.511+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:53 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-22T10:03:53.538+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:53 INFO Executor: Fetching spark://266f60b86faa:35349/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1674381831080
[2023-01-22T10:03:53.644+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:53 INFO TransportClientFactory: Successfully created connection to 266f60b86faa/192.168.64.4:35349 after 58 ms (0 ms spent in bootstraps)
[2023-01-22T10:03:53.656+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:53 INFO Utils: Fetching spark://266f60b86faa:35349/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-5fe63f82-19ae-4b64-8ee2-84599efa96ee/userFiles-dba0ad11-6f97-4cce-97a7-bcdc27251d5f/fetchFileTemp7762907733370891078.tmp
[2023-01-22T10:03:53.922+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:53 INFO Executor: Adding file:/tmp/spark-5fe63f82-19ae-4b64-8ee2-84599efa96ee/userFiles-dba0ad11-6f97-4cce-97a7-bcdc27251d5f/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-22T10:03:53.922+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:53 INFO Executor: Fetching spark://266f60b86faa:35349/jars/gcs-connector-hadoop3-latest.jar with timestamp 1674381831080
[2023-01-22T10:03:53.925+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:53 INFO Utils: Fetching spark://266f60b86faa:35349/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-5fe63f82-19ae-4b64-8ee2-84599efa96ee/userFiles-dba0ad11-6f97-4cce-97a7-bcdc27251d5f/fetchFileTemp9731735994545665122.tmp
[2023-01-22T10:03:54.270+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:54 INFO Executor: Adding file:/tmp/spark-5fe63f82-19ae-4b64-8ee2-84599efa96ee/userFiles-dba0ad11-6f97-4cce-97a7-bcdc27251d5f/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-22T10:03:54.284+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:54 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46211.
[2023-01-22T10:03:54.285+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:54 INFO NettyBlockTransferService: Server created on 266f60b86faa:46211
[2023-01-22T10:03:54.287+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:54 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-22T10:03:54.301+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:54 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 266f60b86faa, 46211, None)
[2023-01-22T10:03:54.307+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:54 INFO BlockManagerMasterEndpoint: Registering block manager 266f60b86faa:46211 with 434.4 MiB RAM, BlockManagerId(driver, 266f60b86faa, 46211, None)
[2023-01-22T10:03:54.313+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:54 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 266f60b86faa, 46211, None)
[2023-01-22T10:03:54.315+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:54 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 266f60b86faa, 46211, None)
[2023-01-22T10:03:55.590+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:55 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-22T10:03:55.612+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:55 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-22T10:03:57.930+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:57 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: gs://entsoe_analytics_1009//opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010100.json.
[2023-01-22T10:03:57.931+0000] {spark_submit.py:495} INFO - org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "gs"
[2023-01-22T10:03:57.931+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2023-01-22T10:03:57.931+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T10:03:57.931+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T10:03:57.932+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T10:03:57.932+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T10:03:57.932+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T10:03:57.932+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T10:03:57.933+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)
[2023-01-22T10:03:57.933+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:370)
[2023-01-22T10:03:57.933+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T10:03:57.933+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T10:03:57.934+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T10:03:57.934+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T10:03:57.934+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T10:03:57.934+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T10:03:57.935+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T10:03:57.935+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T10:03:57.935+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T10:03:57.935+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T10:03:57.936+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T10:03:57.936+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T10:03:57.936+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T10:03:57.936+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T10:03:57.937+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T10:03:57.937+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T10:03:57.937+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T10:03:57.971+0000] {spark_submit.py:495} INFO - Error, ada An error occurred while calling o37.load.
[2023-01-22T10:03:57.971+0000] {spark_submit.py:495} INFO - : org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "gs"
[2023-01-22T10:03:57.972+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2023-01-22T10:03:57.972+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T10:03:57.972+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T10:03:57.972+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T10:03:57.973+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T10:03:57.973+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T10:03:57.974+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T10:03:57.974+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:752)
[2023-01-22T10:03:57.974+0000] {spark_submit.py:495} INFO - at scala.collection.immutable.List.map(List.scala:246)
[2023-01-22T10:03:57.975+0000] {spark_submit.py:495} INFO - at scala.collection.immutable.List.map(List.scala:79)
[2023-01-22T10:03:57.975+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:750)
[2023-01-22T10:03:57.975+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:579)
[2023-01-22T10:03:57.976+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:408)
[2023-01-22T10:03:57.976+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T10:03:57.976+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T10:03:57.976+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T10:03:57.977+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T10:03:57.977+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T10:03:57.977+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T10:03:57.977+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T10:03:57.978+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T10:03:57.978+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T10:03:57.978+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T10:03:57.978+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T10:03:57.979+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T10:03:57.979+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T10:03:57.979+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T10:03:57.979+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T10:03:57.980+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T10:03:57.980+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T10:03:57.980+0000] {spark_submit.py:495} INFO - 
[2023-01-22T10:03:57.980+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T10:03:57.981+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 68, in <module>
[2023-01-22T10:03:57.981+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T10:03:57.981+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 55, in main
[2023-01-22T10:03:57.982+0000] {spark_submit.py:495} INFO - df_spark.write.format("bigquery").option("project", "rafzul-analytics-1009") \
[2023-01-22T10:03:57.982+0000] {spark_submit.py:495} INFO - UnboundLocalError: local variable 'df_spark' referenced before assignment
[2023-01-22T10:03:58.084+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:58 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-22T10:03:58.115+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:58 INFO SparkUI: Stopped Spark web UI at http://266f60b86faa:4041
[2023-01-22T10:03:58.144+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:58 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-22T10:03:58.185+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:58 INFO MemoryStore: MemoryStore cleared
[2023-01-22T10:03:58.194+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:58 INFO BlockManager: BlockManager stopped
[2023-01-22T10:03:58.227+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:58 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-22T10:03:58.231+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:58 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-22T10:03:58.303+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:58 INFO SparkContext: Successfully stopped SparkContext
[2023-01-22T10:03:58.304+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:58 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T10:03:58.307+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-5fe63f82-19ae-4b64-8ee2-84599efa96ee/pyspark-5b9bf05b-324f-4208-bb93-c5fd50045c00
[2023-01-22T10:03:58.317+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-834261ef-ce8b-42ba-8ff7-9d1013a4d222
[2023-01-22T10:03:58.329+0000] {spark_submit.py:495} INFO - 23/01/22 10:03:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-5fe63f82-19ae-4b64-8ee2-84599efa96ee
[2023-01-22T10:03:58.474+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-22T10:03:58.484+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230122T100345, end_date=20230122T100358
[2023-01-22T10:03:58.527+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 407 for task transform_stage_generation (Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 5172)
[2023-01-22T10:03:58.592+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T10:03:58.635+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T10:15:51.607+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T10:15:51.617+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T10:15:51.618+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T10:15:51.618+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-22T10:15:51.618+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T10:15:51.629+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-22T10:15:51.637+0000] {standard_task_runner.py:55} INFO - Started process 6723 to run task
[2023-01-22T10:15:51.641+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '415', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpzpcray7e']
[2023-01-22T10:15:51.644+0000] {standard_task_runner.py:83} INFO - Job 415: Subtask transform_stage_generation
[2023-01-22T10:15:51.706+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 266f60b86faa
[2023-01-22T10:15:51.780+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-22T10:15:51.791+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T10:15:51.792+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-22T10:15:57.158+0000] {spark_submit.py:495} INFO - /opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010100.json
[2023-01-22T10:15:57.245+0000] {spark_submit.py:495} INFO - 23/01/22 10:15:57 INFO SparkContext: Running Spark version 3.3.1
[2023-01-22T10:15:57.350+0000] {spark_submit.py:495} INFO - 23/01/22 10:15:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T10:15:57.491+0000] {spark_submit.py:495} INFO - 23/01/22 10:15:57 INFO ResourceUtils: ==============================================================
[2023-01-22T10:15:57.493+0000] {spark_submit.py:495} INFO - 23/01/22 10:15:57 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-22T10:15:57.494+0000] {spark_submit.py:495} INFO - 23/01/22 10:15:57 INFO ResourceUtils: ==============================================================
[2023-01-22T10:15:57.497+0000] {spark_submit.py:495} INFO - 23/01/22 10:15:57 INFO SparkContext: Submitted application: gcp_playground
[2023-01-22T10:15:57.543+0000] {spark_submit.py:495} INFO - 23/01/22 10:15:57 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-22T10:15:57.554+0000] {spark_submit.py:495} INFO - 23/01/22 10:15:57 INFO ResourceProfile: Limiting resource is cpu
[2023-01-22T10:15:57.556+0000] {spark_submit.py:495} INFO - 23/01/22 10:15:57 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-22T10:15:57.697+0000] {spark_submit.py:495} INFO - 23/01/22 10:15:57 INFO SecurityManager: Changing view acls to: ***
[2023-01-22T10:15:57.699+0000] {spark_submit.py:495} INFO - 23/01/22 10:15:57 INFO SecurityManager: Changing modify acls to: ***
[2023-01-22T10:15:57.700+0000] {spark_submit.py:495} INFO - 23/01/22 10:15:57 INFO SecurityManager: Changing view acls groups to:
[2023-01-22T10:15:57.701+0000] {spark_submit.py:495} INFO - 23/01/22 10:15:57 INFO SecurityManager: Changing modify acls groups to:
[2023-01-22T10:15:57.703+0000] {spark_submit.py:495} INFO - 23/01/22 10:15:57 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-22T10:15:58.609+0000] {spark_submit.py:495} INFO - 23/01/22 10:15:58 INFO Utils: Successfully started service 'sparkDriver' on port 36393.
[2023-01-22T10:15:58.964+0000] {spark_submit.py:495} INFO - 23/01/22 10:15:58 INFO SparkEnv: Registering MapOutputTracker
[2023-01-22T10:15:59.227+0000] {spark_submit.py:495} INFO - 23/01/22 10:15:59 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-22T10:15:59.530+0000] {spark_submit.py:495} INFO - 23/01/22 10:15:59 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-22T10:15:59.542+0000] {spark_submit.py:495} INFO - 23/01/22 10:15:59 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-22T10:15:59.566+0000] {spark_submit.py:495} INFO - 23/01/22 10:15:59 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-22T10:15:59.664+0000] {spark_submit.py:495} INFO - 23/01/22 10:15:59 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-fac010e1-255f-47ba-93b3-57918a938a0a
[2023-01-22T10:15:59.747+0000] {spark_submit.py:495} INFO - 23/01/22 10:15:59 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-22T10:15:59.857+0000] {spark_submit.py:495} INFO - 23/01/22 10:15:59 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-22T10:16:00.639+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:00 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-01-22T10:16:00.748+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:00 INFO SparkContext: Added JAR /opt/spark/gcs-connector-hadoop3-latest.jar at spark://266f60b86faa:36393/jars/gcs-connector-hadoop3-latest.jar with timestamp 1674382557235
[2023-01-22T10:16:00.750+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:00 INFO SparkContext: Added JAR /opt/spark/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://266f60b86faa:36393/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1674382557235
[2023-01-22T10:16:01.090+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:01 INFO Executor: Starting executor ID driver on host 266f60b86faa
[2023-01-22T10:16:01.163+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:01 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-22T10:16:01.233+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:01 INFO Executor: Fetching spark://266f60b86faa:36393/jars/gcs-connector-hadoop3-latest.jar with timestamp 1674382557235
[2023-01-22T10:16:01.638+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:01 INFO TransportClientFactory: Successfully created connection to 266f60b86faa/192.168.64.4:36393 after 196 ms (0 ms spent in bootstraps)
[2023-01-22T10:16:01.681+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:01 INFO Utils: Fetching spark://266f60b86faa:36393/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-e0b47cf6-8977-46c1-80da-388b985fbd06/userFiles-a7bc7c97-21a9-4904-9e4b-82addc537223/fetchFileTemp17615956220784223701.tmp
[2023-01-22T10:16:02.628+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:02 INFO Executor: Adding file:/tmp/spark-e0b47cf6-8977-46c1-80da-388b985fbd06/userFiles-a7bc7c97-21a9-4904-9e4b-82addc537223/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-22T10:16:02.629+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:02 INFO Executor: Fetching spark://266f60b86faa:36393/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1674382557235
[2023-01-22T10:16:02.632+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:02 INFO Utils: Fetching spark://266f60b86faa:36393/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-e0b47cf6-8977-46c1-80da-388b985fbd06/userFiles-a7bc7c97-21a9-4904-9e4b-82addc537223/fetchFileTemp11196929708009809682.tmp
[2023-01-22T10:16:03.104+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:03 INFO Executor: Adding file:/tmp/spark-e0b47cf6-8977-46c1-80da-388b985fbd06/userFiles-a7bc7c97-21a9-4904-9e4b-82addc537223/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-22T10:16:03.127+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:03 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39075.
[2023-01-22T10:16:03.128+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:03 INFO NettyBlockTransferService: Server created on 266f60b86faa:39075
[2023-01-22T10:16:03.132+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:03 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-22T10:16:03.154+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 266f60b86faa, 39075, None)
[2023-01-22T10:16:03.165+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:03 INFO BlockManagerMasterEndpoint: Registering block manager 266f60b86faa:39075 with 434.4 MiB RAM, BlockManagerId(driver, 266f60b86faa, 39075, None)
[2023-01-22T10:16:03.175+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:03 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 266f60b86faa, 39075, None)
[2023-01-22T10:16:03.180+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:03 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 266f60b86faa, 39075, None)
[2023-01-22T10:16:05.175+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:05 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-22T10:16:05.192+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:05 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-22T10:16:06.760+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:06 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: gs://entsoe_analytics_1009//opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010100.json.
[2023-01-22T10:16:06.761+0000] {spark_submit.py:495} INFO - org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "gs"
[2023-01-22T10:16:06.761+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2023-01-22T10:16:06.761+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T10:16:06.761+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T10:16:06.761+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T10:16:06.761+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T10:16:06.762+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T10:16:06.762+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T10:16:06.762+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)
[2023-01-22T10:16:06.762+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:370)
[2023-01-22T10:16:06.762+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T10:16:06.762+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T10:16:06.763+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T10:16:06.763+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T10:16:06.763+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T10:16:06.763+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T10:16:06.763+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T10:16:06.763+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T10:16:06.763+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T10:16:06.764+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T10:16:06.764+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T10:16:06.764+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T10:16:06.764+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T10:16:06.764+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T10:16:06.764+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T10:16:06.764+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T10:16:06.765+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T10:16:06.793+0000] {spark_submit.py:495} INFO - Error, ada An error occurred while calling o37.load.
[2023-01-22T10:16:06.794+0000] {spark_submit.py:495} INFO - : org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "gs"
[2023-01-22T10:16:06.794+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2023-01-22T10:16:06.794+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T10:16:06.794+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T10:16:06.794+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T10:16:06.795+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T10:16:06.795+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T10:16:06.795+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T10:16:06.795+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:752)
[2023-01-22T10:16:06.795+0000] {spark_submit.py:495} INFO - at scala.collection.immutable.List.map(List.scala:246)
[2023-01-22T10:16:06.795+0000] {spark_submit.py:495} INFO - at scala.collection.immutable.List.map(List.scala:79)
[2023-01-22T10:16:06.796+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:750)
[2023-01-22T10:16:06.796+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:579)
[2023-01-22T10:16:06.796+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:408)
[2023-01-22T10:16:06.796+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T10:16:06.796+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T10:16:06.796+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T10:16:06.797+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T10:16:06.797+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T10:16:06.797+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T10:16:06.797+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T10:16:06.797+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T10:16:06.797+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T10:16:06.797+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T10:16:06.797+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T10:16:06.798+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T10:16:06.798+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T10:16:06.798+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T10:16:06.798+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T10:16:06.799+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T10:16:06.799+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T10:16:06.799+0000] {spark_submit.py:495} INFO - 
[2023-01-22T10:16:06.799+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T10:16:06.800+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 68, in <module>
[2023-01-22T10:16:06.800+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T10:16:06.800+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 55, in main
[2023-01-22T10:16:06.800+0000] {spark_submit.py:495} INFO - df_spark.write.format("bigquery").option("project", "rafzul-analytics-1009") \
[2023-01-22T10:16:06.800+0000] {spark_submit.py:495} INFO - UnboundLocalError: local variable 'df_spark' referenced before assignment
[2023-01-22T10:16:06.867+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:06 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-22T10:16:06.887+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:06 INFO SparkUI: Stopped Spark web UI at http://266f60b86faa:4040
[2023-01-22T10:16:06.905+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:06 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-22T10:16:06.925+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:06 INFO MemoryStore: MemoryStore cleared
[2023-01-22T10:16:06.927+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:06 INFO BlockManager: BlockManager stopped
[2023-01-22T10:16:06.946+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:06 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-22T10:16:06.951+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:06 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-22T10:16:07.006+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:07 INFO SparkContext: Successfully stopped SparkContext
[2023-01-22T10:16:07.007+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:07 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T10:16:07.009+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-e0b47cf6-8977-46c1-80da-388b985fbd06/pyspark-49a45bcb-d4dc-42af-9b42-a9f79904245d
[2023-01-22T10:16:07.018+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-e0b47cf6-8977-46c1-80da-388b985fbd06
[2023-01-22T10:16:07.025+0000] {spark_submit.py:495} INFO - 23/01/22 10:16:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-1831118b-686b-4b99-b9d0-3a12bec83dac
[2023-01-22T10:16:07.120+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-22T10:16:07.125+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230122T101551, end_date=20230122T101607
[2023-01-22T10:16:07.154+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 415 for task transform_stage_generation (Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 6723)
[2023-01-22T10:16:07.200+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T10:16:07.222+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T10:19:19.433+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T10:19:19.442+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T10:19:19.443+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T10:19:19.443+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-22T10:19:19.443+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T10:19:19.457+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-22T10:19:19.466+0000] {standard_task_runner.py:55} INFO - Started process 7213 to run task
[2023-01-22T10:19:19.469+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '420', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpkccivd5q']
[2023-01-22T10:19:19.472+0000] {standard_task_runner.py:83} INFO - Job 420: Subtask transform_stage_generation
[2023-01-22T10:19:19.538+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 266f60b86faa
[2023-01-22T10:19:19.610+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-22T10:19:19.619+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T10:19:19.620+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-22T10:19:24.318+0000] {spark_submit.py:495} INFO - /opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010100.json
[2023-01-22T10:19:24.398+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:24 INFO SparkContext: Running Spark version 3.3.1
[2023-01-22T10:19:24.499+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T10:19:24.703+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:24 INFO ResourceUtils: ==============================================================
[2023-01-22T10:19:24.706+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:24 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-22T10:19:24.707+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:24 INFO ResourceUtils: ==============================================================
[2023-01-22T10:19:24.709+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:24 INFO SparkContext: Submitted application: gcp_playground
[2023-01-22T10:19:24.745+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:24 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-22T10:19:24.756+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:24 INFO ResourceProfile: Limiting resource is cpu
[2023-01-22T10:19:24.758+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:24 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-22T10:19:24.858+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:24 INFO SecurityManager: Changing view acls to: ***
[2023-01-22T10:19:24.860+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:24 INFO SecurityManager: Changing modify acls to: ***
[2023-01-22T10:19:24.862+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:24 INFO SecurityManager: Changing view acls groups to:
[2023-01-22T10:19:24.863+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:24 INFO SecurityManager: Changing modify acls groups to:
[2023-01-22T10:19:24.864+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:24 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-22T10:19:25.348+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:25 INFO Utils: Successfully started service 'sparkDriver' on port 40365.
[2023-01-22T10:19:25.435+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:25 INFO SparkEnv: Registering MapOutputTracker
[2023-01-22T10:19:25.511+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:25 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-22T10:19:25.565+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:25 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-22T10:19:25.567+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:25 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-22T10:19:25.575+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:25 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-22T10:19:25.622+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:25 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b9fb988c-754f-4cd5-b75b-7f6fb93f0ae0
[2023-01-22T10:19:25.659+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:25 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-22T10:19:25.697+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:25 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-22T10:19:26.089+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:26 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-22T10:19:26.106+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:26 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2023-01-22T10:19:26.177+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:26 INFO SparkContext: Added JAR /opt/spark/gcs-connector-hadoop3-latest.jar at spark://266f60b86faa:40365/jars/gcs-connector-hadoop3-latest.jar with timestamp 1674382764387
[2023-01-22T10:19:26.178+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:26 INFO SparkContext: Added JAR /opt/spark/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://266f60b86faa:40365/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1674382764387
[2023-01-22T10:19:26.307+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:26 INFO Executor: Starting executor ID driver on host 266f60b86faa
[2023-01-22T10:19:26.330+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:26 INFO Executor: Starting executor with user classpath (userClassPathFirst = true): ''
[2023-01-22T10:19:26.385+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:26 INFO Executor: Fetching spark://266f60b86faa:40365/jars/gcs-connector-hadoop3-latest.jar with timestamp 1674382764387
[2023-01-22T10:19:26.537+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:26 INFO TransportClientFactory: Successfully created connection to 266f60b86faa/192.168.64.4:40365 after 63 ms (0 ms spent in bootstraps)
[2023-01-22T10:19:26.549+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:26 INFO Utils: Fetching spark://266f60b86faa:40365/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-f5ddf727-abaf-47d2-85b0-1b50adf91ce7/userFiles-6c9a05af-730a-452f-a137-75d7c4226fe3/fetchFileTemp12745286620011632307.tmp
[2023-01-22T10:19:26.742+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:26 INFO Executor: Adding file:/tmp/spark-f5ddf727-abaf-47d2-85b0-1b50adf91ce7/userFiles-6c9a05af-730a-452f-a137-75d7c4226fe3/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-22T10:19:26.742+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:26 INFO Executor: Fetching spark://266f60b86faa:40365/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1674382764387
[2023-01-22T10:19:26.746+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:26 INFO Utils: Fetching spark://266f60b86faa:40365/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-f5ddf727-abaf-47d2-85b0-1b50adf91ce7/userFiles-6c9a05af-730a-452f-a137-75d7c4226fe3/fetchFileTemp9422324081169763994.tmp
[2023-01-22T10:19:26.944+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:26 INFO Executor: Adding file:/tmp/spark-f5ddf727-abaf-47d2-85b0-1b50adf91ce7/userFiles-6c9a05af-730a-452f-a137-75d7c4226fe3/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-22T10:19:26.959+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:26 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40605.
[2023-01-22T10:19:26.959+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:26 INFO NettyBlockTransferService: Server created on 266f60b86faa:40605
[2023-01-22T10:19:26.962+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:26 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-22T10:19:26.976+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:26 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 266f60b86faa, 40605, None)
[2023-01-22T10:19:26.981+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:26 INFO BlockManagerMasterEndpoint: Registering block manager 266f60b86faa:40605 with 434.4 MiB RAM, BlockManagerId(driver, 266f60b86faa, 40605, None)
[2023-01-22T10:19:26.986+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:26 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 266f60b86faa, 40605, None)
[2023-01-22T10:19:26.989+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:26 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 266f60b86faa, 40605, None)
[2023-01-22T10:19:27.855+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:27 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-22T10:19:27.867+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:27 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-22T10:19:29.394+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:29 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: gs://entsoe_analytics_1009//opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010100.json.
[2023-01-22T10:19:29.394+0000] {spark_submit.py:495} INFO - org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "gs"
[2023-01-22T10:19:29.394+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2023-01-22T10:19:29.394+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T10:19:29.394+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T10:19:29.395+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T10:19:29.395+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T10:19:29.395+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T10:19:29.395+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T10:19:29.395+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)
[2023-01-22T10:19:29.395+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:370)
[2023-01-22T10:19:29.395+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T10:19:29.395+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T10:19:29.396+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T10:19:29.396+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T10:19:29.397+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T10:19:29.397+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T10:19:29.397+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T10:19:29.397+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T10:19:29.397+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T10:19:29.397+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T10:19:29.397+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T10:19:29.398+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T10:19:29.398+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T10:19:29.398+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T10:19:29.398+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T10:19:29.398+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T10:19:29.399+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T10:19:29.431+0000] {spark_submit.py:495} INFO - Error, ada An error occurred while calling o39.load.
[2023-01-22T10:19:29.431+0000] {spark_submit.py:495} INFO - : org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "gs"
[2023-01-22T10:19:29.431+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2023-01-22T10:19:29.431+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T10:19:29.432+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T10:19:29.432+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T10:19:29.432+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T10:19:29.432+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T10:19:29.432+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T10:19:29.432+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:752)
[2023-01-22T10:19:29.432+0000] {spark_submit.py:495} INFO - at scala.collection.immutable.List.map(List.scala:246)
[2023-01-22T10:19:29.433+0000] {spark_submit.py:495} INFO - at scala.collection.immutable.List.map(List.scala:79)
[2023-01-22T10:19:29.433+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:750)
[2023-01-22T10:19:29.433+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:579)
[2023-01-22T10:19:29.433+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:408)
[2023-01-22T10:19:29.433+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T10:19:29.433+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T10:19:29.433+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T10:19:29.434+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T10:19:29.434+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T10:19:29.434+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T10:19:29.434+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T10:19:29.434+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T10:19:29.434+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T10:19:29.434+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T10:19:29.435+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T10:19:29.435+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T10:19:29.435+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T10:19:29.435+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T10:19:29.435+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T10:19:29.435+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T10:19:29.436+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T10:19:29.436+0000] {spark_submit.py:495} INFO - 
[2023-01-22T10:19:29.436+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T10:19:29.436+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 69, in <module>
[2023-01-22T10:19:29.436+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T10:19:29.437+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 56, in main
[2023-01-22T10:19:29.437+0000] {spark_submit.py:495} INFO - df_spark.write.format("bigquery").option("project", "rafzul-analytics-1009") \
[2023-01-22T10:19:29.437+0000] {spark_submit.py:495} INFO - UnboundLocalError: local variable 'df_spark' referenced before assignment
[2023-01-22T10:19:29.513+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:29 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-22T10:19:29.527+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:29 INFO SparkUI: Stopped Spark web UI at http://266f60b86faa:4041
[2023-01-22T10:19:29.547+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:29 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-22T10:19:29.565+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:29 INFO MemoryStore: MemoryStore cleared
[2023-01-22T10:19:29.565+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:29 INFO BlockManager: BlockManager stopped
[2023-01-22T10:19:29.584+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:29 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-22T10:19:29.589+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:29 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-22T10:19:29.616+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:29 INFO SparkContext: Successfully stopped SparkContext
[2023-01-22T10:19:29.617+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:29 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T10:19:29.618+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-f5ddf727-abaf-47d2-85b0-1b50adf91ce7
[2023-01-22T10:19:29.623+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-f5ddf727-abaf-47d2-85b0-1b50adf91ce7/pyspark-518dce17-49c8-47c2-b497-4dee7e72d074
[2023-01-22T10:19:29.628+0000] {spark_submit.py:495} INFO - 23/01/22 10:19:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-a64838cf-fbc7-41f8-ba5e-a3ffd6e93a96
[2023-01-22T10:19:29.736+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-22T10:19:29.740+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230122T101919, end_date=20230122T101929
[2023-01-22T10:19:29.753+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 420 for task transform_stage_generation (Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 7213)
[2023-01-22T10:19:29.780+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T10:19:29.799+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T10:36:57.157+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T10:36:57.166+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T10:36:57.167+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T10:36:57.167+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-22T10:36:57.167+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T10:36:57.179+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-22T10:36:57.186+0000] {standard_task_runner.py:55} INFO - Started process 437 to run task
[2023-01-22T10:36:57.191+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '457', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpffs5_r_w']
[2023-01-22T10:36:57.194+0000] {standard_task_runner.py:83} INFO - Job 457: Subtask transform_stage_generation
[2023-01-22T10:36:57.269+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host ce1344c6e6d4
[2023-01-22T10:36:57.342+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-22T10:36:57.351+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T10:36:57.352+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-22T10:37:02.178+0000] {spark_submit.py:495} INFO - /opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010100.json
[2023-01-22T10:37:02.355+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:02 INFO SparkContext: Running Spark version 3.3.1
[2023-01-22T10:37:02.570+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T10:37:02.744+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:02 INFO ResourceUtils: ==============================================================
[2023-01-22T10:37:02.745+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:02 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-22T10:37:02.746+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:02 INFO ResourceUtils: ==============================================================
[2023-01-22T10:37:02.747+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:02 INFO SparkContext: Submitted application: gcp_playground
[2023-01-22T10:37:02.792+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:02 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-22T10:37:02.801+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:02 INFO ResourceProfile: Limiting resource is cpu
[2023-01-22T10:37:02.807+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:02 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-22T10:37:02.948+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:02 INFO SecurityManager: Changing view acls to: ***
[2023-01-22T10:37:02.955+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:02 INFO SecurityManager: Changing modify acls to: ***
[2023-01-22T10:37:02.958+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:02 INFO SecurityManager: Changing view acls groups to:
[2023-01-22T10:37:02.962+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:02 INFO SecurityManager: Changing modify acls groups to:
[2023-01-22T10:37:02.972+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:02 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-22T10:37:03.861+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:03 INFO Utils: Successfully started service 'sparkDriver' on port 38401.
[2023-01-22T10:37:03.921+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:03 INFO SparkEnv: Registering MapOutputTracker
[2023-01-22T10:37:04.000+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:04 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-22T10:37:04.062+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:04 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-22T10:37:04.064+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:04 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-22T10:37:04.071+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:04 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-22T10:37:04.119+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:04 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-3f9bad28-fc47-4d44-88e3-9320fcd44525
[2023-01-22T10:37:04.177+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:04 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-22T10:37:04.259+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:04 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-22T10:37:04.921+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:04 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-22T10:37:04.948+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:04 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2023-01-22T10:37:05.017+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:05 ERROR SparkContext: Failed to add /opt/spark/gcs-connector-hadoop3-latest.jar to Spark environment
[2023-01-22T10:37:05.018+0000] {spark_submit.py:495} INFO - java.io.FileNotFoundException: Jar /opt/spark/gcs-connector-hadoop3-latest.jar not found
[2023-01-22T10:37:05.018+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1949)
[2023-01-22T10:37:05.018+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.addJar(SparkContext.scala:2004)
[2023-01-22T10:37:05.018+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:507)
[2023-01-22T10:37:05.018+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:507)
[2023-01-22T10:37:05.019+0000] {spark_submit.py:495} INFO - at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
[2023-01-22T10:37:05.019+0000] {spark_submit.py:495} INFO - at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
[2023-01-22T10:37:05.019+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
[2023-01-22T10:37:05.019+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.<init>(SparkContext.scala:507)
[2023-01-22T10:37:05.019+0000] {spark_submit.py:495} INFO - at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
[2023-01-22T10:37:05.020+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
[2023-01-22T10:37:05.020+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
[2023-01-22T10:37:05.020+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
[2023-01-22T10:37:05.020+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
[2023-01-22T10:37:05.020+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
[2023-01-22T10:37:05.021+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T10:37:05.021+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:238)
[2023-01-22T10:37:05.021+0000] {spark_submit.py:495} INFO - at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
[2023-01-22T10:37:05.021+0000] {spark_submit.py:495} INFO - at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
[2023-01-22T10:37:05.021+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T10:37:05.021+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T10:37:05.022+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T10:37:05.030+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:05 ERROR SparkContext: Failed to add /opt/spark/spark-bigquery-with-dependencies_2.13-0.27.1.jar to Spark environment
[2023-01-22T10:37:05.030+0000] {spark_submit.py:495} INFO - java.io.FileNotFoundException: Jar /opt/spark/spark-bigquery-with-dependencies_2.13-0.27.1.jar not found
[2023-01-22T10:37:05.031+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1949)
[2023-01-22T10:37:05.031+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.addJar(SparkContext.scala:2004)
[2023-01-22T10:37:05.031+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:507)
[2023-01-22T10:37:05.031+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:507)
[2023-01-22T10:37:05.031+0000] {spark_submit.py:495} INFO - at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
[2023-01-22T10:37:05.032+0000] {spark_submit.py:495} INFO - at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
[2023-01-22T10:37:05.032+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
[2023-01-22T10:37:05.032+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.<init>(SparkContext.scala:507)
[2023-01-22T10:37:05.032+0000] {spark_submit.py:495} INFO - at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
[2023-01-22T10:37:05.032+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
[2023-01-22T10:37:05.032+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
[2023-01-22T10:37:05.033+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
[2023-01-22T10:37:05.033+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
[2023-01-22T10:37:05.033+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
[2023-01-22T10:37:05.033+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T10:37:05.033+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:238)
[2023-01-22T10:37:05.034+0000] {spark_submit.py:495} INFO - at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
[2023-01-22T10:37:05.034+0000] {spark_submit.py:495} INFO - at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
[2023-01-22T10:37:05.034+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T10:37:05.034+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T10:37:05.034+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T10:37:05.206+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:05 INFO Executor: Starting executor ID driver on host ce1344c6e6d4
[2023-01-22T10:37:05.218+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:05 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-22T10:37:05.246+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:05 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39155.
[2023-01-22T10:37:05.247+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:05 INFO NettyBlockTransferService: Server created on ce1344c6e6d4:39155
[2023-01-22T10:37:05.250+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:05 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-22T10:37:05.266+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:05 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ce1344c6e6d4, 39155, None)
[2023-01-22T10:37:05.271+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:05 INFO BlockManagerMasterEndpoint: Registering block manager ce1344c6e6d4:39155 with 434.4 MiB RAM, BlockManagerId(driver, ce1344c6e6d4, 39155, None)
[2023-01-22T10:37:05.283+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:05 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ce1344c6e6d4, 39155, None)
[2023-01-22T10:37:05.287+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:05 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ce1344c6e6d4, 39155, None)
[2023-01-22T10:37:07.097+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:07 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-22T10:37:07.117+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:07 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-22T10:37:10.798+0000] {spark_submit.py:495} INFO - Error, ada Path does not exist: gs://entsoe_analytics_1009/opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010100.json
[2023-01-22T10:37:10.799+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T10:37:10.799+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 68, in <module>
[2023-01-22T10:37:10.799+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T10:37:10.799+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 55, in main
[2023-01-22T10:37:10.799+0000] {spark_submit.py:495} INFO - df_spark.write.format("bigquery").option("project", "rafzul-analytics-1009") \
[2023-01-22T10:37:10.800+0000] {spark_submit.py:495} INFO - UnboundLocalError: local variable 'df_spark' referenced before assignment
[2023-01-22T10:37:10.882+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:10 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-22T10:37:10.897+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:10 INFO SparkUI: Stopped Spark web UI at http://ce1344c6e6d4:4041
[2023-01-22T10:37:10.920+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:10 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-22T10:37:10.943+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:10 INFO MemoryStore: MemoryStore cleared
[2023-01-22T10:37:10.944+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:10 INFO BlockManager: BlockManager stopped
[2023-01-22T10:37:10.961+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:10 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-22T10:37:10.968+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:10 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-22T10:37:10.977+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:10 INFO SparkContext: Successfully stopped SparkContext
[2023-01-22T10:37:10.978+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:10 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T10:37:10.978+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:10 INFO ShutdownHookManager: Deleting directory /tmp/spark-6ecb1aa5-b8af-459a-86ba-c01f86ff009f/pyspark-2f33e8b6-40c3-4258-be71-b952ca5d621c
[2023-01-22T10:37:10.984+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:10 INFO ShutdownHookManager: Deleting directory /tmp/spark-6ecb1aa5-b8af-459a-86ba-c01f86ff009f
[2023-01-22T10:37:10.991+0000] {spark_submit.py:495} INFO - 23/01/22 10:37:10 INFO ShutdownHookManager: Deleting directory /tmp/spark-65cca353-8b76-40b0-b712-5ba45ed6bf5d
[2023-01-22T10:37:11.081+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-22T10:37:11.087+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230122T103657, end_date=20230122T103711
[2023-01-22T10:37:11.108+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 457 for task transform_stage_generation (Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 437)
[2023-01-22T10:37:11.126+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T10:37:11.151+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T10:38:09.917+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T10:38:09.931+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T10:38:09.931+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T10:38:09.931+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-22T10:38:09.931+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T10:38:09.943+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-22T10:38:09.953+0000] {standard_task_runner.py:55} INFO - Started process 765 to run task
[2023-01-22T10:38:09.958+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '460', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpqb46xxqw']
[2023-01-22T10:38:09.961+0000] {standard_task_runner.py:83} INFO - Job 460: Subtask transform_stage_generation
[2023-01-22T10:38:10.039+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host ce1344c6e6d4
[2023-01-22T10:38:10.144+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-22T10:38:10.158+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T10:38:10.160+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-22T10:38:16.089+0000] {spark_submit.py:495} INFO - /opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010100.json
[2023-01-22T10:38:16.308+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:16 INFO SparkContext: Running Spark version 3.3.1
[2023-01-22T10:38:16.650+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T10:38:16.853+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:16 INFO ResourceUtils: ==============================================================
[2023-01-22T10:38:16.855+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:16 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-22T10:38:16.856+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:16 INFO ResourceUtils: ==============================================================
[2023-01-22T10:38:16.857+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:16 INFO SparkContext: Submitted application: gcp_playground
[2023-01-22T10:38:16.900+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:16 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-22T10:38:16.911+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:16 INFO ResourceProfile: Limiting resource is cpu
[2023-01-22T10:38:16.914+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:16 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-22T10:38:17.048+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:17 INFO SecurityManager: Changing view acls to: ***
[2023-01-22T10:38:17.049+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:17 INFO SecurityManager: Changing modify acls to: ***
[2023-01-22T10:38:17.067+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:17 INFO SecurityManager: Changing view acls groups to:
[2023-01-22T10:38:17.093+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:17 INFO SecurityManager: Changing modify acls groups to:
[2023-01-22T10:38:17.093+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-22T10:38:18.154+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:18 INFO Utils: Successfully started service 'sparkDriver' on port 37305.
[2023-01-22T10:38:18.243+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:18 INFO SparkEnv: Registering MapOutputTracker
[2023-01-22T10:38:18.377+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:18 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-22T10:38:18.452+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:18 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-22T10:38:18.456+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:18 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-22T10:38:18.465+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:18 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-22T10:38:18.596+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:18 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-8b1cb3ef-6e54-4f23-8f3e-e02d5da82477
[2023-01-22T10:38:18.642+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:18 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-22T10:38:18.680+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:18 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-22T10:38:19.410+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:19 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-01-22T10:38:19.588+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:19 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://ce1344c6e6d4:37305/jars/gcs-connector-hadoop3-latest.jar with timestamp 1674383896287
[2023-01-22T10:38:19.595+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:19 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://ce1344c6e6d4:37305/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1674383896287
[2023-01-22T10:38:19.915+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:19 INFO Executor: Starting executor ID driver on host ce1344c6e6d4
[2023-01-22T10:38:19.939+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:19 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-22T10:38:19.980+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:19 INFO Executor: Fetching spark://ce1344c6e6d4:37305/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1674383896287
[2023-01-22T10:38:20.208+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:20 INFO TransportClientFactory: Successfully created connection to ce1344c6e6d4/192.168.80.8:37305 after 140 ms (0 ms spent in bootstraps)
[2023-01-22T10:38:20.251+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:20 INFO Utils: Fetching spark://ce1344c6e6d4:37305/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-993f26af-5e2d-499d-9dfc-6aab7a986698/userFiles-2312e700-5141-4d57-bf9f-d202080fb1f3/fetchFileTemp2934853145673088814.tmp
[2023-01-22T10:38:20.696+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:20 INFO Executor: Adding file:/tmp/spark-993f26af-5e2d-499d-9dfc-6aab7a986698/userFiles-2312e700-5141-4d57-bf9f-d202080fb1f3/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-22T10:38:20.696+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:20 INFO Executor: Fetching spark://ce1344c6e6d4:37305/jars/gcs-connector-hadoop3-latest.jar with timestamp 1674383896287
[2023-01-22T10:38:20.698+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:20 INFO Utils: Fetching spark://ce1344c6e6d4:37305/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-993f26af-5e2d-499d-9dfc-6aab7a986698/userFiles-2312e700-5141-4d57-bf9f-d202080fb1f3/fetchFileTemp4113726058376928821.tmp
[2023-01-22T10:38:21.199+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:21 INFO Executor: Adding file:/tmp/spark-993f26af-5e2d-499d-9dfc-6aab7a986698/userFiles-2312e700-5141-4d57-bf9f-d202080fb1f3/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-22T10:38:21.229+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46167.
[2023-01-22T10:38:21.230+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:21 INFO NettyBlockTransferService: Server created on ce1344c6e6d4:46167
[2023-01-22T10:38:21.238+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:21 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-22T10:38:21.268+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:21 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ce1344c6e6d4, 46167, None)
[2023-01-22T10:38:21.281+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:21 INFO BlockManagerMasterEndpoint: Registering block manager ce1344c6e6d4:46167 with 434.4 MiB RAM, BlockManagerId(driver, ce1344c6e6d4, 46167, None)
[2023-01-22T10:38:21.315+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:21 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ce1344c6e6d4, 46167, None)
[2023-01-22T10:38:21.321+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:21 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ce1344c6e6d4, 46167, None)
[2023-01-22T10:38:23.929+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:23 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-22T10:38:23.942+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:23 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-22T10:38:28.480+0000] {spark_submit.py:495} INFO - Error, ada Path does not exist: gs://entsoe_analytics_1009/opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010100.json
[2023-01-22T10:38:28.480+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T10:38:28.481+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 68, in <module>
[2023-01-22T10:38:28.481+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T10:38:28.481+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 55, in main
[2023-01-22T10:38:28.482+0000] {spark_submit.py:495} INFO - df_spark.write.format("bigquery").option("project", "rafzul-analytics-1009") \
[2023-01-22T10:38:28.482+0000] {spark_submit.py:495} INFO - UnboundLocalError: local variable 'df_spark' referenced before assignment
[2023-01-22T10:38:28.591+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:28 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-22T10:38:28.619+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:28 INFO SparkUI: Stopped Spark web UI at http://ce1344c6e6d4:4040
[2023-01-22T10:38:28.652+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:28 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-22T10:38:28.679+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:28 INFO MemoryStore: MemoryStore cleared
[2023-01-22T10:38:28.680+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:28 INFO BlockManager: BlockManager stopped
[2023-01-22T10:38:28.699+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:28 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-22T10:38:28.708+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:28 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-22T10:38:28.739+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:28 INFO SparkContext: Successfully stopped SparkContext
[2023-01-22T10:38:28.741+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:28 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T10:38:28.742+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:28 INFO ShutdownHookManager: Deleting directory /tmp/spark-993f26af-5e2d-499d-9dfc-6aab7a986698
[2023-01-22T10:38:28.759+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:28 INFO ShutdownHookManager: Deleting directory /tmp/spark-813a958a-22a4-4134-9c62-55b9ad0f5e45
[2023-01-22T10:38:28.770+0000] {spark_submit.py:495} INFO - 23/01/22 10:38:28 INFO ShutdownHookManager: Deleting directory /tmp/spark-993f26af-5e2d-499d-9dfc-6aab7a986698/pyspark-10c9fe97-6da2-430d-9c97-d8a56b8326b8
[2023-01-22T10:38:29.041+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-22T10:38:29.051+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230122T103809, end_date=20230122T103829
[2023-01-22T10:38:29.080+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 460 for task transform_stage_generation (Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 765)
[2023-01-22T10:38:29.131+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T10:38:29.176+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T10:46:57.563+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T10:46:57.575+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T10:46:57.575+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T10:46:57.575+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-22T10:46:57.575+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T10:46:57.590+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-22T10:46:57.598+0000] {standard_task_runner.py:55} INFO - Started process 1871 to run task
[2023-01-22T10:46:57.603+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '466', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpyz0jnpb7']
[2023-01-22T10:46:57.607+0000] {standard_task_runner.py:83} INFO - Job 466: Subtask transform_stage_generation
[2023-01-22T10:46:57.677+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host ce1344c6e6d4
[2023-01-22T10:46:57.756+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-22T10:46:57.765+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T10:46:57.766+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-22T10:47:03.726+0000] {spark_submit.py:495} INFO - total_generation__DE_TENNET__202101010100__202101010200.json
[2023-01-22T10:47:03.914+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:03 INFO SparkContext: Running Spark version 3.3.1
[2023-01-22T10:47:04.097+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T10:47:04.554+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:04 INFO ResourceUtils: ==============================================================
[2023-01-22T10:47:04.563+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:04 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-22T10:47:04.564+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:04 INFO ResourceUtils: ==============================================================
[2023-01-22T10:47:04.574+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:04 INFO SparkContext: Submitted application: gcp_playground
[2023-01-22T10:47:04.690+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:04 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-22T10:47:04.715+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:04 INFO ResourceProfile: Limiting resource is cpu
[2023-01-22T10:47:04.719+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:04 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-22T10:47:05.020+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:05 INFO SecurityManager: Changing view acls to: ***
[2023-01-22T10:47:05.022+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:05 INFO SecurityManager: Changing modify acls to: ***
[2023-01-22T10:47:05.026+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:05 INFO SecurityManager: Changing view acls groups to:
[2023-01-22T10:47:05.027+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:05 INFO SecurityManager: Changing modify acls groups to:
[2023-01-22T10:47:05.031+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:05 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-22T10:47:06.317+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:06 INFO Utils: Successfully started service 'sparkDriver' on port 41125.
[2023-01-22T10:47:06.470+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:06 INFO SparkEnv: Registering MapOutputTracker
[2023-01-22T10:47:06.720+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:06 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-22T10:47:06.838+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:06 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-22T10:47:06.845+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:06 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-22T10:47:06.865+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:06 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-22T10:47:06.957+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:06 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ba71c55f-da11-4a86-b842-259a7e71932e
[2023-01-22T10:47:07.042+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:07 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-22T10:47:07.144+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:07 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-22T10:47:08.114+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:08 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-01-22T10:47:08.282+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:08 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://ce1344c6e6d4:41125/jars/gcs-connector-hadoop3-latest.jar with timestamp 1674384423890
[2023-01-22T10:47:08.286+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:08 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://ce1344c6e6d4:41125/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1674384423890
[2023-01-22T10:47:08.613+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:08 INFO Executor: Starting executor ID driver on host ce1344c6e6d4
[2023-01-22T10:47:08.653+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:08 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-22T10:47:08.797+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:08 INFO Executor: Fetching spark://ce1344c6e6d4:41125/jars/gcs-connector-hadoop3-latest.jar with timestamp 1674384423890
[2023-01-22T10:47:09.286+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:09 INFO TransportClientFactory: Successfully created connection to ce1344c6e6d4/192.168.80.8:41125 after 149 ms (0 ms spent in bootstraps)
[2023-01-22T10:47:09.331+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:09 INFO Utils: Fetching spark://ce1344c6e6d4:41125/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-94eee711-e046-417f-9073-5a2632aa1297/userFiles-797949a1-70fb-4dc1-8c05-6a6b74e88d07/fetchFileTemp18168962694615435954.tmp
[2023-01-22T10:47:10.358+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:10 INFO Executor: Adding file:/tmp/spark-94eee711-e046-417f-9073-5a2632aa1297/userFiles-797949a1-70fb-4dc1-8c05-6a6b74e88d07/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-22T10:47:10.359+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:10 INFO Executor: Fetching spark://ce1344c6e6d4:41125/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1674384423890
[2023-01-22T10:47:10.363+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:10 INFO Utils: Fetching spark://ce1344c6e6d4:41125/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-94eee711-e046-417f-9073-5a2632aa1297/userFiles-797949a1-70fb-4dc1-8c05-6a6b74e88d07/fetchFileTemp10422710633889330186.tmp
[2023-01-22T10:47:10.925+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:10 INFO Executor: Adding file:/tmp/spark-94eee711-e046-417f-9073-5a2632aa1297/userFiles-797949a1-70fb-4dc1-8c05-6a6b74e88d07/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-22T10:47:10.945+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:10 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41005.
[2023-01-22T10:47:10.947+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:10 INFO NettyBlockTransferService: Server created on ce1344c6e6d4:41005
[2023-01-22T10:47:10.953+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:10 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-22T10:47:10.987+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:10 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ce1344c6e6d4, 41005, None)
[2023-01-22T10:47:11.001+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:10 INFO BlockManagerMasterEndpoint: Registering block manager ce1344c6e6d4:41005 with 434.4 MiB RAM, BlockManagerId(driver, ce1344c6e6d4, 41005, None)
[2023-01-22T10:47:11.010+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:11 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ce1344c6e6d4, 41005, None)
[2023-01-22T10:47:11.013+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:11 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ce1344c6e6d4, 41005, None)
[2023-01-22T10:47:13.447+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:13 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-22T10:47:13.468+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:13 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-22T10:47:18.531+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:18 INFO InMemoryFileIndex: It took 202 ms to list leaf files for 1 paths.
[2023-01-22T10:47:19.136+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:19 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.8 KiB, free 434.2 MiB)
[2023-01-22T10:47:19.301+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:19 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-22T10:47:19.307+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:19 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ce1344c6e6d4:41005 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-22T10:47:19.318+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:19 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-22T10:47:20.188+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:20 INFO FileInputFormat: Total input files to process : 1
[2023-01-22T10:47:20.237+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:20 INFO FileInputFormat: Total input files to process : 1
[2023-01-22T10:47:20.274+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:20 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-22T10:47:20.313+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:20 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-22T10:47:20.314+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:20 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-22T10:47:20.317+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:20 INFO DAGScheduler: Parents of final stage: List()
[2023-01-22T10:47:20.321+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:20 INFO DAGScheduler: Missing parents: List()
[2023-01-22T10:47:20.331+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:20 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-22T10:47:20.454+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:20 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-22T10:47:20.462+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:20 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.2 MiB)
[2023-01-22T10:47:20.464+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:20 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on ce1344c6e6d4:41005 (size: 4.5 KiB, free: 434.4 MiB)
[2023-01-22T10:47:20.465+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:20 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-22T10:47:20.495+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-22T10:47:20.497+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:20 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-22T10:47:20.629+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:20 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (ce1344c6e6d4, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-22T10:47:20.661+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:20 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-22T10:47:20.951+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:20 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010100__202101010200.json:0+9369
[2023-01-22T10:47:21.411+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:21 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3437 bytes result sent to driver
[2023-01-22T10:47:21.439+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:21 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 842 ms on ce1344c6e6d4 (executor driver) (1/1)
[2023-01-22T10:47:21.444+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:21 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-22T10:47:21.457+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:21 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 1.089 s
[2023-01-22T10:47:21.467+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:21 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-22T10:47:21.467+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-22T10:47:21.472+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:21 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 1.195998 s
[2023-01-22T10:47:22.514+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:22 INFO BlockManagerInfo: Removed broadcast_1_piece0 on ce1344c6e6d4:41005 in memory (size: 4.5 KiB, free: 434.4 MiB)
[2023-01-22T10:47:22.564+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:22 INFO BlockManagerInfo: Removed broadcast_0_piece0 on ce1344c6e6d4:41005 in memory (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-22T10:47:32.977+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:32 INFO FileSourceStrategy: Pushed Filters:
[2023-01-22T10:47:32.996+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:32 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-22T10:47:33.017+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:33 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-22T10:47:33.304+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:33 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-22T10:47:33.431+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-22T10:47:33.432+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-22T10:47:33.437+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:33 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-22T10:47:33.439+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-22T10:47:33.440+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-22T10:47:33.444+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-22T10:47:33.872+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:33 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.2 KiB, free 434.2 MiB)
[2023-01-22T10:47:33.914+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:33 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-22T10:47:33.916+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:33 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on ce1344c6e6d4:41005 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-22T10:47:33.918+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:33 INFO SparkContext: Created broadcast 2 from save at BigQueryWriteHelper.java:105
[2023-01-22T10:47:33.950+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-22T10:47:34.178+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:34 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-22T10:47:34.180+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:34 INFO DAGScheduler: Got job 1 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-22T10:47:34.181+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:34 INFO DAGScheduler: Final stage: ResultStage 1 (save at BigQueryWriteHelper.java:105)
[2023-01-22T10:47:34.181+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:34 INFO DAGScheduler: Parents of final stage: List()
[2023-01-22T10:47:34.182+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:34 INFO DAGScheduler: Missing parents: List()
[2023-01-22T10:47:34.186+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:34 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-22T10:47:34.300+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:34 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 215.9 KiB, free 434.0 MiB)
[2023-01-22T10:47:34.313+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:34 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 77.2 KiB, free 433.9 MiB)
[2023-01-22T10:47:34.315+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:34 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on ce1344c6e6d4:41005 (size: 77.2 KiB, free: 434.3 MiB)
[2023-01-22T10:47:34.317+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:34 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-22T10:47:34.319+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-22T10:47:34.319+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:34 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-22T10:47:34.331+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:34 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (ce1344c6e6d4, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-22T10:47:34.334+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:34 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-22T10:47:34.526+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-22T10:47:34.527+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-22T10:47:34.530+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:34 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-22T10:47:34.531+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-22T10:47:34.531+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-22T10:47:34.533+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-22T10:47:34.541+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:34 INFO CodecConfig: Compression: SNAPPY
[2023-01-22T10:47:34.545+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:34 INFO CodecConfig: Compression: SNAPPY
[2023-01-22T10:47:34.593+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:34 INFO ParquetOutputFormat: Parquet block size to 134217728
[2023-01-22T10:47:34.594+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:34 INFO ParquetOutputFormat: Validation is off
[2023-01-22T10:47:34.595+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:34 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-01-22T10:47:34.595+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:34 INFO ParquetOutputFormat: Parquet properties are:
[2023-01-22T10:47:34.595+0000] {spark_submit.py:495} INFO - Parquet page size to 1048576
[2023-01-22T10:47:34.596+0000] {spark_submit.py:495} INFO - Parquet dictionary page size to 1048576
[2023-01-22T10:47:34.596+0000] {spark_submit.py:495} INFO - Dictionary is true
[2023-01-22T10:47:34.596+0000] {spark_submit.py:495} INFO - Writer version is: PARQUET_1_0
[2023-01-22T10:47:34.596+0000] {spark_submit.py:495} INFO - Page size checking is: estimated
[2023-01-22T10:47:34.597+0000] {spark_submit.py:495} INFO - Min row count for page size check is: 100
[2023-01-22T10:47:34.597+0000] {spark_submit.py:495} INFO - Max row count for page size check is: 10000
[2023-01-22T10:47:34.597+0000] {spark_submit.py:495} INFO - Truncate length for column indexes is: 64
[2023-01-22T10:47:34.598+0000] {spark_submit.py:495} INFO - Truncate length for statistics min/max  is: 2147483647
[2023-01-22T10:47:34.598+0000] {spark_submit.py:495} INFO - Bloom filter enabled: false
[2023-01-22T10:47:34.599+0000] {spark_submit.py:495} INFO - Max Bloom filter size for a column is 1048576
[2023-01-22T10:47:34.600+0000] {spark_submit.py:495} INFO - Bloom filter expected number of distinct values are: null
[2023-01-22T10:47:34.600+0000] {spark_submit.py:495} INFO - Page row count limit to 20000
[2023-01-22T10:47:34.601+0000] {spark_submit.py:495} INFO - Writing page checksums is: on
[2023-01-22T10:47:34.689+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:34 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-01-22T10:47:34.689+0000] {spark_submit.py:495} INFO - {
[2023-01-22T10:47:34.689+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-22T10:47:34.690+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-22T10:47:34.690+0000] {spark_submit.py:495} INFO - "name" : "GL_MarketDocument",
[2023-01-22T10:47:34.690+0000] {spark_submit.py:495} INFO - "type" : {
[2023-01-22T10:47:34.690+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-22T10:47:34.691+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-22T10:47:34.691+0000] {spark_submit.py:495} INFO - "name" : "@xmlns",
[2023-01-22T10:47:34.691+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:47:34.691+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.692+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.692+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:47:34.692+0000] {spark_submit.py:495} INFO - "name" : "TimeSeries",
[2023-01-22T10:47:34.692+0000] {spark_submit.py:495} INFO - "type" : {
[2023-01-22T10:47:34.693+0000] {spark_submit.py:495} INFO - "type" : "array",
[2023-01-22T10:47:34.693+0000] {spark_submit.py:495} INFO - "elementType" : {
[2023-01-22T10:47:34.693+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-22T10:47:34.693+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-22T10:47:34.694+0000] {spark_submit.py:495} INFO - "name" : "MktPSRType",
[2023-01-22T10:47:34.698+0000] {spark_submit.py:495} INFO - "type" : {
[2023-01-22T10:47:34.699+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-22T10:47:34.699+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-22T10:47:34.700+0000] {spark_submit.py:495} INFO - "name" : "psrType",
[2023-01-22T10:47:34.700+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:47:34.700+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.701+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.701+0000] {spark_submit.py:495} INFO - } ]
[2023-01-22T10:47:34.702+0000] {spark_submit.py:495} INFO - },
[2023-01-22T10:47:34.702+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.702+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.703+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:47:34.703+0000] {spark_submit.py:495} INFO - "name" : "Period",
[2023-01-22T10:47:34.704+0000] {spark_submit.py:495} INFO - "type" : {
[2023-01-22T10:47:34.704+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-22T10:47:34.704+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-22T10:47:34.705+0000] {spark_submit.py:495} INFO - "name" : "Point",
[2023-01-22T10:47:34.705+0000] {spark_submit.py:495} INFO - "type" : {
[2023-01-22T10:47:34.705+0000] {spark_submit.py:495} INFO - "type" : "array",
[2023-01-22T10:47:34.705+0000] {spark_submit.py:495} INFO - "elementType" : {
[2023-01-22T10:47:34.705+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-22T10:47:34.706+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-22T10:47:34.706+0000] {spark_submit.py:495} INFO - "name" : "position",
[2023-01-22T10:47:34.706+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:47:34.706+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.707+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.707+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:47:34.707+0000] {spark_submit.py:495} INFO - "name" : "quantity",
[2023-01-22T10:47:34.707+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:47:34.707+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.708+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.708+0000] {spark_submit.py:495} INFO - } ]
[2023-01-22T10:47:34.708+0000] {spark_submit.py:495} INFO - },
[2023-01-22T10:47:34.708+0000] {spark_submit.py:495} INFO - "containsNull" : true
[2023-01-22T10:47:34.708+0000] {spark_submit.py:495} INFO - },
[2023-01-22T10:47:34.709+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.709+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.709+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:47:34.709+0000] {spark_submit.py:495} INFO - "name" : "resolution",
[2023-01-22T10:47:34.710+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:47:34.710+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.710+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.710+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:47:34.712+0000] {spark_submit.py:495} INFO - "name" : "timeInterval",
[2023-01-22T10:47:34.712+0000] {spark_submit.py:495} INFO - "type" : {
[2023-01-22T10:47:34.712+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-22T10:47:34.713+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-22T10:47:34.713+0000] {spark_submit.py:495} INFO - "name" : "end",
[2023-01-22T10:47:34.713+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:47:34.713+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.714+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.714+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:47:34.715+0000] {spark_submit.py:495} INFO - "name" : "start",
[2023-01-22T10:47:34.715+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:47:34.715+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.716+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.716+0000] {spark_submit.py:495} INFO - } ]
[2023-01-22T10:47:34.716+0000] {spark_submit.py:495} INFO - },
[2023-01-22T10:47:34.717+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.717+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.717+0000] {spark_submit.py:495} INFO - } ]
[2023-01-22T10:47:34.718+0000] {spark_submit.py:495} INFO - },
[2023-01-22T10:47:34.718+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.718+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.719+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:47:34.719+0000] {spark_submit.py:495} INFO - "name" : "businessType",
[2023-01-22T10:47:34.719+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:47:34.720+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.720+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.720+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:47:34.721+0000] {spark_submit.py:495} INFO - "name" : "curveType",
[2023-01-22T10:47:34.721+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:47:34.721+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.722+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.722+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:47:34.723+0000] {spark_submit.py:495} INFO - "name" : "inBiddingZone_Domain.mRID",
[2023-01-22T10:47:34.723+0000] {spark_submit.py:495} INFO - "type" : {
[2023-01-22T10:47:34.723+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-22T10:47:34.724+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-22T10:47:34.724+0000] {spark_submit.py:495} INFO - "name" : "#text",
[2023-01-22T10:47:34.725+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:47:34.725+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.726+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.726+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:47:34.727+0000] {spark_submit.py:495} INFO - "name" : "@codingScheme",
[2023-01-22T10:47:34.727+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:47:34.728+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.728+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.728+0000] {spark_submit.py:495} INFO - } ]
[2023-01-22T10:47:34.729+0000] {spark_submit.py:495} INFO - },
[2023-01-22T10:47:34.729+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.730+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.730+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:47:34.730+0000] {spark_submit.py:495} INFO - "name" : "mRID",
[2023-01-22T10:47:34.731+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:47:34.731+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.732+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.732+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:47:34.733+0000] {spark_submit.py:495} INFO - "name" : "objectAggregation",
[2023-01-22T10:47:34.733+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:47:34.733+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.734+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.734+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:47:34.734+0000] {spark_submit.py:495} INFO - "name" : "outBiddingZone_Domain.mRID",
[2023-01-22T10:47:34.735+0000] {spark_submit.py:495} INFO - "type" : {
[2023-01-22T10:47:34.735+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-22T10:47:34.735+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-22T10:47:34.735+0000] {spark_submit.py:495} INFO - "name" : "#text",
[2023-01-22T10:47:34.736+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:47:34.736+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.736+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.736+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:47:34.737+0000] {spark_submit.py:495} INFO - "name" : "@codingScheme",
[2023-01-22T10:47:34.737+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:47:34.737+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.737+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.737+0000] {spark_submit.py:495} INFO - } ]
[2023-01-22T10:47:34.737+0000] {spark_submit.py:495} INFO - },
[2023-01-22T10:47:34.737+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.738+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.738+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:47:34.738+0000] {spark_submit.py:495} INFO - "name" : "quantity_Measure_Unit.name",
[2023-01-22T10:47:34.738+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:47:34.738+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.738+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.738+0000] {spark_submit.py:495} INFO - } ]
[2023-01-22T10:47:34.738+0000] {spark_submit.py:495} INFO - },
[2023-01-22T10:47:34.739+0000] {spark_submit.py:495} INFO - "containsNull" : true
[2023-01-22T10:47:34.739+0000] {spark_submit.py:495} INFO - },
[2023-01-22T10:47:34.739+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.739+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.739+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:47:34.739+0000] {spark_submit.py:495} INFO - "name" : "createdDateTime",
[2023-01-22T10:47:34.739+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:47:34.739+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.740+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.740+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:47:34.740+0000] {spark_submit.py:495} INFO - "name" : "mRID",
[2023-01-22T10:47:34.740+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:47:34.740+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.740+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.740+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:47:34.740+0000] {spark_submit.py:495} INFO - "name" : "process.processType",
[2023-01-22T10:47:34.740+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:47:34.741+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.741+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.741+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:47:34.741+0000] {spark_submit.py:495} INFO - "name" : "receiver_MarketParticipant.mRID",
[2023-01-22T10:47:34.741+0000] {spark_submit.py:495} INFO - "type" : {
[2023-01-22T10:47:34.741+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-22T10:47:34.741+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-22T10:47:34.741+0000] {spark_submit.py:495} INFO - "name" : "#text",
[2023-01-22T10:47:34.742+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:47:34.742+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.742+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.742+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:47:34.742+0000] {spark_submit.py:495} INFO - "name" : "@codingScheme",
[2023-01-22T10:47:34.742+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:47:34.742+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.742+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.743+0000] {spark_submit.py:495} INFO - } ]
[2023-01-22T10:47:34.743+0000] {spark_submit.py:495} INFO - },
[2023-01-22T10:47:34.743+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.743+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.743+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:47:34.743+0000] {spark_submit.py:495} INFO - "name" : "receiver_MarketParticipant.marketRole.type",
[2023-01-22T10:47:34.743+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:47:34.743+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.744+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.744+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:47:34.744+0000] {spark_submit.py:495} INFO - "name" : "revisionNumber",
[2023-01-22T10:47:34.744+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:47:34.744+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.744+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.744+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:47:34.744+0000] {spark_submit.py:495} INFO - "name" : "sender_MarketParticipant.mRID",
[2023-01-22T10:47:34.745+0000] {spark_submit.py:495} INFO - "type" : {
[2023-01-22T10:47:34.745+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-22T10:47:34.745+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-22T10:47:34.745+0000] {spark_submit.py:495} INFO - "name" : "#text",
[2023-01-22T10:47:34.745+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:47:34.745+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.745+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.745+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:47:34.746+0000] {spark_submit.py:495} INFO - "name" : "@codingScheme",
[2023-01-22T10:47:34.746+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:47:34.746+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.746+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.746+0000] {spark_submit.py:495} INFO - } ]
[2023-01-22T10:47:34.746+0000] {spark_submit.py:495} INFO - },
[2023-01-22T10:47:34.747+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.747+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.747+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:47:34.747+0000] {spark_submit.py:495} INFO - "name" : "sender_MarketParticipant.marketRole.type",
[2023-01-22T10:47:34.747+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:47:34.748+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.748+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.748+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:47:34.748+0000] {spark_submit.py:495} INFO - "name" : "time_Period.timeInterval",
[2023-01-22T10:47:34.748+0000] {spark_submit.py:495} INFO - "type" : {
[2023-01-22T10:47:34.749+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-22T10:47:34.749+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-22T10:47:34.749+0000] {spark_submit.py:495} INFO - "name" : "end",
[2023-01-22T10:47:34.749+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:47:34.750+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.750+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.750+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:47:34.750+0000] {spark_submit.py:495} INFO - "name" : "start",
[2023-01-22T10:47:34.750+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:47:34.751+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.751+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.751+0000] {spark_submit.py:495} INFO - } ]
[2023-01-22T10:47:34.751+0000] {spark_submit.py:495} INFO - },
[2023-01-22T10:47:34.752+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.752+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.752+0000] {spark_submit.py:495} INFO - }, {
[2023-01-22T10:47:34.752+0000] {spark_submit.py:495} INFO - "name" : "type",
[2023-01-22T10:47:34.752+0000] {spark_submit.py:495} INFO - "type" : "string",
[2023-01-22T10:47:34.753+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.753+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.753+0000] {spark_submit.py:495} INFO - } ]
[2023-01-22T10:47:34.753+0000] {spark_submit.py:495} INFO - },
[2023-01-22T10:47:34.754+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-22T10:47:34.754+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-22T10:47:34.754+0000] {spark_submit.py:495} INFO - } ]
[2023-01-22T10:47:34.754+0000] {spark_submit.py:495} INFO - }
[2023-01-22T10:47:34.754+0000] {spark_submit.py:495} INFO - and corresponding Parquet message type:
[2023-01-22T10:47:34.755+0000] {spark_submit.py:495} INFO - message spark_schema {
[2023-01-22T10:47:34.755+0000] {spark_submit.py:495} INFO - optional group GL_MarketDocument {
[2023-01-22T10:47:34.755+0000] {spark_submit.py:495} INFO - optional binary @xmlns (STRING);
[2023-01-22T10:47:34.755+0000] {spark_submit.py:495} INFO - optional group TimeSeries (LIST) {
[2023-01-22T10:47:34.756+0000] {spark_submit.py:495} INFO - repeated group list {
[2023-01-22T10:47:34.756+0000] {spark_submit.py:495} INFO - optional group element {
[2023-01-22T10:47:34.756+0000] {spark_submit.py:495} INFO - optional group MktPSRType {
[2023-01-22T10:47:34.756+0000] {spark_submit.py:495} INFO - optional binary psrType (STRING);
[2023-01-22T10:47:34.756+0000] {spark_submit.py:495} INFO - }
[2023-01-22T10:47:34.757+0000] {spark_submit.py:495} INFO - optional group Period {
[2023-01-22T10:47:34.757+0000] {spark_submit.py:495} INFO - optional group Point (LIST) {
[2023-01-22T10:47:34.757+0000] {spark_submit.py:495} INFO - repeated group list {
[2023-01-22T10:47:34.757+0000] {spark_submit.py:495} INFO - optional group element {
[2023-01-22T10:47:34.758+0000] {spark_submit.py:495} INFO - optional binary position (STRING);
[2023-01-22T10:47:34.758+0000] {spark_submit.py:495} INFO - optional binary quantity (STRING);
[2023-01-22T10:47:34.758+0000] {spark_submit.py:495} INFO - }
[2023-01-22T10:47:34.758+0000] {spark_submit.py:495} INFO - }
[2023-01-22T10:47:34.758+0000] {spark_submit.py:495} INFO - }
[2023-01-22T10:47:34.759+0000] {spark_submit.py:495} INFO - optional binary resolution (STRING);
[2023-01-22T10:47:34.759+0000] {spark_submit.py:495} INFO - optional group timeInterval {
[2023-01-22T10:47:34.759+0000] {spark_submit.py:495} INFO - optional binary end (STRING);
[2023-01-22T10:47:34.760+0000] {spark_submit.py:495} INFO - optional binary start (STRING);
[2023-01-22T10:47:34.760+0000] {spark_submit.py:495} INFO - }
[2023-01-22T10:47:34.760+0000] {spark_submit.py:495} INFO - }
[2023-01-22T10:47:34.760+0000] {spark_submit.py:495} INFO - optional binary businessType (STRING);
[2023-01-22T10:47:34.760+0000] {spark_submit.py:495} INFO - optional binary curveType (STRING);
[2023-01-22T10:47:34.761+0000] {spark_submit.py:495} INFO - optional group inBiddingZone_Domain.mRID {
[2023-01-22T10:47:34.761+0000] {spark_submit.py:495} INFO - optional binary #text (STRING);
[2023-01-22T10:47:34.761+0000] {spark_submit.py:495} INFO - optional binary @codingScheme (STRING);
[2023-01-22T10:47:34.762+0000] {spark_submit.py:495} INFO - }
[2023-01-22T10:47:34.762+0000] {spark_submit.py:495} INFO - optional binary mRID (STRING);
[2023-01-22T10:47:34.762+0000] {spark_submit.py:495} INFO - optional binary objectAggregation (STRING);
[2023-01-22T10:47:34.762+0000] {spark_submit.py:495} INFO - optional group outBiddingZone_Domain.mRID {
[2023-01-22T10:47:34.763+0000] {spark_submit.py:495} INFO - optional binary #text (STRING);
[2023-01-22T10:47:34.763+0000] {spark_submit.py:495} INFO - optional binary @codingScheme (STRING);
[2023-01-22T10:47:34.763+0000] {spark_submit.py:495} INFO - }
[2023-01-22T10:47:34.763+0000] {spark_submit.py:495} INFO - optional binary quantity_Measure_Unit.name (STRING);
[2023-01-22T10:47:34.763+0000] {spark_submit.py:495} INFO - }
[2023-01-22T10:47:34.764+0000] {spark_submit.py:495} INFO - }
[2023-01-22T10:47:34.764+0000] {spark_submit.py:495} INFO - }
[2023-01-22T10:47:34.764+0000] {spark_submit.py:495} INFO - optional binary createdDateTime (STRING);
[2023-01-22T10:47:34.764+0000] {spark_submit.py:495} INFO - optional binary mRID (STRING);
[2023-01-22T10:47:34.765+0000] {spark_submit.py:495} INFO - optional binary process.processType (STRING);
[2023-01-22T10:47:34.765+0000] {spark_submit.py:495} INFO - optional group receiver_MarketParticipant.mRID {
[2023-01-22T10:47:34.765+0000] {spark_submit.py:495} INFO - optional binary #text (STRING);
[2023-01-22T10:47:34.765+0000] {spark_submit.py:495} INFO - optional binary @codingScheme (STRING);
[2023-01-22T10:47:34.765+0000] {spark_submit.py:495} INFO - }
[2023-01-22T10:47:34.766+0000] {spark_submit.py:495} INFO - optional binary receiver_MarketParticipant.marketRole.type (STRING);
[2023-01-22T10:47:34.766+0000] {spark_submit.py:495} INFO - optional binary revisionNumber (STRING);
[2023-01-22T10:47:34.766+0000] {spark_submit.py:495} INFO - optional group sender_MarketParticipant.mRID {
[2023-01-22T10:47:34.766+0000] {spark_submit.py:495} INFO - optional binary #text (STRING);
[2023-01-22T10:47:34.767+0000] {spark_submit.py:495} INFO - optional binary @codingScheme (STRING);
[2023-01-22T10:47:34.767+0000] {spark_submit.py:495} INFO - }
[2023-01-22T10:47:34.767+0000] {spark_submit.py:495} INFO - optional binary sender_MarketParticipant.marketRole.type (STRING);
[2023-01-22T10:47:34.767+0000] {spark_submit.py:495} INFO - optional group time_Period.timeInterval {
[2023-01-22T10:47:34.768+0000] {spark_submit.py:495} INFO - optional binary end (STRING);
[2023-01-22T10:47:34.768+0000] {spark_submit.py:495} INFO - optional binary start (STRING);
[2023-01-22T10:47:34.768+0000] {spark_submit.py:495} INFO - }
[2023-01-22T10:47:34.768+0000] {spark_submit.py:495} INFO - optional binary type (STRING);
[2023-01-22T10:47:34.768+0000] {spark_submit.py:495} INFO - }
[2023-01-22T10:47:34.769+0000] {spark_submit.py:495} INFO - }
[2023-01-22T10:47:34.769+0000] {spark_submit.py:495} INFO - 
[2023-01-22T10:47:34.769+0000] {spark_submit.py:495} INFO - 
[2023-01-22T10:47:35.061+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:35 INFO CodecPool: Got brand-new compressor [.snappy]
[2023-01-22T10:47:35.508+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:35 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010100__202101010200.json, range: 0-9369, partition values: [empty row]
[2023-01-22T10:47:35.981+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:35 INFO CodeGenerator: Code generated in 389.562141 ms
[2023-01-22T10:47:37.536+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:37 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1674384428433-c44f648b-1f24-4004-a750-ccf7baeb85d5/_temporary/0/_temporary/' directory.
[2023-01-22T10:47:37.537+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:37 INFO FileOutputCommitter: Saved output of task 'attempt_202301221047342440893742698249811_0001_m_000000_1' to gs://entsoe_temp_1009/.spark-bigquery-local-1674384428433-c44f648b-1f24-4004-a750-ccf7baeb85d5/_temporary/0/task_202301221047342440893742698249811_0001_m_000000
[2023-01-22T10:47:37.537+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:37 INFO SparkHadoopMapRedUtil: attempt_202301221047342440893742698249811_0001_m_000000_1: Committed. Elapsed time: 557 ms.
[2023-01-22T10:47:37.546+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:37 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2799 bytes result sent to driver
[2023-01-22T10:47:37.549+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:37 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 3226 ms on ce1344c6e6d4 (executor driver) (1/1)
[2023-01-22T10:47:37.549+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:37 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-22T10:47:37.550+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:37 INFO DAGScheduler: ResultStage 1 (save at BigQueryWriteHelper.java:105) finished in 3.356 s
[2023-01-22T10:47:37.550+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:37 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-22T10:47:37.551+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-22T10:47:37.551+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:37 INFO DAGScheduler: Job 1 finished: save at BigQueryWriteHelper.java:105, took 3.372883 s
[2023-01-22T10:47:37.553+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:37 INFO FileFormatWriter: Start to commit write Job cbcb7a12-eef0-4e9f-a263-df6801c40e36.
[2023-01-22T10:47:38.125+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:38 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1674384428433-c44f648b-1f24-4004-a750-ccf7baeb85d5/_temporary/0/task_202301221047342440893742698249811_0001_m_000000/' directory.
[2023-01-22T10:47:38.380+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:38 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1674384428433-c44f648b-1f24-4004-a750-ccf7baeb85d5/' directory.
[2023-01-22T10:47:38.502+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:38 INFO BlockManagerInfo: Removed broadcast_3_piece0 on ce1344c6e6d4:41005 in memory (size: 77.2 KiB, free: 434.4 MiB)
[2023-01-22T10:47:39.120+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:39 INFO FileFormatWriter: Write Job cbcb7a12-eef0-4e9f-a263-df6801c40e36 committed. Elapsed time: 1566 ms.
[2023-01-22T10:47:39.123+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:39 INFO FileFormatWriter: Finished processing stats for write job cbcb7a12-eef0-4e9f-a263-df6801c40e36.
[2023-01-22T10:47:39.731+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:39 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=entsoe_playground, projectId=rafzul-analytics-1009, tableId=TEST_total_generation_staging}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=null, ignoreUnknownValue=null, sourceUris=[gs://entsoe_temp_1009/.spark-bigquery-local-1674384428433-c44f648b-1f24-4004-a750-ccf7baeb85d5/part-00000-a2aa467e-65c8-468e-ad6a-bf3714c0213b-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=true, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=rafzul-analytics-1009, job=82ac4cb6-90d8-4f24-b53f-1c7901ebaeca, location=asia-southeast1}
[2023-01-22T10:47:41.519+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:41 ERROR BigQueryClient: Unable to create the job to load to rafzul-analytics-1009.entsoe_playground.TEST_total_generation_staging
[2023-01-22T10:47:41.789+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T10:47:41.789+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 68, in <module>
[2023-01-22T10:47:41.789+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T10:47:41.789+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 58, in main
[2023-01-22T10:47:41.789+0000] {spark_submit.py:495} INFO - .save("rafzul-analytics-1009.entsoe_playground.TEST_total_generation_staging")
[2023-01-22T10:47:41.789+0000] {spark_submit.py:495} INFO - File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 968, in save
[2023-01-22T10:47:41.789+0000] {spark_submit.py:495} INFO - File "/opt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py", line 1321, in __call__
[2023-01-22T10:47:41.790+0000] {spark_submit.py:495} INFO - File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 190, in deco
[2023-01-22T10:47:41.790+0000] {spark_submit.py:495} INFO - File "/opt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py", line 326, in get_return_value
[2023-01-22T10:47:41.790+0000] {spark_submit.py:495} INFO - py4j.protocol.Py4JJavaError: An error occurred while calling o43.save.
[2023-01-22T10:47:41.790+0000] {spark_submit.py:495} INFO - : com.google.cloud.bigquery.connector.common.BigQueryConnectorException: Failed to write to BigQuery
[2023-01-22T10:47:41.790+0000] {spark_submit.py:495} INFO - at com.google.cloud.spark.bigquery.write.BigQueryWriteHelper.writeDataFrameToBigQuery(BigQueryWriteHelper.java:110)
[2023-01-22T10:47:41.790+0000] {spark_submit.py:495} INFO - at com.google.cloud.spark.bigquery.write.BigQueryDeprecatedIndirectInsertableRelation.insert(BigQueryDeprecatedIndirectInsertableRelation.java:43)
[2023-01-22T10:47:41.791+0000] {spark_submit.py:495} INFO - at com.google.cloud.spark.bigquery.write.CreatableRelationProviderHelper.createRelation(CreatableRelationProviderHelper.java:51)
[2023-01-22T10:47:41.791+0000] {spark_submit.py:495} INFO - at com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelation(BigQueryRelationProvider.scala:106)
[2023-01-22T10:47:41.791+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
[2023-01-22T10:47:41.791+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
[2023-01-22T10:47:41.791+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
[2023-01-22T10:47:41.791+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
[2023-01-22T10:47:41.791+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
[2023-01-22T10:47:41.791+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-22T10:47:41.791+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-22T10:47:41.791+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-22T10:47:41.791+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-22T10:47:41.792+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-22T10:47:41.792+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
[2023-01-22T10:47:41.792+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
[2023-01-22T10:47:41.792+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
[2023-01-22T10:47:41.792+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
[2023-01-22T10:47:41.792+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
[2023-01-22T10:47:41.792+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-22T10:47:41.792+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2023-01-22T10:47:41.792+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2023-01-22T10:47:41.792+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-22T10:47:41.792+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-22T10:47:41.792+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
[2023-01-22T10:47:41.793+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
[2023-01-22T10:47:41.793+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
[2023-01-22T10:47:41.793+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
[2023-01-22T10:47:41.793+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)
[2023-01-22T10:47:41.793+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)
[2023-01-22T10:47:41.793+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)
[2023-01-22T10:47:41.793+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)
[2023-01-22T10:47:41.793+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
[2023-01-22T10:47:41.793+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T10:47:41.793+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T10:47:41.793+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T10:47:41.793+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T10:47:41.794+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T10:47:41.794+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T10:47:41.794+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T10:47:41.794+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T10:47:41.794+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T10:47:41.794+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T10:47:41.794+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T10:47:41.794+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T10:47:41.794+0000] {spark_submit.py:495} INFO - Caused by: com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryException: Character '.' found in field name: inBiddingZone_Domain.mRID, parquet file: /bigstore/entsoe_temp_1009/.spark-bigquery-local-1674384428433-c44f648b-1f24-4004-a750-ccf7baeb85d5/part-00000-a2aa467e-65c8-468e-ad6a-bf3714c0213b-c000.snappy.parquet.Reading such fields is not yet supported.
[2023-01-22T10:47:41.794+0000] {spark_submit.py:495} INFO - at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.Job.reload(Job.java:419)
[2023-01-22T10:47:41.794+0000] {spark_submit.py:495} INFO - at com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.Job.waitFor(Job.java:252)
[2023-01-22T10:47:41.795+0000] {spark_submit.py:495} INFO - at com.google.cloud.bigquery.connector.common.BigQueryClient.createAndWaitFor(BigQueryClient.java:333)
[2023-01-22T10:47:41.795+0000] {spark_submit.py:495} INFO - at com.google.cloud.bigquery.connector.common.BigQueryClient.createAndWaitFor(BigQueryClient.java:323)
[2023-01-22T10:47:41.795+0000] {spark_submit.py:495} INFO - at com.google.cloud.bigquery.connector.common.BigQueryClient.loadDataIntoTable(BigQueryClient.java:553)
[2023-01-22T10:47:41.795+0000] {spark_submit.py:495} INFO - at com.google.cloud.spark.bigquery.write.BigQueryWriteHelper.loadDataToBigQuery(BigQueryWriteHelper.java:130)
[2023-01-22T10:47:41.795+0000] {spark_submit.py:495} INFO - at com.google.cloud.spark.bigquery.write.BigQueryWriteHelper.writeDataFrameToBigQuery(BigQueryWriteHelper.java:107)
[2023-01-22T10:47:41.795+0000] {spark_submit.py:495} INFO - ... 44 more
[2023-01-22T10:47:41.795+0000] {spark_submit.py:495} INFO - 
[2023-01-22T10:47:41.825+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:41 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-22T10:47:41.833+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:41 INFO SparkUI: Stopped Spark web UI at http://ce1344c6e6d4:4040
[2023-01-22T10:47:41.841+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:41 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-22T10:47:41.850+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:41 INFO MemoryStore: MemoryStore cleared
[2023-01-22T10:47:41.851+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:41 INFO BlockManager: BlockManager stopped
[2023-01-22T10:47:41.853+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:41 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-22T10:47:41.855+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:41 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-22T10:47:41.860+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:41 INFO SparkContext: Successfully stopped SparkContext
[2023-01-22T10:47:41.860+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:41 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T10:47:41.861+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-94eee711-e046-417f-9073-5a2632aa1297/pyspark-1104be6d-9e55-4c18-8796-82cca0acdb7a
[2023-01-22T10:47:41.865+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-380c2291-eb75-4eaa-a951-0b41889c9866
[2023-01-22T10:47:41.869+0000] {spark_submit.py:495} INFO - 23/01/22 10:47:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-94eee711-e046-417f-9073-5a2632aa1297
[2023-01-22T10:47:41.947+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-22T10:47:41.949+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230122T104657, end_date=20230122T104741
[2023-01-22T10:47:41.968+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 466 for task transform_stage_generation (Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 1871)
[2023-01-22T10:47:41.985+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T10:47:41.998+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
