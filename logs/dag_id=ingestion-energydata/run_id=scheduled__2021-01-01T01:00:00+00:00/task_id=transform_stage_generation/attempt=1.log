[2023-01-20T16:16:53.825+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-20T16:16:53.839+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-20T16:16:53.839+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T16:16:53.839+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-20T16:16:53.839+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T16:16:53.859+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-20T16:16:53.881+0000] {standard_task_runner.py:55} INFO - Started process 196 to run task
[2023-01-20T16:16:53.889+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '60', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpx72301rs']
[2023-01-20T16:16:53.895+0000] {standard_task_runner.py:83} INFO - Job 60: Subtask transform_stage_generation
[2023-01-20T16:16:53.975+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T16:16:54.063+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-20T16:16:54.074+0000] {spark_submit.py:222} INFO - Could not load connection string spark_default, defaulting to yarn
[2023-01-20T16:16:54.075+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master yarn --name arrow-spark /opt/***/plugins/scripts/transform_stage.py total_generation 202101010100 202101010200 Europe/Berlin DE_TENNET
[2023-01-20T16:16:54.084+0000] {spark_submit.py:495} INFO - /usr/local/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit: line 27: /opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-class: No such file or directory
[2023-01-20T16:16:54.095+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master yarn --name arrow-spark /opt/***/plugins/scripts/transform_stage.py total_generation 202101010100 202101010200 Europe/Berlin DE_TENNET. Error code is: 127.
[2023-01-20T16:16:54.100+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230120T161653, end_date=20230120T161654
[2023-01-20T16:16:54.115+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 60 for task transform_stage_generation (Cannot execute: spark-submit --master yarn --name arrow-spark /opt/***/plugins/scripts/transform_stage.py total_generation 202101010100 202101010200 Europe/Berlin DE_TENNET. Error code is: 127.; 196)
[2023-01-20T16:16:54.139+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T16:16:54.158+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T16:19:06.350+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-20T16:19:06.364+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-20T16:19:06.364+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T16:19:06.364+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-20T16:19:06.364+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T16:19:06.381+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-20T16:19:06.392+0000] {standard_task_runner.py:55} INFO - Started process 336 to run task
[2023-01-20T16:19:06.397+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '64', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpi7u67u0o']
[2023-01-20T16:19:06.400+0000] {standard_task_runner.py:83} INFO - Job 64: Subtask transform_stage_generation
[2023-01-20T16:19:06.474+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T16:19:06.546+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-20T16:19:06.556+0000] {spark_submit.py:222} INFO - Could not load connection string spark_default, defaulting to yarn
[2023-01-20T16:19:06.557+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master yarn --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 Europe/Berlin DE_TENNET
[2023-01-20T16:19:06.563+0000] {spark_submit.py:495} INFO - /usr/local/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit: line 27: /opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-class: No such file or directory
[2023-01-20T16:19:06.573+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master yarn --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 Europe/Berlin DE_TENNET. Error code is: 127.
[2023-01-20T16:19:06.576+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230120T161906, end_date=20230120T161906
[2023-01-20T16:19:06.588+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 64 for task transform_stage_generation (Cannot execute: spark-submit --master yarn --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 Europe/Berlin DE_TENNET. Error code is: 127.; 336)
[2023-01-20T16:19:06.608+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T16:19:06.629+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T16:22:27.155+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-20T16:22:27.203+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-20T16:22:27.203+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T16:22:27.204+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-20T16:22:27.204+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T16:22:27.254+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-20T16:22:27.277+0000] {standard_task_runner.py:55} INFO - Started process 553 to run task
[2023-01-20T16:22:27.289+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '69', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpcv_xr7ip']
[2023-01-20T16:22:27.296+0000] {standard_task_runner.py:83} INFO - Job 69: Subtask transform_stage_generation
[2023-01-20T16:22:27.500+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T16:22:27.689+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-20T16:22:27.717+0000] {spark_submit.py:222} INFO - Could not load connection string spark_default, defaulting to yarn
[2023-01-20T16:22:27.720+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master yarn --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-20T16:22:27.737+0000] {spark_submit.py:495} INFO - /usr/local/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit: line 27: /opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-class: No such file or directory
[2023-01-20T16:22:27.779+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master yarn --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 127.
[2023-01-20T16:22:27.790+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230120T162227, end_date=20230120T162227
[2023-01-20T16:22:27.831+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 69 for task transform_stage_generation (Cannot execute: spark-submit --master yarn --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 127.; 553)
[2023-01-20T16:22:27.904+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T16:22:27.980+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T16:31:16.656+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-20T16:31:16.667+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-20T16:31:16.667+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T16:31:16.668+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-20T16:31:16.668+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T16:31:16.680+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-20T16:31:16.688+0000] {standard_task_runner.py:55} INFO - Started process 1110 to run task
[2023-01-20T16:31:16.691+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '74', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpxdv41lm5']
[2023-01-20T16:31:16.694+0000] {standard_task_runner.py:83} INFO - Job 74: Subtask transform_stage_generation
[2023-01-20T16:31:16.753+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T16:31:16.843+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-20T16:31:16.852+0000] {spark_submit.py:222} INFO - Could not load connection string spark_default, defaulting to yarn
[2023-01-20T16:31:16.855+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master yarn --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-20T16:31:16.866+0000] {spark_submit.py:495} INFO - /usr/local/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit: line 27: /opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-class: No such file or directory
[2023-01-20T16:31:16.879+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master yarn --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 127.
[2023-01-20T16:31:16.883+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230120T163116, end_date=20230120T163116
[2023-01-20T16:31:16.899+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 74 for task transform_stage_generation (Cannot execute: spark-submit --master yarn --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 127.; 1110)
[2023-01-20T16:31:16.943+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T16:31:16.965+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T17:55:57.553+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-20T17:55:57.564+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-20T17:55:57.565+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T17:55:57.565+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-20T17:55:57.565+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T17:55:57.576+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-20T17:55:57.588+0000] {standard_task_runner.py:55} INFO - Started process 6214 to run task
[2023-01-20T17:55:57.592+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '85', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpq3zofpe4']
[2023-01-20T17:55:57.596+0000] {standard_task_runner.py:83} INFO - Job 85: Subtask transform_stage_generation
[2023-01-20T17:55:57.675+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T17:55:57.751+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-20T17:55:57.752+0000] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-01-20T17:55:57.753+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master  --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-20T17:55:57.760+0000] {spark_submit.py:495} INFO - /usr/local/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit: line 27: /opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-class: No such file or directory
[2023-01-20T17:55:57.771+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master  --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 127.
[2023-01-20T17:55:57.774+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230120T175557, end_date=20230120T175557
[2023-01-20T17:55:57.787+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 85 for task transform_stage_generation (Cannot execute: spark-submit --master  --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 127.; 6214)
[2023-01-20T17:55:57.804+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T17:55:57.825+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T17:59:00.468+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-20T17:59:00.486+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-20T17:59:00.487+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T17:59:00.487+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-20T17:59:00.487+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T17:59:00.511+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-20T17:59:00.530+0000] {standard_task_runner.py:55} INFO - Started process 6425 to run task
[2023-01-20T17:59:00.536+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '88', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp5_c9vvdh']
[2023-01-20T17:59:00.541+0000] {standard_task_runner.py:83} INFO - Job 88: Subtask transform_stage_generation
[2023-01-20T17:59:00.649+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T17:59:00.745+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-20T17:59:00.746+0000] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-01-20T17:59:00.749+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master  --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-20T17:59:00.786+0000] {spark_submit.py:495} INFO - /usr/local/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit: line 27: /opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-class: No such file or directory
[2023-01-20T17:59:00.800+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master  --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 127.
[2023-01-20T17:59:00.806+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230120T175900, end_date=20230120T175900
[2023-01-20T17:59:00.822+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 88 for task transform_stage_generation (Cannot execute: spark-submit --master  --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 127.; 6425)
[2023-01-20T17:59:00.870+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T17:59:00.894+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T18:05:02.154+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-20T18:05:02.171+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-20T18:05:02.171+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T18:05:02.171+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-20T18:05:02.172+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T18:05:02.196+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-20T18:05:02.211+0000] {standard_task_runner.py:55} INFO - Started process 6787 to run task
[2023-01-20T18:05:02.218+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '95', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpda6ut4ct']
[2023-01-20T18:05:02.223+0000] {standard_task_runner.py:83} INFO - Job 95: Subtask transform_stage_generation
[2023-01-20T18:05:02.322+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T18:05:02.443+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-20T18:05:02.445+0000] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-01-20T18:05:02.446+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master  --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-20T18:05:02.456+0000] {spark_submit.py:495} INFO - /usr/local/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit: line 27: /opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-class: No such file or directory
[2023-01-20T18:05:02.469+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master  --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 127.
[2023-01-20T18:05:02.474+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230120T180502, end_date=20230120T180502
[2023-01-20T18:05:02.497+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 95 for task transform_stage_generation (Cannot execute: spark-submit --master  --jars /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/gcs-connector-hadoop3-latest.jar, /opt/spark-3.3.1-bin-hadoop3-scala2.13/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 127.; 6787)
[2023-01-20T18:05:02.557+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T18:05:02.578+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T18:11:16.199+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-20T18:11:16.207+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-20T18:11:16.208+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T18:11:16.208+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-20T18:11:16.208+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T18:11:16.219+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-20T18:11:16.227+0000] {standard_task_runner.py:55} INFO - Started process 7213 to run task
[2023-01-20T18:11:16.230+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '101', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpcqjposfm']
[2023-01-20T18:11:16.233+0000] {standard_task_runner.py:83} INFO - Job 101: Subtask transform_stage_generation
[2023-01-20T18:11:16.289+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T18:11:16.348+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-20T18:11:16.348+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 156, in execute
    self._hook = self._get_hook()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 165, in _get_hook
    return SparkSubmitHook(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 155, in __init__
    raise RuntimeError(
RuntimeError: The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/opt/spark-3.3.1-bin-hadoop3-scala2.13bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH
[2023-01-20T18:11:16.356+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230120T181116, end_date=20230120T181116
[2023-01-20T18:11:16.366+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 101 for task transform_stage_generation (The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/opt/spark-3.3.1-bin-hadoop3-scala2.13bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH; 7213)
[2023-01-20T18:11:16.402+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T18:11:16.420+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T18:12:43.187+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-20T18:12:43.211+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-20T18:12:43.212+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T18:12:43.212+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-20T18:12:43.212+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T18:12:43.244+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-20T18:12:43.259+0000] {standard_task_runner.py:55} INFO - Started process 7308 to run task
[2023-01-20T18:12:43.270+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '105', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp8anc4g1g']
[2023-01-20T18:12:43.278+0000] {standard_task_runner.py:83} INFO - Job 105: Subtask transform_stage_generation
[2023-01-20T18:12:43.444+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T18:12:43.597+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-20T18:12:43.598+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 156, in execute
    self._hook = self._get_hook()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 165, in _get_hook
    return SparkSubmitHook(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 155, in __init__
    raise RuntimeError(
RuntimeError: The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH
[2023-01-20T18:12:43.615+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230120T181243, end_date=20230120T181243
[2023-01-20T18:12:43.636+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 105 for task transform_stage_generation (The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH; 7308)
[2023-01-20T18:12:43.681+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T18:12:43.715+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T18:16:15.998+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-20T18:16:16.025+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-20T18:16:16.025+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T18:16:16.025+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-20T18:16:16.025+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T18:16:16.076+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-20T18:16:16.102+0000] {standard_task_runner.py:55} INFO - Started process 7544 to run task
[2023-01-20T18:16:16.110+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '109', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpd5b_z3t7']
[2023-01-20T18:16:16.117+0000] {standard_task_runner.py:83} INFO - Job 109: Subtask transform_stage_generation
[2023-01-20T18:16:16.276+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T18:16:16.476+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-20T18:16:16.477+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 156, in execute
    self._hook = self._get_hook()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 165, in _get_hook
    return SparkSubmitHook(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 155, in __init__
    raise RuntimeError(
RuntimeError: The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH
[2023-01-20T18:16:16.504+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230120T181616, end_date=20230120T181616
[2023-01-20T18:16:16.514+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 109 for task transform_stage_generation (The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH; 7544)
[2023-01-20T18:16:16.560+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T18:16:16.618+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T18:18:55.646+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-20T18:18:55.664+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-20T18:18:55.665+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T18:18:55.665+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-20T18:18:55.666+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T18:18:55.697+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-20T18:18:55.712+0000] {standard_task_runner.py:55} INFO - Started process 7715 to run task
[2023-01-20T18:18:55.719+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '112', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpw4s4bj_2']
[2023-01-20T18:18:55.723+0000] {standard_task_runner.py:83} INFO - Job 112: Subtask transform_stage_generation
[2023-01-20T18:18:55.817+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 5efae2a07872
[2023-01-20T18:18:55.947+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-20T18:18:55.948+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 156, in execute
    self._hook = self._get_hook()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 165, in _get_hook
    return SparkSubmitHook(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 155, in __init__
    raise RuntimeError(
RuntimeError: The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH
[2023-01-20T18:18:55.961+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230120T181855, end_date=20230120T181855
[2023-01-20T18:18:55.980+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 112 for task transform_stage_generation (The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/opt/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH; 7715)
[2023-01-20T18:18:56.017+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T18:18:56.063+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-20T19:15:54.700+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-20T19:15:54.775+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-20T19:15:54.776+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T19:15:54.777+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-20T19:15:54.779+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-20T19:15:54.833+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-20T19:15:54.848+0000] {standard_task_runner.py:55} INFO - Started process 361 to run task
[2023-01-20T19:15:54.857+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '129', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpxsm2i8hk']
[2023-01-20T19:15:54.860+0000] {standard_task_runner.py:83} INFO - Job 129: Subtask transform_stage_generation
[2023-01-20T19:15:55.109+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 984f5901aea4
[2023-01-20T19:15:55.329+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-20T19:15:55.330+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 156, in execute
    self._hook = self._get_hook()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 165, in _get_hook
    return SparkSubmitHook(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 155, in __init__
    raise RuntimeError(
RuntimeError: The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/usr/local/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH
[2023-01-20T19:15:55.398+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230120T191554, end_date=20230120T191555
[2023-01-20T19:15:55.457+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 129 for task transform_stage_generation (The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/usr/local/spark-3.3.1-bin-hadoop3-scala2.13/bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH; 361)
[2023-01-20T19:15:55.564+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-20T19:15:55.717+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T03:03:17.789+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T03:03:17.813+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T03:03:17.814+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T03:03:17.814+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-21T03:03:17.814+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T03:03:17.846+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-21T03:03:17.863+0000] {standard_task_runner.py:55} INFO - Started process 325 to run task
[2023-01-21T03:03:17.873+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '141', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmps497ehii']
[2023-01-21T03:03:17.879+0000] {standard_task_runner.py:83} INFO - Job 141: Subtask transform_stage_generation
[2023-01-21T03:03:18.039+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 2b3134e2b02d
[2023-01-21T03:03:18.204+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-21T03:03:18.206+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 156, in execute
    self._hook = self._get_hook()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 165, in _get_hook
    return SparkSubmitHook(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 155, in __init__
    raise RuntimeError(
RuntimeError: The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/usr/local/spark/bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH
[2023-01-21T03:03:18.225+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230121T030317, end_date=20230121T030318
[2023-01-21T03:03:18.249+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 141 for task transform_stage_generation (The spark-binary extra can be on of ['spark-submit', 'spark2-submit'] and it was `/usr/local/spark/bin/spark-submit`. Please make sure your spark binary is one of the allowed ones and that it is available on the PATH; 325)
[2023-01-21T03:03:18.285+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T03:03:18.327+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T07:47:28.847+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T07:47:28.862+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T07:47:28.863+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T07:47:28.863+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-21T07:47:28.863+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T07:47:28.884+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-21T07:47:28.894+0000] {standard_task_runner.py:55} INFO - Started process 72 to run task
[2023-01-21T07:47:28.900+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '163', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpbll33cg5']
[2023-01-21T07:47:28.906+0000] {standard_task_runner.py:83} INFO - Job 163: Subtask transform_stage_generation
[2023-01-21T07:47:28.998+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 4d52b49e46b4
[2023-01-21T07:47:29.096+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-21T07:47:29.098+0000] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-01-21T07:47:29.099+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master  --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-21T07:47:29.111+0000] {spark_submit.py:495} INFO - /usr/local/spark/bin/load-spark-env.sh: line 68: ps: command not found
[2023-01-21T07:47:32.688+0000] {spark_submit.py:495} INFO - Exception in thread "main" org.apache.spark.SparkException: Master must either be yarn or start with spark, mesos, k8s, or local
[2023-01-21T07:47:32.689+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.error(SparkSubmit.scala:975)
[2023-01-21T07:47:32.690+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:238)
[2023-01-21T07:47:32.690+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:901)
[2023-01-21T07:47:32.690+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
[2023-01-21T07:47:32.690+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
[2023-01-21T07:47:32.691+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
[2023-01-21T07:47:32.692+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)
[2023-01-21T07:47:32.692+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)
[2023-01-21T07:47:32.692+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
[2023-01-21T07:47:32.730+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master  --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-21T07:47:32.737+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230121T074728, end_date=20230121T074732
[2023-01-21T07:47:32.768+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 163 for task transform_stage_generation (Cannot execute: spark-submit --master  --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 72)
[2023-01-21T07:47:32.808+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T07:47:32.836+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T08:16:38.436+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T08:16:38.451+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T08:16:38.451+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T08:16:38.452+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-21T08:16:38.452+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T08:16:38.467+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-21T08:16:38.477+0000] {standard_task_runner.py:55} INFO - Started process 2286 to run task
[2023-01-21T08:16:38.481+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '172', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpjm05xbvl']
[2023-01-21T08:16:38.484+0000] {standard_task_runner.py:83} INFO - Job 172: Subtask transform_stage_generation
[2023-01-21T08:16:38.565+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 4d52b49e46b4
[2023-01-21T08:16:38.664+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-21T08:16:38.677+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-21T08:16:38.681+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-21T08:16:38.693+0000] {spark_submit.py:495} INFO - /usr/local/spark/bin/load-spark-env.sh: line 68: ps: command not found
[2023-01-21T08:16:41.953+0000] {spark_submit.py:495} INFO - 23/01/21 08:16:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-21T08:16:42.256+0000] {spark_submit.py:495} INFO - 23/01/21 08:16:42 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-21T08:16:42.358+0000] {spark_submit.py:495} INFO - Exception in thread "main" java.io.IOException: Cannot run program "/usr/bin/python3.10": error=2, No such file or directory
[2023-01-21T08:16:42.359+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1128)
[2023-01-21T08:16:42.359+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1071)
[2023-01-21T08:16:42.359+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.PythonRunner$.main(PythonRunner.scala:97)
[2023-01-21T08:16:42.359+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.PythonRunner.main(PythonRunner.scala)
[2023-01-21T08:16:42.360+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-21T08:16:42.360+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-21T08:16:42.360+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-21T08:16:42.360+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-21T08:16:42.360+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
[2023-01-21T08:16:42.360+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:958)
[2023-01-21T08:16:42.361+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
[2023-01-21T08:16:42.361+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
[2023-01-21T08:16:42.361+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
[2023-01-21T08:16:42.361+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)
[2023-01-21T08:16:42.361+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)
[2023-01-21T08:16:42.361+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
[2023-01-21T08:16:42.362+0000] {spark_submit.py:495} INFO - Caused by: java.io.IOException: error=2, No such file or directory
[2023-01-21T08:16:42.362+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessImpl.forkAndExec(Native Method)
[2023-01-21T08:16:42.362+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:340)
[2023-01-21T08:16:42.362+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessImpl.start(ProcessImpl.java:271)
[2023-01-21T08:16:42.362+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1107)
[2023-01-21T08:16:42.362+0000] {spark_submit.py:495} INFO - ... 15 more
[2023-01-21T08:16:42.367+0000] {spark_submit.py:495} INFO - 23/01/21 08:16:42 INFO ShutdownHookManager: Shutdown hook called
[2023-01-21T08:16:42.368+0000] {spark_submit.py:495} INFO - 23/01/21 08:16:42 INFO ShutdownHookManager: Deleting directory /tmp/spark-ce0fd7a3-270e-4354-b72c-7cdd21e52b06
[2023-01-21T08:16:42.445+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-21T08:16:42.450+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230121T081638, end_date=20230121T081642
[2023-01-21T08:16:42.464+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 172 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 2286)
[2023-01-21T08:16:42.500+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T08:16:42.520+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T08:23:55.624+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T08:23:55.632+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T08:23:55.632+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T08:23:55.632+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-21T08:23:55.632+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T08:23:55.642+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-21T08:23:55.649+0000] {standard_task_runner.py:55} INFO - Started process 284 to run task
[2023-01-21T08:23:55.653+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '180', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpn8hul7w4']
[2023-01-21T08:23:55.655+0000] {standard_task_runner.py:83} INFO - Job 180: Subtask transform_stage_generation
[2023-01-21T08:23:55.725+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host d00464a30031
[2023-01-21T08:23:55.802+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-21T08:23:55.812+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-21T08:23:55.814+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-21T08:23:55.823+0000] {spark_submit.py:495} INFO - /usr/local/spark/bin/load-spark-env.sh: line 68: ps: command not found
[2023-01-21T08:23:59.427+0000] {spark_submit.py:495} INFO - 23/01/21 08:23:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-21T08:23:59.755+0000] {spark_submit.py:495} INFO - 23/01/21 08:23:59 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-21T08:23:59.895+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-21T08:23:59.896+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 2, in <module>
[2023-01-21T08:23:59.896+0000] {spark_submit.py:495} INFO - from dotenv import load_dotenv
[2023-01-21T08:23:59.896+0000] {spark_submit.py:495} INFO - ModuleNotFoundError: No module named 'dotenv'
[2023-01-21T08:23:59.915+0000] {spark_submit.py:495} INFO - 23/01/21 08:23:59 INFO ShutdownHookManager: Shutdown hook called
[2023-01-21T08:23:59.918+0000] {spark_submit.py:495} INFO - 23/01/21 08:23:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-8e2e4a9e-6256-4343-b46f-678afc03c705
[2023-01-21T08:24:00.014+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-21T08:24:00.022+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230121T082355, end_date=20230121T082400
[2023-01-21T08:24:00.058+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 180 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 284)
[2023-01-21T08:24:00.092+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T08:24:00.129+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T10:18:15.165+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T10:18:15.191+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T10:18:15.192+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T10:18:15.192+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-21T10:18:15.192+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T10:18:15.224+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-21T10:18:15.240+0000] {standard_task_runner.py:55} INFO - Started process 2218 to run task
[2023-01-21T10:18:15.251+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '193', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp1ey56rip']
[2023-01-21T10:18:15.259+0000] {standard_task_runner.py:83} INFO - Job 193: Subtask transform_stage_generation
[2023-01-21T10:18:15.387+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host b3a2c4a96689
[2023-01-21T10:18:15.534+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-21T10:18:15.555+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-21T10:18:15.561+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-21T10:18:25.329+0000] {spark_submit.py:495} INFO - 23/01/21 10:18:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-21T10:18:26.239+0000] {spark_submit.py:495} INFO - 23/01/21 10:18:26 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-21T10:18:26.390+0000] {spark_submit.py:495} INFO - Exception in thread "main" java.io.IOException: Cannot run program "/usr/bin/python3.10": error=2, No such file or directory
[2023-01-21T10:18:26.391+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1128)
[2023-01-21T10:18:26.392+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1071)
[2023-01-21T10:18:26.392+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.PythonRunner$.main(PythonRunner.scala:97)
[2023-01-21T10:18:26.392+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.PythonRunner.main(PythonRunner.scala)
[2023-01-21T10:18:26.393+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-21T10:18:26.393+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-21T10:18:26.393+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-21T10:18:26.393+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-21T10:18:26.394+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
[2023-01-21T10:18:26.394+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:958)
[2023-01-21T10:18:26.394+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
[2023-01-21T10:18:26.394+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
[2023-01-21T10:18:26.395+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
[2023-01-21T10:18:26.395+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)
[2023-01-21T10:18:26.395+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)
[2023-01-21T10:18:26.395+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
[2023-01-21T10:18:26.396+0000] {spark_submit.py:495} INFO - Caused by: java.io.IOException: error=2, No such file or directory
[2023-01-21T10:18:26.396+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessImpl.forkAndExec(Native Method)
[2023-01-21T10:18:26.396+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:340)
[2023-01-21T10:18:26.396+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessImpl.start(ProcessImpl.java:271)
[2023-01-21T10:18:26.396+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1107)
[2023-01-21T10:18:26.397+0000] {spark_submit.py:495} INFO - ... 15 more
[2023-01-21T10:18:26.405+0000] {spark_submit.py:495} INFO - 23/01/21 10:18:26 INFO ShutdownHookManager: Shutdown hook called
[2023-01-21T10:18:26.408+0000] {spark_submit.py:495} INFO - 23/01/21 10:18:26 INFO ShutdownHookManager: Deleting directory /tmp/spark-ac0f44ff-0598-478f-8ed8-a3d0faf96a6d
[2023-01-21T10:18:26.534+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-21T10:18:26.542+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230121T101815, end_date=20230121T101826
[2023-01-21T10:18:26.571+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 193 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 2218)
[2023-01-21T10:18:26.636+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T10:18:26.681+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T10:45:46.034+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T10:45:46.095+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T10:45:46.096+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T10:45:46.096+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-21T10:45:46.096+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T10:45:46.165+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-21T10:45:46.194+0000] {standard_task_runner.py:55} INFO - Started process 156 to run task
[2023-01-21T10:45:46.218+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '204', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpj5bwfys3']
[2023-01-21T10:45:46.230+0000] {standard_task_runner.py:83} INFO - Job 204: Subtask transform_stage_generation
[2023-01-21T10:45:46.435+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host b3a2c4a96689
[2023-01-21T10:45:46.606+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-21T10:45:46.626+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-21T10:45:46.630+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-21T10:45:51.797+0000] {spark_submit.py:495} INFO - 23/01/21 10:45:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-21T10:45:52.344+0000] {spark_submit.py:495} INFO - 23/01/21 10:45:52 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-21T10:45:52.432+0000] {spark_submit.py:495} INFO - Exception in thread "main" java.io.IOException: Cannot run program "/usr/bin/python3.10": error=2, No such file or directory
[2023-01-21T10:45:52.433+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1128)
[2023-01-21T10:45:52.433+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1071)
[2023-01-21T10:45:52.433+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.PythonRunner$.main(PythonRunner.scala:97)
[2023-01-21T10:45:52.433+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.PythonRunner.main(PythonRunner.scala)
[2023-01-21T10:45:52.433+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-21T10:45:52.434+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-21T10:45:52.434+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-21T10:45:52.434+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-21T10:45:52.434+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
[2023-01-21T10:45:52.434+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:958)
[2023-01-21T10:45:52.434+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
[2023-01-21T10:45:52.434+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
[2023-01-21T10:45:52.435+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
[2023-01-21T10:45:52.435+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)
[2023-01-21T10:45:52.435+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)
[2023-01-21T10:45:52.435+0000] {spark_submit.py:495} INFO - at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
[2023-01-21T10:45:52.436+0000] {spark_submit.py:495} INFO - Caused by: java.io.IOException: error=2, No such file or directory
[2023-01-21T10:45:52.436+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessImpl.forkAndExec(Native Method)
[2023-01-21T10:45:52.436+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:340)
[2023-01-21T10:45:52.437+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessImpl.start(ProcessImpl.java:271)
[2023-01-21T10:45:52.437+0000] {spark_submit.py:495} INFO - at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1107)
[2023-01-21T10:45:52.437+0000] {spark_submit.py:495} INFO - ... 15 more
[2023-01-21T10:45:52.450+0000] {spark_submit.py:495} INFO - 23/01/21 10:45:52 INFO ShutdownHookManager: Shutdown hook called
[2023-01-21T10:45:52.452+0000] {spark_submit.py:495} INFO - 23/01/21 10:45:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-eb8c676b-7aa4-4365-b4ca-79b3b5b989ca
[2023-01-21T10:45:52.615+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-21T10:45:52.626+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230121T104546, end_date=20230121T104552
[2023-01-21T10:45:52.660+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 204 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 156)
[2023-01-21T10:45:52.699+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T10:45:52.767+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T11:29:48.663+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T11:29:48.679+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T11:29:48.679+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T11:29:48.680+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-21T11:29:48.680+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T11:29:48.695+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-21T11:29:48.704+0000] {standard_task_runner.py:55} INFO - Started process 1125 to run task
[2023-01-21T11:29:48.710+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '219', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpnfub2qvq']
[2023-01-21T11:29:48.715+0000] {standard_task_runner.py:83} INFO - Job 219: Subtask transform_stage_generation
[2023-01-21T11:29:48.806+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host b7c4de7fae0b
[2023-01-21T11:29:48.915+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-21T11:29:48.931+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-21T11:29:48.935+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-21T11:29:57.587+0000] {spark_submit.py:495} INFO - 23/01/21 11:29:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-21T11:29:58.339+0000] {spark_submit.py:495} INFO - 23/01/21 11:29:58 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-21T11:29:58.672+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-21T11:29:58.673+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 2, in <module>
[2023-01-21T11:29:58.674+0000] {spark_submit.py:495} INFO - from dotenv import load_dotenv, find_dotenv
[2023-01-21T11:29:58.675+0000] {spark_submit.py:495} INFO - ModuleNotFoundError: No module named 'dotenv'
[2023-01-21T11:29:58.715+0000] {spark_submit.py:495} INFO - 23/01/21 11:29:58 INFO ShutdownHookManager: Shutdown hook called
[2023-01-21T11:29:58.720+0000] {spark_submit.py:495} INFO - 23/01/21 11:29:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-c0ce408d-3076-49bc-b4c4-ce896e904aa4
[2023-01-21T11:29:59.026+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-21T11:29:59.039+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230121T112948, end_date=20230121T112959
[2023-01-21T11:29:59.082+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 219 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 1125)
[2023-01-21T11:29:59.120+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T11:29:59.201+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T11:31:20.913+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T11:31:20.932+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T11:31:20.932+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T11:31:20.933+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-21T11:31:20.933+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T11:31:20.953+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-21T11:31:20.964+0000] {standard_task_runner.py:55} INFO - Started process 1378 to run task
[2023-01-21T11:31:20.971+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '223', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpno4vb9yz']
[2023-01-21T11:31:20.974+0000] {standard_task_runner.py:83} INFO - Job 223: Subtask transform_stage_generation
[2023-01-21T11:31:21.099+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host b7c4de7fae0b
[2023-01-21T11:31:21.296+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-21T11:31:21.319+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-21T11:31:21.321+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-21T11:31:26.787+0000] {spark_submit.py:495} INFO - 23/01/21 11:31:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-21T11:31:27.126+0000] {spark_submit.py:495} INFO - 23/01/21 11:31:27 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-21T11:31:27.243+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 2
[2023-01-21T11:31:27.244+0000] {spark_submit.py:495} INFO - from python-dotenv import load_dotenv, find_dotenv
[2023-01-21T11:31:27.244+0000] {spark_submit.py:495} INFO - ^
[2023-01-21T11:31:27.244+0000] {spark_submit.py:495} INFO - SyntaxError: invalid syntax
[2023-01-21T11:31:27.256+0000] {spark_submit.py:495} INFO - 23/01/21 11:31:27 INFO ShutdownHookManager: Shutdown hook called
[2023-01-21T11:31:27.258+0000] {spark_submit.py:495} INFO - 23/01/21 11:31:27 INFO ShutdownHookManager: Deleting directory /tmp/spark-e424380a-653b-4be5-8e0b-7e29ff077c38
[2023-01-21T11:31:27.327+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-21T11:31:27.332+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230121T113120, end_date=20230121T113127
[2023-01-21T11:31:27.346+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 223 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 1378)
[2023-01-21T11:31:27.398+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T11:31:27.418+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T15:52:49.562+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T15:52:49.590+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T15:52:49.590+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T15:52:49.591+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-21T15:52:49.591+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T15:52:49.616+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-21T15:52:49.629+0000] {standard_task_runner.py:55} INFO - Started process 169 to run task
[2023-01-21T15:52:49.636+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '234', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp7mibqyvf']
[2023-01-21T15:52:49.639+0000] {standard_task_runner.py:83} INFO - Job 234: Subtask transform_stage_generation
[2023-01-21T15:52:49.725+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 673f7b70a136
[2023-01-21T15:52:49.825+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-21T15:52:49.839+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-21T15:52:49.841+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-21T15:52:56.212+0000] {spark_submit.py:495} INFO - 23/01/21 15:52:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-21T15:52:56.802+0000] {spark_submit.py:495} INFO - 23/01/21 15:52:56 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-21T15:52:56.984+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-21T15:52:56.984+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 2, in <module>
[2023-01-21T15:52:56.985+0000] {spark_submit.py:495} INFO - from dotenv import load_dotenv, find_dotenv
[2023-01-21T15:52:56.985+0000] {spark_submit.py:495} INFO - ModuleNotFoundError: No module named 'dotenv'
[2023-01-21T15:52:57.008+0000] {spark_submit.py:495} INFO - 23/01/21 15:52:57 INFO ShutdownHookManager: Shutdown hook called
[2023-01-21T15:52:57.010+0000] {spark_submit.py:495} INFO - 23/01/21 15:52:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-b75af86d-4324-40af-bfe3-ed74bd6e8692
[2023-01-21T15:52:57.105+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-21T15:52:57.111+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230121T155249, end_date=20230121T155257
[2023-01-21T15:52:57.129+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 234 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 169)
[2023-01-21T15:52:57.151+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T15:52:57.190+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T16:54:29.911+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T16:54:29.921+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T16:54:29.922+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T16:54:29.922+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-21T16:54:29.922+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T16:54:29.935+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-21T16:54:29.944+0000] {standard_task_runner.py:55} INFO - Started process 357 to run task
[2023-01-21T16:54:29.949+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '246', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpv8f86g3_']
[2023-01-21T16:54:29.953+0000] {standard_task_runner.py:83} INFO - Job 246: Subtask transform_stage_generation
[2023-01-21T16:54:30.041+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 4260804338b2
[2023-01-21T16:54:30.145+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-21T16:54:30.156+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-21T16:54:30.159+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-21T16:54:33.434+0000] {spark_submit.py:495} INFO - 23/01/21 16:54:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-21T16:54:33.800+0000] {spark_submit.py:495} INFO - 23/01/21 16:54:33 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-21T16:54:33.942+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-21T16:54:33.942+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 2, in <module>
[2023-01-21T16:54:33.943+0000] {spark_submit.py:495} INFO - from dotenv import load_dotenv, find_dotenv
[2023-01-21T16:54:33.943+0000] {spark_submit.py:495} INFO - ModuleNotFoundError: No module named 'dotenv'
[2023-01-21T16:54:33.984+0000] {spark_submit.py:495} INFO - 23/01/21 16:54:33 INFO ShutdownHookManager: Shutdown hook called
[2023-01-21T16:54:33.987+0000] {spark_submit.py:495} INFO - 23/01/21 16:54:33 INFO ShutdownHookManager: Deleting directory /tmp/spark-e663c61f-9a92-43e2-9694-6d403df9379c
[2023-01-21T16:54:34.076+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-21T16:54:34.087+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230121T165429, end_date=20230121T165434
[2023-01-21T16:54:34.110+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 246 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 357)
[2023-01-21T16:54:34.153+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T16:54:34.187+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-21T23:07:13.842+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T23:07:13.856+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-21T23:07:13.857+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T23:07:13.857+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-21T23:07:13.857+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-21T23:07:13.879+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-21T23:07:13.896+0000] {standard_task_runner.py:55} INFO - Started process 4272 to run task
[2023-01-21T23:07:13.903+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '256', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpodos5nw3']
[2023-01-21T23:07:13.909+0000] {standard_task_runner.py:83} INFO - Job 256: Subtask transform_stage_generation
[2023-01-21T23:07:14.008+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 4260804338b2
[2023-01-21T23:07:14.107+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-21T23:07:14.121+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-21T23:07:14.124+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-21T23:07:18.756+0000] {spark_submit.py:495} INFO - 23/01/21 23:07:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-21T23:07:19.234+0000] {spark_submit.py:495} INFO - 23/01/21 23:07:19 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-21T23:07:19.447+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-21T23:07:19.448+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 2, in <module>
[2023-01-21T23:07:19.448+0000] {spark_submit.py:495} INFO - from dotenv import load_dotenv, find_dotenv
[2023-01-21T23:07:19.448+0000] {spark_submit.py:495} INFO - ModuleNotFoundError: No module named 'dotenv'
[2023-01-21T23:07:19.464+0000] {spark_submit.py:495} INFO - 23/01/21 23:07:19 INFO ShutdownHookManager: Shutdown hook called
[2023-01-21T23:07:19.465+0000] {spark_submit.py:495} INFO - 23/01/21 23:07:19 INFO ShutdownHookManager: Deleting directory /tmp/spark-8a33cd29-e460-4c3e-ac17-21a38ada8033
[2023-01-21T23:07:19.536+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-21T23:07:19.542+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230121T230713, end_date=20230121T230719
[2023-01-21T23:07:19.561+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 256 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 4272)
[2023-01-21T23:07:19.596+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-21T23:07:19.624+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T00:11:33.601+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T00:11:33.622+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T00:11:33.623+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T00:11:33.623+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-22T00:11:33.623+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T00:11:33.648+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-22T00:11:33.659+0000] {standard_task_runner.py:55} INFO - Started process 383 to run task
[2023-01-22T00:11:33.665+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '269', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpgmwdt5et']
[2023-01-22T00:11:33.668+0000] {standard_task_runner.py:83} INFO - Job 269: Subtask transform_stage_generation
[2023-01-22T00:11:33.751+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 669043c9475c
[2023-01-22T00:11:33.833+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-22T00:11:33.843+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T00:11:33.845+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-22T00:11:37.373+0000] {spark_submit.py:495} INFO - 23/01/22 00:11:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T00:11:37.788+0000] {spark_submit.py:495} INFO - 23/01/22 00:11:37 WARN DependencyUtils: Local jar /opt/***/ /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-22T00:11:38.455+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T00:11:38.456+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 64, in <module>
[2023-01-22T00:11:38.456+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T00:11:38.456+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 16, in main
[2023-01-22T00:11:38.456+0000] {spark_submit.py:495} INFO - start_label = start.strftime("%Y%m%d%H%M")
[2023-01-22T00:11:38.456+0000] {spark_submit.py:495} INFO - AttributeError: 'str' object has no attribute 'strftime'
[2023-01-22T00:11:38.485+0000] {spark_submit.py:495} INFO - 23/01/22 00:11:38 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T00:11:38.486+0000] {spark_submit.py:495} INFO - 23/01/22 00:11:38 INFO ShutdownHookManager: Deleting directory /tmp/spark-0070f739-c2c4-4a88-ba2e-638a0f0a77c0
[2023-01-22T00:11:38.568+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-22T00:11:38.575+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230122T001133, end_date=20230122T001138
[2023-01-22T00:11:38.593+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 269 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /usr/local/spark/resources/gcs-connector-hadoop3-latest.jar, /usr/local/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 383)
[2023-01-22T00:11:38.656+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T00:11:38.682+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T03:52:53.317+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T03:52:53.331+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T03:52:53.331+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T03:52:53.332+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-22T03:52:53.332+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T03:52:53.350+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-22T03:52:53.361+0000] {standard_task_runner.py:55} INFO - Started process 861 to run task
[2023-01-22T03:52:53.368+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '281', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpc4csf0t_']
[2023-01-22T03:52:53.382+0000] {standard_task_runner.py:83} INFO - Job 281: Subtask transform_stage_generation
[2023-01-22T03:52:53.623+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host e3ae05c9a97e
[2023-01-22T03:52:53.799+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-22T03:52:53.831+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T03:52:53.833+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars /opt/***/spark/resources/gcs-connector-hadoop3-latest.jar, /opt/***/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-22T03:52:59.453+0000] {spark_submit.py:495} INFO - 23/01/22 03:52:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T03:52:59.894+0000] {spark_submit.py:495} INFO - 23/01/22 03:52:59 WARN DependencyUtils: Local jar /opt/***/ /opt/***/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-22T03:53:00.320+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T03:53:00.321+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 64, in <module>
[2023-01-22T03:53:00.321+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T03:53:00.321+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 16, in main
[2023-01-22T03:53:00.321+0000] {spark_submit.py:495} INFO - start_label = start.strftime("%Y%m%d%H%M")
[2023-01-22T03:53:00.321+0000] {spark_submit.py:495} INFO - AttributeError: 'str' object has no attribute 'strftime'
[2023-01-22T03:53:00.341+0000] {spark_submit.py:495} INFO - 23/01/22 03:53:00 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T03:53:00.341+0000] {spark_submit.py:495} INFO - 23/01/22 03:53:00 INFO ShutdownHookManager: Deleting directory /tmp/spark-26f35389-626b-429e-9d4e-25ba16a2bd19
[2023-01-22T03:53:00.401+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars /opt/***/spark/resources/gcs-connector-hadoop3-latest.jar, /opt/***/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-22T03:53:00.404+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230122T035253, end_date=20230122T035300
[2023-01-22T03:53:00.414+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 281 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars /opt/***/spark/resources/gcs-connector-hadoop3-latest.jar, /opt/***/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 861)
[2023-01-22T03:53:00.427+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T03:53:00.443+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T04:19:42.407+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T04:19:42.421+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T04:19:42.421+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T04:19:42.422+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-22T04:19:42.422+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T04:19:42.438+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-22T04:19:42.450+0000] {standard_task_runner.py:55} INFO - Started process 2965 to run task
[2023-01-22T04:19:42.457+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '290', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpv7aps0hr']
[2023-01-22T04:19:42.462+0000] {standard_task_runner.py:83} INFO - Job 290: Subtask transform_stage_generation
[2023-01-22T04:19:42.557+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host e3ae05c9a97e
[2023-01-22T04:19:42.658+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-22T04:19:42.673+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T04:19:42.674+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar, spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-22T04:19:48.916+0000] {spark_submit.py:495} INFO - 23/01/22 04:19:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T04:19:49.762+0000] {spark_submit.py:495} INFO - 23/01/22 04:19:49 WARN DependencyUtils: Local jar /opt/***/gcs-connector-hadoop3-latest.jar does not exist, skipping.
[2023-01-22T04:19:49.764+0000] {spark_submit.py:495} INFO - 23/01/22 04:19:49 WARN DependencyUtils: Local jar /opt/***/ spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-22T04:19:50.972+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T04:19:50.973+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 64, in <module>
[2023-01-22T04:19:50.973+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T04:19:50.973+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 16, in main
[2023-01-22T04:19:50.974+0000] {spark_submit.py:495} INFO - start_label = start.strftime("%Y%m%d%H%M")
[2023-01-22T04:19:50.974+0000] {spark_submit.py:495} INFO - AttributeError: 'str' object has no attribute 'strftime'
[2023-01-22T04:19:51.003+0000] {spark_submit.py:495} INFO - 23/01/22 04:19:51 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T04:19:51.005+0000] {spark_submit.py:495} INFO - 23/01/22 04:19:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-61fae46b-d5ab-4dfe-b9f0-fc94a2901fa8
[2023-01-22T04:19:51.123+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar, spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-22T04:19:51.131+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230122T041942, end_date=20230122T041951
[2023-01-22T04:19:51.158+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 290 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar, spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 2965)
[2023-01-22T04:19:51.206+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T04:19:51.259+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T06:08:46.518+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T06:08:46.573+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T06:08:46.574+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T06:08:46.574+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-22T06:08:46.575+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T06:08:46.630+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-22T06:08:46.657+0000] {standard_task_runner.py:55} INFO - Started process 242 to run task
[2023-01-22T06:08:46.676+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '304', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpo63wwj2x']
[2023-01-22T06:08:46.688+0000] {standard_task_runner.py:83} INFO - Job 304: Subtask transform_stage_generation
[2023-01-22T06:08:46.953+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host d68405340607
[2023-01-22T06:08:47.581+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-22T06:08:47.654+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T06:08:47.665+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-22T06:08:47.701+0000] {spark_submit.py:495} INFO - /home/***/.local/bin/spark-submit: line 27: /opt/spark/bin/spark-class: No such file or directory
[2023-01-22T06:08:47.758+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 127.
[2023-01-22T06:08:47.778+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230122T060846, end_date=20230122T060847
[2023-01-22T06:08:47.836+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 304 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 127.; 242)
[2023-01-22T06:08:47.903+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T06:08:47.978+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T07:45:01.962+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T07:45:01.974+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T07:45:01.975+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T07:45:01.975+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-22T07:45:01.975+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T07:45:01.988+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-22T07:45:01.996+0000] {standard_task_runner.py:55} INFO - Started process 450 to run task
[2023-01-22T07:45:02.001+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '316', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp7c2ztny6']
[2023-01-22T07:45:02.004+0000] {standard_task_runner.py:83} INFO - Job 316: Subtask transform_stage_generation
[2023-01-22T07:45:02.090+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 16233a798013
[2023-01-22T07:45:02.192+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-22T07:45:02.220+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T07:45:02.222+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-22T07:45:04.942+0000] {spark_submit.py:495} INFO - 23/01/22 07:45:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T07:45:05.191+0000] {spark_submit.py:495} INFO - 23/01/22 07:45:05 WARN DependencyUtils: Local jar /opt/***/gcs-connector-hadoop3-latest.jar does not exist, skipping.
[2023-01-22T07:45:05.192+0000] {spark_submit.py:495} INFO - 23/01/22 07:45:05 WARN DependencyUtils: Local jar /opt/***/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-22T07:45:05.554+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T07:45:05.554+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 64, in <module>
[2023-01-22T07:45:05.554+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T07:45:05.554+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 16, in main
[2023-01-22T07:45:05.555+0000] {spark_submit.py:495} INFO - start_label = start.strftime("%Y%m%d%H%M")
[2023-01-22T07:45:05.555+0000] {spark_submit.py:495} INFO - AttributeError: 'str' object has no attribute 'strftime'
[2023-01-22T07:45:05.571+0000] {spark_submit.py:495} INFO - 23/01/22 07:45:05 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T07:45:05.572+0000] {spark_submit.py:495} INFO - 23/01/22 07:45:05 INFO ShutdownHookManager: Deleting directory /tmp/spark-e6e05eb9-c4eb-4e67-8707-d33f2524aaad
[2023-01-22T07:45:05.628+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-22T07:45:05.632+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230122T074501, end_date=20230122T074505
[2023-01-22T07:45:05.644+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 316 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 450)
[2023-01-22T07:45:05.676+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T07:45:05.695+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T07:48:18.644+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T07:48:18.656+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T07:48:18.656+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T07:48:18.656+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-22T07:48:18.657+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T07:48:18.673+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-22T07:48:18.685+0000] {standard_task_runner.py:55} INFO - Started process 761 to run task
[2023-01-22T07:48:18.689+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '320', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpoi7yutsd']
[2023-01-22T07:48:18.693+0000] {standard_task_runner.py:83} INFO - Job 320: Subtask transform_stage_generation
[2023-01-22T07:48:18.762+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 16233a798013
[2023-01-22T07:48:18.831+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-22T07:48:18.840+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T07:48:18.841+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-22T07:48:22.178+0000] {spark_submit.py:495} INFO - 23/01/22 07:48:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T07:48:22.441+0000] {spark_submit.py:495} INFO - 23/01/22 07:48:22 WARN DependencyUtils: Local jar /opt/***/gcs-connector-hadoop3-latest.jar does not exist, skipping.
[2023-01-22T07:48:22.442+0000] {spark_submit.py:495} INFO - 23/01/22 07:48:22 WARN DependencyUtils: Local jar /opt/***/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-22T07:48:23.003+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T07:48:23.003+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 64, in <module>
[2023-01-22T07:48:23.003+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T07:48:23.004+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 28, in main
[2023-01-22T07:48:23.004+0000] {spark_submit.py:495} INFO - SparkSession.builder.appName("gcp_playground")
[2023-01-22T07:48:23.004+0000] {spark_submit.py:495} INFO - NameError: name 'SparkSession' is not defined
[2023-01-22T07:48:23.034+0000] {spark_submit.py:495} INFO - 23/01/22 07:48:23 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T07:48:23.036+0000] {spark_submit.py:495} INFO - 23/01/22 07:48:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-7f6f16ce-735f-4fa7-86d4-4a7fec82652d
[2023-01-22T07:48:23.123+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-22T07:48:23.128+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230122T074818, end_date=20230122T074823
[2023-01-22T07:48:23.146+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 320 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 761)
[2023-01-22T07:48:23.205+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T07:48:23.230+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T07:49:50.265+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T07:49:50.280+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T07:49:50.280+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T07:49:50.280+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-22T07:49:50.281+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T07:49:50.296+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-22T07:49:50.308+0000] {standard_task_runner.py:55} INFO - Started process 1025 to run task
[2023-01-22T07:49:50.314+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '325', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmprg310p_g']
[2023-01-22T07:49:50.318+0000] {standard_task_runner.py:83} INFO - Job 325: Subtask transform_stage_generation
[2023-01-22T07:49:50.403+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 16233a798013
[2023-01-22T07:49:50.499+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-22T07:49:50.509+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T07:49:50.510+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-22T07:49:53.181+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T07:49:53.568+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:53 WARN DependencyUtils: Local jar /opt/***/gcs-connector-hadoop3-latest.jar does not exist, skipping.
[2023-01-22T07:49:53.569+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:53 WARN DependencyUtils: Local jar /opt/***/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-22T07:49:55.369+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:55 INFO SparkContext: Running Spark version 3.3.1
[2023-01-22T07:49:55.399+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:55 INFO ResourceUtils: ==============================================================
[2023-01-22T07:49:55.399+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:55 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-22T07:49:55.400+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:55 INFO ResourceUtils: ==============================================================
[2023-01-22T07:49:55.401+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:55 INFO SparkContext: Submitted application: gcp_playground
[2023-01-22T07:49:55.432+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:55 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-22T07:49:55.438+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:55 INFO ResourceProfile: Limiting resource is cpu
[2023-01-22T07:49:55.440+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:55 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-22T07:49:55.518+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:55 INFO SecurityManager: Changing view acls to: ***
[2023-01-22T07:49:55.519+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:55 INFO SecurityManager: Changing modify acls to: ***
[2023-01-22T07:49:55.520+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:55 INFO SecurityManager: Changing view acls groups to:
[2023-01-22T07:49:55.521+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:55 INFO SecurityManager: Changing modify acls groups to:
[2023-01-22T07:49:55.522+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:55 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-22T07:49:55.896+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:55 INFO Utils: Successfully started service 'sparkDriver' on port 32865.
[2023-01-22T07:49:55.945+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:55 INFO SparkEnv: Registering MapOutputTracker
[2023-01-22T07:49:56.004+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:56 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-22T07:49:56.055+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:56 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-22T07:49:56.056+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:56 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-22T07:49:56.062+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:56 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-22T07:49:56.099+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:56 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-40a7e933-cd8e-4e5f-8309-9cb2f16f1a09
[2023-01-22T07:49:56.122+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:56 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-22T07:49:56.143+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:56 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-22T07:49:56.643+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:56 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-01-22T07:49:56.742+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:56 INFO SparkContext: Added JAR /opt/spark/resources/gcs-connector-hadoop3-latest.jar at spark://16233a798013:32865/jars/gcs-connector-hadoop3-latest.jar with timestamp 1674373795363
[2023-01-22T07:49:56.742+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:56 INFO SparkContext: Added JAR /opt/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://16233a798013:32865/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1674373795363
[2023-01-22T07:49:56.927+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:56 INFO Executor: Starting executor ID driver on host 16233a798013
[2023-01-22T07:49:56.946+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:56 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-22T07:49:56.984+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:56 INFO Executor: Fetching spark://16233a798013:32865/jars/gcs-connector-hadoop3-latest.jar with timestamp 1674373795363
[2023-01-22T07:49:57.156+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:57 INFO TransportClientFactory: Successfully created connection to 16233a798013/192.168.32.7:32865 after 94 ms (0 ms spent in bootstraps)
[2023-01-22T07:49:57.181+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:57 INFO Utils: Fetching spark://16233a798013:32865/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-bc3934f5-7c91-4411-9d45-e3c72c49846c/userFiles-97140927-74ec-482f-943e-d7451fb7a6f7/fetchFileTemp17119910698754061970.tmp
[2023-01-22T07:49:57.710+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:57 INFO Executor: Adding file:/tmp/spark-bc3934f5-7c91-4411-9d45-e3c72c49846c/userFiles-97140927-74ec-482f-943e-d7451fb7a6f7/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-22T07:49:57.710+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:57 INFO Executor: Fetching spark://16233a798013:32865/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1674373795363
[2023-01-22T07:49:57.712+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:57 INFO Utils: Fetching spark://16233a798013:32865/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-bc3934f5-7c91-4411-9d45-e3c72c49846c/userFiles-97140927-74ec-482f-943e-d7451fb7a6f7/fetchFileTemp10683315057589947344.tmp
[2023-01-22T07:49:58.225+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:58 INFO Executor: Adding file:/tmp/spark-bc3934f5-7c91-4411-9d45-e3c72c49846c/userFiles-97140927-74ec-482f-943e-d7451fb7a6f7/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-22T07:49:58.274+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:58 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46529.
[2023-01-22T07:49:58.275+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:58 INFO NettyBlockTransferService: Server created on 16233a798013:46529
[2023-01-22T07:49:58.283+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:58 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-22T07:49:58.311+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:58 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 16233a798013, 46529, None)
[2023-01-22T07:49:58.321+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:58 INFO BlockManagerMasterEndpoint: Registering block manager 16233a798013:46529 with 434.4 MiB RAM, BlockManagerId(driver, 16233a798013, 46529, None)
[2023-01-22T07:49:58.329+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:58 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 16233a798013, 46529, None)
[2023-01-22T07:49:58.331+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:58 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 16233a798013, 46529, None)
[2023-01-22T07:49:59.183+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:59 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-22T07:49:59.187+0000] {spark_submit.py:495} INFO - 23/01/22 07:49:59 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-22T07:50:01.809+0000] {spark_submit.py:495} INFO - 23/01/22 07:50:01 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: gs://entsoe_analytics_1009//opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010100.json.
[2023-01-22T07:50:01.809+0000] {spark_submit.py:495} INFO - org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "gs"
[2023-01-22T07:50:01.810+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2023-01-22T07:50:01.810+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T07:50:01.810+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T07:50:01.811+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T07:50:01.811+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T07:50:01.811+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T07:50:01.811+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T07:50:01.812+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)
[2023-01-22T07:50:01.812+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:370)
[2023-01-22T07:50:01.812+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T07:50:01.812+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T07:50:01.813+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T07:50:01.813+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T07:50:01.813+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T07:50:01.813+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T07:50:01.814+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T07:50:01.814+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T07:50:01.814+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T07:50:01.815+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T07:50:01.815+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T07:50:01.815+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T07:50:01.815+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T07:50:01.816+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T07:50:01.816+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T07:50:01.816+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T07:50:01.817+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T07:50:01.861+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T07:50:01.862+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 67, in <module>
[2023-01-22T07:50:01.866+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T07:50:01.867+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 54, in main
[2023-01-22T07:50:01.867+0000] {spark_submit.py:495} INFO - df_spark.write.format("bigquery").option("project", "rafzul-analytics-1009") \
[2023-01-22T07:50:01.867+0000] {spark_submit.py:495} INFO - UnboundLocalError: local variable 'df_spark' referenced before assignment
[2023-01-22T07:50:01.959+0000] {spark_submit.py:495} INFO - 23/01/22 07:50:01 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-22T07:50:01.977+0000] {spark_submit.py:495} INFO - 23/01/22 07:50:01 INFO SparkUI: Stopped Spark web UI at http://16233a798013:4040
[2023-01-22T07:50:01.997+0000] {spark_submit.py:495} INFO - 23/01/22 07:50:01 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-22T07:50:02.017+0000] {spark_submit.py:495} INFO - 23/01/22 07:50:02 INFO MemoryStore: MemoryStore cleared
[2023-01-22T07:50:02.018+0000] {spark_submit.py:495} INFO - 23/01/22 07:50:02 INFO BlockManager: BlockManager stopped
[2023-01-22T07:50:02.034+0000] {spark_submit.py:495} INFO - 23/01/22 07:50:02 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-22T07:50:02.040+0000] {spark_submit.py:495} INFO - 23/01/22 07:50:02 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-22T07:50:02.076+0000] {spark_submit.py:495} INFO - 23/01/22 07:50:02 INFO SparkContext: Successfully stopped SparkContext
[2023-01-22T07:50:02.077+0000] {spark_submit.py:495} INFO - 23/01/22 07:50:02 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T07:50:02.078+0000] {spark_submit.py:495} INFO - 23/01/22 07:50:02 INFO ShutdownHookManager: Deleting directory /tmp/spark-82b9d49a-0514-4886-af64-f7b79b35e4d9
[2023-01-22T07:50:02.086+0000] {spark_submit.py:495} INFO - 23/01/22 07:50:02 INFO ShutdownHookManager: Deleting directory /tmp/spark-bc3934f5-7c91-4411-9d45-e3c72c49846c/pyspark-5fc69fcf-3bd4-4660-9e0b-f62f957ac2f1
[2023-01-22T07:50:02.094+0000] {spark_submit.py:495} INFO - 23/01/22 07:50:02 INFO ShutdownHookManager: Deleting directory /tmp/spark-bc3934f5-7c91-4411-9d45-e3c72c49846c
[2023-01-22T07:50:02.215+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-22T07:50:02.222+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230122T074950, end_date=20230122T075002
[2023-01-22T07:50:02.258+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 325 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 1025)
[2023-01-22T07:50:02.297+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T07:50:02.330+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T07:58:21.546+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T07:58:21.564+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T07:58:21.565+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T07:58:21.565+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-22T07:58:21.565+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T07:58:21.586+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-22T07:58:21.598+0000] {standard_task_runner.py:55} INFO - Started process 2016 to run task
[2023-01-22T07:58:21.607+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '330', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpltk2dxyf']
[2023-01-22T07:58:21.612+0000] {standard_task_runner.py:83} INFO - Job 330: Subtask transform_stage_generation
[2023-01-22T07:58:21.753+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 16233a798013
[2023-01-22T07:58:21.904+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-22T07:58:21.924+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T07:58:21.929+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-22T07:58:29.042+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T07:58:29.529+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:29 WARN DependencyUtils: Local jar /opt/***/gcs-connector-hadoop3-latest.jar does not exist, skipping.
[2023-01-22T07:58:29.530+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:29 WARN DependencyUtils: Local jar /opt/***/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-22T07:58:32.863+0000] {spark_submit.py:495} INFO - /opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010100.json
[2023-01-22T07:58:33.148+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:33 INFO SparkContext: Running Spark version 3.3.1
[2023-01-22T07:58:33.205+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:33 INFO ResourceUtils: ==============================================================
[2023-01-22T07:58:33.206+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:33 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-22T07:58:33.206+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:33 INFO ResourceUtils: ==============================================================
[2023-01-22T07:58:33.207+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:33 INFO SparkContext: Submitted application: gcp_playground
[2023-01-22T07:58:33.262+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:33 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-22T07:58:33.275+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:33 INFO ResourceProfile: Limiting resource is cpu
[2023-01-22T07:58:33.277+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:33 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-22T07:58:33.404+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:33 INFO SecurityManager: Changing view acls to: ***
[2023-01-22T07:58:33.405+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:33 INFO SecurityManager: Changing modify acls to: ***
[2023-01-22T07:58:33.406+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:33 INFO SecurityManager: Changing view acls groups to:
[2023-01-22T07:58:33.407+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:33 INFO SecurityManager: Changing modify acls groups to:
[2023-01-22T07:58:33.408+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:33 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-22T07:58:34.150+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:34 INFO Utils: Successfully started service 'sparkDriver' on port 34383.
[2023-01-22T07:58:34.282+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:34 INFO SparkEnv: Registering MapOutputTracker
[2023-01-22T07:58:34.453+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:34 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-22T07:58:34.556+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:34 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-22T07:58:34.557+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:34 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-22T07:58:34.567+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:34 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-22T07:58:34.636+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:34 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-2b26c3f0-dddc-4008-ad09-a2a3a1fd61ae
[2023-01-22T07:58:34.703+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:34 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-22T07:58:34.781+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:34 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-22T07:58:35.596+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:35 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-22T07:58:35.657+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:35 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2023-01-22T07:58:35.807+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:35 INFO SparkContext: Added JAR /opt/spark/resources/gcs-connector-hadoop3-latest.jar at spark://16233a798013:34383/jars/gcs-connector-hadoop3-latest.jar with timestamp 1674374313137
[2023-01-22T07:58:35.808+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:35 INFO SparkContext: Added JAR /opt/spark/resources/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://16233a798013:34383/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1674374313137
[2023-01-22T07:58:36.135+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:36 INFO Executor: Starting executor ID driver on host 16233a798013
[2023-01-22T07:58:36.158+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:36 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-22T07:58:36.210+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:36 INFO Executor: Fetching spark://16233a798013:34383/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1674374313137
[2023-01-22T07:58:36.354+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:36 INFO TransportClientFactory: Successfully created connection to 16233a798013/192.168.32.7:34383 after 74 ms (0 ms spent in bootstraps)
[2023-01-22T07:58:36.374+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:36 INFO Utils: Fetching spark://16233a798013:34383/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-30bdaed2-2f51-4ddf-94b4-f578f3ad695f/userFiles-2da060fb-9379-46f4-b043-bbdc19232cd0/fetchFileTemp5218386812464031243.tmp
[2023-01-22T07:58:36.802+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:36 INFO Executor: Adding file:/tmp/spark-30bdaed2-2f51-4ddf-94b4-f578f3ad695f/userFiles-2da060fb-9379-46f4-b043-bbdc19232cd0/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-22T07:58:36.804+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:36 INFO Executor: Fetching spark://16233a798013:34383/jars/gcs-connector-hadoop3-latest.jar with timestamp 1674374313137
[2023-01-22T07:58:36.809+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:36 INFO Utils: Fetching spark://16233a798013:34383/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-30bdaed2-2f51-4ddf-94b4-f578f3ad695f/userFiles-2da060fb-9379-46f4-b043-bbdc19232cd0/fetchFileTemp13886735255540351745.tmp
[2023-01-22T07:58:37.095+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:37 INFO Executor: Adding file:/tmp/spark-30bdaed2-2f51-4ddf-94b4-f578f3ad695f/userFiles-2da060fb-9379-46f4-b043-bbdc19232cd0/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-22T07:58:37.104+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:37 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41867.
[2023-01-22T07:58:37.104+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:37 INFO NettyBlockTransferService: Server created on 16233a798013:41867
[2023-01-22T07:58:37.106+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:37 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-22T07:58:37.115+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:37 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 16233a798013, 41867, None)
[2023-01-22T07:58:37.119+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:37 INFO BlockManagerMasterEndpoint: Registering block manager 16233a798013:41867 with 434.4 MiB RAM, BlockManagerId(driver, 16233a798013, 41867, None)
[2023-01-22T07:58:37.124+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:37 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 16233a798013, 41867, None)
[2023-01-22T07:58:37.125+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:37 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 16233a798013, 41867, None)
[2023-01-22T07:58:37.757+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:37 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-22T07:58:37.761+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:37 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-22T07:58:38.870+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:38 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: gs://entsoe_analytics_1009//opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010100.json.
[2023-01-22T07:58:38.871+0000] {spark_submit.py:495} INFO - org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "gs"
[2023-01-22T07:58:38.871+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2023-01-22T07:58:38.871+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T07:58:38.871+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T07:58:38.871+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T07:58:38.871+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T07:58:38.871+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T07:58:38.871+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T07:58:38.872+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)
[2023-01-22T07:58:38.872+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:370)
[2023-01-22T07:58:38.872+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T07:58:38.872+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T07:58:38.872+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T07:58:38.872+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T07:58:38.872+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T07:58:38.873+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T07:58:38.873+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T07:58:38.873+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T07:58:38.873+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T07:58:38.873+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T07:58:38.873+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T07:58:38.873+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T07:58:38.873+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T07:58:38.874+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T07:58:38.874+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T07:58:38.874+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T07:58:38.874+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T07:58:38.894+0000] {spark_submit.py:495} INFO - Error, ada An error occurred while calling o38.load.
[2023-01-22T07:58:38.895+0000] {spark_submit.py:495} INFO - : org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "gs"
[2023-01-22T07:58:38.895+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2023-01-22T07:58:38.895+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T07:58:38.895+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T07:58:38.895+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T07:58:38.896+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T07:58:38.896+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T07:58:38.896+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T07:58:38.896+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:752)
[2023-01-22T07:58:38.896+0000] {spark_submit.py:495} INFO - at scala.collection.immutable.List.map(List.scala:246)
[2023-01-22T07:58:38.896+0000] {spark_submit.py:495} INFO - at scala.collection.immutable.List.map(List.scala:79)
[2023-01-22T07:58:38.896+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:750)
[2023-01-22T07:58:38.897+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:579)
[2023-01-22T07:58:38.897+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:408)
[2023-01-22T07:58:38.897+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T07:58:38.897+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T07:58:38.897+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T07:58:38.897+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T07:58:38.897+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T07:58:38.898+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T07:58:38.898+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T07:58:38.898+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T07:58:38.898+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T07:58:38.898+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T07:58:38.898+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T07:58:38.899+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T07:58:38.899+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T07:58:38.899+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T07:58:38.899+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T07:58:38.899+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T07:58:38.899+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T07:58:38.899+0000] {spark_submit.py:495} INFO - 
[2023-01-22T07:58:38.900+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T07:58:38.900+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 69, in <module>
[2023-01-22T07:58:38.900+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T07:58:38.900+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 56, in main
[2023-01-22T07:58:38.900+0000] {spark_submit.py:495} INFO - df_spark.write.format("bigquery").option("project", "rafzul-analytics-1009") \
[2023-01-22T07:58:38.900+0000] {spark_submit.py:495} INFO - UnboundLocalError: local variable 'df_spark' referenced before assignment
[2023-01-22T07:58:38.973+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:38 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-22T07:58:38.985+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:38 INFO SparkUI: Stopped Spark web UI at http://16233a798013:4041
[2023-01-22T07:58:39.002+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:39 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-22T07:58:39.019+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:39 INFO MemoryStore: MemoryStore cleared
[2023-01-22T07:58:39.019+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:39 INFO BlockManager: BlockManager stopped
[2023-01-22T07:58:39.029+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:39 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-22T07:58:39.033+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:39 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-22T07:58:39.062+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:39 INFO SparkContext: Successfully stopped SparkContext
[2023-01-22T07:58:39.062+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:39 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T07:58:39.063+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-30bdaed2-2f51-4ddf-94b4-f578f3ad695f
[2023-01-22T07:58:39.076+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-30bdaed2-2f51-4ddf-94b4-f578f3ad695f/pyspark-4330ca6b-852a-45ac-b413-780b3059451b
[2023-01-22T07:58:39.082+0000] {spark_submit.py:495} INFO - 23/01/22 07:58:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-f60bee33-67b7-44bf-a8d3-5200c60b3f12
[2023-01-22T07:58:39.154+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-22T07:58:39.159+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230122T075821, end_date=20230122T075839
[2023-01-22T07:58:39.171+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 330 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 2016)
[2023-01-22T07:58:39.221+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T07:58:39.236+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T08:38:41.403+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T08:38:41.420+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T08:38:41.420+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T08:38:41.421+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-22T08:38:41.421+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T08:38:41.433+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-22T08:38:41.440+0000] {standard_task_runner.py:55} INFO - Started process 359 to run task
[2023-01-22T08:38:41.445+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '373', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp78l3ba9q']
[2023-01-22T08:38:41.448+0000] {standard_task_runner.py:83} INFO - Job 373: Subtask transform_stage_generation
[2023-01-22T08:38:41.513+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 9d8db2d97423
[2023-01-22T08:38:41.590+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-22T08:38:41.600+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T08:38:41.601+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-22T08:38:45.754+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T08:38:46.063+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:46 WARN DependencyUtils: Local jar /opt/***/gcs-connector-hadoop3-latest.jar does not exist, skipping.
[2023-01-22T08:38:46.063+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:46 WARN DependencyUtils: Local jar /opt/***/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-22T08:38:47.296+0000] {spark_submit.py:495} INFO - /opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010100.json
[2023-01-22T08:38:47.392+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:47 INFO SparkContext: Running Spark version 3.3.1
[2023-01-22T08:38:47.429+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:47 INFO ResourceUtils: ==============================================================
[2023-01-22T08:38:47.430+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:47 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-22T08:38:47.431+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:47 INFO ResourceUtils: ==============================================================
[2023-01-22T08:38:47.432+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:47 INFO SparkContext: Submitted application: gcp_playground
[2023-01-22T08:38:47.467+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:47 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-22T08:38:47.475+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:47 INFO ResourceProfile: Limiting resource is cpu
[2023-01-22T08:38:47.476+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:47 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-22T08:38:47.563+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:47 INFO SecurityManager: Changing view acls to: ***
[2023-01-22T08:38:47.564+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:47 INFO SecurityManager: Changing modify acls to: ***
[2023-01-22T08:38:47.564+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:47 INFO SecurityManager: Changing view acls groups to:
[2023-01-22T08:38:47.565+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:47 INFO SecurityManager: Changing modify acls groups to:
[2023-01-22T08:38:47.565+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:47 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-22T08:38:47.968+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:47 INFO Utils: Successfully started service 'sparkDriver' on port 37125.
[2023-01-22T08:38:48.022+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO SparkEnv: Registering MapOutputTracker
[2023-01-22T08:38:48.081+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-22T08:38:48.164+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-22T08:38:48.165+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-22T08:38:48.171+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-22T08:38:48.203+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ac15dfe1-7fef-4917-be0e-6d0f11272ee4
[2023-01-22T08:38:48.226+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-22T08:38:48.247+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-22T08:38:48.602+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-01-22T08:38:48.690+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 ERROR SparkContext: Failed to add file:/opt/***/gcs-connector-hadoop3-latest.jar to Spark environment
[2023-01-22T08:38:48.690+0000] {spark_submit.py:495} INFO - java.io.FileNotFoundException: Jar /opt/***/gcs-connector-hadoop3-latest.jar not found
[2023-01-22T08:38:48.690+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1949)
[2023-01-22T08:38:48.691+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.addJar(SparkContext.scala:2004)
[2023-01-22T08:38:48.691+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:507)
[2023-01-22T08:38:48.691+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:507)
[2023-01-22T08:38:48.691+0000] {spark_submit.py:495} INFO - at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
[2023-01-22T08:38:48.691+0000] {spark_submit.py:495} INFO - at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
[2023-01-22T08:38:48.691+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
[2023-01-22T08:38:48.691+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.<init>(SparkContext.scala:507)
[2023-01-22T08:38:48.691+0000] {spark_submit.py:495} INFO - at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
[2023-01-22T08:38:48.691+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
[2023-01-22T08:38:48.691+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
[2023-01-22T08:38:48.692+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
[2023-01-22T08:38:48.692+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
[2023-01-22T08:38:48.692+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
[2023-01-22T08:38:48.692+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T08:38:48.692+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:238)
[2023-01-22T08:38:48.692+0000] {spark_submit.py:495} INFO - at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
[2023-01-22T08:38:48.692+0000] {spark_submit.py:495} INFO - at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
[2023-01-22T08:38:48.692+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T08:38:48.692+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T08:38:48.692+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T08:38:48.692+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 ERROR SparkContext: Failed to add file:/opt/***/spark-bigquery-with-dependencies_2.13-0.27.1.jar to Spark environment
[2023-01-22T08:38:48.693+0000] {spark_submit.py:495} INFO - java.io.FileNotFoundException: Jar /opt/***/spark-bigquery-with-dependencies_2.13-0.27.1.jar not found
[2023-01-22T08:38:48.693+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1949)
[2023-01-22T08:38:48.693+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.addJar(SparkContext.scala:2004)
[2023-01-22T08:38:48.693+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:507)
[2023-01-22T08:38:48.693+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:507)
[2023-01-22T08:38:48.693+0000] {spark_submit.py:495} INFO - at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
[2023-01-22T08:38:48.693+0000] {spark_submit.py:495} INFO - at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
[2023-01-22T08:38:48.693+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
[2023-01-22T08:38:48.694+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.<init>(SparkContext.scala:507)
[2023-01-22T08:38:48.694+0000] {spark_submit.py:495} INFO - at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
[2023-01-22T08:38:48.694+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
[2023-01-22T08:38:48.694+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
[2023-01-22T08:38:48.694+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
[2023-01-22T08:38:48.694+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
[2023-01-22T08:38:48.694+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
[2023-01-22T08:38:48.695+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T08:38:48.695+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:238)
[2023-01-22T08:38:48.695+0000] {spark_submit.py:495} INFO - at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
[2023-01-22T08:38:48.695+0000] {spark_submit.py:495} INFO - at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
[2023-01-22T08:38:48.695+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T08:38:48.695+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T08:38:48.696+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T08:38:48.813+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO Executor: Starting executor ID driver on host 9d8db2d97423
[2023-01-22T08:38:48.825+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-22T08:38:48.860+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45679.
[2023-01-22T08:38:48.860+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO NettyBlockTransferService: Server created on 9d8db2d97423:45679
[2023-01-22T08:38:48.863+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-22T08:38:48.877+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 9d8db2d97423, 45679, None)
[2023-01-22T08:38:48.883+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO BlockManagerMasterEndpoint: Registering block manager 9d8db2d97423:45679 with 434.4 MiB RAM, BlockManagerId(driver, 9d8db2d97423, 45679, None)
[2023-01-22T08:38:48.890+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 9d8db2d97423, 45679, None)
[2023-01-22T08:38:48.892+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:48 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 9d8db2d97423, 45679, None)
[2023-01-22T08:38:49.698+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:49 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-22T08:38:49.707+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:49 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-22T08:38:51.343+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:51 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: gs://entsoe_analytics_1009//opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010100.json.
[2023-01-22T08:38:51.343+0000] {spark_submit.py:495} INFO - org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "gs"
[2023-01-22T08:38:51.343+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2023-01-22T08:38:51.344+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T08:38:51.344+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T08:38:51.344+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T08:38:51.344+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T08:38:51.344+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T08:38:51.344+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T08:38:51.345+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)
[2023-01-22T08:38:51.345+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:370)
[2023-01-22T08:38:51.345+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T08:38:51.345+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T08:38:51.345+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T08:38:51.346+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T08:38:51.346+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T08:38:51.346+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T08:38:51.346+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T08:38:51.346+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T08:38:51.347+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T08:38:51.347+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T08:38:51.347+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T08:38:51.347+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T08:38:51.347+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T08:38:51.348+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T08:38:51.348+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T08:38:51.348+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T08:38:51.348+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T08:38:51.385+0000] {spark_submit.py:495} INFO - Error, ada An error occurred while calling o37.load.
[2023-01-22T08:38:51.386+0000] {spark_submit.py:495} INFO - : org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "gs"
[2023-01-22T08:38:51.386+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2023-01-22T08:38:51.386+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T08:38:51.386+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T08:38:51.387+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T08:38:51.387+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T08:38:51.387+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T08:38:51.387+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T08:38:51.388+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:752)
[2023-01-22T08:38:51.388+0000] {spark_submit.py:495} INFO - at scala.collection.immutable.List.map(List.scala:246)
[2023-01-22T08:38:51.388+0000] {spark_submit.py:495} INFO - at scala.collection.immutable.List.map(List.scala:79)
[2023-01-22T08:38:51.388+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:750)
[2023-01-22T08:38:51.388+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:579)
[2023-01-22T08:38:51.388+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:408)
[2023-01-22T08:38:51.389+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T08:38:51.389+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T08:38:51.389+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T08:38:51.389+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T08:38:51.389+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T08:38:51.389+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T08:38:51.390+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T08:38:51.390+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T08:38:51.390+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T08:38:51.390+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T08:38:51.390+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T08:38:51.391+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T08:38:51.391+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T08:38:51.391+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T08:38:51.391+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T08:38:51.391+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T08:38:51.391+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T08:38:51.392+0000] {spark_submit.py:495} INFO - 
[2023-01-22T08:38:51.392+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T08:38:51.392+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 68, in <module>
[2023-01-22T08:38:51.392+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T08:38:51.392+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 55, in main
[2023-01-22T08:38:51.392+0000] {spark_submit.py:495} INFO - df_spark.write.format("bigquery").option("project", "rafzul-analytics-1009") \
[2023-01-22T08:38:51.393+0000] {spark_submit.py:495} INFO - UnboundLocalError: local variable 'df_spark' referenced before assignment
[2023-01-22T08:38:51.472+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:51 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-22T08:38:51.491+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:51 INFO SparkUI: Stopped Spark web UI at http://9d8db2d97423:4040
[2023-01-22T08:38:51.516+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:51 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-22T08:38:51.561+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:51 INFO MemoryStore: MemoryStore cleared
[2023-01-22T08:38:51.562+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:51 INFO BlockManager: BlockManager stopped
[2023-01-22T08:38:51.598+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:51 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-22T08:38:51.609+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:51 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-22T08:38:51.638+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:51 INFO SparkContext: Successfully stopped SparkContext
[2023-01-22T08:38:51.645+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:51 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T08:38:51.646+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-2eddc059-6165-4f9b-8a32-6723a661bc0f/pyspark-1daf798c-5639-412c-b7c2-fd43024b60b0
[2023-01-22T08:38:51.656+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-f1c93bf4-3833-404e-b926-dbb257354a5a
[2023-01-22T08:38:51.663+0000] {spark_submit.py:495} INFO - 23/01/22 08:38:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-2eddc059-6165-4f9b-8a32-6723a661bc0f
[2023-01-22T08:38:51.753+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-22T08:38:51.757+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230122T083841, end_date=20230122T083851
[2023-01-22T08:38:51.772+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 373 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 359)
[2023-01-22T08:38:51.813+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T08:38:51.834+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-22T08:48:47.816+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T08:48:47.825+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [queued]>
[2023-01-22T08:48:47.825+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T08:48:47.826+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-22T08:48:47.826+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-22T08:48:47.837+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): transform_stage_generation> on 2021-01-01 01:00:00+00:00
[2023-01-22T08:48:47.844+0000] {standard_task_runner.py:55} INFO - Started process 1493 to run task
[2023-01-22T08:48:47.848+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ingestion-energydata', 'transform_stage_generation', 'scheduled__2021-01-01T01:00:00+00:00', '--job-id', '379', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpuest1vu4']
[2023-01-22T08:48:47.851+0000] {standard_task_runner.py:83} INFO - Job 379: Subtask transform_stage_generation
[2023-01-22T08:48:47.914+0000] {task_command.py:389} INFO - Running <TaskInstance: ingestion-energydata.transform_stage_generation scheduled__2021-01-01T01:00:00+00:00 [running]> on host 9d8db2d97423
[2023-01-22T08:48:47.985+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=ingestion-energydata
AIRFLOW_CTX_TASK_ID=transform_stage_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T01:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T01:00:00+00:00
[2023-01-22T08:48:47.994+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-22T08:48:47.995+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET
[2023-01-22T08:48:51.058+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-22T08:48:51.372+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:51 WARN DependencyUtils: Local jar /opt/***/gcs-connector-hadoop3-latest.jar does not exist, skipping.
[2023-01-22T08:48:51.373+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:51 WARN DependencyUtils: Local jar /opt/***/spark-bigquery-with-dependencies_2.13-0.27.1.jar does not exist, skipping.
[2023-01-22T08:48:52.546+0000] {spark_submit.py:495} INFO - /opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010100.json
[2023-01-22T08:48:52.644+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:52 INFO SparkContext: Running Spark version 3.3.1
[2023-01-22T08:48:52.699+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:52 INFO ResourceUtils: ==============================================================
[2023-01-22T08:48:52.706+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:52 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-22T08:48:52.706+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:52 INFO ResourceUtils: ==============================================================
[2023-01-22T08:48:52.707+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:52 INFO SparkContext: Submitted application: gcp_playground
[2023-01-22T08:48:52.736+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:52 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-22T08:48:52.743+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:52 INFO ResourceProfile: Limiting resource is cpu
[2023-01-22T08:48:52.745+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:52 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-22T08:48:52.817+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:52 INFO SecurityManager: Changing view acls to: ***
[2023-01-22T08:48:52.817+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:52 INFO SecurityManager: Changing modify acls to: ***
[2023-01-22T08:48:52.818+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:52 INFO SecurityManager: Changing view acls groups to:
[2023-01-22T08:48:52.818+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:52 INFO SecurityManager: Changing modify acls groups to:
[2023-01-22T08:48:52.819+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:52 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-22T08:48:53.226+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:53 INFO Utils: Successfully started service 'sparkDriver' on port 37739.
[2023-01-22T08:48:53.296+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:53 INFO SparkEnv: Registering MapOutputTracker
[2023-01-22T08:48:53.365+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:53 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-22T08:48:53.432+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:53 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-22T08:48:53.433+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:53 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-22T08:48:53.441+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:53 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-22T08:48:53.487+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:53 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-81f7c4a0-a9f7-4edd-a8cc-320c7b32abcc
[2023-01-22T08:48:53.517+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:53 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-22T08:48:53.561+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:53 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-22T08:48:54.073+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:54 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-01-22T08:48:54.228+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:54 ERROR SparkContext: Failed to add file:/opt/***/gcs-connector-hadoop3-latest.jar to Spark environment
[2023-01-22T08:48:54.229+0000] {spark_submit.py:495} INFO - java.io.FileNotFoundException: Jar /opt/***/gcs-connector-hadoop3-latest.jar not found
[2023-01-22T08:48:54.229+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1949)
[2023-01-22T08:48:54.229+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.addJar(SparkContext.scala:2004)
[2023-01-22T08:48:54.229+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:507)
[2023-01-22T08:48:54.230+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:507)
[2023-01-22T08:48:54.230+0000] {spark_submit.py:495} INFO - at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
[2023-01-22T08:48:54.230+0000] {spark_submit.py:495} INFO - at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
[2023-01-22T08:48:54.230+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
[2023-01-22T08:48:54.230+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.<init>(SparkContext.scala:507)
[2023-01-22T08:48:54.230+0000] {spark_submit.py:495} INFO - at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
[2023-01-22T08:48:54.231+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
[2023-01-22T08:48:54.231+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
[2023-01-22T08:48:54.231+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
[2023-01-22T08:48:54.231+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
[2023-01-22T08:48:54.231+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
[2023-01-22T08:48:54.231+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T08:48:54.232+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:238)
[2023-01-22T08:48:54.232+0000] {spark_submit.py:495} INFO - at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
[2023-01-22T08:48:54.232+0000] {spark_submit.py:495} INFO - at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
[2023-01-22T08:48:54.232+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T08:48:54.232+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T08:48:54.232+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T08:48:54.236+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:54 ERROR SparkContext: Failed to add file:/opt/***/spark-bigquery-with-dependencies_2.13-0.27.1.jar to Spark environment
[2023-01-22T08:48:54.237+0000] {spark_submit.py:495} INFO - java.io.FileNotFoundException: Jar /opt/***/spark-bigquery-with-dependencies_2.13-0.27.1.jar not found
[2023-01-22T08:48:54.237+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1949)
[2023-01-22T08:48:54.237+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.addJar(SparkContext.scala:2004)
[2023-01-22T08:48:54.237+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:507)
[2023-01-22T08:48:54.238+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:507)
[2023-01-22T08:48:54.238+0000] {spark_submit.py:495} INFO - at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
[2023-01-22T08:48:54.238+0000] {spark_submit.py:495} INFO - at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
[2023-01-22T08:48:54.238+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterable.foreach(Iterable.scala:926)
[2023-01-22T08:48:54.238+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.<init>(SparkContext.scala:507)
[2023-01-22T08:48:54.238+0000] {spark_submit.py:495} INFO - at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
[2023-01-22T08:48:54.238+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
[2023-01-22T08:48:54.239+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
[2023-01-22T08:48:54.239+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
[2023-01-22T08:48:54.239+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
[2023-01-22T08:48:54.239+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
[2023-01-22T08:48:54.239+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T08:48:54.240+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:238)
[2023-01-22T08:48:54.240+0000] {spark_submit.py:495} INFO - at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
[2023-01-22T08:48:54.240+0000] {spark_submit.py:495} INFO - at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
[2023-01-22T08:48:54.240+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T08:48:54.240+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T08:48:54.240+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T08:48:54.376+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:54 INFO Executor: Starting executor ID driver on host 9d8db2d97423
[2023-01-22T08:48:54.390+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:54 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-22T08:48:54.429+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:54 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33217.
[2023-01-22T08:48:54.430+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:54 INFO NettyBlockTransferService: Server created on 9d8db2d97423:33217
[2023-01-22T08:48:54.434+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:54 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-22T08:48:54.445+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:54 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 9d8db2d97423, 33217, None)
[2023-01-22T08:48:54.455+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:54 INFO BlockManagerMasterEndpoint: Registering block manager 9d8db2d97423:33217 with 434.4 MiB RAM, BlockManagerId(driver, 9d8db2d97423, 33217, None)
[2023-01-22T08:48:54.475+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:54 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 9d8db2d97423, 33217, None)
[2023-01-22T08:48:54.477+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:54 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 9d8db2d97423, 33217, None)
[2023-01-22T08:48:55.194+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:55 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-22T08:48:55.199+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:55 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-22T08:48:56.796+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:56 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: gs://entsoe_analytics_1009//opt/***/plugins/scripts/transform_raw_staging.py__DE_TENNET__total_generation__202101010100.json.
[2023-01-22T08:48:56.797+0000] {spark_submit.py:495} INFO - org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "gs"
[2023-01-22T08:48:56.797+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2023-01-22T08:48:56.797+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T08:48:56.797+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T08:48:56.797+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T08:48:56.797+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T08:48:56.798+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T08:48:56.798+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T08:48:56.798+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)
[2023-01-22T08:48:56.798+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:370)
[2023-01-22T08:48:56.798+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T08:48:56.798+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T08:48:56.799+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T08:48:56.799+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T08:48:56.799+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T08:48:56.799+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T08:48:56.799+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T08:48:56.799+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T08:48:56.799+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T08:48:56.800+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T08:48:56.800+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T08:48:56.800+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T08:48:56.800+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T08:48:56.800+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T08:48:56.800+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T08:48:56.800+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T08:48:56.800+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T08:48:56.849+0000] {spark_submit.py:495} INFO - Error, ada An error occurred while calling o37.load.
[2023-01-22T08:48:56.849+0000] {spark_submit.py:495} INFO - : org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "gs"
[2023-01-22T08:48:56.849+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
[2023-01-22T08:48:56.850+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
[2023-01-22T08:48:56.850+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
[2023-01-22T08:48:56.850+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
[2023-01-22T08:48:56.850+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
[2023-01-22T08:48:56.850+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
[2023-01-22T08:48:56.850+0000] {spark_submit.py:495} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
[2023-01-22T08:48:56.851+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:752)
[2023-01-22T08:48:56.851+0000] {spark_submit.py:495} INFO - at scala.collection.immutable.List.map(List.scala:246)
[2023-01-22T08:48:56.851+0000] {spark_submit.py:495} INFO - at scala.collection.immutable.List.map(List.scala:79)
[2023-01-22T08:48:56.851+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:750)
[2023-01-22T08:48:56.851+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:579)
[2023-01-22T08:48:56.851+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:408)
[2023-01-22T08:48:56.852+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
[2023-01-22T08:48:56.852+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
[2023-01-22T08:48:56.852+0000] {spark_submit.py:495} INFO - at scala.Option.getOrElse(Option.scala:201)
[2023-01-22T08:48:56.852+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
[2023-01-22T08:48:56.852+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)
[2023-01-22T08:48:56.853+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-22T08:48:56.853+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-22T08:48:56.854+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-22T08:48:56.854+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-22T08:48:56.854+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-22T08:48:56.854+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-22T08:48:56.854+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-22T08:48:56.854+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-22T08:48:56.855+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-22T08:48:56.855+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-01-22T08:48:56.855+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-01-22T08:48:56.855+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-22T08:48:56.855+0000] {spark_submit.py:495} INFO - 
[2023-01-22T08:48:56.855+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-22T08:48:56.856+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 67, in <module>
[2023-01-22T08:48:56.856+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-22T08:48:56.856+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 54, in main
[2023-01-22T08:48:56.856+0000] {spark_submit.py:495} INFO - df_spark.write.format("bigquery").option("project", "rafzul-analytics-1009") \
[2023-01-22T08:48:56.856+0000] {spark_submit.py:495} INFO - UnboundLocalError: local variable 'df_spark' referenced before assignment
[2023-01-22T08:48:56.964+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:56 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-22T08:48:56.992+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:56 INFO SparkUI: Stopped Spark web UI at http://9d8db2d97423:4040
[2023-01-22T08:48:57.023+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:57 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-22T08:48:57.044+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:57 INFO MemoryStore: MemoryStore cleared
[2023-01-22T08:48:57.046+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:57 INFO BlockManager: BlockManager stopped
[2023-01-22T08:48:57.060+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:57 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-22T08:48:57.066+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:57 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-22T08:48:57.080+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:57 INFO SparkContext: Successfully stopped SparkContext
[2023-01-22T08:48:57.080+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:57 INFO ShutdownHookManager: Shutdown hook called
[2023-01-22T08:48:57.081+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-4eb560d6-c11a-452f-8d14-b2123ce76aed/pyspark-eb452fe4-eb33-4650-9585-8edc037e780a
[2023-01-22T08:48:57.092+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-4eb560d6-c11a-452f-8d14-b2123ce76aed
[2023-01-22T08:48:57.099+0000] {spark_submit.py:495} INFO - 23/01/22 08:48:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-2430c496-8884-4f68-97c6-22115c374539
[2023-01-22T08:48:57.277+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.
[2023-01-22T08:48:57.284+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=ingestion-energydata, task_id=transform_stage_generation, execution_date=20210101T010000, start_date=20230122T084847, end_date=20230122T084857
[2023-01-22T08:48:57.311+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 379 for task transform_stage_generation (Cannot execute: spark-submit --master local --jars gcs-connector-hadoop3-latest.jar,spark-bigquery-with-dependencies_2.13-0.27.1.jar --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010100 202101010200 DE_TENNET. Error code is: 1.; 1493)
[2023-01-22T08:48:57.353+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-22T08:48:57.380+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
