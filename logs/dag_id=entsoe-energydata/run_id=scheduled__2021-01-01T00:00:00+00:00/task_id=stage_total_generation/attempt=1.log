[2023-01-30T10:54:42.412+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-30T10:54:42.596+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-30T10:54:42.597+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-30T10:54:42.598+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-30T10:54:42.598+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-30T10:54:42.695+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 00:00:00+00:00
[2023-01-30T10:54:42.743+0000] {standard_task_runner.py:55} INFO - Started process 220 to run task
[2023-01-30T10:54:42.779+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata', 'stage_total_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '532', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp7ms6gkur']
[2023-01-30T10:54:42.791+0000] {standard_task_runner.py:83} INFO - Job 532: Subtask stage_total_generation
[2023-01-30T10:54:43.230+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 5b6f5d29e531
[2023-01-30T10:54:43.593+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-30T10:54:43.632+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-30T10:54:43.635+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 398, in submit
    spark_submit_cmd = self._build_spark_submit_command(application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 344, in _build_spark_submit_command
    self.log.info("Spark-Submit cmd: %s", self._mask_cmd(connection_cmd))
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 256, in _mask_cmd
    " ".join(connection_cmd),
TypeError: sequence item 5: expected str instance, module found
[2023-01-30T10:54:43.671+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=entsoe-energydata, task_id=stage_total_generation, execution_date=20210101T000000, start_date=20230130T105442, end_date=20230130T105443
[2023-01-30T10:54:43.713+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 532 for task stage_total_generation (sequence item 5: expected str instance, module found; 220)
[2023-01-30T10:54:43.761+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-30T10:54:43.848+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-30T10:57:22.845+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-30T10:57:22.903+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-30T10:57:22.904+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-30T10:57:22.904+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-30T10:57:22.905+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-30T10:57:22.974+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 00:00:00+00:00
[2023-01-30T10:57:23.005+0000] {standard_task_runner.py:55} INFO - Started process 413 to run task
[2023-01-30T10:57:23.037+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata', 'stage_total_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '542', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpe_pb8btd']
[2023-01-30T10:57:23.057+0000] {standard_task_runner.py:83} INFO - Job 542: Subtask stage_total_generation
[2023-01-30T10:57:23.369+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 5b6f5d29e531
[2023-01-30T10:57:23.752+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-30T10:57:23.796+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-30T10:57:23.804+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-30T10:58:03.010+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:03 INFO SparkContext: Running Spark version 3.3.1
[2023-01-30T10:58:03.431+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-30T10:58:04.683+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:04 INFO ResourceUtils: ==============================================================
[2023-01-30T10:58:04.695+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:04 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-30T10:58:04.710+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:04 INFO ResourceUtils: ==============================================================
[2023-01-30T10:58:04.720+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:04 INFO SparkContext: Submitted application: gcp_playground
[2023-01-30T10:58:05.069+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:05 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-30T10:58:05.099+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:05 INFO ResourceProfile: Limiting resource is cpu
[2023-01-30T10:58:05.112+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:05 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-30T10:58:05.521+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:05 INFO SecurityManager: Changing view acls to: ***
[2023-01-30T10:58:05.526+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:05 INFO SecurityManager: Changing modify acls to: ***
[2023-01-30T10:58:05.548+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:05 INFO SecurityManager: Changing view acls groups to:
[2023-01-30T10:58:05.554+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:05 INFO SecurityManager: Changing modify acls groups to:
[2023-01-30T10:58:05.557+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:05 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-30T10:58:07.594+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:07 INFO Utils: Successfully started service 'sparkDriver' on port 44331.
[2023-01-30T10:58:07.896+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:07 INFO SparkEnv: Registering MapOutputTracker
[2023-01-30T10:58:08.351+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:08 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-30T10:58:08.527+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:08 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-30T10:58:08.531+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:08 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-30T10:58:08.574+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:08 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-30T10:58:08.780+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:08 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ae5761d9-97da-4186-9e72-88818b4ea117
[2023-01-30T10:58:08.912+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:08 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-30T10:58:09.113+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:09 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-30T10:58:11.858+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:11 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-01-30T10:58:12.314+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:12 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://5b6f5d29e531:44331/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675076282952
[2023-01-30T10:58:12.316+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:12 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://5b6f5d29e531:44331/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675076282952
[2023-01-30T10:58:13.101+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:13 INFO Executor: Starting executor ID driver on host 5b6f5d29e531
[2023-01-30T10:58:13.216+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:13 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-30T10:58:13.341+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:13 INFO Executor: Fetching spark://5b6f5d29e531:44331/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675076282952
[2023-01-30T10:58:14.295+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:14 INFO TransportClientFactory: Successfully created connection to 5b6f5d29e531/172.21.0.7:44331 after 477 ms (0 ms spent in bootstraps)
[2023-01-30T10:58:14.472+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:14 INFO Utils: Fetching spark://5b6f5d29e531:44331/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-329c1663-c868-4030-86d1-539a1b83dcb9/userFiles-b1d6c099-5b09-45cf-a2a2-955da3b7a166/fetchFileTemp10191969632069556691.tmp
[2023-01-30T10:58:17.901+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:17 INFO Executor: Adding file:/tmp/spark-329c1663-c868-4030-86d1-539a1b83dcb9/userFiles-b1d6c099-5b09-45cf-a2a2-955da3b7a166/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-30T10:58:17.903+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:17 INFO Executor: Fetching spark://5b6f5d29e531:44331/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675076282952
[2023-01-30T10:58:17.962+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:17 INFO Utils: Fetching spark://5b6f5d29e531:44331/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-329c1663-c868-4030-86d1-539a1b83dcb9/userFiles-b1d6c099-5b09-45cf-a2a2-955da3b7a166/fetchFileTemp10278028650895300850.tmp
[2023-01-30T10:58:19.575+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:19 INFO Executor: Adding file:/tmp/spark-329c1663-c868-4030-86d1-539a1b83dcb9/userFiles-b1d6c099-5b09-45cf-a2a2-955da3b7a166/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-30T10:58:19.669+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:19 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36935.
[2023-01-30T10:58:19.669+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:19 INFO NettyBlockTransferService: Server created on 5b6f5d29e531:36935
[2023-01-30T10:58:19.691+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:19 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-30T10:58:19.788+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:19 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 5b6f5d29e531, 36935, None)
[2023-01-30T10:58:19.828+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:19 INFO BlockManagerMasterEndpoint: Registering block manager 5b6f5d29e531:36935 with 434.4 MiB RAM, BlockManagerId(driver, 5b6f5d29e531, 36935, None)
[2023-01-30T10:58:19.853+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:19 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 5b6f5d29e531, 36935, None)
[2023-01-30T10:58:19.864+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:19 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 5b6f5d29e531, 36935, None)
[2023-01-30T10:58:27.699+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:27 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-30T10:58:27.753+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:27 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-30T10:58:49.003+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-30T10:58:49.004+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 233, in <module>
[2023-01-30T10:58:49.004+0000] {spark_submit.py:495} INFO - main(metrics_label=metrics_label, start=start, end=end, country_code=country_code)
[2023-01-30T10:58:49.004+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 220, in main
[2023-01-30T10:58:49.005+0000] {spark_submit.py:495} INFO - run_method = method(metrics_label, start, end, country_code, **params)
[2023-01-30T10:58:49.005+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 176, in transform_generation
[2023-01-30T10:58:49.005+0000] {spark_submit.py:495} INFO - full_df = self._base_timeseries(metrics_label, start, end, country_code)
[2023-01-30T10:58:49.006+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 82, in _base_timeseries
[2023-01-30T10:58:49.008+0000] {spark_submit.py:495} INFO - raise e
[2023-01-30T10:58:49.008+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 79, in _base_timeseries
[2023-01-30T10:58:49.012+0000] {spark_submit.py:495} INFO - .load(path)
[2023-01-30T10:58:49.016+0000] {spark_submit.py:495} INFO - File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 177, in load
[2023-01-30T10:58:49.016+0000] {spark_submit.py:495} INFO - File "/opt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py", line 1321, in __call__
[2023-01-30T10:58:49.017+0000] {spark_submit.py:495} INFO - File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 196, in deco
[2023-01-30T10:58:49.170+0000] {spark_submit.py:495} INFO - pyspark.sql.utils.AnalysisException: Path does not exist: gs://entsoe_analytics_1009/TEST_total_generation__DE_TENNET__202101010000__202101010100.json
[2023-01-30T10:58:50.040+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:50 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-30T10:58:50.107+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:50 INFO SparkUI: Stopped Spark web UI at http://5b6f5d29e531:4040
[2023-01-30T10:58:50.394+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:50 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-30T10:58:50.488+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:50 INFO MemoryStore: MemoryStore cleared
[2023-01-30T10:58:50.492+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:50 INFO BlockManager: BlockManager stopped
[2023-01-30T10:58:50.596+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:50 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-30T10:58:50.610+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:50 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-30T10:58:50.672+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:50 INFO SparkContext: Successfully stopped SparkContext
[2023-01-30T10:58:50.675+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:50 INFO ShutdownHookManager: Shutdown hook called
[2023-01-30T10:58:50.680+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:50 INFO ShutdownHookManager: Deleting directory /tmp/spark-329c1663-c868-4030-86d1-539a1b83dcb9/pyspark-2799de00-c6bd-4302-9304-651baa614d74
[2023-01-30T10:58:50.722+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:50 INFO ShutdownHookManager: Deleting directory /tmp/spark-e4c06fae-5722-4180-98b1-ff4c091f19ad
[2023-01-30T10:58:50.752+0000] {spark_submit.py:495} INFO - 23/01/30 10:58:50 INFO ShutdownHookManager: Deleting directory /tmp/spark-329c1663-c868-4030-86d1-539a1b83dcb9
[2023-01-30T10:58:50.979+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-30T10:58:50.987+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=entsoe-energydata, task_id=stage_total_generation, execution_date=20210101T000000, start_date=20230130T105722, end_date=20230130T105850
[2023-01-30T10:58:51.017+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 542 for task stage_total_generation (Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 413)
[2023-01-30T10:58:51.056+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-30T10:58:51.096+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-30T18:09:25.886+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-30T18:09:25.969+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-30T18:09:25.970+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-30T18:09:25.971+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-30T18:09:25.971+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-30T18:09:26.041+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 00:00:00+00:00
[2023-01-30T18:09:26.078+0000] {standard_task_runner.py:55} INFO - Started process 13042 to run task
[2023-01-30T18:09:26.106+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata', 'stage_total_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '574', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpr60ykv39']
[2023-01-30T18:09:26.124+0000] {standard_task_runner.py:83} INFO - Job 574: Subtask stage_total_generation
[2023-01-30T18:09:26.547+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 64b72c0214de
[2023-01-30T18:09:26.956+0000] {abstractoperator.py:592} ERROR - Exception rendering Jinja template for task 'stage_total_generation', field '_application_args'. Template: ['total_generation', "{{ data_interval_start.format('YYYYMMDDHHmm') }}", "{{ data_interval_end.format('YYYYMMDDHHmm') }}", '{{ params.country_code }}', '{{ start_date }}', '{{ end_date }}']
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/abstractoperator.py", line 585, in _do_render_template_fields
    rendered_content = self.render_template(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/abstractoperator.py", line 657, in render_template
    return [self.render_template(element, context, jinja_env, oids) for element in value]
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/abstractoperator.py", line 657, in <listcomp>
    return [self.render_template(element, context, jinja_env, oids) for element in value]
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/abstractoperator.py", line 646, in render_template
    return render_template_to_string(template, context)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/helpers.py", line 288, in render_template_to_string
    return render_template(template, cast(MutableMapping[str, Any], context), native=False)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/helpers.py", line 283, in render_template
    return "".join(nodes)
  File "<template>", line 12, in root
  File "/home/airflow/.local/lib/python3.10/site-packages/jinja2/runtime.py", line 852, in _fail_with_undefined_error
    raise self._undefined_exception(self._undefined_message)
jinja2.exceptions.UndefinedError: 'start_date' is undefined
[2023-01-30T18:09:26.990+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 1378, in _run_raw_task
    self._execute_task_with_callbacks(context, test_mode)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 1497, in _execute_task_with_callbacks
    task_orig = self.render_templates(context=context)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 2119, in render_templates
    original_task.render_template_fields(context)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1196, in render_template_fields
    self._do_render_template_fields(self, self.template_fields, context, jinja_env, set())
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/abstractoperator.py", line 585, in _do_render_template_fields
    rendered_content = self.render_template(
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/abstractoperator.py", line 657, in render_template
    return [self.render_template(element, context, jinja_env, oids) for element in value]
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/abstractoperator.py", line 657, in <listcomp>
    return [self.render_template(element, context, jinja_env, oids) for element in value]
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/abstractoperator.py", line 646, in render_template
    return render_template_to_string(template, context)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/helpers.py", line 288, in render_template_to_string
    return render_template(template, cast(MutableMapping[str, Any], context), native=False)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/helpers.py", line 283, in render_template
    return "".join(nodes)
  File "<template>", line 12, in root
  File "/home/airflow/.local/lib/python3.10/site-packages/jinja2/runtime.py", line 852, in _fail_with_undefined_error
    raise self._undefined_exception(self._undefined_message)
jinja2.exceptions.UndefinedError: 'start_date' is undefined
[2023-01-30T18:09:27.060+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=entsoe-energydata, task_id=stage_total_generation, execution_date=20210101T000000, start_date=20230130T180925, end_date=20230130T180927
[2023-01-30T18:09:27.114+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 574 for task stage_total_generation ('start_date' is undefined; 13042)
[2023-01-30T18:09:27.230+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-30T18:09:27.345+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-30T18:17:21.764+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-30T18:17:21.784+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-30T18:17:21.784+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-30T18:17:21.784+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-30T18:17:21.785+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-30T18:17:21.806+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 00:00:00+00:00
[2023-01-30T18:17:21.819+0000] {standard_task_runner.py:55} INFO - Started process 14006 to run task
[2023-01-30T18:17:21.825+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata', 'stage_total_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '594', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpo0bq2676']
[2023-01-30T18:17:21.830+0000] {standard_task_runner.py:83} INFO - Job 594: Subtask stage_total_generation
[2023-01-30T18:17:21.951+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 64b72c0214de
[2023-01-30T18:17:22.074+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-30T18:17:22.087+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-30T18:17:22.089+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-30T18:17:42.134+0000] {spark_submit.py:495} INFO - 23/01/30 18:17:42 INFO SparkContext: Running Spark version 3.3.1
[2023-01-30T18:17:42.523+0000] {spark_submit.py:495} INFO - 23/01/30 18:17:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-30T18:17:42.798+0000] {spark_submit.py:495} INFO - 23/01/30 18:17:42 INFO ResourceUtils: ==============================================================
[2023-01-30T18:17:42.801+0000] {spark_submit.py:495} INFO - 23/01/30 18:17:42 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-30T18:17:42.805+0000] {spark_submit.py:495} INFO - 23/01/30 18:17:42 INFO ResourceUtils: ==============================================================
[2023-01-30T18:17:42.809+0000] {spark_submit.py:495} INFO - 23/01/30 18:17:42 INFO SparkContext: Submitted application: gcp_playground
[2023-01-30T18:17:42.928+0000] {spark_submit.py:495} INFO - 23/01/30 18:17:42 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-30T18:17:42.939+0000] {spark_submit.py:495} INFO - 23/01/30 18:17:42 INFO ResourceProfile: Limiting resource is cpu
[2023-01-30T18:17:42.944+0000] {spark_submit.py:495} INFO - 23/01/30 18:17:42 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-30T18:17:43.210+0000] {spark_submit.py:495} INFO - 23/01/30 18:17:43 INFO SecurityManager: Changing view acls to: ***
[2023-01-30T18:17:43.214+0000] {spark_submit.py:495} INFO - 23/01/30 18:17:43 INFO SecurityManager: Changing modify acls to: ***
[2023-01-30T18:17:43.219+0000] {spark_submit.py:495} INFO - 23/01/30 18:17:43 INFO SecurityManager: Changing view acls groups to:
[2023-01-30T18:17:43.221+0000] {spark_submit.py:495} INFO - 23/01/30 18:17:43 INFO SecurityManager: Changing modify acls groups to:
[2023-01-30T18:17:43.222+0000] {spark_submit.py:495} INFO - 23/01/30 18:17:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-30T18:17:47.823+0000] {spark_submit.py:495} INFO - 23/01/30 18:17:47 INFO Utils: Successfully started service 'sparkDriver' on port 35503.
[2023-01-30T18:17:48.221+0000] {spark_submit.py:495} INFO - 23/01/30 18:17:48 INFO SparkEnv: Registering MapOutputTracker
[2023-01-30T18:17:48.689+0000] {spark_submit.py:495} INFO - 23/01/30 18:17:48 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-30T18:17:49.112+0000] {spark_submit.py:495} INFO - 23/01/30 18:17:49 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-30T18:17:49.132+0000] {spark_submit.py:495} INFO - 23/01/30 18:17:49 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-30T18:17:49.250+0000] {spark_submit.py:495} INFO - 23/01/30 18:17:49 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-30T18:17:49.575+0000] {spark_submit.py:495} INFO - 23/01/30 18:17:49 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-289e049f-4af3-4031-8922-90e47e5a4a12
[2023-01-30T18:17:49.952+0000] {spark_submit.py:495} INFO - 23/01/30 18:17:49 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-30T18:17:50.197+0000] {spark_submit.py:495} INFO - 23/01/30 18:17:50 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-30T18:17:53.758+0000] {spark_submit.py:495} INFO - 23/01/30 18:17:53 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-30T18:17:53.837+0000] {spark_submit.py:495} INFO - 23/01/30 18:17:53 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2023-01-30T18:17:54.185+0000] {spark_submit.py:495} INFO - 23/01/30 18:17:54 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://64b72c0214de:35503/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675102662093
[2023-01-30T18:17:54.188+0000] {spark_submit.py:495} INFO - 23/01/30 18:17:54 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://64b72c0214de:35503/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675102662093
[2023-01-30T18:17:55.017+0000] {spark_submit.py:495} INFO - 23/01/30 18:17:55 INFO Executor: Starting executor ID driver on host 64b72c0214de
[2023-01-30T18:17:55.096+0000] {spark_submit.py:495} INFO - 23/01/30 18:17:55 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-30T18:17:55.229+0000] {spark_submit.py:495} INFO - 23/01/30 18:17:55 INFO Executor: Fetching spark://64b72c0214de:35503/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675102662093
[2023-01-30T18:17:55.912+0000] {spark_submit.py:495} INFO - 23/01/30 18:17:55 INFO TransportClientFactory: Successfully created connection to 64b72c0214de/172.19.0.6:35503 after 506 ms (0 ms spent in bootstraps)
[2023-01-30T18:17:56.035+0000] {spark_submit.py:495} INFO - 23/01/30 18:17:56 INFO Utils: Fetching spark://64b72c0214de:35503/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-a119f2b4-8c06-403d-9061-b11e0bcc69fd/userFiles-5dd4985e-0f70-49c7-b83c-64461e7ebb0b/fetchFileTemp15242514920210280597.tmp
[2023-01-30T18:17:59.664+0000] {spark_submit.py:495} INFO - 23/01/30 18:17:59 INFO Executor: Adding file:/tmp/spark-a119f2b4-8c06-403d-9061-b11e0bcc69fd/userFiles-5dd4985e-0f70-49c7-b83c-64461e7ebb0b/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-30T18:17:59.666+0000] {spark_submit.py:495} INFO - 23/01/30 18:17:59 INFO Executor: Fetching spark://64b72c0214de:35503/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675102662093
[2023-01-30T18:17:59.666+0000] {spark_submit.py:495} INFO - 23/01/30 18:17:59 INFO Utils: Fetching spark://64b72c0214de:35503/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-a119f2b4-8c06-403d-9061-b11e0bcc69fd/userFiles-5dd4985e-0f70-49c7-b83c-64461e7ebb0b/fetchFileTemp4980361363895678693.tmp
[2023-01-30T18:18:03.276+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:03 INFO Executor: Adding file:/tmp/spark-a119f2b4-8c06-403d-9061-b11e0bcc69fd/userFiles-5dd4985e-0f70-49c7-b83c-64461e7ebb0b/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-30T18:18:03.356+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:03 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36659.
[2023-01-30T18:18:03.357+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:03 INFO NettyBlockTransferService: Server created on 64b72c0214de:36659
[2023-01-30T18:18:03.552+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:03 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-30T18:18:03.868+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 64b72c0214de, 36659, None)
[2023-01-30T18:18:03.904+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:03 INFO BlockManagerMasterEndpoint: Registering block manager 64b72c0214de:36659 with 434.4 MiB RAM, BlockManagerId(driver, 64b72c0214de, 36659, None)
[2023-01-30T18:18:03.912+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:03 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 64b72c0214de, 36659, None)
[2023-01-30T18:18:03.928+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:03 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 64b72c0214de, 36659, None)
[2023-01-30T18:18:08.421+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:08 INFO AsyncEventQueue: Process of event SparkListenerResourceProfileAdded(Profile: id = 0, executor resources: cores -> name: cores, amount: 1, script: , vendor: ,memory -> name: memory, amount: 1024, script: , vendor: ,offHeap -> name: offHeap, amount: 0, script: , vendor: , task resources: cpus -> name: cpus, amount: 1.0) by listener AppStatusListener took 1.061161467s.
[2023-01-30T18:18:16.247+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:16 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-30T18:18:16.355+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:16 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-30T18:18:33.170+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-30T18:18:33.171+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 266, in <module>
[2023-01-30T18:18:33.171+0000] {spark_submit.py:495} INFO - main(
[2023-01-30T18:18:33.171+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 243, in main
[2023-01-30T18:18:33.171+0000] {spark_submit.py:495} INFO - run_method = method(
[2023-01-30T18:18:33.172+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 185, in transform_generation
[2023-01-30T18:18:33.172+0000] {spark_submit.py:495} INFO - full_df = self._base_timeseries(
[2023-01-30T18:18:33.173+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 84, in _base_timeseries
[2023-01-30T18:18:33.173+0000] {spark_submit.py:495} INFO - raise e
[2023-01-30T18:18:33.173+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 81, in _base_timeseries
[2023-01-30T18:18:33.174+0000] {spark_submit.py:495} INFO - .load(path)
[2023-01-30T18:18:33.174+0000] {spark_submit.py:495} INFO - File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 177, in load
[2023-01-30T18:18:33.174+0000] {spark_submit.py:495} INFO - File "/opt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py", line 1321, in __call__
[2023-01-30T18:18:33.174+0000] {spark_submit.py:495} INFO - File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 196, in deco
[2023-01-30T18:18:33.236+0000] {spark_submit.py:495} INFO - pyspark.sql.utils.AnalysisException: Path does not exist: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010600.json
[2023-01-30T18:18:33.555+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:33 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-30T18:18:33.619+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:33 INFO SparkUI: Stopped Spark web UI at http://64b72c0214de:4041
[2023-01-30T18:18:33.672+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:33 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-30T18:18:33.790+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:33 INFO MemoryStore: MemoryStore cleared
[2023-01-30T18:18:33.801+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:33 INFO BlockManager: BlockManager stopped
[2023-01-30T18:18:33.831+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:33 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-30T18:18:33.850+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:33 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-30T18:18:33.962+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:33 INFO SparkContext: Successfully stopped SparkContext
[2023-01-30T18:18:33.963+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:33 INFO ShutdownHookManager: Shutdown hook called
[2023-01-30T18:18:33.964+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:33 INFO ShutdownHookManager: Deleting directory /tmp/spark-a119f2b4-8c06-403d-9061-b11e0bcc69fd
[2023-01-30T18:18:34.025+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-63dc6364-be15-418a-9993-668d197fb13f
[2023-01-30T18:18:34.050+0000] {spark_submit.py:495} INFO - 23/01/30 18:18:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-a119f2b4-8c06-403d-9061-b11e0bcc69fd/pyspark-c0c5fcd9-6bf8-4f62-a2ed-3fd4cfc825e8
[2023-01-30T18:18:34.596+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-30T18:18:34.610+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=entsoe-energydata, task_id=stage_total_generation, execution_date=20210101T000000, start_date=20230130T181721, end_date=20230130T181834
[2023-01-30T18:18:34.676+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 594 for task stage_total_generation (Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 14006)
[2023-01-30T18:18:34.763+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-30T18:18:34.814+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-30T18:24:56.174+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-30T18:24:56.215+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-30T18:24:56.216+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-30T18:24:56.217+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-30T18:24:56.217+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-30T18:24:56.285+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 00:00:00+00:00
[2023-01-30T18:24:56.323+0000] {standard_task_runner.py:55} INFO - Started process 16498 to run task
[2023-01-30T18:24:56.370+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata', 'stage_total_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '617', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpqj819p5f']
[2023-01-30T18:24:56.399+0000] {standard_task_runner.py:83} INFO - Job 617: Subtask stage_total_generation
[2023-01-30T18:24:56.844+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 64b72c0214de
[2023-01-30T18:24:57.306+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-30T18:24:57.360+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-30T18:24:57.368+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-30T18:25:16.960+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:16 INFO SparkContext: Running Spark version 3.3.1
[2023-01-30T18:25:17.321+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-30T18:25:18.032+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:18 INFO ResourceUtils: ==============================================================
[2023-01-30T18:25:18.062+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:18 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-30T18:25:18.092+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:18 INFO ResourceUtils: ==============================================================
[2023-01-30T18:25:18.092+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:18 INFO SparkContext: Submitted application: gcp_playground
[2023-01-30T18:25:18.515+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:18 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-30T18:25:18.609+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:18 INFO ResourceProfile: Limiting resource is cpu
[2023-01-30T18:25:18.636+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:18 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-30T18:25:18.841+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:18 INFO SecurityManager: Changing view acls to: ***
[2023-01-30T18:25:18.843+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:18 INFO SecurityManager: Changing modify acls to: ***
[2023-01-30T18:25:18.844+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:18 INFO SecurityManager: Changing view acls groups to:
[2023-01-30T18:25:18.849+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:18 INFO SecurityManager: Changing modify acls groups to:
[2023-01-30T18:25:18.851+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:18 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-30T18:25:21.412+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:21 INFO Utils: Successfully started service 'sparkDriver' on port 37223.
[2023-01-30T18:25:21.639+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:21 INFO SparkEnv: Registering MapOutputTracker
[2023-01-30T18:25:21.846+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:21 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-30T18:25:22.159+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:22 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-30T18:25:22.169+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:22 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-30T18:25:22.187+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:22 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-30T18:25:22.382+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:22 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-041385d7-f598-49cd-b501-9d6f5f083536
[2023-01-30T18:25:22.547+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:22 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-30T18:25:22.644+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:22 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-30T18:25:23.795+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:23 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-30T18:25:23.827+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:23 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2023-01-30T18:25:23.982+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:23 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://64b72c0214de:37223/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675103116921
[2023-01-30T18:25:23.983+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:23 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://64b72c0214de:37223/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675103116921
[2023-01-30T18:25:24.625+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:24 INFO Executor: Starting executor ID driver on host 64b72c0214de
[2023-01-30T18:25:24.665+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:24 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-30T18:25:24.762+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:24 INFO Executor: Fetching spark://64b72c0214de:37223/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675103116921
[2023-01-30T18:25:25.072+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:25 INFO TransportClientFactory: Successfully created connection to 64b72c0214de/172.19.0.6:37223 after 203 ms (0 ms spent in bootstraps)
[2023-01-30T18:25:25.120+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:25 INFO Utils: Fetching spark://64b72c0214de:37223/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-cbfac6c4-2610-4814-bbca-85100b289986/userFiles-0701acb8-eb3a-4d15-a6be-3d57ed052803/fetchFileTemp18234459627872430133.tmp
[2023-01-30T18:25:25.928+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:25 INFO Executor: Adding file:/tmp/spark-cbfac6c4-2610-4814-bbca-85100b289986/userFiles-0701acb8-eb3a-4d15-a6be-3d57ed052803/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-30T18:25:25.928+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:25 INFO Executor: Fetching spark://64b72c0214de:37223/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675103116921
[2023-01-30T18:25:25.933+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:25 INFO Utils: Fetching spark://64b72c0214de:37223/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-cbfac6c4-2610-4814-bbca-85100b289986/userFiles-0701acb8-eb3a-4d15-a6be-3d57ed052803/fetchFileTemp4095256938141345276.tmp
[2023-01-30T18:25:26.741+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:26 INFO Executor: Adding file:/tmp/spark-cbfac6c4-2610-4814-bbca-85100b289986/userFiles-0701acb8-eb3a-4d15-a6be-3d57ed052803/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-30T18:25:26.780+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:26 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44669.
[2023-01-30T18:25:26.780+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:26 INFO NettyBlockTransferService: Server created on 64b72c0214de:44669
[2023-01-30T18:25:26.783+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:26 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-30T18:25:26.806+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:26 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 64b72c0214de, 44669, None)
[2023-01-30T18:25:26.821+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:26 INFO BlockManagerMasterEndpoint: Registering block manager 64b72c0214de:44669 with 434.4 MiB RAM, BlockManagerId(driver, 64b72c0214de, 44669, None)
[2023-01-30T18:25:26.848+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:26 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 64b72c0214de, 44669, None)
[2023-01-30T18:25:26.848+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:26 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 64b72c0214de, 44669, None)
[2023-01-30T18:25:30.074+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:30 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-30T18:25:30.107+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:30 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-30T18:25:38.771+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:38 INFO InMemoryFileIndex: It took 480 ms to list leaf files for 1 paths.
[2023-01-30T18:25:40.074+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:40 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-30T18:25:40.495+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:40 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-30T18:25:40.510+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:40 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 64b72c0214de:44669 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-30T18:25:40.552+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:40 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-30T18:25:43.170+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:43 INFO FileInputFormat: Total input files to process : 1
[2023-01-30T18:25:43.254+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:43 INFO FileInputFormat: Total input files to process : 1
[2023-01-30T18:25:43.352+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:43 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-30T18:25:43.478+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:43 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-30T18:25:43.494+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:43 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-30T18:25:43.496+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:43 INFO DAGScheduler: Parents of final stage: List()
[2023-01-30T18:25:43.513+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:43 INFO DAGScheduler: Missing parents: List()
[2023-01-30T18:25:43.530+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:43 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-30T18:25:43.760+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:43 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-30T18:25:43.776+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:43 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-30T18:25:43.779+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:43 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 64b72c0214de:44669 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-30T18:25:43.782+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:43 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-30T18:25:43.885+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-30T18:25:43.890+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:43 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-30T18:25:44.119+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:44 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (64b72c0214de, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-30T18:25:44.256+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:44 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-30T18:25:44.697+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:44 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010000__202101010100.json:0+9369
[2023-01-30T18:25:45.589+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:45 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3437 bytes result sent to driver
[2023-01-30T18:25:45.664+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:45 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1617 ms on 64b72c0214de (executor driver) (1/1)
[2023-01-30T18:25:45.685+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:45 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-30T18:25:45.695+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:45 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 2.111 s
[2023-01-30T18:25:45.713+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:45 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-30T18:25:45.714+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-30T18:25:45.720+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:45 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 2.366377 s
[2023-01-30T18:25:50.030+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:50 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 64b72c0214de:44669 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-30T18:25:50.227+0000] {spark_submit.py:495} INFO - 23/01/30 18:25:50 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 64b72c0214de:44669 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-30T18:35:25.336+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-30T18:35:25.361+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-30T18:35:25.361+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-30T18:35:25.361+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-30T18:35:25.362+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-30T18:35:25.385+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 00:00:00+00:00
[2023-01-30T18:35:25.397+0000] {standard_task_runner.py:55} INFO - Started process 199 to run task
[2023-01-30T18:35:25.403+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata', 'stage_total_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '656', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpq0rhcdvz']
[2023-01-30T18:35:25.408+0000] {standard_task_runner.py:83} INFO - Job 656: Subtask stage_total_generation
[2023-01-30T18:35:25.533+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 767c6fe0d003
[2023-01-30T18:35:25.670+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-30T18:35:25.687+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-30T18:35:25.691+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-30T18:35:45.119+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:45 INFO SparkContext: Running Spark version 3.3.1
[2023-01-30T18:35:45.897+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-30T18:35:46.337+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:46 INFO ResourceUtils: ==============================================================
[2023-01-30T18:35:46.343+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:46 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-30T18:35:46.357+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:46 INFO ResourceUtils: ==============================================================
[2023-01-30T18:35:46.357+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:46 INFO SparkContext: Submitted application: gcp_playground
[2023-01-30T18:35:46.471+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:46 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-30T18:35:46.486+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:46 INFO ResourceProfile: Limiting resource is cpu
[2023-01-30T18:35:46.489+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:46 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-30T18:35:46.692+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:46 INFO SecurityManager: Changing view acls to: ***
[2023-01-30T18:35:46.699+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:46 INFO SecurityManager: Changing modify acls to: ***
[2023-01-30T18:35:46.699+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:46 INFO SecurityManager: Changing view acls groups to:
[2023-01-30T18:35:46.700+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:46 INFO SecurityManager: Changing modify acls groups to:
[2023-01-30T18:35:46.704+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-30T18:35:48.466+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:48 INFO Utils: Successfully started service 'sparkDriver' on port 42787.
[2023-01-30T18:35:48.774+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:48 INFO SparkEnv: Registering MapOutputTracker
[2023-01-30T18:35:49.063+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:49 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-30T18:35:49.150+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:49 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-30T18:35:49.153+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:49 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-30T18:35:49.203+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:49 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-30T18:35:49.482+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:49 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-5352adc1-2b6b-4a00-931f-9c21bce0db6d
[2023-01-30T18:35:49.624+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:49 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-30T18:35:49.763+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:49 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-30T18:35:52.108+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:52 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-30T18:35:52.108+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:52 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2023-01-30T18:35:52.113+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:52 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[2023-01-30T18:35:52.129+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:52 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[2023-01-30T18:35:52.130+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:52 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[2023-01-30T18:35:52.254+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:52 INFO Utils: Successfully started service 'SparkUI' on port 4045.
[2023-01-30T18:35:52.517+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:52 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://767c6fe0d003:42787/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675103745086
[2023-01-30T18:35:52.523+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:52 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://767c6fe0d003:42787/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675103745086
[2023-01-30T18:35:53.650+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:53 INFO Executor: Starting executor ID driver on host 767c6fe0d003
[2023-01-30T18:35:53.721+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:53 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-30T18:35:53.905+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:53 INFO Executor: Fetching spark://767c6fe0d003:42787/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675103745086
[2023-01-30T18:35:54.869+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:54 INFO TransportClientFactory: Successfully created connection to 767c6fe0d003/172.19.0.9:42787 after 681 ms (0 ms spent in bootstraps)
[2023-01-30T18:35:55.047+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:55 INFO Utils: Fetching spark://767c6fe0d003:42787/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-9383433c-1006-4552-8036-e4010a2e553b/userFiles-7c146a40-2ee4-407f-b22b-f4d70a4c2cae/fetchFileTemp14429372659641057540.tmp
[2023-01-30T18:35:56.874+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:56 INFO Executor: Adding file:/tmp/spark-9383433c-1006-4552-8036-e4010a2e553b/userFiles-7c146a40-2ee4-407f-b22b-f4d70a4c2cae/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-30T18:35:56.886+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:56 INFO Executor: Fetching spark://767c6fe0d003:42787/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675103745086
[2023-01-30T18:35:56.887+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:56 INFO Utils: Fetching spark://767c6fe0d003:42787/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-9383433c-1006-4552-8036-e4010a2e553b/userFiles-7c146a40-2ee4-407f-b22b-f4d70a4c2cae/fetchFileTemp18276566753297745565.tmp
[2023-01-30T18:35:58.051+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:58 INFO Executor: Adding file:/tmp/spark-9383433c-1006-4552-8036-e4010a2e553b/userFiles-7c146a40-2ee4-407f-b22b-f4d70a4c2cae/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-30T18:35:58.157+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:58 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40287.
[2023-01-30T18:35:58.158+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:58 INFO NettyBlockTransferService: Server created on 767c6fe0d003:40287
[2023-01-30T18:35:58.162+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:58 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-30T18:35:58.243+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:58 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 767c6fe0d003, 40287, None)
[2023-01-30T18:35:58.279+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:58 INFO BlockManagerMasterEndpoint: Registering block manager 767c6fe0d003:40287 with 434.4 MiB RAM, BlockManagerId(driver, 767c6fe0d003, 40287, None)
[2023-01-30T18:35:58.301+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:58 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 767c6fe0d003, 40287, None)
[2023-01-30T18:35:58.317+0000] {spark_submit.py:495} INFO - 23/01/30 18:35:58 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 767c6fe0d003, 40287, None)
[2023-01-30T18:36:04.987+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:04 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-30T18:36:05.028+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:05 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-30T18:36:23.355+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:23 INFO InMemoryFileIndex: It took 984 ms to list leaf files for 1 paths.
[2023-01-30T18:36:25.886+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:25 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-30T18:36:26.949+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:26 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-30T18:36:26.971+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:26 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 767c6fe0d003:40287 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-30T18:36:27.006+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:27 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-30T18:36:30.641+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:30 INFO FileInputFormat: Total input files to process : 1
[2023-01-30T18:36:30.765+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:30 INFO FileInputFormat: Total input files to process : 1
[2023-01-30T18:36:30.929+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:30 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-30T18:36:31.070+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:31 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-30T18:36:31.077+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:31 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-30T18:36:31.083+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:31 INFO DAGScheduler: Parents of final stage: List()
[2023-01-30T18:36:31.109+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:31 INFO DAGScheduler: Missing parents: List()
[2023-01-30T18:36:31.181+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:31 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-30T18:36:31.817+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:31 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-30T18:36:31.845+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:31 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-30T18:36:31.854+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:31 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 767c6fe0d003:40287 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-30T18:36:31.891+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:31 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-30T18:36:31.968+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-30T18:36:31.972+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:31 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-30T18:36:32.565+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:32 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-30T18:36:32.932+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:32 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-30T18:36:34.918+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:34 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010000__202101010100.json:0+9369
[2023-01-30T18:36:36.854+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:36 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3480 bytes result sent to driver
[2023-01-30T18:36:36.995+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:36 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 4659 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-30T18:36:37.018+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:37 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-30T18:36:37.058+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:37 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 5.667 s
[2023-01-30T18:36:37.082+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:37 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-30T18:36:37.092+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-30T18:36:37.104+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:37 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 6.165952 s
[2023-01-30T18:36:44.387+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:44 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 767c6fe0d003:40287 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-30T18:36:46.095+0000] {spark_submit.py:495} INFO - 23/01/30 18:36:46 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 767c6fe0d003:40287 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-30T18:37:08.132+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:08 INFO FileSourceStrategy: Pushed Filters:
[2023-01-30T18:37:08.141+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:08 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-30T18:37:08.155+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:08 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-30T18:37:11.375+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:11 INFO CodeGenerator: Code generated in 1210.20687 ms
[2023-01-30T18:37:11.393+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:11 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-30T18:37:11.438+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:11 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-30T18:37:11.440+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:11 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 767c6fe0d003:40287 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-30T18:37:11.445+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:11 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:42
[2023-01-30T18:37:11.478+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:11 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-30T18:37:11.835+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:11 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:42
[2023-01-30T18:37:11.847+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:11 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:42) with 1 output partitions
[2023-01-30T18:37:11.850+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:11 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:42)
[2023-01-30T18:37:11.851+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:11 INFO DAGScheduler: Parents of final stage: List()
[2023-01-30T18:37:11.851+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:11 INFO DAGScheduler: Missing parents: List()
[2023-01-30T18:37:11.862+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:11 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:42), which has no missing parents
[2023-01-30T18:37:11.928+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:11 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.9 KiB, free 434.2 MiB)
[2023-01-30T18:37:11.954+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:11 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.8 KiB, free 434.1 MiB)
[2023-01-30T18:37:11.959+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:11 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 767c6fe0d003:40287 (size: 7.8 KiB, free: 434.4 MiB)
[2023-01-30T18:37:11.963+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:11 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-30T18:37:11.967+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:42) (first 15 tasks are for partitions Vector(0))
[2023-01-30T18:37:11.970+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:11 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-30T18:37:11.992+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:11 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-30T18:37:12.010+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:12 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-30T18:37:12.683+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:12 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-30T18:37:13.801+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:13 INFO CodeGenerator: Code generated in 918.364253 ms
[2023-01-30T18:37:14.818+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:14 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1628 bytes result sent to driver
[2023-01-30T18:37:14.847+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:14 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2868 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-30T18:37:14.850+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:14 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-30T18:37:14.854+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:14 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:42) finished in 2.947 s
[2023-01-30T18:37:14.854+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:14 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-30T18:37:14.855+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-30T18:37:14.863+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:14 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:42, took 3.014320 s
[2023-01-30T18:37:15.321+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:15 INFO FileSourceStrategy: Pushed Filters:
[2023-01-30T18:37:15.326+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:15 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-30T18:37:15.334+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:15 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-30T18:37:15.718+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:15 INFO CodeGenerator: Code generated in 120.189047 ms
[2023-01-30T18:37:15.799+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:15 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 433.9 MiB)
[2023-01-30T18:37:16.043+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:16 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-30T18:37:16.046+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:16 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 767c6fe0d003:40287 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-30T18:37:16.056+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:16 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:45
[2023-01-30T18:37:16.071+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:16 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-30T18:37:16.571+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:16 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:45
[2023-01-30T18:37:16.572+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:16 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:45) with 1 output partitions
[2023-01-30T18:37:16.572+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:16 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:45)
[2023-01-30T18:37:16.573+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:16 INFO DAGScheduler: Parents of final stage: List()
[2023-01-30T18:37:16.573+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:16 INFO DAGScheduler: Missing parents: List()
[2023-01-30T18:37:16.582+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:16 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:45), which has no missing parents
[2023-01-30T18:37:16.618+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:16 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.9 KiB, free 433.9 MiB)
[2023-01-30T18:37:16.662+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:16 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.8 KiB, free 433.9 MiB)
[2023-01-30T18:37:16.679+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:16 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 767c6fe0d003:40287 (size: 7.8 KiB, free: 434.3 MiB)
[2023-01-30T18:37:16.685+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:16 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-30T18:37:16.689+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:45) (first 15 tasks are for partitions Vector(0))
[2023-01-30T18:37:16.691+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:16 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-30T18:37:16.699+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:16 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-30T18:37:16.701+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:16 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-30T18:37:16.878+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:16 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-30T18:37:17.243+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:17 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1585 bytes result sent to driver
[2023-01-30T18:37:17.321+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:17 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 625 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-30T18:37:17.322+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:17 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-30T18:37:17.329+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:17 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:45) finished in 0.748 s
[2023-01-30T18:37:17.330+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:17 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-30T18:37:17.330+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-30T18:37:17.338+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:17 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:45, took 0.766629 s
[2023-01-30T18:37:18.274+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:18 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 767c6fe0d003:40287 in memory (size: 7.8 KiB, free: 434.3 MiB)
[2023-01-30T18:37:18.419+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:18 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 767c6fe0d003:40287 in memory (size: 7.8 KiB, free: 434.3 MiB)
[2023-01-30T18:37:19.442+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:19 INFO FileSourceStrategy: Pushed Filters:
[2023-01-30T18:37:19.447+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:19 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-30T18:37:19.471+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:19 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-30T18:37:20.827+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:20 INFO CodeGenerator: Code generated in 475.743746 ms
[2023-01-30T18:37:20.838+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:20 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-30T18:37:20.870+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:20 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-30T18:37:20.873+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:20 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 767c6fe0d003:40287 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-30T18:37:20.875+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:20 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-30T18:37:20.882+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:20 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-30T18:37:21.038+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-30T18:37:21.060+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-30T18:37:21.062+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-30T18:37:21.062+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO DAGScheduler: Parents of final stage: List()
[2023-01-30T18:37:21.062+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO DAGScheduler: Missing parents: List()
[2023-01-30T18:37:21.074+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-30T18:37:21.095+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.7 MiB)
[2023-01-30T18:37:21.109+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.7 MiB)
[2023-01-30T18:37:21.112+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 767c6fe0d003:40287 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-30T18:37:21.115+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-30T18:37:21.124+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-30T18:37:21.127+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-30T18:37:21.134+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-30T18:37:21.135+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-30T18:37:21.198+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-30T18:37:21.391+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-01-30T18:37:21.421+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 289 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-30T18:37:21.423+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-30T18:37:21.428+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 0.350 s
[2023-01-30T18:37:21.428+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-30T18:37:21.429+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-30T18:37:21.429+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 0.386772 s
[2023-01-30T18:37:21.549+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO FileSourceStrategy: Pushed Filters:
[2023-01-30T18:37:21.550+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-30T18:37:21.550+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-30T18:37:21.596+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO CodeGenerator: Code generated in 30.662601 ms
[2023-01-30T18:37:21.611+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.5 MiB)
[2023-01-30T18:37:21.639+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-30T18:37:21.642+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 767c6fe0d003:40287 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-30T18:37:21.642+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:63
[2023-01-30T18:37:21.646+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-30T18:37:21.739+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:63
[2023-01-30T18:37:21.794+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:63) with 1 output partitions
[2023-01-30T18:37:21.794+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:63)
[2023-01-30T18:37:21.794+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO DAGScheduler: Parents of final stage: List()
[2023-01-30T18:37:21.796+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO DAGScheduler: Missing parents: List()
[2023-01-30T18:37:21.806+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:63), which has no missing parents
[2023-01-30T18:37:21.830+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.7 KiB, free 433.4 MiB)
[2023-01-30T18:37:21.868+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.4 MiB)
[2023-01-30T18:37:21.872+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 767c6fe0d003:40287 (size: 7.7 KiB, free: 434.2 MiB)
[2023-01-30T18:37:21.874+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-30T18:37:21.881+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:63) (first 15 tasks are for partitions Vector(0))
[2023-01-30T18:37:21.887+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-30T18:37:21.922+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-30T18:37:21.928+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:21 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-30T18:37:22.007+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:22 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-30T18:37:22.223+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:22 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1585 bytes result sent to driver
[2023-01-30T18:37:22.223+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:22 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 304 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-30T18:37:22.223+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:22 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-30T18:37:22.224+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:22 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:63) finished in 0.421 s
[2023-01-30T18:37:22.224+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:22 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-30T18:37:22.224+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-30T18:37:22.224+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:22 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:63, took 0.485028 s
[2023-01-30T18:37:23.344+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:23 INFO FileSourceStrategy: Pushed Filters:
[2023-01-30T18:37:23.344+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:23 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-30T18:37:23.344+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:23 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-30T18:37:23.645+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:23 INFO CodeGenerator: Code generated in 218.155785 ms
[2023-01-30T18:37:23.654+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:23 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.2 MiB)
[2023-01-30T18:37:23.694+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:23 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-30T18:37:23.695+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:23 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 767c6fe0d003:40287 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-30T18:37:23.706+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:23 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-30T18:37:23.714+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:23 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-30T18:37:23.876+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:23 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-30T18:37:23.879+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:23 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) with 1 output partitions
[2023-01-30T18:37:23.879+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:23 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201)
[2023-01-30T18:37:23.880+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:23 INFO DAGScheduler: Parents of final stage: List()
[2023-01-30T18:37:23.880+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:23 INFO DAGScheduler: Missing parents: List()
[2023-01-30T18:37:23.887+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:23 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201), which has no missing parents
[2023-01-30T18:37:23.891+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:23 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.1 MiB)
[2023-01-30T18:37:23.908+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:23 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.1 MiB)
[2023-01-30T18:37:23.911+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:23 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 767c6fe0d003:40287 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-30T18:37:23.913+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:23 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-30T18:37:23.918+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) (first 15 tasks are for partitions Vector(0))
[2023-01-30T18:37:23.919+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:23 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-30T18:37:23.922+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:23 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-30T18:37:23.923+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:23 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-30T18:37:23.948+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:23 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-30T18:37:24.109+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2558 bytes result sent to driver
[2023-01-30T18:37:24.117+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 200 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-30T18:37:24.125+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-30T18:37:24.126+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) finished in 0.233 s
[2023-01-30T18:37:24.126+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-30T18:37:24.127+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-30T18:37:24.137+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201, took 0.243698 s
[2023-01-30T18:37:24.297+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO FileSourceStrategy: Pushed Filters:
[2023-01-30T18:37:24.297+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-30T18:37:24.302+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-30T18:37:24.505+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO CodeGenerator: Code generated in 158.912764 ms
[2023-01-30T18:37:24.570+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 432.9 MiB)
[2023-01-30T18:37:24.589+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 767c6fe0d003:40287 in memory (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-30T18:37:24.611+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.9 MiB)
[2023-01-30T18:37:24.614+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 767c6fe0d003:40287 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-30T18:37:24.623+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-30T18:37:24.623+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-30T18:37:24.739+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-30T18:37:24.740+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-30T18:37:24.741+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-30T18:37:24.741+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO DAGScheduler: Parents of final stage: List()
[2023-01-30T18:37:24.741+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO DAGScheduler: Missing parents: List()
[2023-01-30T18:37:24.760+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-30T18:37:24.761+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 767c6fe0d003:40287 in memory (size: 7.7 KiB, free: 434.2 MiB)
[2023-01-30T18:37:24.762+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 432.9 MiB)
[2023-01-30T18:37:24.767+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.9 MiB)
[2023-01-30T18:37:24.774+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 767c6fe0d003:40287 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-30T18:37:24.775+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-30T18:37:24.778+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-30T18:37:24.782+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-30T18:37:24.785+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-30T18:37:24.786+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-30T18:37:24.825+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-30T18:37:24.885+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:24 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 767c6fe0d003:40287 in memory (size: 12.7 KiB, free: 434.2 MiB)
[2023-01-30T18:37:25.005+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-01-30T18:37:25.009+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 222 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-30T18:37:25.022+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 0.251 s
[2023-01-30T18:37:25.022+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-30T18:37:25.023+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-30T18:37:25.027+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-30T18:37:25.027+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 0.285053 s
[2023-01-30T18:37:25.214+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO FileSourceStrategy: Pushed Filters:
[2023-01-30T18:37:25.222+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-30T18:37:25.223+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-30T18:37:25.401+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO CodeGenerator: Code generated in 121.622803 ms
[2023-01-30T18:37:25.412+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.8 MiB)
[2023-01-30T18:37:25.476+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.7 MiB)
[2023-01-30T18:37:25.478+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 767c6fe0d003:40287 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-30T18:37:25.479+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-30T18:37:25.482+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-30T18:37:25.663+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-30T18:37:25.664+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-30T18:37:25.664+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-30T18:37:25.664+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO DAGScheduler: Parents of final stage: List()
[2023-01-30T18:37:25.664+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO DAGScheduler: Missing parents: List()
[2023-01-30T18:37:25.675+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-30T18:37:25.675+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.7 MiB)
[2023-01-30T18:37:25.691+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.7 MiB)
[2023-01-30T18:37:25.694+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 767c6fe0d003:40287 (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-30T18:37:25.702+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-30T18:37:25.703+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-30T18:37:25.703+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-30T18:37:25.703+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-30T18:37:25.703+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-30T18:37:25.747+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-30T18:37:25.877+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1729 bytes result sent to driver
[2023-01-30T18:37:25.878+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 177 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-30T18:37:25.879+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-30T18:37:25.880+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 0.208 s
[2023-01-30T18:37:25.881+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-30T18:37:25.881+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-30T18:37:25.883+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:25 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 0.220634 s
[2023-01-30T18:37:28.882+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:28 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 767c6fe0d003:40287 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-30T18:37:28.961+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:28 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 767c6fe0d003:40287 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-30T18:37:29.021+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:29 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 767c6fe0d003:40287 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-30T18:37:29.078+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:29 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 767c6fe0d003:40287 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-30T18:37:29.161+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:29 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 767c6fe0d003:40287 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-30T18:37:29.185+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:29 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 767c6fe0d003:40287 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-30T18:37:29.244+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:29 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 767c6fe0d003:40287 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-30T18:37:31.604+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:31 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 767c6fe0d003:40287 in memory (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-30T18:37:37.263+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:37 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-30T18:37:37.315+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-30T18:37:37.317+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-30T18:37:37.319+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:37 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-30T18:37:37.324+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-30T18:37:37.324+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-30T18:37:37.329+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-30T18:37:38.214+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:38 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-30T18:37:38.217+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:38 INFO DAGScheduler: Got job 8 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-30T18:37:38.217+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:38 INFO DAGScheduler: Final stage: ResultStage 8 (save at BigQueryWriteHelper.java:105)
[2023-01-30T18:37:38.218+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:38 INFO DAGScheduler: Parents of final stage: List()
[2023-01-30T18:37:38.218+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:38 INFO DAGScheduler: Missing parents: List()
[2023-01-30T18:37:38.226+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:38 INFO DAGScheduler: Submitting ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-30T18:37:38.430+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:38 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 209.9 KiB, free 434.0 MiB)
[2023-01-30T18:37:38.462+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:38 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 74.3 KiB, free 433.9 MiB)
[2023-01-30T18:37:38.463+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:38 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 767c6fe0d003:40287 (size: 74.3 KiB, free: 434.3 MiB)
[2023-01-30T18:37:38.469+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:38 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513
[2023-01-30T18:37:38.473+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-30T18:37:38.474+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:38 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2023-01-30T18:37:38.483+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:38 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7484 bytes) taskResourceAssignments Map()
[2023-01-30T18:37:38.491+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:38 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2023-01-30T18:37:38.668+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-30T18:37:38.669+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-30T18:37:38.673+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:38 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-30T18:37:38.675+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-30T18:37:38.676+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-30T18:37:38.682+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-30T18:37:38.710+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:38 INFO CodecConfig: Compression: SNAPPY
[2023-01-30T18:37:38.739+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:38 INFO CodecConfig: Compression: SNAPPY
[2023-01-30T18:37:38.961+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:38 INFO ParquetOutputFormat: Parquet block size to 134217728
[2023-01-30T18:37:38.964+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:38 INFO ParquetOutputFormat: Validation is off
[2023-01-30T18:37:38.964+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:38 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-01-30T18:37:38.965+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:38 INFO ParquetOutputFormat: Parquet properties are:
[2023-01-30T18:37:38.965+0000] {spark_submit.py:495} INFO - Parquet page size to 1048576
[2023-01-30T18:37:38.965+0000] {spark_submit.py:495} INFO - Parquet dictionary page size to 1048576
[2023-01-30T18:37:38.966+0000] {spark_submit.py:495} INFO - Dictionary is true
[2023-01-30T18:37:38.966+0000] {spark_submit.py:495} INFO - Writer version is: PARQUET_1_0
[2023-01-30T18:37:38.966+0000] {spark_submit.py:495} INFO - Page size checking is: estimated
[2023-01-30T18:37:38.966+0000] {spark_submit.py:495} INFO - Min row count for page size check is: 100
[2023-01-30T18:37:38.966+0000] {spark_submit.py:495} INFO - Max row count for page size check is: 10000
[2023-01-30T18:37:38.966+0000] {spark_submit.py:495} INFO - Truncate length for column indexes is: 64
[2023-01-30T18:37:38.966+0000] {spark_submit.py:495} INFO - Truncate length for statistics min/max  is: 2147483647
[2023-01-30T18:37:38.967+0000] {spark_submit.py:495} INFO - Bloom filter enabled: false
[2023-01-30T18:37:38.967+0000] {spark_submit.py:495} INFO - Max Bloom filter size for a column is 1048576
[2023-01-30T18:37:38.967+0000] {spark_submit.py:495} INFO - Bloom filter expected number of distinct values are: null
[2023-01-30T18:37:38.967+0000] {spark_submit.py:495} INFO - Page row count limit to 20000
[2023-01-30T18:37:38.967+0000] {spark_submit.py:495} INFO - Writing page checksums is: on
[2023-01-30T18:37:39.417+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:39 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-01-30T18:37:39.418+0000] {spark_submit.py:495} INFO - {
[2023-01-30T18:37:39.418+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-30T18:37:39.418+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-30T18:37:39.418+0000] {spark_submit.py:495} INFO - "name" : "measured_at",
[2023-01-30T18:37:39.419+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-30T18:37:39.419+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-30T18:37:39.419+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:37:39.419+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:37:39.419+0000] {spark_submit.py:495} INFO - "name" : "created_at",
[2023-01-30T18:37:39.420+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-30T18:37:39.420+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-30T18:37:39.420+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:37:39.420+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:37:39.420+0000] {spark_submit.py:495} INFO - "name" : "Biomass - Actual Aggregated",
[2023-01-30T18:37:39.420+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:37:39.421+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:37:39.421+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:37:39.421+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:37:39.421+0000] {spark_submit.py:495} INFO - "name" : "Fossil Brown coal/Lignite - Actual Aggregated",
[2023-01-30T18:37:39.421+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:37:39.422+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:37:39.422+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:37:39.422+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:37:39.422+0000] {spark_submit.py:495} INFO - "name" : "Fossil Gas - Actual Aggregated",
[2023-01-30T18:37:39.422+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:37:39.422+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:37:39.423+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:37:39.442+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:37:39.443+0000] {spark_submit.py:495} INFO - "name" : "Fossil Hard coal - Actual Aggregated",
[2023-01-30T18:37:39.448+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:37:39.448+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:37:39.449+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:37:39.449+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:37:39.449+0000] {spark_submit.py:495} INFO - "name" : "Fossil Oil - Actual Aggregated",
[2023-01-30T18:37:39.449+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:37:39.450+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:37:39.450+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:37:39.450+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:37:39.450+0000] {spark_submit.py:495} INFO - "name" : "Geothermal - Actual Aggregated",
[2023-01-30T18:37:39.450+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:37:39.450+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:37:39.451+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:37:39.451+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:37:39.453+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Aggregated",
[2023-01-30T18:37:39.453+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:37:39.454+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:37:39.454+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:37:39.454+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:37:39.454+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Consumption",
[2023-01-30T18:37:39.455+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:37:39.455+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:37:39.455+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:37:39.455+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:37:39.456+0000] {spark_submit.py:495} INFO - "name" : "Hydro Run-of-river and poundage - Actual Aggregated",
[2023-01-30T18:37:39.456+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:37:39.456+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:37:39.456+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:37:39.456+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:37:39.456+0000] {spark_submit.py:495} INFO - "name" : "Hydro Water Reservoir - Actual Aggregated",
[2023-01-30T18:37:39.456+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:37:39.457+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:37:39.457+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:37:39.457+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:37:39.457+0000] {spark_submit.py:495} INFO - "name" : "Nuclear - Actual Aggregated",
[2023-01-30T18:37:39.457+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:37:39.457+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:37:39.458+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:37:39.458+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:37:39.458+0000] {spark_submit.py:495} INFO - "name" : "Other - Actual Aggregated",
[2023-01-30T18:37:39.458+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:37:39.458+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:37:39.458+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:37:39.458+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:37:39.459+0000] {spark_submit.py:495} INFO - "name" : "Other renewable - Actual Aggregated",
[2023-01-30T18:37:39.459+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:37:39.459+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:37:39.459+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:37:39.460+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:37:39.461+0000] {spark_submit.py:495} INFO - "name" : "Solar - Actual Aggregated",
[2023-01-30T18:37:39.461+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:37:39.461+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:37:39.461+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:37:39.462+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:37:39.462+0000] {spark_submit.py:495} INFO - "name" : "Waste - Actual Aggregated",
[2023-01-30T18:37:39.462+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:37:39.462+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:37:39.462+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:37:39.462+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:37:39.463+0000] {spark_submit.py:495} INFO - "name" : "Wind Offshore - Actual Aggregated",
[2023-01-30T18:37:39.463+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:37:39.463+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:37:39.463+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:37:39.463+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:37:39.464+0000] {spark_submit.py:495} INFO - "name" : "Wind Onshore - Actual Aggregated",
[2023-01-30T18:37:39.464+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:37:39.464+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:37:39.464+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:37:39.464+0000] {spark_submit.py:495} INFO - } ]
[2023-01-30T18:37:39.464+0000] {spark_submit.py:495} INFO - }
[2023-01-30T18:37:39.465+0000] {spark_submit.py:495} INFO - and corresponding Parquet message type:
[2023-01-30T18:37:39.465+0000] {spark_submit.py:495} INFO - message spark_schema {
[2023-01-30T18:37:39.465+0000] {spark_submit.py:495} INFO - required int96 measured_at;
[2023-01-30T18:37:39.465+0000] {spark_submit.py:495} INFO - required int96 created_at;
[2023-01-30T18:37:39.465+0000] {spark_submit.py:495} INFO - optional double Biomass - Actual Aggregated;
[2023-01-30T18:37:39.465+0000] {spark_submit.py:495} INFO - optional double Fossil Brown coal/Lignite - Actual Aggregated;
[2023-01-30T18:37:39.466+0000] {spark_submit.py:495} INFO - optional double Fossil Gas - Actual Aggregated;
[2023-01-30T18:37:39.466+0000] {spark_submit.py:495} INFO - optional double Fossil Hard coal - Actual Aggregated;
[2023-01-30T18:37:39.466+0000] {spark_submit.py:495} INFO - optional double Fossil Oil - Actual Aggregated;
[2023-01-30T18:37:39.466+0000] {spark_submit.py:495} INFO - optional double Geothermal - Actual Aggregated;
[2023-01-30T18:37:39.466+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Aggregated;
[2023-01-30T18:37:39.466+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Consumption;
[2023-01-30T18:37:39.467+0000] {spark_submit.py:495} INFO - optional double Hydro Run-of-river and poundage - Actual Aggregated;
[2023-01-30T18:37:39.467+0000] {spark_submit.py:495} INFO - optional double Hydro Water Reservoir - Actual Aggregated;
[2023-01-30T18:37:39.467+0000] {spark_submit.py:495} INFO - optional double Nuclear - Actual Aggregated;
[2023-01-30T18:37:39.467+0000] {spark_submit.py:495} INFO - optional double Other - Actual Aggregated;
[2023-01-30T18:37:39.467+0000] {spark_submit.py:495} INFO - optional double Other renewable - Actual Aggregated;
[2023-01-30T18:37:39.468+0000] {spark_submit.py:495} INFO - optional double Solar - Actual Aggregated;
[2023-01-30T18:37:39.468+0000] {spark_submit.py:495} INFO - optional double Waste - Actual Aggregated;
[2023-01-30T18:37:39.468+0000] {spark_submit.py:495} INFO - optional double Wind Offshore - Actual Aggregated;
[2023-01-30T18:37:39.468+0000] {spark_submit.py:495} INFO - optional double Wind Onshore - Actual Aggregated;
[2023-01-30T18:37:39.468+0000] {spark_submit.py:495} INFO - }
[2023-01-30T18:37:39.468+0000] {spark_submit.py:495} INFO - 
[2023-01-30T18:37:39.469+0000] {spark_submit.py:495} INFO - 
[2023-01-30T18:37:40.223+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:40 INFO CodecPool: Got brand-new compressor [.snappy]
[2023-01-30T18:37:44.374+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:44 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675103753230-f8c9765d-b075-49c8-824a-968ebb5be9a0/_temporary/0/_temporary/' directory.
[2023-01-30T18:37:44.375+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:44 INFO FileOutputCommitter: Saved output of task 'attempt_202301301837371496947249418982562_0008_m_000000_8' to gs://entsoe_temp_1009/.spark-bigquery-local-1675103753230-f8c9765d-b075-49c8-824a-968ebb5be9a0/_temporary/0/task_202301301837371496947249418982562_0008_m_000000
[2023-01-30T18:37:44.387+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:44 INFO SparkHadoopMapRedUtil: attempt_202301301837371496947249418982562_0008_m_000000_8: Committed. Elapsed time: 1131 ms.
[2023-01-30T18:37:44.423+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:44 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2588 bytes result sent to driver
[2023-01-30T18:37:44.426+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:44 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 5946 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-30T18:37:44.426+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:44 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2023-01-30T18:37:44.428+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:44 INFO DAGScheduler: ResultStage 8 (save at BigQueryWriteHelper.java:105) finished in 6.189 s
[2023-01-30T18:37:44.430+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:44 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-30T18:37:44.430+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2023-01-30T18:37:44.432+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:44 INFO DAGScheduler: Job 8 finished: save at BigQueryWriteHelper.java:105, took 6.216966 s
[2023-01-30T18:37:44.435+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:44 INFO FileFormatWriter: Start to commit write Job 905c925e-ed00-45d3-bc6a-f77f1a85581b.
[2023-01-30T18:37:45.333+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:45 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675103753230-f8c9765d-b075-49c8-824a-968ebb5be9a0/_temporary/0/task_202301301837371496947249418982562_0008_m_000000/' directory.
[2023-01-30T18:37:45.763+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:45 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 767c6fe0d003:40287 in memory (size: 74.3 KiB, free: 434.4 MiB)
[2023-01-30T18:37:46.103+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:46 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675103753230-f8c9765d-b075-49c8-824a-968ebb5be9a0/' directory.
[2023-01-30T18:37:47.509+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:47 INFO FileFormatWriter: Write Job 905c925e-ed00-45d3-bc6a-f77f1a85581b committed. Elapsed time: 3073 ms.
[2023-01-30T18:37:47.545+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:47 INFO FileFormatWriter: Finished processing stats for write job 905c925e-ed00-45d3-bc6a-f77f1a85581b.
[2023-01-30T18:37:49.240+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:49 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=entsoe_analytics_1009, projectId=rafzul-analytics-1009, tableId=TEST_total_generation_staging}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=null, ignoreUnknownValue=null, sourceUris=[gs://entsoe_temp_1009/.spark-bigquery-local-1675103753230-f8c9765d-b075-49c8-824a-968ebb5be9a0/part-00000-da353a57-bd6e-4f6e-93fc-39fe15132d37-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=true, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=rafzul-analytics-1009, job=d1c7e7b9-6511-47da-9199-34e4d3c11684, location=US}
[2023-01-30T18:37:55.647+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:55 INFO BigQueryClient: Done loading to rafzul-analytics-1009.entsoe_analytics_1009.TEST_total_generation_staging. jobId: JobId{project=rafzul-analytics-1009, job=d1c7e7b9-6511-47da-9199-34e4d3c11684, location=US}
[2023-01-30T18:37:56.617+0000] {spark_submit.py:495} INFO - MANTEP!!!!
[2023-01-30T18:37:57.042+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:57 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-30T18:37:57.138+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:57 INFO SparkUI: Stopped Spark web UI at http://767c6fe0d003:4045
[2023-01-30T18:37:57.184+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:57 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-30T18:37:57.339+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:57 INFO MemoryStore: MemoryStore cleared
[2023-01-30T18:37:57.349+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:57 INFO BlockManager: BlockManager stopped
[2023-01-30T18:37:57.372+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:57 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-30T18:37:57.405+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:57 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-30T18:37:57.471+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:57 INFO SparkContext: Successfully stopped SparkContext
[2023-01-30T18:37:57.472+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:57 INFO ShutdownHookManager: Shutdown hook called
[2023-01-30T18:37:57.476+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-9383433c-1006-4552-8036-e4010a2e553b
[2023-01-30T18:37:57.498+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-9383433c-1006-4552-8036-e4010a2e553b/pyspark-32dca5bf-ed58-4980-a819-f0e02b220cf3
[2023-01-30T18:37:57.513+0000] {spark_submit.py:495} INFO - 23/01/30 18:37:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-e3276331-954c-41de-9330-c725c304a35f
[2023-01-30T18:37:57.901+0000] {taskinstance.py:1322} INFO - Marking task as SUCCESS. dag_id=entsoe-energydata, task_id=stage_total_generation, execution_date=20210101T000000, start_date=20230130T183525, end_date=20230130T183757
[2023-01-30T18:37:57.992+0000] {local_task_job.py:159} INFO - Task exited with return code 0
[2023-01-30T18:37:58.050+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-30T18:44:22.166+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-30T18:44:22.299+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-30T18:44:22.300+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-30T18:44:22.300+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-30T18:44:22.301+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-30T18:44:22.456+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 00:00:00+00:00
[2023-01-30T18:44:22.531+0000] {standard_task_runner.py:55} INFO - Started process 2711 to run task
[2023-01-30T18:44:22.590+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata', 'stage_total_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '668', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpbcxr52p9']
[2023-01-30T18:44:22.611+0000] {standard_task_runner.py:83} INFO - Job 668: Subtask stage_total_generation
[2023-01-30T18:44:24.473+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 767c6fe0d003
[2023-01-30T18:44:25.636+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-30T18:44:25.798+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-30T18:44:25.835+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-30T18:45:25.675+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:25 INFO SparkContext: Running Spark version 3.3.1
[2023-01-30T18:45:25.953+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-30T18:45:26.353+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:26 INFO ResourceUtils: ==============================================================
[2023-01-30T18:45:26.368+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:26 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-30T18:45:26.375+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:26 INFO ResourceUtils: ==============================================================
[2023-01-30T18:45:26.379+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:26 INFO SparkContext: Submitted application: gcp_playground
[2023-01-30T18:45:26.572+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:26 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-30T18:45:26.591+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:26 INFO ResourceProfile: Limiting resource is cpu
[2023-01-30T18:45:26.592+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:26 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-30T18:45:27.311+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:27 INFO SecurityManager: Changing view acls to: ***
[2023-01-30T18:45:27.317+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:27 INFO SecurityManager: Changing modify acls to: ***
[2023-01-30T18:45:27.322+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:27 INFO SecurityManager: Changing view acls groups to:
[2023-01-30T18:45:27.340+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:27 INFO SecurityManager: Changing modify acls groups to:
[2023-01-30T18:45:27.357+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:27 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-30T18:45:31.565+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:31 INFO Utils: Successfully started service 'sparkDriver' on port 33859.
[2023-01-30T18:45:31.939+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:31 INFO SparkEnv: Registering MapOutputTracker
[2023-01-30T18:45:34.611+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:34 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-30T18:45:35.333+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:35 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-30T18:45:35.337+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:35 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-30T18:45:35.382+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:35 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-30T18:45:35.715+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:35 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ad1fc0c8-74e5-4151-8f63-cb40dd2893f3
[2023-01-30T18:45:36.170+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:36 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-30T18:45:36.514+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:36 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-30T18:45:40.368+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:40 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-30T18:45:40.399+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:40 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2023-01-30T18:45:40.430+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:40 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[2023-01-30T18:45:40.707+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:40 INFO Utils: Successfully started service 'SparkUI' on port 4043.
[2023-01-30T18:45:41.714+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:41 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://767c6fe0d003:33859/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675104325646
[2023-01-30T18:45:41.717+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:41 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://767c6fe0d003:33859/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675104325646
[2023-01-30T18:45:43.235+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:43 INFO Executor: Starting executor ID driver on host 767c6fe0d003
[2023-01-30T18:45:43.291+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:43 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-30T18:45:43.387+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:43 INFO Executor: Fetching spark://767c6fe0d003:33859/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675104325646
[2023-01-30T18:45:43.975+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:43 INFO TransportClientFactory: Successfully created connection to 767c6fe0d003/172.19.0.9:33859 after 338 ms (0 ms spent in bootstraps)
[2023-01-30T18:45:44.264+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:44 INFO Utils: Fetching spark://767c6fe0d003:33859/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-c73f2945-7173-462d-a976-230308f3462c/userFiles-0d75a797-9eae-4b9c-bcf3-6299e0a7f0b3/fetchFileTemp1467650803339205945.tmp
[2023-01-30T18:45:48.784+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:48 INFO Executor: Adding file:/tmp/spark-c73f2945-7173-462d-a976-230308f3462c/userFiles-0d75a797-9eae-4b9c-bcf3-6299e0a7f0b3/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-30T18:45:48.787+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:48 INFO Executor: Fetching spark://767c6fe0d003:33859/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675104325646
[2023-01-30T18:45:48.804+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:48 INFO Utils: Fetching spark://767c6fe0d003:33859/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-c73f2945-7173-462d-a976-230308f3462c/userFiles-0d75a797-9eae-4b9c-bcf3-6299e0a7f0b3/fetchFileTemp7892795128200355569.tmp
[2023-01-30T18:45:52.235+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:52 INFO Executor: Adding file:/tmp/spark-c73f2945-7173-462d-a976-230308f3462c/userFiles-0d75a797-9eae-4b9c-bcf3-6299e0a7f0b3/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-30T18:45:52.431+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:52 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38739.
[2023-01-30T18:45:52.432+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:52 INFO NettyBlockTransferService: Server created on 767c6fe0d003:38739
[2023-01-30T18:45:52.466+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:52 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-30T18:45:52.599+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:52 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 767c6fe0d003, 38739, None)
[2023-01-30T18:45:52.633+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:52 INFO BlockManagerMasterEndpoint: Registering block manager 767c6fe0d003:38739 with 434.4 MiB RAM, BlockManagerId(driver, 767c6fe0d003, 38739, None)
[2023-01-30T18:45:52.707+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:52 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 767c6fe0d003, 38739, None)
[2023-01-30T18:45:52.714+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:52 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 767c6fe0d003, 38739, None)
[2023-01-30T18:45:57.637+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:57 INFO AsyncEventQueue: Process of event SparkListenerResourceProfileAdded(Profile: id = 0, executor resources: cores -> name: cores, amount: 1, script: , vendor: ,memory -> name: memory, amount: 1024, script: , vendor: ,offHeap -> name: offHeap, amount: 0, script: , vendor: , task resources: cpus -> name: cpus, amount: 1.0) by listener HeartbeatReceiver took 1.276298187s.
[2023-01-30T18:45:58.019+0000] {spark_submit.py:495} INFO - 23/01/30 18:45:58 INFO AsyncEventQueue: Process of event SparkListenerResourceProfileAdded(Profile: id = 0, executor resources: cores -> name: cores, amount: 1, script: , vendor: ,memory -> name: memory, amount: 1024, script: , vendor: ,offHeap -> name: offHeap, amount: 0, script: , vendor: , task resources: cpus -> name: cpus, amount: 1.0) by listener AppStatusListener took 1.768828946s.
[2023-01-30T18:46:10.443+0000] {spark_submit.py:495} INFO - 23/01/30 18:46:10 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-30T18:46:10.571+0000] {spark_submit.py:495} INFO - 23/01/30 18:46:10 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-30T18:46:48.882+0000] {spark_submit.py:495} INFO - 23/01/30 18:46:48 INFO InMemoryFileIndex: It took 684 ms to list leaf files for 1 paths.
[2023-01-30T18:46:49.562+0000] {spark_submit.py:495} INFO - 23/01/30 18:46:49 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-30T18:46:49.841+0000] {spark_submit.py:495} INFO - 23/01/30 18:46:49 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-30T18:46:49.848+0000] {spark_submit.py:495} INFO - 23/01/30 18:46:49 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 767c6fe0d003:38739 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-30T18:46:49.908+0000] {spark_submit.py:495} INFO - 23/01/30 18:46:49 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-30T18:46:55.048+0000] {spark_submit.py:495} INFO - 23/01/30 18:46:55 INFO FileInputFormat: Total input files to process : 1
[2023-01-30T18:46:55.235+0000] {spark_submit.py:495} INFO - 23/01/30 18:46:55 INFO FileInputFormat: Total input files to process : 1
[2023-01-30T18:46:55.407+0000] {spark_submit.py:495} INFO - 23/01/30 18:46:55 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-30T18:46:55.665+0000] {spark_submit.py:495} INFO - 23/01/30 18:46:55 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-30T18:46:55.670+0000] {spark_submit.py:495} INFO - 23/01/30 18:46:55 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-30T18:46:55.677+0000] {spark_submit.py:495} INFO - 23/01/30 18:46:55 INFO DAGScheduler: Parents of final stage: List()
[2023-01-30T18:46:55.718+0000] {spark_submit.py:495} INFO - 23/01/30 18:46:55 INFO DAGScheduler: Missing parents: List()
[2023-01-30T18:46:55.799+0000] {spark_submit.py:495} INFO - 23/01/30 18:46:55 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-30T18:46:57.501+0000] {spark_submit.py:495} INFO - 23/01/30 18:46:57 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-30T18:46:57.533+0000] {spark_submit.py:495} INFO - 23/01/30 18:46:57 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-30T18:46:57.538+0000] {spark_submit.py:495} INFO - 23/01/30 18:46:57 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 767c6fe0d003:38739 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-30T18:46:57.555+0000] {spark_submit.py:495} INFO - 23/01/30 18:46:57 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-30T18:46:57.695+0000] {spark_submit.py:495} INFO - 23/01/30 18:46:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-30T18:46:57.696+0000] {spark_submit.py:495} INFO - 23/01/30 18:46:57 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-30T18:46:58.155+0000] {spark_submit.py:495} INFO - 23/01/30 18:46:58 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-30T18:46:58.330+0000] {spark_submit.py:495} INFO - 23/01/30 18:46:58 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-30T18:47:00.579+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:00 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010000__202101010100.json:0+9369
[2023-01-30T18:47:03.040+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:03 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3480 bytes result sent to driver
[2023-01-30T18:47:03.190+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:03 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 5244 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-30T18:47:03.238+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:03 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-30T18:47:03.321+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:03 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 7.045 s
[2023-01-30T18:47:03.429+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:03 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-30T18:47:03.434+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-30T18:47:03.463+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:03 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 8.045330 s
[2023-01-30T18:47:05.505+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:05 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 767c6fe0d003:38739 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-30T18:47:10.434+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:10 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 767c6fe0d003:38739 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-30T18:47:48.160+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:48 INFO FileSourceStrategy: Pushed Filters:
[2023-01-30T18:47:48.178+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:48 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-30T18:47:48.202+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:48 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-30T18:47:56.491+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:56 INFO CodeGenerator: Code generated in 4867.302622 ms
[2023-01-30T18:47:56.788+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:56 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-30T18:47:57.291+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:57 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-30T18:47:57.303+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:57 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 767c6fe0d003:38739 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-30T18:47:57.317+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:57 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-30T18:47:57.417+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:57 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-30T18:47:58.261+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:58 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-30T18:47:58.276+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:58 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-30T18:47:58.277+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:58 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-30T18:47:58.278+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:58 INFO DAGScheduler: Parents of final stage: List()
[2023-01-30T18:47:58.290+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:58 INFO DAGScheduler: Missing parents: List()
[2023-01-30T18:47:58.327+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:58 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-30T18:47:58.430+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:58 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-30T18:47:58.440+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:58 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-30T18:47:58.446+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:58 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 767c6fe0d003:38739 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-30T18:47:58.458+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:58 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-30T18:47:58.518+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-30T18:47:58.519+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:58 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-30T18:47:58.566+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:58 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-30T18:47:58.567+0000] {spark_submit.py:495} INFO - 23/01/30 18:47:58 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-30T18:48:01.922+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:01 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-30T18:48:06.877+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:06 INFO CodeGenerator: Code generated in 3066.82116 ms
[2023-01-30T18:48:10.511+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:10 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1628 bytes result sent to driver
[2023-01-30T18:48:10.561+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:10 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 12015 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-30T18:48:10.621+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:10 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-30T18:48:10.630+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:10 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 12.214 s
[2023-01-30T18:48:10.639+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:10 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-30T18:48:10.640+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-30T18:48:10.640+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:10 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 12.374798 s
[2023-01-30T18:48:11.823+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:11 INFO FileSourceStrategy: Pushed Filters:
[2023-01-30T18:48:11.825+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:11 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-30T18:48:11.825+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:11 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-30T18:48:12.350+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:12 INFO CodeGenerator: Code generated in 391.708193 ms
[2023-01-30T18:48:12.619+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:12 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-30T18:48:12.900+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:12 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-30T18:48:12.932+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:12 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 767c6fe0d003:38739 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-30T18:48:12.932+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:12 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-30T18:48:12.933+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-30T18:48:13.153+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:13 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-30T18:48:13.160+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:13 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-30T18:48:13.161+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:13 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-30T18:48:13.162+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:13 INFO DAGScheduler: Parents of final stage: List()
[2023-01-30T18:48:13.162+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:13 INFO DAGScheduler: Missing parents: List()
[2023-01-30T18:48:13.175+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:13 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-30T18:48:13.265+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:13 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-30T18:48:13.428+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:13 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-30T18:48:13.451+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:13 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 767c6fe0d003:38739 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-30T18:48:13.454+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:13 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-30T18:48:13.529+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-30T18:48:13.531+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:13 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-30T18:48:13.540+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:13 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-30T18:48:13.617+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:13 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-30T18:48:13.771+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:13 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-30T18:48:16.084+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:16 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1628 bytes result sent to driver
[2023-01-30T18:48:16.130+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:16 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 2593 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-30T18:48:16.147+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:16 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 2.950 s
[2023-01-30T18:48:16.192+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:16 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-30T18:48:16.194+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:16 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-30T18:48:16.241+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-30T18:48:16.242+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:16 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 3.084877 s
[2023-01-30T18:48:16.932+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:16 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 767c6fe0d003:38739 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-30T18:48:17.007+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:17 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 767c6fe0d003:38739 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-30T18:48:17.319+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:17 INFO FileSourceStrategy: Pushed Filters:
[2023-01-30T18:48:17.331+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:17 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-30T18:48:17.332+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:17 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-30T18:48:21.719+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:21 INFO CodeGenerator: Code generated in 3969.211205 ms
[2023-01-30T18:48:22.119+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:22 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-30T18:48:22.272+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:22 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-30T18:48:22.279+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:22 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 767c6fe0d003:38739 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-30T18:48:22.285+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:22 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-30T18:48:22.295+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:22 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-30T18:48:22.423+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:22 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-30T18:48:22.430+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:22 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-30T18:48:22.430+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:22 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-30T18:48:22.431+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:22 INFO DAGScheduler: Parents of final stage: List()
[2023-01-30T18:48:22.432+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:22 INFO DAGScheduler: Missing parents: List()
[2023-01-30T18:48:22.445+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:22 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-30T18:48:22.472+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:22 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.7 MiB)
[2023-01-30T18:48:22.519+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:22 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.7 MiB)
[2023-01-30T18:48:22.546+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:22 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 767c6fe0d003:38739 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-30T18:48:22.580+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:22 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-30T18:48:22.650+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-30T18:48:22.651+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:22 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-30T18:48:22.664+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:22 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-30T18:48:22.665+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:22 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-30T18:48:22.837+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:22 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-30T18:48:23.645+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:23 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-01-30T18:48:23.696+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:23 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 1031 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-30T18:48:23.715+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:23 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 1.246 s
[2023-01-30T18:48:23.716+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:23 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-30T18:48:23.717+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:23 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-30T18:48:23.717+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-30T18:48:23.720+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:23 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 1.287235 s
[2023-01-30T18:48:24.133+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:24 INFO FileSourceStrategy: Pushed Filters:
[2023-01-30T18:48:24.134+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:24 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-30T18:48:24.138+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:24 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-30T18:48:24.282+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:24 INFO CodeGenerator: Code generated in 85.059787 ms
[2023-01-30T18:48:24.331+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:24 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.5 MiB)
[2023-01-30T18:48:24.599+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:24 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-30T18:48:24.606+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:24 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 767c6fe0d003:38739 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-30T18:48:24.609+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:24 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-30T18:48:24.626+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:24 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-30T18:48:24.774+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:24 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-30T18:48:24.781+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:24 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-30T18:48:24.785+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:24 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-30T18:48:24.786+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:24 INFO DAGScheduler: Parents of final stage: List()
[2023-01-30T18:48:24.787+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:24 INFO DAGScheduler: Missing parents: List()
[2023-01-30T18:48:24.812+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:24 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-30T18:48:24.936+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:24 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-30T18:48:24.985+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:24 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-01-30T18:48:24.991+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:24 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 767c6fe0d003:38739 (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-30T18:48:24.994+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:24 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-30T18:48:25.022+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-30T18:48:25.027+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:25 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-30T18:48:25.028+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:25 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-30T18:48:25.028+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:25 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-30T18:48:25.131+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:25 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-30T18:48:25.392+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:25 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1586 bytes result sent to driver
[2023-01-30T18:48:25.393+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:25 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 368 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-30T18:48:25.393+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:25 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-30T18:48:25.394+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:25 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 0.560 s
[2023-01-30T18:48:25.394+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:25 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-30T18:48:25.395+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-30T18:48:25.396+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:25 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 0.618654 s
[2023-01-30T18:48:26.830+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:26 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 767c6fe0d003:38739 in memory (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-30T18:48:27.084+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:27 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 767c6fe0d003:38739 in memory (size: 7.6 KiB, free: 434.3 MiB)
[2023-01-30T18:48:31.935+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:31 INFO FileSourceStrategy: Pushed Filters:
[2023-01-30T18:48:32.036+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:32 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-30T18:48:32.038+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:32 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-30T18:48:38.730+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:38 INFO CodeGenerator: Code generated in 4561.487656 ms
[2023-01-30T18:48:38.746+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:38 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.3 MiB)
[2023-01-30T18:48:38.859+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:38 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-30T18:48:38.876+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:38 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 767c6fe0d003:38739 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-30T18:48:38.881+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:38 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-30T18:48:38.922+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-30T18:48:39.071+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:39 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-30T18:48:39.074+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:39 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) with 1 output partitions
[2023-01-30T18:48:39.075+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:39 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201)
[2023-01-30T18:48:39.076+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:39 INFO DAGScheduler: Parents of final stage: List()
[2023-01-30T18:48:39.076+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:39 INFO DAGScheduler: Missing parents: List()
[2023-01-30T18:48:39.099+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:39 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201), which has no missing parents
[2023-01-30T18:48:39.153+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:39 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.2 MiB)
[2023-01-30T18:48:39.170+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:39 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.2 MiB)
[2023-01-30T18:48:39.184+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:39 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 767c6fe0d003:38739 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-30T18:48:39.201+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:39 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-30T18:48:39.205+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) (first 15 tasks are for partitions Vector(0))
[2023-01-30T18:48:39.211+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:39 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-30T18:48:39.223+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:39 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-30T18:48:39.227+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:39 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-30T18:48:39.367+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:39 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-30T18:48:39.974+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:39 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2558 bytes result sent to driver
[2023-01-30T18:48:39.994+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:39 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 752 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-30T18:48:39.995+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:39 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-30T18:48:39.995+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:39 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) finished in 0.892 s
[2023-01-30T18:48:39.995+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:39 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-30T18:48:39.996+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-30T18:48:39.996+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:39 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201, took 0.927595 s
[2023-01-30T18:48:41.395+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:41 INFO FileSourceStrategy: Pushed Filters:
[2023-01-30T18:48:41.395+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:41 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-30T18:48:41.396+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:41 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-30T18:48:41.921+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:41 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 767c6fe0d003:38739 in memory (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-30T18:48:42.106+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:42 INFO CodeGenerator: Code generated in 689.248584 ms
[2023-01-30T18:48:42.454+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:42 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 433.1 MiB)
[2023-01-30T18:48:43.019+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:43 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.0 MiB)
[2023-01-30T18:48:43.037+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:43 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 767c6fe0d003:38739 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-30T18:48:43.052+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:43 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-30T18:48:43.214+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:43 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-30T18:48:44.025+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:44 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-30T18:48:44.053+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:44 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-30T18:48:44.054+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:44 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-30T18:48:44.054+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:44 INFO DAGScheduler: Parents of final stage: List()
[2023-01-30T18:48:44.055+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:44 INFO DAGScheduler: Missing parents: List()
[2023-01-30T18:48:44.104+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:44 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-30T18:48:44.215+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:44 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 433.0 MiB)
[2023-01-30T18:48:44.264+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:44 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 433.0 MiB)
[2023-01-30T18:48:44.286+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:44 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 767c6fe0d003:38739 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-30T18:48:44.295+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:44 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-30T18:48:44.313+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-30T18:48:44.325+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:44 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-30T18:48:44.358+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:44 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-30T18:48:44.367+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:44 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-30T18:48:44.493+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:44 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-30T18:48:46.190+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:46 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-01-30T18:48:46.266+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:46 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 1915 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-30T18:48:46.267+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:46 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-30T18:48:46.276+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:46 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 2.123 s
[2023-01-30T18:48:46.283+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:46 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-30T18:48:46.284+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-30T18:48:46.284+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:46 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 2.247758 s
[2023-01-30T18:48:47.490+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:47 INFO FileSourceStrategy: Pushed Filters:
[2023-01-30T18:48:47.500+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:47 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-30T18:48:47.501+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:47 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-30T18:48:48.869+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:48 INFO CodeGenerator: Code generated in 774.1474 ms
[2023-01-30T18:48:48.944+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:48 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.8 MiB)
[2023-01-30T18:48:49.164+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:49 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.7 MiB)
[2023-01-30T18:48:49.168+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:49 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 767c6fe0d003:38739 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-30T18:48:49.209+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:49 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-30T18:48:49.209+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:49 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-30T18:48:49.489+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:49 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-30T18:48:49.492+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:49 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-30T18:48:49.493+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:49 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-30T18:48:49.507+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:49 INFO DAGScheduler: Parents of final stage: List()
[2023-01-30T18:48:49.508+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:49 INFO DAGScheduler: Missing parents: List()
[2023-01-30T18:48:49.516+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:49 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-30T18:48:49.545+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:49 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.7 MiB)
[2023-01-30T18:48:49.581+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:49 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.7 MiB)
[2023-01-30T18:48:49.583+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:49 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 767c6fe0d003:38739 (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-30T18:48:49.584+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:49 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-30T18:48:49.587+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-30T18:48:49.589+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:49 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-30T18:48:49.597+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:49 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-30T18:48:49.601+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:49 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-30T18:48:49.699+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:49 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-30T18:48:49.962+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:49 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1729 bytes result sent to driver
[2023-01-30T18:48:49.970+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:49 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 372 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-30T18:48:49.970+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:49 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-30T18:48:49.971+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:49 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 0.428 s
[2023-01-30T18:48:49.971+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:49 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-30T18:48:49.971+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-30T18:48:49.971+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:49 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 0.483350 s
[2023-01-30T18:48:56.803+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:56 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 767c6fe0d003:38739 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-30T18:48:56.948+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:56 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 767c6fe0d003:38739 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-30T18:48:57.156+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:57 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 767c6fe0d003:38739 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-30T18:48:57.370+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:57 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 767c6fe0d003:38739 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-30T18:48:57.552+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:57 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 767c6fe0d003:38739 in memory (size: 12.6 KiB, free: 434.3 MiB)
[2023-01-30T18:48:57.680+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:57 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 767c6fe0d003:38739 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-30T18:48:57.825+0000] {spark_submit.py:495} INFO - 23/01/30 18:48:57 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 767c6fe0d003:38739 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-30T18:49:57.486+0000] {spark_submit.py:495} INFO - 23/01/30 18:49:57 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-30T18:49:57.608+0000] {spark_submit.py:495} INFO - 23/01/30 18:49:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-30T18:49:57.609+0000] {spark_submit.py:495} INFO - 23/01/30 18:49:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-30T18:49:57.647+0000] {spark_submit.py:495} INFO - 23/01/30 18:49:57 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-30T18:49:57.649+0000] {spark_submit.py:495} INFO - 23/01/30 18:49:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-30T18:49:57.649+0000] {spark_submit.py:495} INFO - 23/01/30 18:49:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-30T18:49:57.667+0000] {spark_submit.py:495} INFO - 23/01/30 18:49:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-30T18:50:01.988+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:01 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-30T18:50:02.009+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:02 INFO DAGScheduler: Got job 8 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-30T18:50:02.010+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:02 INFO DAGScheduler: Final stage: ResultStage 8 (save at BigQueryWriteHelper.java:105)
[2023-01-30T18:50:02.010+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:02 INFO DAGScheduler: Parents of final stage: List()
[2023-01-30T18:50:02.013+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:02 INFO DAGScheduler: Missing parents: List()
[2023-01-30T18:50:02.045+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:02 INFO DAGScheduler: Submitting ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-30T18:50:03.843+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:03 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 209.9 KiB, free 433.7 MiB)
[2023-01-30T18:50:03.899+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:03 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 74.2 KiB, free 433.7 MiB)
[2023-01-30T18:50:03.907+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:03 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 767c6fe0d003:38739 (size: 74.2 KiB, free: 434.3 MiB)
[2023-01-30T18:50:03.923+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:03 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513
[2023-01-30T18:50:04.057+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-30T18:50:04.097+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:04 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2023-01-30T18:50:04.156+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:04 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7484 bytes) taskResourceAssignments Map()
[2023-01-30T18:50:04.186+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:04 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2023-01-30T18:50:05.209+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-30T18:50:05.212+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-30T18:50:05.220+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:05 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-30T18:50:05.221+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-30T18:50:05.226+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-30T18:50:05.252+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-30T18:50:05.732+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:05 INFO CodecConfig: Compression: SNAPPY
[2023-01-30T18:50:05.918+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:05 INFO CodecConfig: Compression: SNAPPY
[2023-01-30T18:50:06.471+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:06 INFO ParquetOutputFormat: Parquet block size to 134217728
[2023-01-30T18:50:06.472+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:06 INFO ParquetOutputFormat: Validation is off
[2023-01-30T18:50:06.473+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:06 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-01-30T18:50:06.474+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:06 INFO ParquetOutputFormat: Parquet properties are:
[2023-01-30T18:50:06.475+0000] {spark_submit.py:495} INFO - Parquet page size to 1048576
[2023-01-30T18:50:06.482+0000] {spark_submit.py:495} INFO - Parquet dictionary page size to 1048576
[2023-01-30T18:50:06.485+0000] {spark_submit.py:495} INFO - Dictionary is true
[2023-01-30T18:50:06.487+0000] {spark_submit.py:495} INFO - Writer version is: PARQUET_1_0
[2023-01-30T18:50:06.488+0000] {spark_submit.py:495} INFO - Page size checking is: estimated
[2023-01-30T18:50:06.488+0000] {spark_submit.py:495} INFO - Min row count for page size check is: 100
[2023-01-30T18:50:06.493+0000] {spark_submit.py:495} INFO - Max row count for page size check is: 10000
[2023-01-30T18:50:06.495+0000] {spark_submit.py:495} INFO - Truncate length for column indexes is: 64
[2023-01-30T18:50:06.497+0000] {spark_submit.py:495} INFO - Truncate length for statistics min/max  is: 2147483647
[2023-01-30T18:50:06.498+0000] {spark_submit.py:495} INFO - Bloom filter enabled: false
[2023-01-30T18:50:06.499+0000] {spark_submit.py:495} INFO - Max Bloom filter size for a column is 1048576
[2023-01-30T18:50:06.500+0000] {spark_submit.py:495} INFO - Bloom filter expected number of distinct values are: null
[2023-01-30T18:50:06.502+0000] {spark_submit.py:495} INFO - Page row count limit to 20000
[2023-01-30T18:50:06.503+0000] {spark_submit.py:495} INFO - Writing page checksums is: on
[2023-01-30T18:50:07.716+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:07 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-01-30T18:50:07.717+0000] {spark_submit.py:495} INFO - {
[2023-01-30T18:50:07.718+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-30T18:50:07.718+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-30T18:50:07.718+0000] {spark_submit.py:495} INFO - "name" : "measured_at",
[2023-01-30T18:50:07.719+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-30T18:50:07.726+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-30T18:50:07.733+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:50:07.772+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:50:07.773+0000] {spark_submit.py:495} INFO - "name" : "created_at",
[2023-01-30T18:50:07.774+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-30T18:50:07.774+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-30T18:50:07.863+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:50:07.864+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:50:07.865+0000] {spark_submit.py:495} INFO - "name" : "Biomass - Actual Aggregated",
[2023-01-30T18:50:07.865+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:50:07.866+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:50:07.866+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:50:07.866+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:50:07.943+0000] {spark_submit.py:495} INFO - "name" : "Fossil Brown coal/Lignite - Actual Aggregated",
[2023-01-30T18:50:07.944+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:50:07.945+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:50:07.945+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:50:07.946+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:50:07.950+0000] {spark_submit.py:495} INFO - "name" : "Fossil Gas - Actual Aggregated",
[2023-01-30T18:50:07.951+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:50:07.951+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:50:07.952+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:50:07.952+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:50:07.952+0000] {spark_submit.py:495} INFO - "name" : "Fossil Hard coal - Actual Aggregated",
[2023-01-30T18:50:07.953+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:50:07.953+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:50:07.954+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:50:07.954+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:50:07.955+0000] {spark_submit.py:495} INFO - "name" : "Fossil Oil - Actual Aggregated",
[2023-01-30T18:50:07.956+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:50:07.957+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:50:07.957+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:50:07.964+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:50:07.965+0000] {spark_submit.py:495} INFO - "name" : "Geothermal - Actual Aggregated",
[2023-01-30T18:50:07.965+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:50:07.966+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:50:07.973+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:50:08.053+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:50:08.054+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Aggregated",
[2023-01-30T18:50:08.055+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:50:08.055+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:50:08.056+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:50:08.056+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:50:08.057+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Consumption",
[2023-01-30T18:50:08.057+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:50:08.058+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:50:08.058+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:50:08.059+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:50:08.137+0000] {spark_submit.py:495} INFO - "name" : "Hydro Run-of-river and poundage - Actual Aggregated",
[2023-01-30T18:50:08.140+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:50:08.176+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:50:08.213+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:50:08.229+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:50:08.237+0000] {spark_submit.py:495} INFO - "name" : "Hydro Water Reservoir - Actual Aggregated",
[2023-01-30T18:50:08.238+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:50:08.240+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:50:08.247+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:50:08.256+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:50:08.262+0000] {spark_submit.py:495} INFO - "name" : "Nuclear - Actual Aggregated",
[2023-01-30T18:50:08.265+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:50:08.266+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:50:08.267+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:50:08.269+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:50:08.276+0000] {spark_submit.py:495} INFO - "name" : "Other - Actual Aggregated",
[2023-01-30T18:50:08.277+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:50:08.279+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:50:08.281+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:50:08.282+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:50:08.284+0000] {spark_submit.py:495} INFO - "name" : "Other renewable - Actual Aggregated",
[2023-01-30T18:50:08.290+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:50:08.291+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:50:08.292+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:50:08.297+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:50:08.381+0000] {spark_submit.py:495} INFO - "name" : "Solar - Actual Aggregated",
[2023-01-30T18:50:08.382+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:50:08.402+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:50:08.403+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:50:08.447+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:50:08.448+0000] {spark_submit.py:495} INFO - "name" : "Waste - Actual Aggregated",
[2023-01-30T18:50:08.449+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:50:08.515+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:50:08.517+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:50:08.518+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:50:08.555+0000] {spark_submit.py:495} INFO - "name" : "Wind Offshore - Actual Aggregated",
[2023-01-30T18:50:08.556+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:50:08.585+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:50:08.586+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:50:08.686+0000] {spark_submit.py:495} INFO - }, {
[2023-01-30T18:50:08.687+0000] {spark_submit.py:495} INFO - "name" : "Wind Onshore - Actual Aggregated",
[2023-01-30T18:50:08.693+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-30T18:50:08.696+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-30T18:50:08.743+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-30T18:50:08.750+0000] {spark_submit.py:495} INFO - } ]
[2023-01-30T18:50:08.766+0000] {spark_submit.py:495} INFO - }
[2023-01-30T18:50:08.974+0000] {spark_submit.py:495} INFO - and corresponding Parquet message type:
[2023-01-30T18:50:09.004+0000] {spark_submit.py:495} INFO - message spark_schema {
[2023-01-30T18:50:09.008+0000] {spark_submit.py:495} INFO - required int96 measured_at;
[2023-01-30T18:50:09.056+0000] {spark_submit.py:495} INFO - required int96 created_at;
[2023-01-30T18:50:09.072+0000] {spark_submit.py:495} INFO - optional double Biomass - Actual Aggregated;
[2023-01-30T18:50:09.091+0000] {spark_submit.py:495} INFO - optional double Fossil Brown coal/Lignite - Actual Aggregated;
[2023-01-30T18:50:09.093+0000] {spark_submit.py:495} INFO - optional double Fossil Gas - Actual Aggregated;
[2023-01-30T18:50:09.094+0000] {spark_submit.py:495} INFO - optional double Fossil Hard coal - Actual Aggregated;
[2023-01-30T18:50:09.113+0000] {spark_submit.py:495} INFO - optional double Fossil Oil - Actual Aggregated;
[2023-01-30T18:50:09.115+0000] {spark_submit.py:495} INFO - optional double Geothermal - Actual Aggregated;
[2023-01-30T18:50:09.116+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Aggregated;
[2023-01-30T18:50:09.118+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Consumption;
[2023-01-30T18:50:09.118+0000] {spark_submit.py:495} INFO - optional double Hydro Run-of-river and poundage - Actual Aggregated;
[2023-01-30T18:50:09.118+0000] {spark_submit.py:495} INFO - optional double Hydro Water Reservoir - Actual Aggregated;
[2023-01-30T18:50:09.128+0000] {spark_submit.py:495} INFO - optional double Nuclear - Actual Aggregated;
[2023-01-30T18:50:09.129+0000] {spark_submit.py:495} INFO - optional double Other - Actual Aggregated;
[2023-01-30T18:50:09.130+0000] {spark_submit.py:495} INFO - optional double Other renewable - Actual Aggregated;
[2023-01-30T18:50:09.130+0000] {spark_submit.py:495} INFO - optional double Solar - Actual Aggregated;
[2023-01-30T18:50:09.131+0000] {spark_submit.py:495} INFO - optional double Waste - Actual Aggregated;
[2023-01-30T18:50:09.132+0000] {spark_submit.py:495} INFO - optional double Wind Offshore - Actual Aggregated;
[2023-01-30T18:50:09.132+0000] {spark_submit.py:495} INFO - optional double Wind Onshore - Actual Aggregated;
[2023-01-30T18:50:09.133+0000] {spark_submit.py:495} INFO - }
[2023-01-30T18:50:09.133+0000] {spark_submit.py:495} INFO - 
[2023-01-30T18:50:09.134+0000] {spark_submit.py:495} INFO - 
[2023-01-30T18:50:13.123+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:13 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 767c6fe0d003:38739 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-30T18:50:13.219+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:13 INFO CodecPool: Got brand-new compressor [.snappy]
[2023-01-30T18:50:24.980+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:24 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675104342795-414e3e78-9fc2-4b56-bc19-bb25d498bce7/_temporary/0/_temporary/' directory.
[2023-01-30T18:50:24.984+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:24 INFO FileOutputCommitter: Saved output of task 'attempt_202301301850006699081924782084917_0008_m_000000_8' to gs://entsoe_temp_1009/.spark-bigquery-local-1675104342795-414e3e78-9fc2-4b56-bc19-bb25d498bce7/_temporary/0/task_202301301850006699081924782084917_0008_m_000000
[2023-01-30T18:50:25.010+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:25 INFO SparkHadoopMapRedUtil: attempt_202301301850006699081924782084917_0008_m_000000_8: Committed. Elapsed time: 2552 ms.
[2023-01-30T18:50:25.185+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:25 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2588 bytes result sent to driver
[2023-01-30T18:50:25.200+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:25 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 21072 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-30T18:50:25.201+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:25 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2023-01-30T18:50:25.223+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:25 INFO DAGScheduler: ResultStage 8 (save at BigQueryWriteHelper.java:105) finished in 23.142 s
[2023-01-30T18:50:25.224+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:25 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-30T18:50:25.224+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2023-01-30T18:50:25.224+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:25 INFO DAGScheduler: Job 8 finished: save at BigQueryWriteHelper.java:105, took 23.215425 s
[2023-01-30T18:50:25.224+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:25 INFO FileFormatWriter: Start to commit write Job 2799871c-c558-4137-b00a-96c41078f225.
[2023-01-30T18:50:26.507+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:26 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675104342795-414e3e78-9fc2-4b56-bc19-bb25d498bce7/_temporary/0/task_202301301850006699081924782084917_0008_m_000000/' directory.
[2023-01-30T18:50:26.978+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:26 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675104342795-414e3e78-9fc2-4b56-bc19-bb25d498bce7/' directory.
[2023-01-30T18:50:27.602+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:27 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 767c6fe0d003:38739 in memory (size: 74.2 KiB, free: 434.4 MiB)
[2023-01-30T18:50:27.851+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:27 INFO FileFormatWriter: Write Job 2799871c-c558-4137-b00a-96c41078f225 committed. Elapsed time: 2574 ms.
[2023-01-30T18:50:27.899+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:27 INFO FileFormatWriter: Finished processing stats for write job 2799871c-c558-4137-b00a-96c41078f225.
[2023-01-30T18:50:30.222+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:30 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=entsoe_analytics_1009, projectId=rafzul-analytics-1009, tableId=TEST_total_generation_staging}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=null, ignoreUnknownValue=null, sourceUris=[gs://entsoe_temp_1009/.spark-bigquery-local-1675104342795-414e3e78-9fc2-4b56-bc19-bb25d498bce7/part-00000-dcc583c4-001e-4709-983b-36627fe2c9af-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=true, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=rafzul-analytics-1009, job=5eb5bef9-c0fb-43a3-a580-5cbdcbb608ce, location=US}
[2023-01-30T18:50:35.363+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:35 INFO BigQueryClient: Done loading to rafzul-analytics-1009.entsoe_analytics_1009.TEST_total_generation_staging. jobId: JobId{project=rafzul-analytics-1009, job=5eb5bef9-c0fb-43a3-a580-5cbdcbb608ce, location=US}
[2023-01-30T18:50:36.491+0000] {spark_submit.py:495} INFO - MANTEP!!!!
[2023-01-30T18:50:36.973+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:36 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-30T18:50:37.016+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:37 INFO SparkUI: Stopped Spark web UI at http://767c6fe0d003:4043
[2023-01-30T18:50:37.070+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:37 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-30T18:50:37.143+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:37 INFO MemoryStore: MemoryStore cleared
[2023-01-30T18:50:37.147+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:37 INFO BlockManager: BlockManager stopped
[2023-01-30T18:50:37.164+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:37 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-30T18:50:37.185+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:37 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-30T18:50:37.212+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:37 INFO SparkContext: Successfully stopped SparkContext
[2023-01-30T18:50:37.213+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:37 INFO ShutdownHookManager: Shutdown hook called
[2023-01-30T18:50:37.215+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-c73f2945-7173-462d-a976-230308f3462c/pyspark-3a40a57f-01ca-4b04-9890-807960527527
[2023-01-30T18:50:37.233+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-337164ca-f3d8-4d9b-9e36-bf75e8a7dfe2
[2023-01-30T18:50:37.252+0000] {spark_submit.py:495} INFO - 23/01/30 18:50:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-c73f2945-7173-462d-a976-230308f3462c
[2023-01-30T18:50:37.644+0000] {taskinstance.py:1322} INFO - Marking task as SUCCESS. dag_id=entsoe-energydata, task_id=stage_total_generation, execution_date=20210101T000000, start_date=20230130T184422, end_date=20230130T185037
[2023-01-30T18:50:37.749+0000] {local_task_job.py:159} INFO - Task exited with return code 0
[2023-01-30T18:50:37.803+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T01:10:47.194+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-31T01:10:47.230+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-31T01:10:47.232+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T01:10:47.235+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T01:10:47.240+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T01:10:47.313+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 00:00:00+00:00
[2023-01-31T01:10:47.351+0000] {standard_task_runner.py:55} INFO - Started process 1770 to run task
[2023-01-31T01:10:47.378+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata', 'stage_total_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '685', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpobdm5cxk']
[2023-01-31T01:10:47.388+0000] {standard_task_runner.py:83} INFO - Job 685: Subtask stage_total_generation
[2023-01-31T01:10:47.711+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 767c6fe0d003
[2023-01-31T01:10:48.007+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-31T01:10:48.046+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T01:10:48.055+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-31T01:11:36.049+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:36 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T01:11:36.876+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T01:11:37.867+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:37 INFO ResourceUtils: ==============================================================
[2023-01-31T01:11:37.874+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:37 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T01:11:37.884+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:37 INFO ResourceUtils: ==============================================================
[2023-01-31T01:11:37.917+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:37 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T01:11:38.062+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:38 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T01:11:38.074+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:38 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T01:11:38.078+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:38 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T01:11:38.558+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:38 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T01:11:38.559+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:38 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T01:11:38.559+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:38 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T01:11:38.572+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:38 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T01:11:38.587+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:38 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T01:11:42.032+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:42 INFO Utils: Successfully started service 'sparkDriver' on port 44869.
[2023-01-31T01:11:42.365+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:42 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T01:11:42.684+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:42 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T01:11:42.844+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:42 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T01:11:42.847+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:42 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T01:11:42.879+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:42 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T01:11:43.054+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:43 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d2fd4662-4b04-4c84-8731-fec931f8db2a
[2023-01-31T01:11:43.303+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:43 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T01:11:43.685+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:43 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T01:11:45.568+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:45 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T01:11:45.570+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:45 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2023-01-31T01:11:45.572+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:45 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[2023-01-31T01:11:45.578+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:45 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[2023-01-31T01:11:45.579+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:45 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[2023-01-31T01:11:45.618+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:45 INFO Utils: Successfully started service 'SparkUI' on port 4045.
[2023-01-31T01:11:45.792+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:45 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://767c6fe0d003:44869/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675127495825
[2023-01-31T01:11:45.795+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:45 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://767c6fe0d003:44869/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675127495825
[2023-01-31T01:11:46.406+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:46 INFO Executor: Starting executor ID driver on host 767c6fe0d003
[2023-01-31T01:11:46.449+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:46 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T01:11:46.630+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:46 INFO Executor: Fetching spark://767c6fe0d003:44869/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675127495825
[2023-01-31T01:11:47.054+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:47 INFO TransportClientFactory: Successfully created connection to 767c6fe0d003/172.19.0.6:44869 after 295 ms (0 ms spent in bootstraps)
[2023-01-31T01:11:47.094+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:47 INFO Utils: Fetching spark://767c6fe0d003:44869/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-f4459ab4-a409-4038-b53a-9ac8290f695b/userFiles-d288c9a8-0063-4372-aa7d-91d302ae5447/fetchFileTemp14221502919827788721.tmp
[2023-01-31T01:11:48.337+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:48 INFO Executor: Adding file:/tmp/spark-f4459ab4-a409-4038-b53a-9ac8290f695b/userFiles-d288c9a8-0063-4372-aa7d-91d302ae5447/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T01:11:48.339+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:48 INFO Executor: Fetching spark://767c6fe0d003:44869/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675127495825
[2023-01-31T01:11:48.341+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:48 INFO Utils: Fetching spark://767c6fe0d003:44869/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-f4459ab4-a409-4038-b53a-9ac8290f695b/userFiles-d288c9a8-0063-4372-aa7d-91d302ae5447/fetchFileTemp16892186410768599149.tmp
[2023-01-31T01:11:48.750+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:48 INFO Executor: Adding file:/tmp/spark-f4459ab4-a409-4038-b53a-9ac8290f695b/userFiles-d288c9a8-0063-4372-aa7d-91d302ae5447/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T01:11:48.791+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:48 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35147.
[2023-01-31T01:11:48.791+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:48 INFO NettyBlockTransferService: Server created on 767c6fe0d003:35147
[2023-01-31T01:11:48.792+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:48 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T01:11:48.814+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:48 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 767c6fe0d003, 35147, None)
[2023-01-31T01:11:48.838+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:48 INFO BlockManagerMasterEndpoint: Registering block manager 767c6fe0d003:35147 with 434.4 MiB RAM, BlockManagerId(driver, 767c6fe0d003, 35147, None)
[2023-01-31T01:11:48.857+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:48 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 767c6fe0d003, 35147, None)
[2023-01-31T01:11:48.859+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:48 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 767c6fe0d003, 35147, None)
[2023-01-31T01:11:52.262+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:52 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T01:11:52.274+0000] {spark_submit.py:495} INFO - 23/01/31 01:11:52 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T01:12:00.631+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:00 INFO InMemoryFileIndex: It took 376 ms to list leaf files for 1 paths.
[2023-01-31T01:12:01.634+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:01 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T01:12:01.943+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:01 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T01:12:02.002+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:02 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 767c6fe0d003:35147 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T01:12:02.079+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:02 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T01:12:04.643+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:04 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T01:12:04.734+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:04 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T01:12:05.054+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:05 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T01:12:05.265+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:05 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T01:12:05.283+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:05 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T01:12:05.310+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:05 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T01:12:05.366+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:05 INFO DAGScheduler: Missing parents: List()
[2023-01-31T01:12:05.423+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:05 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T01:12:06.273+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:06 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T01:12:06.377+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:06 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T01:12:06.379+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:06 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 767c6fe0d003:35147 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T01:12:06.397+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:06 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T01:12:06.626+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T01:12:06.627+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:06 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T01:12:06.964+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:06 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T01:12:07.249+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:07 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T01:12:08.190+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:08 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010000__202101010100.json:0+9369
[2023-01-31T01:12:09.254+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:09 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3437 bytes result sent to driver
[2023-01-31T01:12:09.381+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:09 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2472 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-31T01:12:09.408+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:09 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T01:12:09.521+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:09 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 3.613 s
[2023-01-31T01:12:09.638+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:09 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T01:12:09.647+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T01:12:09.679+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:09 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 4.617075 s
[2023-01-31T01:12:21.406+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:21 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 767c6fe0d003:35147 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T01:12:21.542+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:21 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 767c6fe0d003:35147 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T01:12:55.386+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:55 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T01:12:55.394+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:55 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T01:12:55.398+0000] {spark_submit.py:495} INFO - 23/01/31 01:12:55 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T01:13:00.122+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:00 INFO CodeGenerator: Code generated in 2770.378683 ms
[2023-01-31T01:13:00.142+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:00 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T01:13:00.205+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:00 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T01:13:00.206+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:00 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 767c6fe0d003:35147 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T01:13:00.255+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:00 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T01:13:00.370+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:00 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T01:13:01.069+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:01 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T01:13:01.073+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:01 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T01:13:01.073+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:01 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T01:13:01.073+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:01 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T01:13:01.073+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:01 INFO DAGScheduler: Missing parents: List()
[2023-01-31T01:13:01.073+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:01 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T01:13:01.108+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:01 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T01:13:01.115+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:01 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T01:13:01.117+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:01 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 767c6fe0d003:35147 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T01:13:01.125+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:01 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T01:13:01.126+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T01:13:01.127+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:01 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T01:13:01.141+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:01 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T01:13:01.155+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:01 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T01:13:02.427+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:02 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T01:13:04.169+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:04 INFO CodeGenerator: Code generated in 1560.840556 ms
[2023-01-31T01:13:05.666+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:05 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1671 bytes result sent to driver
[2023-01-31T01:13:05.704+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:05 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 4565 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-31T01:13:05.704+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:05 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T01:13:05.709+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:05 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 4.611 s
[2023-01-31T01:13:05.709+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:05 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T01:13:05.710+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T01:13:05.710+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:05 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 4.640470 s
[2023-01-31T01:13:06.378+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:06 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T01:13:06.379+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:06 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T01:13:06.379+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:06 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T01:13:06.453+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:06 INFO CodeGenerator: Code generated in 59.143695 ms
[2023-01-31T01:13:06.530+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:06 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T01:13:06.601+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:06 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T01:13:06.624+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:06 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 767c6fe0d003:35147 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T01:13:06.629+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:06 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T01:13:06.635+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:06 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T01:13:06.902+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:06 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T01:13:06.907+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:06 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T01:13:06.907+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:06 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T01:13:06.907+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:06 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T01:13:06.908+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:06 INFO DAGScheduler: Missing parents: List()
[2023-01-31T01:13:06.926+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:06 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T01:13:06.934+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:06 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 767c6fe0d003:35147 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T01:13:06.940+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:06 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T01:13:06.954+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:06 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T01:13:06.959+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:06 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 767c6fe0d003:35147 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T01:13:06.961+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:06 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T01:13:06.964+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T01:13:06.966+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:06 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T01:13:06.978+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:06 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T01:13:06.982+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:06 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T01:13:07.023+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:07 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T01:13:07.439+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:07 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1585 bytes result sent to driver
[2023-01-31T01:13:07.450+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:07 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 474 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-31T01:13:07.452+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:07 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T01:13:07.470+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:07 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 0.541 s
[2023-01-31T01:13:07.471+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:07 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T01:13:07.471+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T01:13:07.471+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:07 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 0.571084 s
[2023-01-31T01:13:08.584+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:08 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T01:13:08.602+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:08 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T01:13:08.622+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:08 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T01:13:09.466+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:09 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 767c6fe0d003:35147 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T01:13:10.238+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:10 INFO CodeGenerator: Code generated in 898.640236 ms
[2023-01-31T01:13:10.242+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:10 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T01:13:10.266+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:10 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T01:13:10.270+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:10 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 767c6fe0d003:35147 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T01:13:10.284+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:10 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T01:13:10.285+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:10 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T01:13:10.340+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:10 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T01:13:10.352+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:10 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T01:13:10.365+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:10 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T01:13:10.368+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:10 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T01:13:10.369+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:10 INFO DAGScheduler: Missing parents: List()
[2023-01-31T01:13:10.404+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:10 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T01:13:10.510+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:10 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.7 MiB)
[2023-01-31T01:13:10.545+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:10 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.7 MiB)
[2023-01-31T01:13:10.550+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:10 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 767c6fe0d003:35147 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T01:13:10.551+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:10 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T01:13:10.557+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T01:13:10.557+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:10 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T01:13:10.557+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:10 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T01:13:10.560+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:10 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T01:13:10.684+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:10 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T01:13:11.077+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:11 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1755 bytes result sent to driver
[2023-01-31T01:13:11.098+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:11 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 542 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-31T01:13:11.124+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:11 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T01:13:11.136+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:11 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 0.724 s
[2023-01-31T01:13:11.137+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:11 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T01:13:11.137+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T01:13:11.137+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:11 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 0.794071 s
[2023-01-31T01:13:11.610+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:11 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T01:13:11.611+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:11 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T01:13:11.611+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:11 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T01:13:11.634+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:11 INFO CodeGenerator: Code generated in 17.756243 ms
[2023-01-31T01:13:11.645+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:11 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.5 MiB)
[2023-01-31T01:13:11.676+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:11 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T01:13:11.685+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:11 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 767c6fe0d003:35147 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T01:13:11.687+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:11 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T01:13:11.691+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:11 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T01:13:11.879+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:11 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T01:13:11.879+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:11 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T01:13:11.880+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:11 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T01:13:11.880+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:11 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T01:13:11.880+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:11 INFO DAGScheduler: Missing parents: List()
[2023-01-31T01:13:11.906+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:11 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T01:13:11.931+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:11 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-31T01:13:11.964+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:11 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-01-31T01:13:11.971+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:11 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 767c6fe0d003:35147 (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T01:13:11.974+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:11 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T01:13:11.976+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T01:13:11.977+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:11 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T01:13:11.987+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:11 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T01:13:11.989+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:11 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T01:13:12.008+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:12 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T01:13:12.184+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:12 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 767c6fe0d003:35147 in memory (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T01:13:12.685+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:12 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1629 bytes result sent to driver
[2023-01-31T01:13:12.688+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:12 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 701 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-31T01:13:12.689+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:12 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T01:13:12.743+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:12 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 0.785 s
[2023-01-31T01:13:12.744+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:12 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T01:13:12.744+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T01:13:12.744+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:12 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 0.846086 s
[2023-01-31T01:13:14.116+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-31T01:13:14.117+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 266, in <module>
[2023-01-31T01:13:14.118+0000] {spark_submit.py:495} INFO - main(
[2023-01-31T01:13:14.118+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 243, in main
[2023-01-31T01:13:14.119+0000] {spark_submit.py:495} INFO - run_method = method(
[2023-01-31T01:13:14.119+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 196, in transform_generation
[2023-01-31T01:13:14.119+0000] {spark_submit.py:495} INFO - all_df = parse_datetimeindex(self.spark, df_ts, df_nonts, tz=None)
[2023-01-31T01:13:14.120+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/helpers/parsers.py", line 61, in parse_datetimeindex
[2023-01-31T01:13:14.120+0000] {spark_submit.py:495} INFO - date_index = spark.sql(
[2023-01-31T01:13:14.120+0000] {spark_submit.py:495} INFO - File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py", line 1034, in sql
[2023-01-31T01:13:14.120+0000] {spark_submit.py:495} INFO - File "/opt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py", line 1321, in __call__
[2023-01-31T01:13:14.120+0000] {spark_submit.py:495} INFO - File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 196, in deco
[2023-01-31T01:13:14.184+0000] {spark_submit.py:495} INFO - pyspark.sql.utils.ParseException:
[2023-01-31T01:13:14.187+0000] {spark_submit.py:495} INFO - Syntax error at or near ':': extra input ':'(line 1, pos 59)
[2023-01-31T01:13:14.188+0000] {spark_submit.py:495} INFO - 
[2023-01-31T01:13:14.188+0000] {spark_submit.py:495} INFO - == SQL ==
[2023-01-31T01:13:14.188+0000] {spark_submit.py:495} INFO - SELECT sequence(to_utc_timestamp('2021-01-01 00:00:00', +07:00), to_utc_timestamp('2021-01-01 01:00:00', +07:00), INTERVAL 15 MINUTES) as measured_at
[2023-01-31T01:13:14.188+0000] {spark_submit.py:495} INFO - -----------------------------------------------------------^^^
[2023-01-31T01:13:14.188+0000] {spark_submit.py:495} INFO - 
[2023-01-31T01:13:15.122+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:15 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-31T01:13:15.191+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:15 INFO SparkUI: Stopped Spark web UI at http://767c6fe0d003:4045
[2023-01-31T01:13:15.287+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:15 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-31T01:13:15.423+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:15 INFO MemoryStore: MemoryStore cleared
[2023-01-31T01:13:15.431+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:15 INFO BlockManager: BlockManager stopped
[2023-01-31T01:13:15.439+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:15 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-31T01:13:15.627+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:15 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-31T01:13:15.708+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:15 INFO SparkContext: Successfully stopped SparkContext
[2023-01-31T01:13:15.710+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:15 INFO ShutdownHookManager: Shutdown hook called
[2023-01-31T01:13:15.712+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:15 INFO ShutdownHookManager: Deleting directory /tmp/spark-f4459ab4-a409-4038-b53a-9ac8290f695b
[2023-01-31T01:13:15.778+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:15 INFO ShutdownHookManager: Deleting directory /tmp/spark-daabce28-de2c-480f-9525-4ee975a8e17d
[2023-01-31T01:13:15.824+0000] {spark_submit.py:495} INFO - 23/01/31 01:13:15 INFO ShutdownHookManager: Deleting directory /tmp/spark-f4459ab4-a409-4038-b53a-9ac8290f695b/pyspark-7bc6295c-b60f-4e34-8b79-cea7bfba2a31
[2023-01-31T01:13:16.871+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-31T01:13:16.889+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=entsoe-energydata, task_id=stage_total_generation, execution_date=20210101T000000, start_date=20230131T011047, end_date=20230131T011316
[2023-01-31T01:13:17.093+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 685 for task stage_total_generation (Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 1770)
[2023-01-31T01:13:17.151+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-31T01:13:17.202+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T01:22:21.503+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-31T01:22:21.525+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-31T01:22:21.525+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T01:22:21.526+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T01:22:21.526+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T01:22:21.551+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 00:00:00+00:00
[2023-01-31T01:22:21.564+0000] {standard_task_runner.py:55} INFO - Started process 4824 to run task
[2023-01-31T01:22:21.575+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata', 'stage_total_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '710', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp4e6k5x20']
[2023-01-31T01:22:21.580+0000] {standard_task_runner.py:83} INFO - Job 710: Subtask stage_total_generation
[2023-01-31T01:22:21.774+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 767c6fe0d003
[2023-01-31T01:22:22.032+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-31T01:22:22.075+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T01:22:22.077+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-31T01:22:50.914+0000] {spark_submit.py:495} INFO - 23/01/31 01:22:50 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T01:22:52.410+0000] {spark_submit.py:495} INFO - 23/01/31 01:22:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T01:22:53.493+0000] {spark_submit.py:495} INFO - 23/01/31 01:22:53 INFO ResourceUtils: ==============================================================
[2023-01-31T01:22:53.516+0000] {spark_submit.py:495} INFO - 23/01/31 01:22:53 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T01:22:53.540+0000] {spark_submit.py:495} INFO - 23/01/31 01:22:53 INFO ResourceUtils: ==============================================================
[2023-01-31T01:22:53.543+0000] {spark_submit.py:495} INFO - 23/01/31 01:22:53 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T01:22:53.903+0000] {spark_submit.py:495} INFO - 23/01/31 01:22:53 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T01:22:53.991+0000] {spark_submit.py:495} INFO - 23/01/31 01:22:53 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T01:22:54.016+0000] {spark_submit.py:495} INFO - 23/01/31 01:22:54 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T01:22:55.410+0000] {spark_submit.py:495} INFO - 23/01/31 01:22:55 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T01:22:55.424+0000] {spark_submit.py:495} INFO - 23/01/31 01:22:55 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T01:22:55.449+0000] {spark_submit.py:495} INFO - 23/01/31 01:22:55 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T01:22:55.453+0000] {spark_submit.py:495} INFO - 23/01/31 01:22:55 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T01:22:55.459+0000] {spark_submit.py:495} INFO - 23/01/31 01:22:55 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T01:22:58.839+0000] {spark_submit.py:495} INFO - 23/01/31 01:22:58 INFO Utils: Successfully started service 'sparkDriver' on port 38431.
[2023-01-31T01:22:59.139+0000] {spark_submit.py:495} INFO - 23/01/31 01:22:59 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T01:22:59.802+0000] {spark_submit.py:495} INFO - 23/01/31 01:22:59 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T01:23:00.390+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:00 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T01:23:00.414+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:00 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T01:23:00.498+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:00 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T01:23:01.187+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:01 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-2e694f54-2ff7-4454-af5a-a1eacae94b76
[2023-01-31T01:23:01.493+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:01 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T01:23:01.755+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:01 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T01:23:04.624+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:04 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T01:23:04.627+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:04 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2023-01-31T01:23:04.638+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:04 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[2023-01-31T01:23:04.652+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:04 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[2023-01-31T01:23:04.662+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:04 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[2023-01-31T01:23:04.663+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:04 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
[2023-01-31T01:23:04.787+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:04 INFO Utils: Successfully started service 'SparkUI' on port 4046.
[2023-01-31T01:23:05.306+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:05 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://767c6fe0d003:38431/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675128170485
[2023-01-31T01:23:05.316+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:05 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://767c6fe0d003:38431/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675128170485
[2023-01-31T01:23:06.373+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:06 INFO Executor: Starting executor ID driver on host 767c6fe0d003
[2023-01-31T01:23:06.459+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:06 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T01:23:06.571+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:06 INFO Executor: Fetching spark://767c6fe0d003:38431/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675128170485
[2023-01-31T01:23:07.238+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:07 INFO TransportClientFactory: Successfully created connection to 767c6fe0d003/172.19.0.6:38431 after 456 ms (0 ms spent in bootstraps)
[2023-01-31T01:23:07.326+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:07 INFO Utils: Fetching spark://767c6fe0d003:38431/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-49b76f13-35a1-4dea-9d76-9049cd5af5d3/userFiles-6cf76b38-7743-47b3-afc1-f7a3e44460a8/fetchFileTemp1028384534971342097.tmp
[2023-01-31T01:23:09.827+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:09 INFO Executor: Adding file:/tmp/spark-49b76f13-35a1-4dea-9d76-9049cd5af5d3/userFiles-6cf76b38-7743-47b3-afc1-f7a3e44460a8/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T01:23:09.828+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:09 INFO Executor: Fetching spark://767c6fe0d003:38431/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675128170485
[2023-01-31T01:23:09.830+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:09 INFO Utils: Fetching spark://767c6fe0d003:38431/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-49b76f13-35a1-4dea-9d76-9049cd5af5d3/userFiles-6cf76b38-7743-47b3-afc1-f7a3e44460a8/fetchFileTemp5305815905218270628.tmp
[2023-01-31T01:23:11.050+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:11 INFO Executor: Adding file:/tmp/spark-49b76f13-35a1-4dea-9d76-9049cd5af5d3/userFiles-6cf76b38-7743-47b3-afc1-f7a3e44460a8/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T01:23:11.183+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:11 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37381.
[2023-01-31T01:23:11.183+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:11 INFO NettyBlockTransferService: Server created on 767c6fe0d003:37381
[2023-01-31T01:23:11.186+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:11 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T01:23:11.206+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:11 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 767c6fe0d003, 37381, None)
[2023-01-31T01:23:11.219+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:11 INFO BlockManagerMasterEndpoint: Registering block manager 767c6fe0d003:37381 with 434.4 MiB RAM, BlockManagerId(driver, 767c6fe0d003, 37381, None)
[2023-01-31T01:23:11.240+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:11 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 767c6fe0d003, 37381, None)
[2023-01-31T01:23:11.284+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:11 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 767c6fe0d003, 37381, None)
[2023-01-31T01:23:16.954+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:16 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T01:23:17.030+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:17 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T01:23:30.173+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:30 INFO InMemoryFileIndex: It took 326 ms to list leaf files for 1 paths.
[2023-01-31T01:23:30.607+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:30 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T01:23:30.811+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:30 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T01:23:30.818+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:30 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 767c6fe0d003:37381 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T01:23:30.827+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:30 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T01:23:32.286+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:32 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T01:23:32.362+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:32 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T01:23:32.452+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:32 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T01:23:32.490+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:32 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T01:23:32.490+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:32 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T01:23:32.490+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:32 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T01:23:32.495+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:32 INFO DAGScheduler: Missing parents: List()
[2023-01-31T01:23:32.501+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:32 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T01:23:32.712+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:32 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T01:23:32.724+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:32 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T01:23:32.742+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:32 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 767c6fe0d003:37381 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T01:23:32.744+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:32 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T01:23:33.010+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T01:23:33.019+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:33 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T01:23:33.692+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:33 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T01:23:33.868+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:33 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T01:23:34.312+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:34 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010000__202101010100.json:0+9369
[2023-01-31T01:23:35.765+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:35 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3437 bytes result sent to driver
[2023-01-31T01:23:35.850+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:35 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2307 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-31T01:23:35.865+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:35 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T01:23:35.897+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:35 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 3.293 s
[2023-01-31T01:23:35.921+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:35 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T01:23:35.926+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T01:23:35.936+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:35 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 3.487421 s
[2023-01-31T01:23:37.666+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:37 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 767c6fe0d003:37381 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T01:23:37.692+0000] {spark_submit.py:495} INFO - 23/01/31 01:23:37 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 767c6fe0d003:37381 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T01:24:00.702+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:00 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T01:24:00.707+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:00 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T01:24:00.714+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:00 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T01:24:05.836+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:05 INFO CodeGenerator: Code generated in 2471.637319 ms
[2023-01-31T01:24:05.979+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:05 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T01:24:06.443+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:06 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T01:24:06.448+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:06 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 767c6fe0d003:37381 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T01:24:06.503+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:06 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T01:24:06.574+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:06 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T01:24:06.803+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:06 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T01:24:06.803+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:06 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T01:24:06.804+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:06 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T01:24:06.805+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:06 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T01:24:06.806+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:06 INFO DAGScheduler: Missing parents: List()
[2023-01-31T01:24:06.806+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:06 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T01:24:06.837+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:06 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T01:24:06.848+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:06 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T01:24:06.870+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:06 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 767c6fe0d003:37381 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T01:24:06.888+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:06 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T01:24:06.894+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T01:24:06.903+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:06 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T01:24:06.927+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:06 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T01:24:06.938+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:06 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T01:24:07.890+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:07 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T01:24:10.618+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:10 INFO CodeGenerator: Code generated in 2258.136651 ms
[2023-01-31T01:24:13.027+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:13 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1628 bytes result sent to driver
[2023-01-31T01:24:13.070+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:13 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 6150 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-31T01:24:13.071+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:13 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 6.249 s
[2023-01-31T01:24:13.072+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:13 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T01:24:13.083+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:13 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T01:24:13.084+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T01:24:13.086+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:13 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 6.285462 s
[2023-01-31T01:24:13.673+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:13 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T01:24:13.677+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:13 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T01:24:13.682+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:13 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T01:24:14.024+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:14 INFO CodeGenerator: Code generated in 172.191705 ms
[2023-01-31T01:24:14.036+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:14 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T01:24:14.077+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:14 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T01:24:14.080+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:14 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 767c6fe0d003:37381 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T01:24:14.098+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:14 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T01:24:14.105+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:14 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T01:24:14.145+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:14 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T01:24:14.148+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:14 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T01:24:14.148+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:14 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T01:24:14.149+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:14 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T01:24:14.149+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:14 INFO DAGScheduler: Missing parents: List()
[2023-01-31T01:24:14.158+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:14 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T01:24:14.202+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:14 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T01:24:14.238+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:14 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T01:24:14.239+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:14 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 767c6fe0d003:37381 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T01:24:14.246+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:14 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T01:24:14.252+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T01:24:14.253+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:14 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T01:24:14.263+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:14 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T01:24:14.267+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:14 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T01:24:14.421+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:14 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T01:24:15.961+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:15 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1585 bytes result sent to driver
[2023-01-31T01:24:15.985+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:15 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1725 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-31T01:24:16.076+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:16 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 1.869 s
[2023-01-31T01:24:16.077+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:16 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T01:24:16.175+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:16 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T01:24:16.195+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T01:24:16.195+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:16 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 2.032258 s
[2023-01-31T01:24:19.102+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:19 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T01:24:19.162+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:19 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T01:24:19.194+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:19 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T01:24:21.429+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:21 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 767c6fe0d003:37381 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T01:24:21.706+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:21 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 767c6fe0d003:37381 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T01:24:23.032+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:23 INFO CodeGenerator: Code generated in 2885.178428 ms
[2023-01-31T01:24:23.270+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:23 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T01:24:24.187+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:24 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T01:24:24.199+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:24 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 767c6fe0d003:37381 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T01:24:24.233+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:24 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T01:24:24.292+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:24 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T01:24:24.826+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:24 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T01:24:24.835+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:24 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T01:24:24.836+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:24 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T01:24:24.838+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:24 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T01:24:24.839+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:24 INFO DAGScheduler: Missing parents: List()
[2023-01-31T01:24:24.852+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:24 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T01:24:24.942+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:24 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.7 MiB)
[2023-01-31T01:24:25.056+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:25 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.7 MiB)
[2023-01-31T01:24:25.076+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:25 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 767c6fe0d003:37381 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T01:24:25.096+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:25 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T01:24:25.109+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T01:24:25.109+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:25 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T01:24:25.118+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:25 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T01:24:25.202+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:25 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T01:24:25.203+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:25 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T01:24:25.878+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:25 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-01-31T01:24:25.941+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:25 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 829 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-31T01:24:25.950+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:25 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T01:24:25.950+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:25 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 1.081 s
[2023-01-31T01:24:25.951+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:25 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T01:24:25.963+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T01:24:25.964+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:25 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 1.128591 s
[2023-01-31T01:24:26.789+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:26 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T01:24:26.815+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:26 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T01:24:26.819+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:26 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T01:24:27.726+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:27 INFO CodeGenerator: Code generated in 460.85765 ms
[2023-01-31T01:24:27.834+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:27 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.5 MiB)
[2023-01-31T01:24:28.156+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:28 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T01:24:28.162+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:28 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 767c6fe0d003:37381 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T01:24:28.172+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:28 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T01:24:28.198+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:28 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T01:24:28.612+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:28 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T01:24:28.624+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:28 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T01:24:28.625+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:28 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T01:24:28.625+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:28 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T01:24:28.626+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:28 INFO DAGScheduler: Missing parents: List()
[2023-01-31T01:24:28.639+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:28 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T01:24:28.694+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:28 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-31T01:24:28.733+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:28 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-01-31T01:24:28.738+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:28 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 767c6fe0d003:37381 (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T01:24:28.750+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:28 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T01:24:28.767+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T01:24:28.777+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:28 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T01:24:28.785+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:28 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T01:24:28.786+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:28 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T01:24:29.023+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:29 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T01:24:30.272+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:30 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1586 bytes result sent to driver
[2023-01-31T01:24:30.321+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:30 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 1531 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-31T01:24:30.322+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:30 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T01:24:30.332+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:30 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 1.689 s
[2023-01-31T01:24:30.332+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:30 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T01:24:30.343+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T01:24:30.344+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:30 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 1.718167 s
[2023-01-31T01:24:32.082+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-31T01:24:32.082+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 266, in <module>
[2023-01-31T01:24:32.083+0000] {spark_submit.py:495} INFO - main(
[2023-01-31T01:24:32.084+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 243, in main
[2023-01-31T01:24:32.084+0000] {spark_submit.py:495} INFO - run_method = method(
[2023-01-31T01:24:32.085+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 196, in transform_generation
[2023-01-31T01:24:32.085+0000] {spark_submit.py:495} INFO - all_df = parse_datetimeindex(self.spark, df_ts, df_nonts, tz=None)
[2023-01-31T01:24:32.086+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/helpers/parsers.py", line 61, in parse_datetimeindex
[2023-01-31T01:24:32.086+0000] {spark_submit.py:495} INFO - date_index = spark.sql(
[2023-01-31T01:24:32.086+0000] {spark_submit.py:495} INFO - File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py", line 1034, in sql
[2023-01-31T01:24:32.087+0000] {spark_submit.py:495} INFO - File "/opt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py", line 1321, in __call__
[2023-01-31T01:24:32.091+0000] {spark_submit.py:495} INFO - File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 196, in deco
[2023-01-31T01:24:32.144+0000] {spark_submit.py:495} INFO - pyspark.sql.utils.ParseException:
[2023-01-31T01:24:32.145+0000] {spark_submit.py:495} INFO - Syntax error at or near ':': extra input ':'(line 1, pos 59)
[2023-01-31T01:24:32.145+0000] {spark_submit.py:495} INFO - 
[2023-01-31T01:24:32.146+0000] {spark_submit.py:495} INFO - == SQL ==
[2023-01-31T01:24:32.146+0000] {spark_submit.py:495} INFO - SELECT sequence(to_utc_timestamp('2021-01-01 01:00:00', +07:00), to_utc_timestamp('2021-01-01 02:00:00', +07:00), INTERVAL 15 MINUTES) as measured_at
[2023-01-31T01:24:32.147+0000] {spark_submit.py:495} INFO - -----------------------------------------------------------^^^
[2023-01-31T01:24:32.147+0000] {spark_submit.py:495} INFO - 
[2023-01-31T01:24:33.289+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:33 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-31T01:24:33.566+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:33 INFO SparkUI: Stopped Spark web UI at http://767c6fe0d003:4046
[2023-01-31T01:24:34.039+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:34 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-31T01:24:34.252+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:34 INFO MemoryStore: MemoryStore cleared
[2023-01-31T01:24:34.259+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:34 INFO BlockManager: BlockManager stopped
[2023-01-31T01:24:34.315+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:34 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-31T01:24:34.388+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:34 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-31T01:24:34.460+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:34 INFO SparkContext: Successfully stopped SparkContext
[2023-01-31T01:24:34.465+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:34 INFO ShutdownHookManager: Shutdown hook called
[2023-01-31T01:24:34.474+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-184a339e-8704-4a5d-a5ef-f4530da0f4ab
[2023-01-31T01:24:34.507+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-49b76f13-35a1-4dea-9d76-9049cd5af5d3/pyspark-f67184de-c19e-41f7-9456-4f3a07f9dd33
[2023-01-31T01:24:34.562+0000] {spark_submit.py:495} INFO - 23/01/31 01:24:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-49b76f13-35a1-4dea-9d76-9049cd5af5d3
[2023-01-31T01:24:36.005+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-31T01:24:36.015+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=entsoe-energydata, task_id=stage_total_generation, execution_date=20210101T000000, start_date=20230131T012221, end_date=20230131T012436
[2023-01-31T01:24:36.101+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 710 for task stage_total_generation (Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 4824)
[2023-01-31T01:24:36.170+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-31T01:24:36.229+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T01:26:24.256+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-31T01:26:24.396+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-31T01:26:24.397+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T01:26:24.397+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T01:26:24.398+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T01:26:24.475+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 00:00:00+00:00
[2023-01-31T01:26:24.512+0000] {standard_task_runner.py:55} INFO - Started process 6504 to run task
[2023-01-31T01:26:24.523+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata', 'stage_total_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '734', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmptett5ios']
[2023-01-31T01:26:24.531+0000] {standard_task_runner.py:83} INFO - Job 734: Subtask stage_total_generation
[2023-01-31T01:26:24.796+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 767c6fe0d003
[2023-01-31T01:26:25.117+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-31T01:26:25.162+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T01:26:25.165+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-31T01:26:48.614+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:48 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T01:26:49.142+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T01:26:50.658+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:50 INFO ResourceUtils: ==============================================================
[2023-01-31T01:26:50.681+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:50 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T01:26:50.725+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:50 INFO ResourceUtils: ==============================================================
[2023-01-31T01:26:50.726+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:50 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T01:26:50.918+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:50 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T01:26:50.941+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:50 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T01:26:50.949+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:50 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T01:26:51.332+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:51 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T01:26:51.333+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:51 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T01:26:51.370+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:51 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T01:26:51.372+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:51 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T01:26:51.374+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:51 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T01:26:52.482+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:52 INFO Utils: Successfully started service 'sparkDriver' on port 36873.
[2023-01-31T01:26:52.914+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:52 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T01:26:53.205+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:53 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T01:26:53.312+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:53 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T01:26:53.314+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:53 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T01:26:53.362+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:53 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T01:26:53.572+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:53 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-58b2e511-de9c-4035-ab45-ee7d8782523e
[2023-01-31T01:26:53.630+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:53 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T01:26:53.726+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:53 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T01:26:55.362+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:55 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T01:26:55.380+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:55 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2023-01-31T01:26:55.490+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:55 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://767c6fe0d003:36873/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675128408573
[2023-01-31T01:26:55.504+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:55 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://767c6fe0d003:36873/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675128408573
[2023-01-31T01:26:55.894+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:55 INFO Executor: Starting executor ID driver on host 767c6fe0d003
[2023-01-31T01:26:55.958+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:55 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T01:26:56.054+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:56 INFO Executor: Fetching spark://767c6fe0d003:36873/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675128408573
[2023-01-31T01:26:56.306+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:56 INFO TransportClientFactory: Successfully created connection to 767c6fe0d003/172.19.0.6:36873 after 205 ms (0 ms spent in bootstraps)
[2023-01-31T01:26:56.345+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:56 INFO Utils: Fetching spark://767c6fe0d003:36873/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-7bac0889-c155-4985-b677-736493d0effa/userFiles-8eaa0585-a393-4039-bcba-511e36df9985/fetchFileTemp4094508166429842220.tmp
[2023-01-31T01:26:56.953+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:56 INFO Executor: Adding file:/tmp/spark-7bac0889-c155-4985-b677-736493d0effa/userFiles-8eaa0585-a393-4039-bcba-511e36df9985/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T01:26:56.953+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:56 INFO Executor: Fetching spark://767c6fe0d003:36873/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675128408573
[2023-01-31T01:26:56.955+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:56 INFO Utils: Fetching spark://767c6fe0d003:36873/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-7bac0889-c155-4985-b677-736493d0effa/userFiles-8eaa0585-a393-4039-bcba-511e36df9985/fetchFileTemp14956147694171402700.tmp
[2023-01-31T01:26:57.830+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:57 INFO Executor: Adding file:/tmp/spark-7bac0889-c155-4985-b677-736493d0effa/userFiles-8eaa0585-a393-4039-bcba-511e36df9985/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T01:26:57.854+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:57 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35855.
[2023-01-31T01:26:57.855+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:57 INFO NettyBlockTransferService: Server created on 767c6fe0d003:35855
[2023-01-31T01:26:57.858+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:57 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T01:26:57.882+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:57 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 767c6fe0d003, 35855, None)
[2023-01-31T01:26:57.934+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:57 INFO BlockManagerMasterEndpoint: Registering block manager 767c6fe0d003:35855 with 434.4 MiB RAM, BlockManagerId(driver, 767c6fe0d003, 35855, None)
[2023-01-31T01:26:57.947+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:57 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 767c6fe0d003, 35855, None)
[2023-01-31T01:26:57.954+0000] {spark_submit.py:495} INFO - 23/01/31 01:26:57 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 767c6fe0d003, 35855, None)
[2023-01-31T01:27:03.254+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:03 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T01:27:03.336+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:03 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T01:27:18.998+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:18 INFO InMemoryFileIndex: It took 870 ms to list leaf files for 1 paths.
[2023-01-31T01:27:19.949+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:19 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T01:27:20.580+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:20 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T01:27:20.593+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:20 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 767c6fe0d003:35855 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T01:27:20.607+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:20 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T01:27:24.940+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:24 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T01:27:25.227+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:25 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T01:27:25.723+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:25 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T01:27:25.985+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:25 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T01:27:25.987+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:25 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T01:27:25.992+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:25 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T01:27:26.005+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:26 INFO DAGScheduler: Missing parents: List()
[2023-01-31T01:27:26.093+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:26 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T01:27:27.318+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:27 INFO AsyncEventQueue: Process of event SparkListenerJobStart(0,1675128446019,ArraySeq(org.apache.spark.scheduler.StageInfo@19474926),{spark.master=local, spark.driver.port=36873, spark.submit.pyFiles=, spark.app.startTime=1675128408573, spark.rdd.compress=True, spark.executor.id=driver, spark.app.name=gcp_playground, spark.submit.deployMode=client, spark.hadoop.google.cloud.auth.service.account.json.keyfile=/.google/credentials/google_credentials.json, spark.driver.host=767c6fe0d003, spark.app.id=local-1675128415742, spark.app.submitTime=1675128403259, spark.hadoop.fs.AbstractFileSystem.gs.impl=google.cloud.hadoop.fs.gcs.GoogleHadoopFS, spark.sql.session.timeZone=UTC, spark.hadoop.google.cloud.auth.service.account.enable=true, spark.app.initial.jar.urls=spark://767c6fe0d003:36873/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar,spark://767c6fe0d003:36873/jars/gcs-connector-hadoop3-latest.jar, spark.executor.extraJavaOptions=-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED, spark.jars=/opt/spark/jars/gcs-connector-hadoop3-latest.jar, /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar, spark.sql.warehouse.dir=file:/opt/***/spark-warehouse, spark.serializer.objectStreamReset=100, spark.driver.extraJavaOptions=-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED}) by listener AppStatusListener took 1.114269102s.
[2023-01-31T01:27:28.130+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:28 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T01:27:28.197+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:28 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T01:27:28.198+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:28 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 767c6fe0d003:35855 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T01:27:28.202+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:28 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T01:27:28.251+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T01:27:28.258+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:28 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T01:27:29.128+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:29 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T01:27:29.468+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:29 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T01:27:31.248+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:31 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010000__202101010100.json:0+9369
[2023-01-31T01:27:34.071+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:34 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3437 bytes result sent to driver
[2023-01-31T01:27:34.474+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:34 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 5941 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-31T01:27:34.482+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:34 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T01:27:34.498+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:34 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 7.864 s
[2023-01-31T01:27:34.510+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:34 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T01:27:34.511+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T01:27:34.518+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:34 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 8.790065 s
[2023-01-31T01:27:37.682+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:37 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 767c6fe0d003:35855 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T01:27:37.826+0000] {spark_submit.py:495} INFO - 23/01/31 01:27:37 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 767c6fe0d003:35855 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T01:28:07.109+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:07 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T01:28:07.158+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:07 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T01:28:07.211+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:07 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T01:28:09.738+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:09 INFO CodeGenerator: Code generated in 1015.166568 ms
[2023-01-31T01:28:09.798+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:09 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T01:28:09.861+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:09 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T01:28:09.868+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:09 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 767c6fe0d003:35855 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T01:28:09.874+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:09 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T01:28:09.949+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T01:28:10.174+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:10 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T01:28:10.181+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:10 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T01:28:10.181+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:10 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T01:28:10.181+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:10 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T01:28:10.182+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:10 INFO DAGScheduler: Missing parents: List()
[2023-01-31T01:28:10.194+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:10 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T01:28:10.225+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:10 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T01:28:10.243+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:10 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T01:28:10.247+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:10 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 767c6fe0d003:35855 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T01:28:10.252+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:10 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T01:28:10.257+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T01:28:10.262+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:10 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T01:28:10.281+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:10 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T01:28:10.290+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:10 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T01:28:10.393+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:10 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T01:28:10.704+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:10 INFO CodeGenerator: Code generated in 265.312768 ms
[2023-01-31T01:28:11.005+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:11 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1628 bytes result sent to driver
[2023-01-31T01:28:11.013+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:11 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 741 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-31T01:28:11.015+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:11 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 0.810 s
[2023-01-31T01:28:11.017+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:11 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T01:28:11.023+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:11 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T01:28:11.027+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T01:28:11.029+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:11 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 0.846063 s
[2023-01-31T01:28:11.187+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:11 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T01:28:11.190+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:11 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T01:28:11.191+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:11 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T01:28:11.297+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:11 INFO CodeGenerator: Code generated in 64.490576 ms
[2023-01-31T01:28:11.317+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:11 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T01:28:11.364+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:11 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T01:28:11.370+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:11 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 767c6fe0d003:35855 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T01:28:11.374+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:11 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T01:28:11.376+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:11 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T01:28:11.480+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:11 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T01:28:11.484+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:11 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T01:28:11.485+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:11 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T01:28:11.485+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:11 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T01:28:11.485+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:11 INFO DAGScheduler: Missing parents: List()
[2023-01-31T01:28:11.492+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:11 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T01:28:11.508+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:11 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T01:28:11.525+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:11 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T01:28:11.527+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:11 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 767c6fe0d003:35855 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T01:28:11.536+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:11 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T01:28:11.546+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T01:28:11.546+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:11 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T01:28:11.554+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:11 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T01:28:11.555+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:11 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T01:28:11.590+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:11 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T01:28:11.771+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:11 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1585 bytes result sent to driver
[2023-01-31T01:28:11.788+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:11 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 234 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-31T01:28:11.788+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:11 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T01:28:11.792+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:11 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 0.301 s
[2023-01-31T01:28:11.793+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:11 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T01:28:11.797+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T01:28:11.797+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:11 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 0.314172 s
[2023-01-31T01:28:12.087+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T01:28:12.093+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T01:28:12.099+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T01:28:12.512+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO CodeGenerator: Code generated in 232.032274 ms
[2023-01-31T01:28:12.531+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T01:28:12.607+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T01:28:12.610+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 767c6fe0d003:35855 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T01:28:12.613+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T01:28:12.647+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T01:28:12.793+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T01:28:12.795+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T01:28:12.796+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T01:28:12.798+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T01:28:12.799+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO DAGScheduler: Missing parents: List()
[2023-01-31T01:28:12.799+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T01:28:12.802+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.6 MiB)
[2023-01-31T01:28:12.824+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.6 MiB)
[2023-01-31T01:28:12.826+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 767c6fe0d003:35855 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T01:28:12.831+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T01:28:12.833+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T01:28:12.836+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T01:28:12.842+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T01:28:12.842+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T01:28:12.873+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:12 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T01:28:13.072+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:13 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-01-31T01:28:13.152+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:13 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 310 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-31T01:28:13.152+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:13 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T01:28:13.154+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:13 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 0.356 s
[2023-01-31T01:28:13.210+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:13 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T01:28:13.211+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T01:28:13.213+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:13 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 0.417534 s
[2023-01-31T01:28:13.230+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:13 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 767c6fe0d003:35855 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T01:28:13.345+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:13 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 767c6fe0d003:35855 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T01:28:13.418+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:13 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T01:28:13.419+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:13 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T01:28:13.422+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:13 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T01:28:13.522+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:13 INFO CodeGenerator: Code generated in 42.754729 ms
[2023-01-31T01:28:13.537+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:13 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.5 MiB)
[2023-01-31T01:28:13.595+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:13 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T01:28:13.600+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:13 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 767c6fe0d003:35855 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T01:28:13.605+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:13 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T01:28:13.605+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:13 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T01:28:13.713+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:13 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T01:28:13.714+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:13 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T01:28:13.714+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:13 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T01:28:13.714+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:13 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T01:28:13.714+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:13 INFO DAGScheduler: Missing parents: List()
[2023-01-31T01:28:13.720+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:13 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T01:28:13.758+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:13 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-31T01:28:13.758+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:13 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-01-31T01:28:13.759+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:13 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 767c6fe0d003:35855 (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T01:28:13.764+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:13 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T01:28:13.768+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T01:28:13.769+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:13 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T01:28:13.775+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:13 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (767c6fe0d003, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T01:28:13.779+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:13 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T01:28:13.808+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:13 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T01:28:13.948+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:13 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1586 bytes result sent to driver
[2023-01-31T01:28:13.952+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:13 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 177 ms on 767c6fe0d003 (executor driver) (1/1)
[2023-01-31T01:28:13.958+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:13 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T01:28:13.958+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:13 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 0.235 s
[2023-01-31T01:28:13.958+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:13 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T01:28:13.958+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T01:28:13.959+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:13 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 0.244094 s
[2023-01-31T01:28:14.228+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-31T01:28:14.228+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 266, in <module>
[2023-01-31T01:28:14.229+0000] {spark_submit.py:495} INFO - main(
[2023-01-31T01:28:14.229+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 243, in main
[2023-01-31T01:28:14.229+0000] {spark_submit.py:495} INFO - run_method = method(
[2023-01-31T01:28:14.229+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 196, in transform_generation
[2023-01-31T01:28:14.229+0000] {spark_submit.py:495} INFO - all_df = parse_datetimeindex(self.spark, df_ts, df_nonts, tz=None)
[2023-01-31T01:28:14.230+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/helpers/parsers.py", line 61, in parse_datetimeindex
[2023-01-31T01:28:14.230+0000] {spark_submit.py:495} INFO - date_index = spark.sql(
[2023-01-31T01:28:14.230+0000] {spark_submit.py:495} INFO - File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py", line 1034, in sql
[2023-01-31T01:28:14.230+0000] {spark_submit.py:495} INFO - File "/opt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py", line 1321, in __call__
[2023-01-31T01:28:14.230+0000] {spark_submit.py:495} INFO - File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 196, in deco
[2023-01-31T01:28:14.247+0000] {spark_submit.py:495} INFO - pyspark.sql.utils.ParseException:
[2023-01-31T01:28:14.248+0000] {spark_submit.py:495} INFO - Syntax error at or near ':': extra input ':'(line 1, pos 59)
[2023-01-31T01:28:14.248+0000] {spark_submit.py:495} INFO - 
[2023-01-31T01:28:14.248+0000] {spark_submit.py:495} INFO - == SQL ==
[2023-01-31T01:28:14.249+0000] {spark_submit.py:495} INFO - SELECT sequence(to_utc_timestamp('2021-01-01 00:00:00', +07:00), to_utc_timestamp('2021-01-01 01:00:00', +07:00), INTERVAL 15 MINUTES) as measured_at
[2023-01-31T01:28:14.249+0000] {spark_submit.py:495} INFO - -----------------------------------------------------------^^^
[2023-01-31T01:28:14.249+0000] {spark_submit.py:495} INFO - 
[2023-01-31T01:28:14.653+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:14 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-31T01:28:14.694+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:14 INFO SparkUI: Stopped Spark web UI at http://767c6fe0d003:4041
[2023-01-31T01:28:14.752+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:14 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-31T01:28:14.831+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:14 INFO MemoryStore: MemoryStore cleared
[2023-01-31T01:28:14.831+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:14 INFO BlockManager: BlockManager stopped
[2023-01-31T01:28:14.842+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:14 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-31T01:28:14.848+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:14 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-31T01:28:14.886+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:14 INFO SparkContext: Successfully stopped SparkContext
[2023-01-31T01:28:14.886+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:14 INFO ShutdownHookManager: Shutdown hook called
[2023-01-31T01:28:14.887+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:14 INFO ShutdownHookManager: Deleting directory /tmp/spark-a9ee553d-084a-43b8-a784-0b211ea333f8
[2023-01-31T01:28:14.908+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:14 INFO ShutdownHookManager: Deleting directory /tmp/spark-7bac0889-c155-4985-b677-736493d0effa
[2023-01-31T01:28:14.946+0000] {spark_submit.py:495} INFO - 23/01/31 01:28:14 INFO ShutdownHookManager: Deleting directory /tmp/spark-7bac0889-c155-4985-b677-736493d0effa/pyspark-3ad846e4-0bd0-485f-a33b-b125aef67af3
[2023-01-31T01:28:15.291+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-31T01:28:15.297+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=entsoe-energydata, task_id=stage_total_generation, execution_date=20210101T000000, start_date=20230131T012624, end_date=20230131T012815
[2023-01-31T01:28:15.322+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 734 for task stage_total_generation (Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 6504)
[2023-01-31T01:28:15.381+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-31T01:28:15.497+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T03:42:46.222+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-31T03:42:46.275+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-31T03:42:46.276+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T03:42:46.277+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T03:42:46.277+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T03:42:46.345+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 00:00:00+00:00
[2023-01-31T03:42:46.384+0000] {standard_task_runner.py:55} INFO - Started process 404 to run task
[2023-01-31T03:42:46.405+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata', 'stage_total_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '804', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpck_pf7in']
[2023-01-31T03:42:46.415+0000] {standard_task_runner.py:83} INFO - Job 804: Subtask stage_total_generation
[2023-01-31T03:42:46.747+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 2f101bc15e7a
[2023-01-31T03:42:47.117+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-31T03:42:47.154+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T03:42:47.158+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-31T03:44:12.704+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:12 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T03:44:14.755+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T03:44:20.617+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:20 INFO ResourceUtils: ==============================================================
[2023-01-31T03:44:20.622+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:20 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T03:44:20.625+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:20 INFO ResourceUtils: ==============================================================
[2023-01-31T03:44:20.629+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:20 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T03:44:21.481+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:21 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T03:44:21.516+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:21 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T03:44:21.524+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:21 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T03:44:24.015+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:24 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T03:44:24.026+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:24 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T03:44:24.032+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:24 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T03:44:24.037+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:24 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T03:44:24.042+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:24 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T03:44:36.248+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:36 INFO Utils: Successfully started service 'sparkDriver' on port 41105.
[2023-01-31T03:44:37.073+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:37 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T03:44:38.397+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:38 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T03:44:38.924+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:38 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T03:44:38.937+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:38 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T03:44:39.049+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:39 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T03:44:39.947+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:39 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-92973629-2506-4652-8b21-b099118ac161
[2023-01-31T03:44:40.696+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:40 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T03:44:41.354+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:41 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T03:44:43.838+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:43 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T03:44:43.852+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:43 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2023-01-31T03:44:43.854+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:43 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[2023-01-31T03:44:43.856+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:43 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[2023-01-31T03:44:43.857+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:43 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[2023-01-31T03:44:43.912+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:43 INFO Utils: Successfully started service 'SparkUI' on port 4045.
[2023-01-31T03:44:44.255+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:44 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://2f101bc15e7a:41105/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675136652424
[2023-01-31T03:44:44.260+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:44 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://2f101bc15e7a:41105/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675136652424
[2023-01-31T03:44:45.046+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:45 INFO Executor: Starting executor ID driver on host 2f101bc15e7a
[2023-01-31T03:44:45.215+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:45 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T03:44:45.746+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:45 INFO Executor: Fetching spark://2f101bc15e7a:41105/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675136652424
[2023-01-31T03:44:46.730+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:46 INFO TransportClientFactory: Successfully created connection to 2f101bc15e7a/172.19.0.4:41105 after 940 ms (0 ms spent in bootstraps)
[2023-01-31T03:44:47.075+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:47 INFO Utils: Fetching spark://2f101bc15e7a:41105/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-c1a58af4-b588-481a-9ca4-0eb1b206b6d0/userFiles-6f018a3f-93c2-48da-8895-27be1756d135/fetchFileTemp13909389044398329358.tmp
[2023-01-31T03:44:49.920+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:49 INFO Executor: Adding file:/tmp/spark-c1a58af4-b588-481a-9ca4-0eb1b206b6d0/userFiles-6f018a3f-93c2-48da-8895-27be1756d135/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T03:44:49.926+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:49 INFO Executor: Fetching spark://2f101bc15e7a:41105/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675136652424
[2023-01-31T03:44:49.927+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:49 INFO Utils: Fetching spark://2f101bc15e7a:41105/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-c1a58af4-b588-481a-9ca4-0eb1b206b6d0/userFiles-6f018a3f-93c2-48da-8895-27be1756d135/fetchFileTemp14688154306041808152.tmp
[2023-01-31T03:44:50.787+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:50 INFO Executor: Adding file:/tmp/spark-c1a58af4-b588-481a-9ca4-0eb1b206b6d0/userFiles-6f018a3f-93c2-48da-8895-27be1756d135/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T03:44:50.866+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:50 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39501.
[2023-01-31T03:44:50.867+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:50 INFO NettyBlockTransferService: Server created on 2f101bc15e7a:39501
[2023-01-31T03:44:50.873+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:50 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T03:44:50.947+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:50 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 2f101bc15e7a, 39501, None)
[2023-01-31T03:44:50.982+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:50 INFO BlockManagerMasterEndpoint: Registering block manager 2f101bc15e7a:39501 with 434.4 MiB RAM, BlockManagerId(driver, 2f101bc15e7a, 39501, None)
[2023-01-31T03:44:51.059+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:51 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 2f101bc15e7a, 39501, None)
[2023-01-31T03:44:51.072+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:51 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 2f101bc15e7a, 39501, None)
[2023-01-31T03:44:56.535+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:56 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T03:44:56.633+0000] {spark_submit.py:495} INFO - 23/01/31 03:44:56 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T03:45:55.437+0000] {spark_submit.py:495} INFO - 23/01/31 03:45:55 INFO InMemoryFileIndex: It took 1841 ms to list leaf files for 1 paths.
[2023-01-31T03:46:00.376+0000] {spark_submit.py:495} INFO - 23/01/31 03:46:00 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T03:46:02.152+0000] {spark_submit.py:495} INFO - 23/01/31 03:46:02 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T03:46:02.283+0000] {spark_submit.py:495} INFO - 23/01/31 03:46:02 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 2f101bc15e7a:39501 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T03:46:02.476+0000] {spark_submit.py:495} INFO - 23/01/31 03:46:02 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T03:46:19.830+0000] {spark_submit.py:495} INFO - 23/01/31 03:46:19 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T03:46:20.162+0000] {spark_submit.py:495} INFO - 23/01/31 03:46:20 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T03:46:20.327+0000] {spark_submit.py:495} INFO - 23/01/31 03:46:20 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T03:46:20.462+0000] {spark_submit.py:495} INFO - 23/01/31 03:46:20 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T03:46:20.471+0000] {spark_submit.py:495} INFO - 23/01/31 03:46:20 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T03:46:20.481+0000] {spark_submit.py:495} INFO - 23/01/31 03:46:20 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T03:46:20.542+0000] {spark_submit.py:495} INFO - 23/01/31 03:46:20 INFO DAGScheduler: Missing parents: List()
[2023-01-31T03:46:20.694+0000] {spark_submit.py:495} INFO - 23/01/31 03:46:20 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T03:46:22.111+0000] {spark_submit.py:495} INFO - 23/01/31 03:46:22 INFO AsyncEventQueue: Process of event SparkListenerJobStart(0,1675136780545,ArraySeq(org.apache.spark.scheduler.StageInfo@41565cc1),{spark.master=local, spark.driver.port=41105, spark.submit.pyFiles=, spark.app.startTime=1675136652424, spark.rdd.compress=True, spark.executor.id=driver, spark.app.name=gcp_playground, spark.submit.deployMode=client, spark.hadoop.google.cloud.auth.service.account.json.keyfile=/.google/credentials/google_credentials.json, spark.driver.host=2f101bc15e7a, spark.app.id=local-1675136684673, spark.app.submitTime=1675136615166, spark.hadoop.fs.AbstractFileSystem.gs.impl=google.cloud.hadoop.fs.gcs.GoogleHadoopFS, spark.sql.session.timeZone=UTC, spark.hadoop.google.cloud.auth.service.account.enable=true, spark.app.initial.jar.urls=spark://2f101bc15e7a:41105/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar,spark://2f101bc15e7a:41105/jars/gcs-connector-hadoop3-latest.jar, spark.executor.extraJavaOptions=-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED, spark.jars=/opt/spark/jars/gcs-connector-hadoop3-latest.jar, /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar, spark.sql.warehouse.dir=file:/opt/***/spark-warehouse, spark.serializer.objectStreamReset=100, spark.driver.extraJavaOptions=-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED}) by listener AppStatusListener took 1.30990765s.
[2023-01-31T03:46:23.592+0000] {spark_submit.py:495} INFO - 23/01/31 03:46:23 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T03:46:23.927+0000] {spark_submit.py:495} INFO - 23/01/31 03:46:23 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T03:46:23.984+0000] {spark_submit.py:495} INFO - 23/01/31 03:46:23 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 2f101bc15e7a:39501 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T03:46:24.092+0000] {spark_submit.py:495} INFO - 23/01/31 03:46:24 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T03:46:24.697+0000] {spark_submit.py:495} INFO - 23/01/31 03:46:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T03:46:24.705+0000] {spark_submit.py:495} INFO - 23/01/31 03:46:24 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T03:46:25.895+0000] {spark_submit.py:495} INFO - 23/01/31 03:46:25 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (2f101bc15e7a, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T03:46:26.214+0000] {spark_submit.py:495} INFO - 23/01/31 03:46:26 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T03:46:29.859+0000] {spark_submit.py:495} INFO - 23/01/31 03:46:29 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010000__202101010100.json:0+9369
[2023-01-31T03:46:33.423+0000] {spark_submit.py:495} INFO - 23/01/31 03:46:33 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3480 bytes result sent to driver
[2023-01-31T03:46:33.631+0000] {spark_submit.py:495} INFO - 23/01/31 03:46:33 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 8609 ms on 2f101bc15e7a (executor driver) (1/1)
[2023-01-31T03:46:33.659+0000] {spark_submit.py:495} INFO - 23/01/31 03:46:33 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T03:46:33.702+0000] {spark_submit.py:495} INFO - 23/01/31 03:46:33 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 12.719 s
[2023-01-31T03:46:33.762+0000] {spark_submit.py:495} INFO - 23/01/31 03:46:33 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T03:46:33.780+0000] {spark_submit.py:495} INFO - 23/01/31 03:46:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T03:46:33.807+0000] {spark_submit.py:495} INFO - 23/01/31 03:46:33 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 13.475991 s
[2023-01-31T03:47:26.155+0000] {spark_submit.py:495} INFO - 23/01/31 03:47:26 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 2f101bc15e7a:39501 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T03:47:26.330+0000] {spark_submit.py:495} INFO - 23/01/31 03:47:26 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 2f101bc15e7a:39501 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T03:48:18.230+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:18 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T03:48:18.251+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:18 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T03:48:18.252+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:18 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T03:48:29.402+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:29 INFO CodeGenerator: Code generated in 3843.495321 ms
[2023-01-31T03:48:30.022+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:30 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T03:48:31.079+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:31 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T03:48:31.127+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:31 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 2f101bc15e7a:39501 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T03:48:31.155+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:31 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T03:48:31.263+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:31 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T03:48:34.228+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:34 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T03:48:34.263+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:34 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T03:48:34.264+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:34 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T03:48:34.265+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:34 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T03:48:34.265+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:34 INFO DAGScheduler: Missing parents: List()
[2023-01-31T03:48:34.335+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:34 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T03:48:34.696+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:34 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T03:48:34.704+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:34 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T03:48:34.767+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:34 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 2f101bc15e7a:39501 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T03:48:34.791+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:34 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T03:48:34.847+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T03:48:34.866+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:34 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T03:48:36.040+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:36 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (2f101bc15e7a, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T03:48:36.188+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:36 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T03:48:39.810+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:39 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T03:48:46.262+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:46 INFO CodeGenerator: Code generated in 3013.128652 ms
[2023-01-31T03:48:49.728+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:49 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1628 bytes result sent to driver
[2023-01-31T03:48:49.933+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:49 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 14757 ms on 2f101bc15e7a (executor driver) (1/1)
[2023-01-31T03:48:49.943+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:49 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T03:48:49.963+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:49 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 15.272 s
[2023-01-31T03:48:49.965+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:49 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T03:48:49.966+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T03:48:49.990+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:49 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 15.757476 s
[2023-01-31T03:48:52.635+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:52 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T03:48:52.637+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:52 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T03:48:52.650+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:52 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T03:48:54.179+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:54 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 2f101bc15e7a:39501 in memory (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T03:48:54.672+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:54 INFO CodeGenerator: Code generated in 1065.226253 ms
[2023-01-31T03:48:55.011+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:55 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T03:48:55.454+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:55 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T03:48:55.471+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:55 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 2f101bc15e7a:39501 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T03:48:55.500+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:55 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T03:48:55.566+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:55 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T03:48:56.673+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:56 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T03:48:56.685+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:56 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T03:48:56.686+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:56 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T03:48:56.690+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:56 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T03:48:56.696+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:56 INFO DAGScheduler: Missing parents: List()
[2023-01-31T03:48:56.759+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:56 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T03:48:56.906+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:56 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T03:48:56.991+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:56 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T03:48:57.010+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:57 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 2f101bc15e7a:39501 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T03:48:57.059+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:57 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T03:48:57.089+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T03:48:57.098+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:57 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T03:48:57.151+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:57 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (2f101bc15e7a, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T03:48:57.224+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:57 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T03:48:57.725+0000] {spark_submit.py:495} INFO - 23/01/31 03:48:57 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T03:49:00.964+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:00 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1585 bytes result sent to driver
[2023-01-31T03:49:01.111+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:01 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 3972 ms on 2f101bc15e7a (executor driver) (1/1)
[2023-01-31T03:49:01.124+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:01 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T03:49:01.128+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:01 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 4.291 s
[2023-01-31T03:49:01.138+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:01 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T03:49:01.139+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T03:49:01.151+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:01 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 4.465078 s
[2023-01-31T03:49:06.660+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:06 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T03:49:06.675+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:06 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T03:49:06.678+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:06 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T03:49:08.064+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:08 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 2f101bc15e7a:39501 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T03:49:08.283+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:08 INFO CodeGenerator: Code generated in 725.14452 ms
[2023-01-31T03:49:08.319+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:08 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T03:49:08.619+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:08 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T03:49:08.642+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:08 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 2f101bc15e7a:39501 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T03:49:08.679+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:08 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T03:49:08.777+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:08 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T03:49:09.574+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:09 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T03:49:09.593+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:09 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T03:49:09.594+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:09 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T03:49:09.608+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:09 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T03:49:09.609+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:09 INFO DAGScheduler: Missing parents: List()
[2023-01-31T03:49:09.639+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:09 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T03:49:09.716+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:09 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.7 MiB)
[2023-01-31T03:49:09.746+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:09 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.7 MiB)
[2023-01-31T03:49:09.760+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:09 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 2f101bc15e7a:39501 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T03:49:09.767+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:09 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T03:49:09.775+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T03:49:09.777+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:09 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T03:49:09.785+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:09 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (2f101bc15e7a, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T03:49:09.796+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:09 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T03:49:09.918+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:09 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T03:49:13.224+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:13 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1755 bytes result sent to driver
[2023-01-31T03:49:13.318+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:13 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 3499 ms on 2f101bc15e7a (executor driver) (1/1)
[2023-01-31T03:49:13.320+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:13 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T03:49:13.554+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:13 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 3.718 s
[2023-01-31T03:49:13.560+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:13 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T03:49:13.562+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T03:49:13.603+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:13 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 4.024560 s
[2023-01-31T03:49:15.769+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:15 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T03:49:15.782+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:15 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T03:49:15.791+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:15 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T03:49:16.529+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:16 INFO CodeGenerator: Code generated in 138.868347 ms
[2023-01-31T03:49:16.547+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:16 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.5 MiB)
[2023-01-31T03:49:16.674+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:16 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T03:49:16.676+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:16 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 2f101bc15e7a:39501 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T03:49:16.676+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:16 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T03:49:16.677+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:16 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T03:49:17.504+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:17 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T03:49:17.505+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:17 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T03:49:17.505+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:17 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T03:49:17.506+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:17 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T03:49:17.530+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:17 INFO DAGScheduler: Missing parents: List()
[2023-01-31T03:49:17.661+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:17 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T03:49:18.477+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:18 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-31T03:49:19.026+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:18 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-01-31T03:49:19.046+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:19 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 2f101bc15e7a:39501 (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T03:49:19.074+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:19 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T03:49:19.113+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T03:49:19.123+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:19 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T03:49:19.267+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:19 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (2f101bc15e7a, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T03:49:19.355+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:19 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T03:49:20.252+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:20 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T03:49:22.112+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:22 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 2f101bc15e7a:39501 in memory (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T03:49:24.212+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:24 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1628 bytes result sent to driver
[2023-01-31T03:49:24.222+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:24 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 5022 ms on 2f101bc15e7a (executor driver) (1/1)
[2023-01-31T03:49:24.224+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:24 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T03:49:24.271+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:24 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 6.540 s
[2023-01-31T03:49:24.273+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:24 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T03:49:24.274+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T03:49:24.274+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:24 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 6.870855 s
[2023-01-31T03:49:32.321+0000] {spark_submit.py:495} INFO - Traceback (most recent call last):
[2023-01-31T03:49:32.322+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 266, in <module>
[2023-01-31T03:49:32.331+0000] {spark_submit.py:495} INFO - main(
[2023-01-31T03:49:32.331+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 243, in main
[2023-01-31T03:49:32.332+0000] {spark_submit.py:495} INFO - run_method = method(
[2023-01-31T03:49:32.332+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/scripts/transform_raw_staging.py", line 196, in transform_generation
[2023-01-31T03:49:32.333+0000] {spark_submit.py:495} INFO - all_df = parse_datetimeindex(self.spark, df_ts, df_nonts, tz=None)
[2023-01-31T03:49:32.333+0000] {spark_submit.py:495} INFO - File "/opt/***/plugins/helpers/parsers.py", line 70, in parse_datetimeindex
[2023-01-31T03:49:32.334+0000] {spark_submit.py:495} INFO - date_index = date_index.selectExpr(
[2023-01-31T03:49:32.352+0000] {spark_submit.py:495} INFO - File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py", line 2048, in selectExpr
[2023-01-31T03:49:32.355+0000] {spark_submit.py:495} INFO - File "/opt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py", line 1321, in __call__
[2023-01-31T03:49:32.360+0000] {spark_submit.py:495} INFO - File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 196, in deco
[2023-01-31T03:49:32.368+0000] {spark_submit.py:495} INFO - pyspark.sql.utils.ParseException:
[2023-01-31T03:49:32.377+0000] {spark_submit.py:495} INFO - Syntax error at or near ':': extra input ':'(line 1, pos 43)
[2023-01-31T03:49:32.383+0000] {spark_submit.py:495} INFO - 
[2023-01-31T03:49:32.392+0000] {spark_submit.py:495} INFO - == SQL ==
[2023-01-31T03:49:32.401+0000] {spark_submit.py:495} INFO - to_utc_timestamp('2023-01-31 03:42:28', +07:00) as created_at
[2023-01-31T03:49:32.407+0000] {spark_submit.py:495} INFO - -------------------------------------------^^^
[2023-01-31T03:49:32.411+0000] {spark_submit.py:495} INFO - 
[2023-01-31T03:49:33.810+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:33 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-31T03:49:34.938+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:34 INFO SparkUI: Stopped Spark web UI at http://2f101bc15e7a:4045
[2023-01-31T03:49:36.231+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:36 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-31T03:49:36.546+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:36 INFO MemoryStore: MemoryStore cleared
[2023-01-31T03:49:36.711+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:36 INFO BlockManager: BlockManager stopped
[2023-01-31T03:49:37.020+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:37 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-31T03:49:37.156+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:37 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-31T03:49:37.763+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:37 INFO SparkContext: Successfully stopped SparkContext
[2023-01-31T03:49:37.855+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:37 INFO ShutdownHookManager: Shutdown hook called
[2023-01-31T03:49:37.928+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-c1a58af4-b588-481a-9ca4-0eb1b206b6d0
[2023-01-31T03:49:38.169+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:38 INFO ShutdownHookManager: Deleting directory /tmp/spark-c1a58af4-b588-481a-9ca4-0eb1b206b6d0/pyspark-a88325ea-9c63-4d2a-b62a-dc15268b326e
[2023-01-31T03:49:38.274+0000] {spark_submit.py:495} INFO - 23/01/31 03:49:38 INFO ShutdownHookManager: Deleting directory /tmp/spark-81337a6c-02b9-4fbd-ba4e-e06014cb801f
[2023-01-31T03:49:41.568+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.
[2023-01-31T03:49:41.614+0000] {taskinstance.py:1322} INFO - Marking task as UP_FOR_RETRY. dag_id=entsoe-energydata, task_id=stage_total_generation, execution_date=20210101T000000, start_date=20230131T034246, end_date=20230131T034941
[2023-01-31T03:49:41.885+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 804 for task stage_total_generation (Cannot execute: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET. Error code is: 1.; 404)
[2023-01-31T03:49:42.009+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-01-31T03:49:42.238+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T03:54:02.383+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-31T03:54:02.444+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-31T03:54:02.446+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T03:54:02.446+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T03:54:02.447+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T03:54:02.549+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 00:00:00+00:00
[2023-01-31T03:54:02.733+0000] {standard_task_runner.py:55} INFO - Started process 2271 to run task
[2023-01-31T03:54:02.856+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata', 'stage_total_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '815', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp_ej4aboy']
[2023-01-31T03:54:02.903+0000] {standard_task_runner.py:83} INFO - Job 815: Subtask stage_total_generation
[2023-01-31T03:54:03.741+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 2f101bc15e7a
[2023-01-31T03:54:05.864+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-31T03:54:05.948+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T03:54:05.961+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-31T03:55:18.371+0000] {spark_submit.py:495} INFO - 23/01/31 03:55:18 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T03:55:20.825+0000] {spark_submit.py:495} INFO - 23/01/31 03:55:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T03:55:25.201+0000] {spark_submit.py:495} INFO - 23/01/31 03:55:25 INFO ResourceUtils: ==============================================================
[2023-01-31T03:55:25.205+0000] {spark_submit.py:495} INFO - 23/01/31 03:55:25 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T03:55:25.209+0000] {spark_submit.py:495} INFO - 23/01/31 03:55:25 INFO ResourceUtils: ==============================================================
[2023-01-31T03:55:25.283+0000] {spark_submit.py:495} INFO - 23/01/31 03:55:25 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T03:55:26.455+0000] {spark_submit.py:495} INFO - 23/01/31 03:55:26 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T03:55:26.587+0000] {spark_submit.py:495} INFO - 23/01/31 03:55:26 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T03:55:26.600+0000] {spark_submit.py:495} INFO - 23/01/31 03:55:26 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T03:55:27.730+0000] {spark_submit.py:495} INFO - 23/01/31 03:55:27 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T03:55:27.739+0000] {spark_submit.py:495} INFO - 23/01/31 03:55:27 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T03:55:27.749+0000] {spark_submit.py:495} INFO - 23/01/31 03:55:27 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T03:55:27.787+0000] {spark_submit.py:495} INFO - 23/01/31 03:55:27 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T03:55:27.808+0000] {spark_submit.py:495} INFO - 23/01/31 03:55:27 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T03:55:34.260+0000] {spark_submit.py:495} INFO - 23/01/31 03:55:34 INFO Utils: Successfully started service 'sparkDriver' on port 40295.
[2023-01-31T03:55:35.833+0000] {spark_submit.py:495} INFO - 23/01/31 03:55:35 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T03:55:37.897+0000] {spark_submit.py:495} INFO - 23/01/31 03:55:37 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T03:55:38.717+0000] {spark_submit.py:495} INFO - 23/01/31 03:55:38 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T03:55:38.750+0000] {spark_submit.py:495} INFO - 23/01/31 03:55:38 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T03:55:38.993+0000] {spark_submit.py:495} INFO - 23/01/31 03:55:38 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T03:55:39.561+0000] {spark_submit.py:495} INFO - 23/01/31 03:55:39 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-0e97a002-77ca-4d54-81f8-a2affdea1114
[2023-01-31T03:55:40.661+0000] {spark_submit.py:495} INFO - 23/01/31 03:55:40 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T03:55:41.186+0000] {spark_submit.py:495} INFO - 23/01/31 03:55:41 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T03:55:51.068+0000] {spark_submit.py:495} INFO - 23/01/31 03:55:51 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T03:55:51.631+0000] {spark_submit.py:495} INFO - 23/01/31 03:55:51 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2023-01-31T03:55:52.223+0000] {spark_submit.py:495} INFO - 23/01/31 03:55:52 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://2f101bc15e7a:40295/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675137318176
[2023-01-31T03:55:52.223+0000] {spark_submit.py:495} INFO - 23/01/31 03:55:52 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://2f101bc15e7a:40295/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675137318176
[2023-01-31T03:55:57.021+0000] {spark_submit.py:495} INFO - 23/01/31 03:55:57 INFO Executor: Starting executor ID driver on host 2f101bc15e7a
[2023-01-31T03:55:57.351+0000] {spark_submit.py:495} INFO - 23/01/31 03:55:57 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T03:55:58.157+0000] {spark_submit.py:495} INFO - 23/01/31 03:55:58 INFO Executor: Fetching spark://2f101bc15e7a:40295/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675137318176
[2023-01-31T03:56:02.462+0000] {spark_submit.py:495} INFO - 23/01/31 03:56:02 INFO TransportClientFactory: Successfully created connection to 2f101bc15e7a/172.19.0.4:40295 after 2186 ms (0 ms spent in bootstraps)
[2023-01-31T03:56:02.962+0000] {spark_submit.py:495} INFO - 23/01/31 03:56:02 INFO Utils: Fetching spark://2f101bc15e7a:40295/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-6e28a2af-e54b-41cf-b6bb-f894ec751596/userFiles-47f43fd7-0222-4396-8486-95312bac03da/fetchFileTemp16613033559253339906.tmp
[2023-01-31T03:56:13.882+0000] {spark_submit.py:495} INFO - 23/01/31 03:56:13 INFO Executor: Told to re-register on heartbeat
[2023-01-31T03:56:13.894+0000] {spark_submit.py:495} INFO - 23/01/31 03:56:13 INFO BlockManager: BlockManager null re-registering with master
[2023-01-31T03:56:14.056+0000] {spark_submit.py:495} INFO - 23/01/31 03:56:14 INFO BlockManagerMaster: Registering BlockManager null
[2023-01-31T03:56:14.479+0000] {spark_submit.py:495} INFO - 23/01/31 03:56:14 ERROR Inbox: Ignoring error
[2023-01-31T03:56:14.480+0000] {spark_submit.py:495} INFO - java.lang.NullPointerException
[2023-01-31T03:56:14.480+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)
[2023-01-31T03:56:14.481+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)
[2023-01-31T03:56:14.481+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-01-31T03:56:14.482+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-01-31T03:56:14.492+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-01-31T03:56:14.492+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-01-31T03:56:14.493+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-01-31T03:56:14.494+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-01-31T03:56:14.494+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-01-31T03:56:14.529+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-31T03:56:14.530+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-31T03:56:14.615+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-31T03:56:14.616+0000] {spark_submit.py:495} INFO - 23/01/31 03:56:14 WARN Executor: Issue communicating with driver in heartbeater
[2023-01-31T03:56:14.617+0000] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-01-31T03:56:14.617+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-01-31T03:56:14.618+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-01-31T03:56:14.618+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2023-01-31T03:56:14.691+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2023-01-31T03:56:14.692+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:79)
[2023-01-31T03:56:14.739+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:643)
[2023-01-31T03:56:14.740+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2023-01-31T03:56:14.740+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2023-01-31T03:56:14.741+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
[2023-01-31T03:56:14.742+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2023-01-31T03:56:14.742+0000] {spark_submit.py:495} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2023-01-31T03:56:14.843+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-01-31T03:56:14.844+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2023-01-31T03:56:14.844+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2023-01-31T03:56:14.845+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-31T03:56:14.845+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-31T03:56:14.911+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-31T03:56:14.912+0000] {spark_submit.py:495} INFO - Caused by: java.lang.NullPointerException
[2023-01-31T03:56:14.992+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)
[2023-01-31T03:56:15.021+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)
[2023-01-31T03:56:15.055+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-01-31T03:56:15.055+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-01-31T03:56:15.056+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-01-31T03:56:15.057+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-01-31T03:56:15.057+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-01-31T03:56:15.058+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-01-31T03:56:15.177+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-01-31T03:56:15.178+0000] {spark_submit.py:495} INFO - ... 3 more
[2023-01-31T03:56:15.247+0000] {spark_submit.py:495} INFO - 23/01/31 03:56:15 INFO Executor: Adding file:/tmp/spark-6e28a2af-e54b-41cf-b6bb-f894ec751596/userFiles-47f43fd7-0222-4396-8486-95312bac03da/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T03:56:15.248+0000] {spark_submit.py:495} INFO - 23/01/31 03:56:15 INFO Executor: Fetching spark://2f101bc15e7a:40295/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675137318176
[2023-01-31T03:56:15.293+0000] {spark_submit.py:495} INFO - 23/01/31 03:56:15 INFO Utils: Fetching spark://2f101bc15e7a:40295/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-6e28a2af-e54b-41cf-b6bb-f894ec751596/userFiles-47f43fd7-0222-4396-8486-95312bac03da/fetchFileTemp8209004843281566768.tmp
[2023-01-31T03:56:23.318+0000] {spark_submit.py:495} INFO - 23/01/31 03:56:23 INFO Executor: Told to re-register on heartbeat
[2023-01-31T03:56:23.322+0000] {spark_submit.py:495} INFO - 23/01/31 03:56:23 INFO BlockManager: BlockManager null re-registering with master
[2023-01-31T03:56:23.335+0000] {spark_submit.py:495} INFO - 23/01/31 03:56:23 INFO BlockManagerMaster: Registering BlockManager null
[2023-01-31T03:56:23.369+0000] {spark_submit.py:495} INFO - 23/01/31 03:56:23 ERROR Inbox: Ignoring error
[2023-01-31T03:56:23.370+0000] {spark_submit.py:495} INFO - java.lang.NullPointerException
[2023-01-31T03:56:23.372+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)
[2023-01-31T03:56:23.372+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)
[2023-01-31T03:56:23.373+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-01-31T03:56:23.373+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-01-31T03:56:23.374+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-01-31T03:56:23.455+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-01-31T03:56:23.456+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-01-31T03:56:23.456+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-01-31T03:56:23.457+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-01-31T03:56:23.458+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-31T03:56:23.458+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-31T03:56:23.562+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-31T03:56:23.563+0000] {spark_submit.py:495} INFO - 23/01/31 03:56:23 WARN Executor: Issue communicating with driver in heartbeater
[2023-01-31T03:56:23.564+0000] {spark_submit.py:495} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2023-01-31T03:56:23.565+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2023-01-31T03:56:23.565+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2023-01-31T03:56:23.566+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
[2023-01-31T03:56:23.645+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
[2023-01-31T03:56:23.645+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:79)
[2023-01-31T03:56:23.646+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:643)
[2023-01-31T03:56:23.717+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)
[2023-01-31T03:56:23.718+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)
[2023-01-31T03:56:23.719+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
[2023-01-31T03:56:23.719+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)
[2023-01-31T03:56:23.720+0000] {spark_submit.py:495} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2023-01-31T03:56:23.720+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-01-31T03:56:23.721+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2023-01-31T03:56:23.721+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2023-01-31T03:56:23.722+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-31T03:56:23.722+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-31T03:56:23.838+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-31T03:56:23.840+0000] {spark_submit.py:495} INFO - Caused by: java.lang.NullPointerException
[2023-01-31T03:56:23.840+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:579)
[2023-01-31T03:56:23.841+0000] {spark_submit.py:495} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:121)
[2023-01-31T03:56:23.841+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2023-01-31T03:56:23.842+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2023-01-31T03:56:23.922+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2023-01-31T03:56:23.946+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2023-01-31T03:56:23.947+0000] {spark_submit.py:495} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2023-01-31T03:56:23.948+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2023-01-31T03:56:23.948+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[2023-01-31T03:56:23.949+0000] {spark_submit.py:495} INFO - ... 3 more
[2023-01-31T03:56:28.014+0000] {spark_submit.py:495} INFO - 23/01/31 03:56:28 INFO Executor: Adding file:/tmp/spark-6e28a2af-e54b-41cf-b6bb-f894ec751596/userFiles-47f43fd7-0222-4396-8486-95312bac03da/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T03:56:28.211+0000] {spark_submit.py:495} INFO - 23/01/31 03:56:28 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39635.
[2023-01-31T03:56:28.212+0000] {spark_submit.py:495} INFO - 23/01/31 03:56:28 INFO NettyBlockTransferService: Server created on 2f101bc15e7a:39635
[2023-01-31T03:56:28.241+0000] {spark_submit.py:495} INFO - 23/01/31 03:56:28 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T03:56:28.425+0000] {spark_submit.py:495} INFO - 23/01/31 03:56:28 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 2f101bc15e7a, 39635, None)
[2023-01-31T03:56:28.507+0000] {spark_submit.py:495} INFO - 23/01/31 03:56:28 INFO BlockManagerMasterEndpoint: Registering block manager 2f101bc15e7a:39635 with 434.4 MiB RAM, BlockManagerId(driver, 2f101bc15e7a, 39635, None)
[2023-01-31T03:56:28.610+0000] {spark_submit.py:495} INFO - 23/01/31 03:56:28 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 2f101bc15e7a, 39635, None)
[2023-01-31T03:56:28.704+0000] {spark_submit.py:495} INFO - 23/01/31 03:56:28 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 2f101bc15e7a, 39635, None)
[2023-01-31T03:56:33.346+0000] {spark_submit.py:495} INFO - 23/01/31 03:56:33 INFO Executor: Told to re-register on heartbeat
[2023-01-31T03:56:33.443+0000] {spark_submit.py:495} INFO - 23/01/31 03:56:33 INFO BlockManager: BlockManager BlockManagerId(driver, 2f101bc15e7a, 39635, None) re-registering with master
[2023-01-31T03:56:33.443+0000] {spark_submit.py:495} INFO - 23/01/31 03:56:33 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 2f101bc15e7a, 39635, None)
[2023-01-31T03:56:33.546+0000] {spark_submit.py:495} INFO - 23/01/31 03:56:33 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 2f101bc15e7a, 39635, None)
[2023-01-31T03:56:33.550+0000] {spark_submit.py:495} INFO - 23/01/31 03:56:33 INFO BlockManager: Reporting 0 blocks to the master.
[2023-01-31T03:56:50.971+0000] {spark_submit.py:495} INFO - 23/01/31 03:56:50 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T03:56:51.060+0000] {spark_submit.py:495} INFO - 23/01/31 03:56:51 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T03:57:57.900+0000] {spark_submit.py:495} INFO - 23/01/31 03:57:57 INFO InMemoryFileIndex: It took 749 ms to list leaf files for 1 paths.
[2023-01-31T03:58:04.470+0000] {spark_submit.py:495} INFO - 23/01/31 03:58:04 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T03:58:05.845+0000] {spark_submit.py:495} INFO - 23/01/31 03:58:05 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T03:58:05.976+0000] {spark_submit.py:495} INFO - 23/01/31 03:58:05 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 2f101bc15e7a:39635 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T03:58:06.411+0000] {spark_submit.py:495} INFO - 23/01/31 03:58:06 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T03:58:29.386+0000] {spark_submit.py:495} INFO - 23/01/31 03:58:29 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T03:58:29.762+0000] {spark_submit.py:495} INFO - 23/01/31 03:58:29 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T03:58:30.964+0000] {spark_submit.py:495} INFO - 23/01/31 03:58:30 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T03:58:31.577+0000] {spark_submit.py:495} INFO - 23/01/31 03:58:31 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T03:58:31.608+0000] {spark_submit.py:495} INFO - 23/01/31 03:58:31 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T03:58:31.647+0000] {spark_submit.py:495} INFO - 23/01/31 03:58:31 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T03:58:31.821+0000] {spark_submit.py:495} INFO - 23/01/31 03:58:31 INFO DAGScheduler: Missing parents: List()
[2023-01-31T03:58:31.913+0000] {spark_submit.py:495} INFO - 23/01/31 03:58:31 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T03:58:33.430+0000] {spark_submit.py:495} INFO - 23/01/31 03:58:33 INFO AsyncEventQueue: Process of event SparkListenerJobStart(0,1675137511822,ArraySeq(org.apache.spark.scheduler.StageInfo@63a3ec47),{spark.master=local, spark.driver.port=40295, spark.submit.pyFiles=, spark.app.startTime=1675137318176, spark.rdd.compress=True, spark.executor.id=driver, spark.app.name=gcp_playground, spark.submit.deployMode=client, spark.hadoop.google.cloud.auth.service.account.json.keyfile=/.google/credentials/google_credentials.json, spark.driver.host=2f101bc15e7a, spark.app.id=local-1675137355010, spark.app.submitTime=1675137301605, spark.hadoop.fs.AbstractFileSystem.gs.impl=google.cloud.hadoop.fs.gcs.GoogleHadoopFS, spark.sql.session.timeZone=UTC, spark.hadoop.google.cloud.auth.service.account.enable=true, spark.app.initial.jar.urls=spark://2f101bc15e7a:40295/jars/gcs-connector-hadoop3-latest.jar,spark://2f101bc15e7a:40295/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar, spark.executor.extraJavaOptions=-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED, spark.jars=/opt/spark/jars/gcs-connector-hadoop3-latest.jar, /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar, spark.sql.warehouse.dir=file:/opt/***/spark-warehouse, spark.serializer.objectStreamReset=100, spark.driver.extraJavaOptions=-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED}) by listener AppStatusListener took 1.559937919s.
[2023-01-31T03:58:36.212+0000] {spark_submit.py:495} INFO - 23/01/31 03:58:36 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T03:58:36.235+0000] {spark_submit.py:495} INFO - 23/01/31 03:58:36 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T03:58:36.240+0000] {spark_submit.py:495} INFO - 23/01/31 03:58:36 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 2f101bc15e7a:39635 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T03:58:36.249+0000] {spark_submit.py:495} INFO - 23/01/31 03:58:36 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T03:58:36.786+0000] {spark_submit.py:495} INFO - 23/01/31 03:58:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T03:58:36.828+0000] {spark_submit.py:495} INFO - 23/01/31 03:58:36 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T03:58:39.536+0000] {spark_submit.py:495} INFO - 23/01/31 03:58:39 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (2f101bc15e7a, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T03:58:40.949+0000] {spark_submit.py:495} INFO - 23/01/31 03:58:40 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T03:58:46.687+0000] {spark_submit.py:495} INFO - 23/01/31 03:58:46 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010000__202101010100.json:0+9369
[2023-01-31T03:58:52.248+0000] {spark_submit.py:495} INFO - 23/01/31 03:58:52 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3480 bytes result sent to driver
[2023-01-31T03:58:52.332+0000] {spark_submit.py:495} INFO - 23/01/31 03:58:52 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 13732 ms on 2f101bc15e7a (executor driver) (1/1)
[2023-01-31T03:58:52.373+0000] {spark_submit.py:495} INFO - 23/01/31 03:58:52 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T03:58:52.403+0000] {spark_submit.py:495} INFO - 23/01/31 03:58:52 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 19.439 s
[2023-01-31T03:58:52.427+0000] {spark_submit.py:495} INFO - 23/01/31 03:58:52 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T03:58:52.430+0000] {spark_submit.py:495} INFO - 23/01/31 03:58:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T03:58:52.442+0000] {spark_submit.py:495} INFO - 23/01/31 03:58:52 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 21.473324 s
[2023-01-31T03:58:53.457+0000] {spark_submit.py:495} INFO - 23/01/31 03:58:53 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 2f101bc15e7a:39635 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T03:58:53.553+0000] {spark_submit.py:495} INFO - 23/01/31 03:58:53 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 2f101bc15e7a:39635 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T03:59:50.571+0000] {spark_submit.py:495} INFO - 23/01/31 03:59:50 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T03:59:50.583+0000] {spark_submit.py:495} INFO - 23/01/31 03:59:50 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T03:59:50.597+0000] {spark_submit.py:495} INFO - 23/01/31 03:59:50 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T04:00:02.097+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:02 INFO CodeGenerator: Code generated in 5687.573357 ms
[2023-01-31T04:00:02.210+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:02 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T04:00:03.087+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:03 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T04:00:03.106+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:03 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 2f101bc15e7a:39635 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T04:00:03.199+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:03 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T04:00:04.139+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:04 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T04:00:06.245+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:06 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T04:00:06.251+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:06 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T04:00:06.251+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:06 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T04:00:06.261+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:06 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:00:06.281+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:06 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:00:06.338+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:06 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T04:00:06.521+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:06 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T04:00:06.545+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:06 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T04:00:06.549+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:06 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 2f101bc15e7a:39635 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T04:00:06.554+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:06 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:00:06.560+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:00:06.562+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:06 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T04:00:06.613+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:06 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (2f101bc15e7a, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T04:00:06.650+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:06 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T04:00:09.400+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:09 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T04:00:12.570+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:12 INFO CodeGenerator: Code generated in 1758.761848 ms
[2023-01-31T04:00:15.418+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:15 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1671 bytes result sent to driver
[2023-01-31T04:00:15.472+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:15 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 8880 ms on 2f101bc15e7a (executor driver) (1/1)
[2023-01-31T04:00:15.496+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:15 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T04:00:15.514+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:15 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 9.005 s
[2023-01-31T04:00:15.568+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:15 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:00:15.577+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T04:00:15.601+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:15 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 9.351171 s
[2023-01-31T04:00:17.655+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:17 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T04:00:17.658+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:17 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T04:00:17.659+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:17 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T04:00:18.179+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:18 INFO CodeGenerator: Code generated in 372.204692 ms
[2023-01-31T04:00:18.342+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:18 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T04:00:18.857+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:18 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T04:00:18.876+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:18 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 2f101bc15e7a:39635 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T04:00:18.897+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:18 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T04:00:19.037+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:19 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T04:00:19.579+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:19 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T04:00:19.591+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:19 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T04:00:19.592+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:19 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T04:00:19.593+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:19 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:00:19.607+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:19 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:00:19.653+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:19 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T04:00:20.169+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:20 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T04:00:20.305+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:20 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T04:00:20.375+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:20 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 2f101bc15e7a:39635 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T04:00:20.394+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:20 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:00:20.407+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:00:20.435+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:20 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T04:00:20.463+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:20 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (2f101bc15e7a, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T04:00:20.519+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:20 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T04:00:21.255+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:21 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T04:00:22.051+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:22 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 2f101bc15e7a:39635 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T04:00:23.332+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:23 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1628 bytes result sent to driver
[2023-01-31T04:00:23.426+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:23 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 2975 ms on 2f101bc15e7a (executor driver) (1/1)
[2023-01-31T04:00:23.435+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:23 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T04:00:23.442+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:23 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 3.695 s
[2023-01-31T04:00:23.463+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:23 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:00:23.468+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T04:00:23.478+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:23 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 3.897068 s
[2023-01-31T04:00:26.991+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:26 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T04:00:26.996+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:26 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T04:00:27.000+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:26 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T04:00:28.738+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:28 INFO CodeGenerator: Code generated in 1198.070021 ms
[2023-01-31T04:00:29.034+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:29 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T04:00:29.135+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:29 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 2f101bc15e7a:39635 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T04:00:29.415+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:29 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T04:00:29.440+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:29 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 2f101bc15e7a:39635 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T04:00:29.464+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:29 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T04:00:29.512+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:29 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T04:00:29.920+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:29 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T04:00:29.927+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:29 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T04:00:29.928+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:29 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T04:00:29.933+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:29 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:00:29.942+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:29 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:00:29.959+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:29 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T04:00:30.035+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:30 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.7 MiB)
[2023-01-31T04:00:30.094+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:30 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.7 MiB)
[2023-01-31T04:00:30.136+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:30 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 2f101bc15e7a:39635 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T04:00:30.157+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:30 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:00:30.170+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:00:30.173+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:30 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T04:00:30.202+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:30 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (2f101bc15e7a, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T04:00:30.211+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:30 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T04:00:30.824+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:30 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T04:00:33.358+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:33 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-01-31T04:00:33.497+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:33 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 3298 ms on 2f101bc15e7a (executor driver) (1/1)
[2023-01-31T04:00:33.500+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:33 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T04:00:33.619+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:33 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 3.532 s
[2023-01-31T04:00:33.619+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:33 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:00:33.620+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T04:00:33.621+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:33 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 3.693627 s
[2023-01-31T04:00:35.202+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:35 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T04:00:35.240+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:35 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T04:00:35.318+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:35 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T04:00:36.205+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:36 INFO CodeGenerator: Code generated in 509.545938 ms
[2023-01-31T04:00:36.413+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:36 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.5 MiB)
[2023-01-31T04:00:36.788+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:36 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T04:00:36.918+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:36 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 2f101bc15e7a:39635 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T04:00:36.954+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:36 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T04:00:37.043+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:37 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T04:00:38.120+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:38 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 2f101bc15e7a:39635 in memory (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T04:00:38.619+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:38 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T04:00:38.691+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:38 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T04:00:38.692+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:38 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T04:00:38.693+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:38 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:00:38.693+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:38 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:00:38.694+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:38 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T04:00:38.733+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:38 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.5 MiB)
[2023-01-31T04:00:38.734+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:38 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.5 MiB)
[2023-01-31T04:00:38.739+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:38 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 2f101bc15e7a:39635 (size: 7.6 KiB, free: 434.3 MiB)
[2023-01-31T04:00:38.762+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:38 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:00:38.763+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:00:38.764+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:38 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T04:00:38.773+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:38 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (2f101bc15e7a, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T04:00:38.782+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:38 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T04:00:38.867+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:38 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T04:00:41.338+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:41 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1586 bytes result sent to driver
[2023-01-31T04:00:41.344+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:41 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 2575 ms on 2f101bc15e7a (executor driver) (1/1)
[2023-01-31T04:00:41.356+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:41 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T04:00:41.373+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:41 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 2.661 s
[2023-01-31T04:00:41.376+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:41 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:00:41.378+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T04:00:41.397+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:41 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 2.776568 s
[2023-01-31T04:00:47.527+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:47 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T04:00:47.588+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:47 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T04:00:47.637+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:47 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T04:00:48.798+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:48 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 2f101bc15e7a:39635 in memory (size: 7.6 KiB, free: 434.3 MiB)
[2023-01-31T04:00:53.102+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:53 INFO CodeGenerator: Code generated in 4415.791917 ms
[2023-01-31T04:00:53.149+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:53 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.3 MiB)
[2023-01-31T04:00:53.216+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:53 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T04:00:53.233+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:53 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 2f101bc15e7a:39635 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T04:00:53.240+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:53 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T04:00:53.244+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:53 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T04:00:54.147+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:54 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T04:00:54.150+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:54 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) with 1 output partitions
[2023-01-31T04:00:54.157+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:54 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201)
[2023-01-31T04:00:54.158+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:54 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:00:54.228+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:54 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:00:54.257+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:54 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201), which has no missing parents
[2023-01-31T04:00:54.585+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:54 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.2 MiB)
[2023-01-31T04:00:54.635+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:54 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.2 MiB)
[2023-01-31T04:00:54.639+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:54 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 2f101bc15e7a:39635 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T04:00:54.645+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:54 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:00:54.652+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:00:54.654+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:54 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T04:00:54.664+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:54 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (2f101bc15e7a, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T04:00:54.671+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:54 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T04:00:54.748+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:54 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T04:00:55.522+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:55 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2558 bytes result sent to driver
[2023-01-31T04:00:55.536+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:55 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 877 ms on 2f101bc15e7a (executor driver) (1/1)
[2023-01-31T04:00:55.546+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:55 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) finished in 1.194 s
[2023-01-31T04:00:55.546+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:55 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:00:55.554+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:55 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T04:00:55.556+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T04:00:55.569+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:55 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201, took 1.413192 s
[2023-01-31T04:00:57.222+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:57 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T04:00:57.320+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:57 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T04:00:57.322+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:57 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T04:00:57.982+0000] {spark_submit.py:495} INFO - 23/01/31 04:00:57 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 2f101bc15e7a:39635 in memory (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T04:01:02.135+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:02 INFO CodeGenerator: Code generated in 3015.170751 ms
[2023-01-31T04:01:02.244+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:02 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 433.1 MiB)
[2023-01-31T04:01:02.665+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:02 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.0 MiB)
[2023-01-31T04:01:02.705+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:02 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 2f101bc15e7a:39635 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T04:01:02.722+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:02 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T04:01:02.850+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:02 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T04:01:04.778+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:04 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T04:01:04.784+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:04 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T04:01:04.789+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:04 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T04:01:04.790+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:04 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:01:04.802+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:04 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:01:04.827+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:04 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T04:01:05.001+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:04 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 433.0 MiB)
[2023-01-31T04:01:05.108+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:05 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 433.0 MiB)
[2023-01-31T04:01:05.124+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:05 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 2f101bc15e7a:39635 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T04:01:05.144+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:05 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:01:05.165+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:01:05.181+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:05 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T04:01:05.209+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:05 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (2f101bc15e7a, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T04:01:05.220+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:05 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T04:01:05.520+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:05 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T04:01:08.803+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:08 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-01-31T04:01:08.910+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:08 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 3658 ms on 2f101bc15e7a (executor driver) (1/1)
[2023-01-31T04:01:08.924+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:08 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T04:01:08.938+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:08 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 4.054 s
[2023-01-31T04:01:08.939+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:08 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:01:09.124+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T04:01:09.131+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:09 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 4.343929 s
[2023-01-31T04:01:11.482+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:11 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T04:01:11.555+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:11 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T04:01:11.593+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:11 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T04:01:14.201+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:14 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 2f101bc15e7a:39635 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T04:01:19.668+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:19 INFO CodeGenerator: Code generated in 4625.036209 ms
[2023-01-31T04:01:20.183+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:20 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.8 MiB)
[2023-01-31T04:01:20.850+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:20 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.8 MiB)
[2023-01-31T04:01:20.859+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:20 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 2f101bc15e7a:39635 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T04:01:20.984+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:20 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T04:01:21.161+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:21 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T04:01:22.649+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:22 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T04:01:22.682+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:22 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T04:01:22.694+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:22 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T04:01:22.695+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:22 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:01:22.701+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:22 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:01:22.753+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:22 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T04:01:22.912+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:22 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.8 MiB)
[2023-01-31T04:01:22.974+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:22 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.7 MiB)
[2023-01-31T04:01:22.982+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:22 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 2f101bc15e7a:39635 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T04:01:22.991+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:22 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:01:22.991+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:01:22.998+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:22 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T04:01:23.011+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:23 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (2f101bc15e7a, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T04:01:23.046+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:23 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T04:01:23.439+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:23 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T04:01:25.391+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:25 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1729 bytes result sent to driver
[2023-01-31T04:01:25.406+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:25 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 2395 ms on 2f101bc15e7a (executor driver) (1/1)
[2023-01-31T04:01:25.420+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:25 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 2.575 s
[2023-01-31T04:01:25.421+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:25 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:01:25.464+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:25 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T04:01:25.487+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T04:01:25.524+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:25 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 2.858093 s
[2023-01-31T04:01:26.659+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:26 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 2f101bc15e7a:39635 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T04:01:34.323+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:34 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 2f101bc15e7a:39635 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T04:01:34.640+0000] {spark_submit.py:495} INFO - 23/01/31 04:01:34 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 2f101bc15e7a:39635 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T04:05:19.074+0000] {spark_submit.py:495} INFO - 23/01/31 04:05:18 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,ArraySeq(),HashMap()) by listener AppStatusListener took 6.508399679s.
[2023-01-31T04:05:23.620+0000] {spark_submit.py:495} INFO - 23/01/31 04:05:23 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.TimeoutException: Cannot receive any reply from 2f101bc15e7a:40295 in 10000 milliseconds
[2023-01-31T04:05:56.536+0000] {spark_submit.py:495} INFO - 23/01/31 04:05:56 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T04:05:56.829+0000] {spark_submit.py:495} INFO - 23/01/31 04:05:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T04:05:56.829+0000] {spark_submit.py:495} INFO - 23/01/31 04:05:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T04:05:56.854+0000] {spark_submit.py:495} INFO - 23/01/31 04:05:56 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T04:05:56.855+0000] {spark_submit.py:495} INFO - 23/01/31 04:05:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T04:05:56.855+0000] {spark_submit.py:495} INFO - 23/01/31 04:05:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T04:05:56.886+0000] {spark_submit.py:495} INFO - 23/01/31 04:05:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T04:05:58.624+0000] {spark_submit.py:495} INFO - 23/01/31 04:05:58 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-31T04:05:58.627+0000] {spark_submit.py:495} INFO - 23/01/31 04:05:58 INFO DAGScheduler: Got job 8 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:05:58.629+0000] {spark_submit.py:495} INFO - 23/01/31 04:05:58 INFO DAGScheduler: Final stage: ResultStage 8 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:05:58.630+0000] {spark_submit.py:495} INFO - 23/01/31 04:05:58 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:05:58.630+0000] {spark_submit.py:495} INFO - 23/01/31 04:05:58 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:05:58.636+0000] {spark_submit.py:495} INFO - 23/01/31 04:05:58 INFO DAGScheduler: Submitting ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:05:58.742+0000] {spark_submit.py:495} INFO - 23/01/31 04:05:58 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 209.9 KiB, free 433.0 MiB)
[2023-01-31T04:05:58.747+0000] {spark_submit.py:495} INFO - 23/01/31 04:05:58 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 74.3 KiB, free 433.0 MiB)
[2023-01-31T04:05:58.758+0000] {spark_submit.py:495} INFO - 23/01/31 04:05:58 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 2f101bc15e7a:39635 (size: 74.3 KiB, free: 434.2 MiB)
[2023-01-31T04:05:58.759+0000] {spark_submit.py:495} INFO - 23/01/31 04:05:58 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:05:58.762+0000] {spark_submit.py:495} INFO - 23/01/31 04:05:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:05:58.763+0000] {spark_submit.py:495} INFO - 23/01/31 04:05:58 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2023-01-31T04:05:58.772+0000] {spark_submit.py:495} INFO - 23/01/31 04:05:58 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (2f101bc15e7a, executor driver, partition 0, PROCESS_LOCAL, 7484 bytes) taskResourceAssignments Map()
[2023-01-31T04:05:58.780+0000] {spark_submit.py:495} INFO - 23/01/31 04:05:58 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2023-01-31T04:05:58.919+0000] {spark_submit.py:495} INFO - 23/01/31 04:05:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T04:05:58.922+0000] {spark_submit.py:495} INFO - 23/01/31 04:05:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T04:05:58.925+0000] {spark_submit.py:495} INFO - 23/01/31 04:05:58 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T04:05:58.926+0000] {spark_submit.py:495} INFO - 23/01/31 04:05:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T04:05:58.926+0000] {spark_submit.py:495} INFO - 23/01/31 04:05:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T04:05:58.930+0000] {spark_submit.py:495} INFO - 23/01/31 04:05:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T04:05:58.950+0000] {spark_submit.py:495} INFO - 23/01/31 04:05:58 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T04:05:58.958+0000] {spark_submit.py:495} INFO - 23/01/31 04:05:58 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T04:05:59.160+0000] {spark_submit.py:495} INFO - 23/01/31 04:05:59 INFO ParquetOutputFormat: Parquet block size to 134217728
[2023-01-31T04:05:59.162+0000] {spark_submit.py:495} INFO - 23/01/31 04:05:59 INFO ParquetOutputFormat: Validation is off
[2023-01-31T04:05:59.168+0000] {spark_submit.py:495} INFO - 23/01/31 04:05:59 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-01-31T04:05:59.169+0000] {spark_submit.py:495} INFO - 23/01/31 04:05:59 INFO ParquetOutputFormat: Parquet properties are:
[2023-01-31T04:05:59.171+0000] {spark_submit.py:495} INFO - Parquet page size to 1048576
[2023-01-31T04:05:59.176+0000] {spark_submit.py:495} INFO - Parquet dictionary page size to 1048576
[2023-01-31T04:05:59.177+0000] {spark_submit.py:495} INFO - Dictionary is true
[2023-01-31T04:05:59.187+0000] {spark_submit.py:495} INFO - Writer version is: PARQUET_1_0
[2023-01-31T04:05:59.190+0000] {spark_submit.py:495} INFO - Page size checking is: estimated
[2023-01-31T04:05:59.194+0000] {spark_submit.py:495} INFO - Min row count for page size check is: 100
[2023-01-31T04:05:59.201+0000] {spark_submit.py:495} INFO - Max row count for page size check is: 10000
[2023-01-31T04:05:59.211+0000] {spark_submit.py:495} INFO - Truncate length for column indexes is: 64
[2023-01-31T04:05:59.214+0000] {spark_submit.py:495} INFO - Truncate length for statistics min/max  is: 2147483647
[2023-01-31T04:05:59.216+0000] {spark_submit.py:495} INFO - Bloom filter enabled: false
[2023-01-31T04:05:59.218+0000] {spark_submit.py:495} INFO - Max Bloom filter size for a column is 1048576
[2023-01-31T04:05:59.229+0000] {spark_submit.py:495} INFO - Bloom filter expected number of distinct values are: null
[2023-01-31T04:05:59.232+0000] {spark_submit.py:495} INFO - Page row count limit to 20000
[2023-01-31T04:05:59.259+0000] {spark_submit.py:495} INFO - Writing page checksums is: on
[2023-01-31T04:05:59.563+0000] {spark_submit.py:495} INFO - 23/01/31 04:05:59 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-01-31T04:05:59.564+0000] {spark_submit.py:495} INFO - {
[2023-01-31T04:05:59.564+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-31T04:05:59.564+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-31T04:05:59.564+0000] {spark_submit.py:495} INFO - "name" : "measured_at",
[2023-01-31T04:05:59.564+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T04:05:59.564+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T04:05:59.565+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:05:59.565+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:05:59.565+0000] {spark_submit.py:495} INFO - "name" : "created_at",
[2023-01-31T04:05:59.565+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T04:05:59.565+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T04:05:59.565+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:05:59.566+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:05:59.566+0000] {spark_submit.py:495} INFO - "name" : "Biomass - Actual Aggregated",
[2023-01-31T04:05:59.566+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:05:59.566+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:05:59.566+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:05:59.567+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:05:59.567+0000] {spark_submit.py:495} INFO - "name" : "Fossil Brown coal/Lignite - Actual Aggregated",
[2023-01-31T04:05:59.567+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:05:59.567+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:05:59.567+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:05:59.568+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:05:59.568+0000] {spark_submit.py:495} INFO - "name" : "Fossil Gas - Actual Aggregated",
[2023-01-31T04:05:59.568+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:05:59.568+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:05:59.568+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:05:59.568+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:05:59.569+0000] {spark_submit.py:495} INFO - "name" : "Fossil Hard coal - Actual Aggregated",
[2023-01-31T04:05:59.569+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:05:59.569+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:05:59.570+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:05:59.570+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:05:59.570+0000] {spark_submit.py:495} INFO - "name" : "Fossil Oil - Actual Aggregated",
[2023-01-31T04:05:59.570+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:05:59.570+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:05:59.570+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:05:59.571+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:05:59.571+0000] {spark_submit.py:495} INFO - "name" : "Geothermal - Actual Aggregated",
[2023-01-31T04:05:59.571+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:05:59.571+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:05:59.571+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:05:59.571+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:05:59.572+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Aggregated",
[2023-01-31T04:05:59.572+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:05:59.572+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:05:59.614+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:05:59.614+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:05:59.614+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Consumption",
[2023-01-31T04:05:59.615+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:05:59.615+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:05:59.615+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:05:59.615+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:05:59.628+0000] {spark_submit.py:495} INFO - "name" : "Hydro Run-of-river and poundage - Actual Aggregated",
[2023-01-31T04:05:59.628+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:05:59.628+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:05:59.628+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:05:59.632+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:05:59.634+0000] {spark_submit.py:495} INFO - "name" : "Hydro Water Reservoir - Actual Aggregated",
[2023-01-31T04:05:59.645+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:05:59.645+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:05:59.659+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:05:59.664+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:05:59.664+0000] {spark_submit.py:495} INFO - "name" : "Nuclear - Actual Aggregated",
[2023-01-31T04:05:59.664+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:05:59.664+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:05:59.664+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:05:59.665+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:05:59.665+0000] {spark_submit.py:495} INFO - "name" : "Other - Actual Aggregated",
[2023-01-31T04:05:59.665+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:05:59.665+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:05:59.669+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:05:59.669+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:05:59.669+0000] {spark_submit.py:495} INFO - "name" : "Other renewable - Actual Aggregated",
[2023-01-31T04:05:59.670+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:05:59.670+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:05:59.670+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:05:59.670+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:05:59.670+0000] {spark_submit.py:495} INFO - "name" : "Solar - Actual Aggregated",
[2023-01-31T04:05:59.670+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:05:59.671+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:05:59.671+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:05:59.671+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:05:59.671+0000] {spark_submit.py:495} INFO - "name" : "Waste - Actual Aggregated",
[2023-01-31T04:05:59.671+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:05:59.671+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:05:59.672+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:05:59.672+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:05:59.672+0000] {spark_submit.py:495} INFO - "name" : "Wind Offshore - Actual Aggregated",
[2023-01-31T04:05:59.672+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:05:59.674+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:05:59.680+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:05:59.687+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:05:59.687+0000] {spark_submit.py:495} INFO - "name" : "Wind Onshore - Actual Aggregated",
[2023-01-31T04:05:59.687+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:05:59.687+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:05:59.687+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:05:59.688+0000] {spark_submit.py:495} INFO - } ]
[2023-01-31T04:05:59.688+0000] {spark_submit.py:495} INFO - }
[2023-01-31T04:05:59.688+0000] {spark_submit.py:495} INFO - and corresponding Parquet message type:
[2023-01-31T04:05:59.694+0000] {spark_submit.py:495} INFO - message spark_schema {
[2023-01-31T04:05:59.698+0000] {spark_submit.py:495} INFO - required int96 measured_at;
[2023-01-31T04:05:59.705+0000] {spark_submit.py:495} INFO - required int96 created_at;
[2023-01-31T04:05:59.705+0000] {spark_submit.py:495} INFO - optional double Biomass - Actual Aggregated;
[2023-01-31T04:05:59.706+0000] {spark_submit.py:495} INFO - optional double Fossil Brown coal/Lignite - Actual Aggregated;
[2023-01-31T04:05:59.706+0000] {spark_submit.py:495} INFO - optional double Fossil Gas - Actual Aggregated;
[2023-01-31T04:05:59.706+0000] {spark_submit.py:495} INFO - optional double Fossil Hard coal - Actual Aggregated;
[2023-01-31T04:05:59.706+0000] {spark_submit.py:495} INFO - optional double Fossil Oil - Actual Aggregated;
[2023-01-31T04:05:59.706+0000] {spark_submit.py:495} INFO - optional double Geothermal - Actual Aggregated;
[2023-01-31T04:05:59.707+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Aggregated;
[2023-01-31T04:05:59.707+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Consumption;
[2023-01-31T04:05:59.707+0000] {spark_submit.py:495} INFO - optional double Hydro Run-of-river and poundage - Actual Aggregated;
[2023-01-31T04:05:59.707+0000] {spark_submit.py:495} INFO - optional double Hydro Water Reservoir - Actual Aggregated;
[2023-01-31T04:05:59.707+0000] {spark_submit.py:495} INFO - optional double Nuclear - Actual Aggregated;
[2023-01-31T04:05:59.707+0000] {spark_submit.py:495} INFO - optional double Other - Actual Aggregated;
[2023-01-31T04:05:59.708+0000] {spark_submit.py:495} INFO - optional double Other renewable - Actual Aggregated;
[2023-01-31T04:05:59.708+0000] {spark_submit.py:495} INFO - optional double Solar - Actual Aggregated;
[2023-01-31T04:05:59.710+0000] {spark_submit.py:495} INFO - optional double Waste - Actual Aggregated;
[2023-01-31T04:05:59.712+0000] {spark_submit.py:495} INFO - optional double Wind Offshore - Actual Aggregated;
[2023-01-31T04:05:59.712+0000] {spark_submit.py:495} INFO - optional double Wind Onshore - Actual Aggregated;
[2023-01-31T04:05:59.712+0000] {spark_submit.py:495} INFO - }
[2023-01-31T04:05:59.712+0000] {spark_submit.py:495} INFO - 
[2023-01-31T04:05:59.712+0000] {spark_submit.py:495} INFO - 
[2023-01-31T04:06:01.008+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:01 INFO CodecPool: Got brand-new compressor [.snappy]
[2023-01-31T04:06:01.289+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:01 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 2f101bc15e7a:39635 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T04:06:01.388+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:01 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 2f101bc15e7a:39635 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T04:06:01.600+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:01 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 2f101bc15e7a:39635 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T04:06:01.668+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:01 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 2f101bc15e7a:39635 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T04:06:06.501+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:06 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675137355010-f21b82d8-d466-49cf-af83-9f92f1359223/_temporary/0/_temporary/' directory.
[2023-01-31T04:06:06.502+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:06 INFO FileOutputCommitter: Saved output of task 'attempt_202301310405587494316556344910024_0008_m_000000_8' to gs://entsoe_temp_1009/.spark-bigquery-local-1675137355010-f21b82d8-d466-49cf-af83-9f92f1359223/_temporary/0/task_202301310405587494316556344910024_0008_m_000000
[2023-01-31T04:06:06.504+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:06 INFO SparkHadoopMapRedUtil: attempt_202301310405587494316556344910024_0008_m_000000_8: Committed. Elapsed time: 1645 ms.
[2023-01-31T04:06:06.547+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:06 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2588 bytes result sent to driver
[2023-01-31T04:06:06.552+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:06 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 7785 ms on 2f101bc15e7a (executor driver) (1/1)
[2023-01-31T04:06:06.553+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:06 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2023-01-31T04:06:06.553+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:06 INFO DAGScheduler: ResultStage 8 (save at BigQueryWriteHelper.java:105) finished in 7.921 s
[2023-01-31T04:06:06.554+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:06 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:06:06.554+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2023-01-31T04:06:06.555+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:06 INFO DAGScheduler: Job 8 finished: save at BigQueryWriteHelper.java:105, took 7.930841 s
[2023-01-31T04:06:06.558+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:06 INFO FileFormatWriter: Start to commit write Job 9e3fa2c3-020a-4812-88aa-ae29f6f713dd.
[2023-01-31T04:06:07.482+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:07 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675137355010-f21b82d8-d466-49cf-af83-9f92f1359223/_temporary/0/task_202301310405587494316556344910024_0008_m_000000/' directory.
[2023-01-31T04:06:07.838+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:07 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675137355010-f21b82d8-d466-49cf-af83-9f92f1359223/' directory.
[2023-01-31T04:06:08.344+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:08 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 2f101bc15e7a:39635 in memory (size: 74.3 KiB, free: 434.4 MiB)
[2023-01-31T04:06:08.638+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:08 INFO FileFormatWriter: Write Job 9e3fa2c3-020a-4812-88aa-ae29f6f713dd committed. Elapsed time: 2076 ms.
[2023-01-31T04:06:08.677+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:08 INFO FileFormatWriter: Finished processing stats for write job 9e3fa2c3-020a-4812-88aa-ae29f6f713dd.
[2023-01-31T04:06:10.432+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:10 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=entsoe_analytics_1009, projectId=rafzul-analytics-1009, tableId=TEST_total_generation_staging}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=null, ignoreUnknownValue=null, sourceUris=[gs://entsoe_temp_1009/.spark-bigquery-local-1675137355010-f21b82d8-d466-49cf-af83-9f92f1359223/part-00000-955c944b-5630-4bda-8fa5-60b09d9ac13c-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=true, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=rafzul-analytics-1009, job=76f37a7d-a49d-4919-9939-a30b8091bba4, location=US}
[2023-01-31T04:06:13.929+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:13 INFO BigQueryClient: Done loading to rafzul-analytics-1009.entsoe_analytics_1009.TEST_total_generation_staging. jobId: JobId{project=rafzul-analytics-1009, job=76f37a7d-a49d-4919-9939-a30b8091bba4, location=US}
[2023-01-31T04:06:14.751+0000] {spark_submit.py:495} INFO - MANTEP!!!!
[2023-01-31T04:06:14.944+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:14 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-31T04:06:14.962+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:14 INFO SparkUI: Stopped Spark web UI at http://2f101bc15e7a:4041
[2023-01-31T04:06:14.983+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:14 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-31T04:06:15.018+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:15 INFO MemoryStore: MemoryStore cleared
[2023-01-31T04:06:15.019+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:15 INFO BlockManager: BlockManager stopped
[2023-01-31T04:06:15.025+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:15 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-31T04:06:15.032+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:15 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-31T04:06:15.045+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:15 INFO SparkContext: Successfully stopped SparkContext
[2023-01-31T04:06:15.047+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:15 INFO ShutdownHookManager: Shutdown hook called
[2023-01-31T04:06:15.048+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:15 INFO ShutdownHookManager: Deleting directory /tmp/spark-6e28a2af-e54b-41cf-b6bb-f894ec751596
[2023-01-31T04:06:15.057+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:15 INFO ShutdownHookManager: Deleting directory /tmp/spark-6e28a2af-e54b-41cf-b6bb-f894ec751596/pyspark-6a555098-f919-4a81-bfe5-044d5dab1366
[2023-01-31T04:06:15.067+0000] {spark_submit.py:495} INFO - 23/01/31 04:06:15 INFO ShutdownHookManager: Deleting directory /tmp/spark-378828cb-0c6d-4db4-8627-b3a1c89a0c13
[2023-01-31T04:06:15.259+0000] {taskinstance.py:1322} INFO - Marking task as SUCCESS. dag_id=entsoe-energydata, task_id=stage_total_generation, execution_date=20210101T000000, start_date=20230131T035402, end_date=20230131T040615
[2023-01-31T04:06:15.356+0000] {local_task_job.py:159} INFO - Task exited with return code 0
[2023-01-31T04:06:15.398+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T04:24:15.605+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-31T04:24:15.648+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-31T04:24:15.656+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T04:24:15.656+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T04:24:15.657+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T04:24:15.733+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 00:00:00+00:00
[2023-01-31T04:24:15.750+0000] {standard_task_runner.py:55} INFO - Started process 259 to run task
[2023-01-31T04:24:15.761+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata', 'stage_total_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '840', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp6c60lrpv']
[2023-01-31T04:24:15.769+0000] {standard_task_runner.py:83} INFO - Job 840: Subtask stage_total_generation
[2023-01-31T04:24:15.977+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 81d5fcd0285b
[2023-01-31T04:24:16.222+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-31T04:24:16.262+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T04:24:16.269+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-31T04:24:39.713+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:39 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T04:24:40.576+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T04:24:41.766+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:41 INFO ResourceUtils: ==============================================================
[2023-01-31T04:24:41.792+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:41 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T04:24:41.793+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:41 INFO ResourceUtils: ==============================================================
[2023-01-31T04:24:41.794+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:41 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T04:24:42.064+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:42 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T04:24:42.093+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:42 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T04:24:42.111+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:42 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T04:24:42.747+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:42 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T04:24:42.758+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:42 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T04:24:42.768+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:42 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T04:24:42.769+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:42 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T04:24:42.770+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:42 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T04:24:45.169+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:45 INFO Utils: Successfully started service 'sparkDriver' on port 37881.
[2023-01-31T04:24:45.536+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:45 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T04:24:45.832+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:45 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T04:24:46.140+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:46 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T04:24:46.156+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:46 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T04:24:46.176+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:46 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T04:24:46.378+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:46 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-8f04f250-ab58-47c0-a6ef-c0084bc8ae71
[2023-01-31T04:24:46.521+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:46 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T04:24:46.699+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:46 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T04:24:48.436+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:48 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T04:24:48.512+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:48 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2023-01-31T04:24:48.904+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:48 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://81d5fcd0285b:37881/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675139079661
[2023-01-31T04:24:48.914+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:48 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://81d5fcd0285b:37881/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675139079661
[2023-01-31T04:24:49.633+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:49 INFO Executor: Starting executor ID driver on host 81d5fcd0285b
[2023-01-31T04:24:49.671+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:49 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T04:24:49.783+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:49 INFO Executor: Fetching spark://81d5fcd0285b:37881/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675139079661
[2023-01-31T04:24:50.116+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:50 INFO TransportClientFactory: Successfully created connection to 81d5fcd0285b/172.19.0.9:37881 after 195 ms (0 ms spent in bootstraps)
[2023-01-31T04:24:50.187+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:50 INFO Utils: Fetching spark://81d5fcd0285b:37881/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-0ac15da1-368f-45a6-987e-2b523089f0f2/userFiles-1bbff517-5506-4b96-9daf-27c5b5e14059/fetchFileTemp11995569038076835219.tmp
[2023-01-31T04:24:52.128+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:52 INFO Executor: Adding file:/tmp/spark-0ac15da1-368f-45a6-987e-2b523089f0f2/userFiles-1bbff517-5506-4b96-9daf-27c5b5e14059/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T04:24:52.129+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:52 INFO Executor: Fetching spark://81d5fcd0285b:37881/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675139079661
[2023-01-31T04:24:52.130+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:52 INFO Utils: Fetching spark://81d5fcd0285b:37881/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-0ac15da1-368f-45a6-987e-2b523089f0f2/userFiles-1bbff517-5506-4b96-9daf-27c5b5e14059/fetchFileTemp13299411996999454664.tmp
[2023-01-31T04:24:52.935+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:52 INFO Executor: Adding file:/tmp/spark-0ac15da1-368f-45a6-987e-2b523089f0f2/userFiles-1bbff517-5506-4b96-9daf-27c5b5e14059/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T04:24:52.996+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:52 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37445.
[2023-01-31T04:24:52.997+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:52 INFO NettyBlockTransferService: Server created on 81d5fcd0285b:37445
[2023-01-31T04:24:53.000+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:53 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T04:24:53.064+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:53 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 37445, None)
[2023-01-31T04:24:53.100+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:53 INFO BlockManagerMasterEndpoint: Registering block manager 81d5fcd0285b:37445 with 434.4 MiB RAM, BlockManagerId(driver, 81d5fcd0285b, 37445, None)
[2023-01-31T04:24:53.124+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:53 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 37445, None)
[2023-01-31T04:24:53.125+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:53 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 81d5fcd0285b, 37445, None)
[2023-01-31T04:24:57.044+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:57 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T04:24:57.070+0000] {spark_submit.py:495} INFO - 23/01/31 04:24:57 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T04:25:11.419+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:11 INFO InMemoryFileIndex: It took 548 ms to list leaf files for 1 paths.
[2023-01-31T04:25:12.366+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:12 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T04:25:12.887+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:12 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T04:25:12.893+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:12 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 81d5fcd0285b:37445 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T04:25:12.923+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:12 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T04:25:14.532+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:14 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T04:25:14.594+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:14 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T04:25:14.687+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:14 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T04:25:14.735+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:14 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T04:25:14.742+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:14 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T04:25:14.744+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:14 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:25:14.748+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:14 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:25:14.768+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:14 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T04:25:14.993+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:14 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T04:25:15.008+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:15 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T04:25:15.011+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:15 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 81d5fcd0285b:37445 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T04:25:15.013+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:15 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:25:15.072+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:25:15.073+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:15 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T04:25:15.188+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:15 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T04:25:15.246+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:15 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T04:25:15.692+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:15 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010000__202101010100.json:0+9369
[2023-01-31T04:25:16.619+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:16 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3437 bytes result sent to driver
[2023-01-31T04:25:16.694+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:16 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1545 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:25:16.749+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:16 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 1.914 s
[2023-01-31T04:25:16.770+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:16 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:25:16.782+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:16 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T04:25:16.783+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T04:25:16.804+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:16 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 2.112805 s
[2023-01-31T04:25:20.737+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:20 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 81d5fcd0285b:37445 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T04:25:20.852+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:20 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 81d5fcd0285b:37445 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T04:25:29.547+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:29 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T04:25:29.548+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:29 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T04:25:29.572+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:29 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T04:25:30.715+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:30 INFO CodeGenerator: Code generated in 450.680847 ms
[2023-01-31T04:25:30.749+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:30 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T04:25:30.780+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:30 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T04:25:30.783+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:30 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 81d5fcd0285b:37445 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T04:25:30.786+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:30 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T04:25:30.805+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T04:25:31.012+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:31 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T04:25:31.014+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:31 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T04:25:31.014+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:31 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T04:25:31.014+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:31 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:25:31.015+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:31 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:25:31.026+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:31 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T04:25:31.061+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:31 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T04:25:31.080+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:31 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T04:25:31.084+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:31 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 81d5fcd0285b:37445 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T04:25:31.087+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:31 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:25:31.092+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:25:31.092+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:31 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T04:25:31.100+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:31 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T04:25:31.102+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:31 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T04:25:31.262+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:31 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T04:25:31.570+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:31 INFO CodeGenerator: Code generated in 232.989363 ms
[2023-01-31T04:25:31.885+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:31 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1628 bytes result sent to driver
[2023-01-31T04:25:31.916+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:31 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 821 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:25:31.917+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:31 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T04:25:31.922+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:31 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 0.887 s
[2023-01-31T04:25:31.922+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:31 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:25:31.922+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T04:25:31.922+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:31 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 0.907216 s
[2023-01-31T04:25:32.042+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:32 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T04:25:32.044+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:32 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T04:25:32.045+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:32 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T04:25:32.168+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:32 INFO CodeGenerator: Code generated in 62.84181 ms
[2023-01-31T04:25:32.187+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:32 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T04:25:32.212+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:32 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T04:25:32.214+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:32 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 81d5fcd0285b:37445 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T04:25:32.216+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:32 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T04:25:32.220+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:32 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T04:25:32.255+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:32 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T04:25:32.259+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:32 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T04:25:32.260+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:32 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T04:25:32.261+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:32 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:25:32.261+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:32 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:25:32.268+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:32 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T04:25:32.272+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:32 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T04:25:32.279+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:32 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T04:25:32.281+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:32 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 81d5fcd0285b:37445 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T04:25:32.288+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:32 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:25:32.288+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:25:32.289+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:32 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T04:25:32.292+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:32 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T04:25:32.298+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:32 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T04:25:32.325+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:32 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T04:25:32.501+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:32 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1585 bytes result sent to driver
[2023-01-31T04:25:32.522+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:32 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 231 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:25:32.522+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:32 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T04:25:32.529+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:32 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 0.259 s
[2023-01-31T04:25:32.530+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:32 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:25:32.530+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T04:25:32.530+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:32 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 0.269625 s
[2023-01-31T04:25:32.983+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:32 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T04:25:33.005+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:33 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T04:25:33.006+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:33 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T04:25:33.762+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:33 INFO CodeGenerator: Code generated in 518.890991 ms
[2023-01-31T04:25:33.812+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:33 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T04:25:33.889+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:33 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T04:25:33.895+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:33 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 81d5fcd0285b:37445 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T04:25:33.912+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:33 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T04:25:33.965+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T04:25:34.076+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T04:25:34.077+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T04:25:34.078+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T04:25:34.078+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:25:34.078+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:25:34.085+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T04:25:34.085+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.6 MiB)
[2023-01-31T04:25:34.170+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.6 MiB)
[2023-01-31T04:25:34.172+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 81d5fcd0285b:37445 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T04:25:34.176+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:25:34.190+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:25:34.193+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T04:25:34.199+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T04:25:34.201+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T04:25:34.272+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T04:25:34.494+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-01-31T04:25:34.516+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 318 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:25:34.522+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T04:25:34.526+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 0.443 s
[2023-01-31T04:25:34.526+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:25:34.527+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T04:25:34.527+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 0.450340 s
[2023-01-31T04:25:34.626+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T04:25:34.626+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T04:25:34.628+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T04:25:34.730+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO CodeGenerator: Code generated in 63.487476 ms
[2023-01-31T04:25:34.757+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.4 MiB)
[2023-01-31T04:25:34.911+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T04:25:34.916+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 81d5fcd0285b:37445 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T04:25:34.918+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T04:25:34.919+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T04:25:34.954+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T04:25:34.956+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T04:25:34.957+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T04:25:34.957+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:25:34.957+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:25:34.960+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T04:25:34.964+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:34 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-31T04:25:35.033+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:35 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-01-31T04:25:35.052+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:35 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 81d5fcd0285b:37445 (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T04:25:35.056+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:35 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:25:35.057+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:25:35.069+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:35 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T04:25:35.073+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:35 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T04:25:35.074+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:35 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T04:25:35.109+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:35 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T04:25:35.326+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:35 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1586 bytes result sent to driver
[2023-01-31T04:25:35.330+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:35 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 258 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:25:35.330+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:35 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T04:25:35.333+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:35 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 0.370 s
[2023-01-31T04:25:35.334+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:35 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:25:35.335+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T04:25:35.335+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:35 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 0.378925 s
[2023-01-31T04:25:36.151+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:36 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T04:25:36.160+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:36 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T04:25:36.175+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:36 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T04:25:36.658+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:36 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 81d5fcd0285b:37445 in memory (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T04:25:36.762+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:36 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 81d5fcd0285b:37445 in memory (size: 7.7 KiB, free: 434.2 MiB)
[2023-01-31T04:25:36.835+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:36 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 81d5fcd0285b:37445 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T04:25:36.886+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:36 INFO CodeGenerator: Code generated in 331.459058 ms
[2023-01-31T04:25:36.908+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:36 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.2 MiB)
[2023-01-31T04:25:36.959+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:36 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 81d5fcd0285b:37445 in memory (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T04:25:36.977+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:36 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T04:25:36.980+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:36 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 81d5fcd0285b:37445 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T04:25:36.984+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:36 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T04:25:36.986+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:36 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T04:25:37.185+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:37 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T04:25:37.188+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:37 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) with 1 output partitions
[2023-01-31T04:25:37.188+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:37 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201)
[2023-01-31T04:25:37.189+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:37 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:25:37.189+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:37 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:25:37.199+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:37 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201), which has no missing parents
[2023-01-31T04:25:37.218+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:37 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.2 MiB)
[2023-01-31T04:25:37.244+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:37 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.2 MiB)
[2023-01-31T04:25:37.245+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:37 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 81d5fcd0285b:37445 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T04:25:37.249+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:37 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:25:37.258+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:25:37.261+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:37 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T04:25:37.274+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:37 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T04:25:37.277+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:37 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T04:25:37.307+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:37 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T04:25:37.481+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:37 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2558 bytes result sent to driver
[2023-01-31T04:25:37.482+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:37 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 208 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:25:37.483+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:37 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T04:25:37.483+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:37 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) finished in 0.292 s
[2023-01-31T04:25:37.483+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:37 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:25:37.483+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T04:25:37.484+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:37 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201, took 0.298381 s
[2023-01-31T04:25:37.627+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:37 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T04:25:37.629+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:37 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T04:25:37.629+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:37 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T04:25:37.785+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:37 INFO CodeGenerator: Code generated in 102.00583 ms
[2023-01-31T04:25:37.831+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:37 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 433.0 MiB)
[2023-01-31T04:25:37.949+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:37 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.0 MiB)
[2023-01-31T04:25:37.953+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:37 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 81d5fcd0285b:37445 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T04:25:37.954+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:37 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T04:25:37.965+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:37 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T04:25:38.180+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T04:25:38.182+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T04:25:38.183+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T04:25:38.183+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:25:38.183+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:25:38.190+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T04:25:38.192+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 432.9 MiB)
[2023-01-31T04:25:38.223+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.9 MiB)
[2023-01-31T04:25:38.224+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 81d5fcd0285b:37445 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T04:25:38.226+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:25:38.230+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:25:38.233+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T04:25:38.239+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T04:25:38.245+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T04:25:38.284+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T04:25:38.466+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-01-31T04:25:38.470+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 232 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:25:38.471+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T04:25:38.474+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 0.287 s
[2023-01-31T04:25:38.475+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:25:38.475+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T04:25:38.475+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 0.291745 s
[2023-01-31T04:25:38.697+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T04:25:38.709+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T04:25:38.709+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T04:25:38.874+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO CodeGenerator: Code generated in 52.622464 ms
[2023-01-31T04:25:38.876+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.7 MiB)
[2023-01-31T04:25:38.898+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.7 MiB)
[2023-01-31T04:25:38.906+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 81d5fcd0285b:37445 (size: 34.5 KiB, free: 434.1 MiB)
[2023-01-31T04:25:38.909+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T04:25:38.921+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T04:25:39.148+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:39 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T04:25:39.152+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:39 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T04:25:39.153+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:39 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T04:25:39.153+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:39 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:25:39.153+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:39 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:25:39.157+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:39 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T04:25:39.164+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:39 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.7 MiB)
[2023-01-31T04:25:39.185+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:39 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.6 MiB)
[2023-01-31T04:25:39.189+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:39 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 81d5fcd0285b:37445 (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T04:25:39.192+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:39 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:25:39.196+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:25:39.196+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:39 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T04:25:39.209+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:39 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T04:25:39.209+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:39 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T04:25:39.229+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:39 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T04:25:39.406+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:39 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1729 bytes result sent to driver
[2023-01-31T04:25:39.423+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:39 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 221 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:25:39.428+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:39 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T04:25:39.429+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:39 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 0.264 s
[2023-01-31T04:25:39.429+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:39 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:25:39.429+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T04:25:39.429+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:39 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 0.277068 s
[2023-01-31T04:25:41.604+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:41 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 81d5fcd0285b:37445 in memory (size: 12.6 KiB, free: 434.1 MiB)
[2023-01-31T04:25:41.687+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:41 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 81d5fcd0285b:37445 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T04:25:41.772+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:41 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 81d5fcd0285b:37445 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T04:25:41.833+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:41 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 81d5fcd0285b:37445 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T04:25:41.917+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:41 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 81d5fcd0285b:37445 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T04:25:42.000+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:42 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 81d5fcd0285b:37445 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T04:25:42.085+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:42 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 81d5fcd0285b:37445 in memory (size: 13.4 KiB, free: 434.3 MiB)
[2023-01-31T04:25:42.170+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:42 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 81d5fcd0285b:37445 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T04:25:44.469+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:44 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 81d5fcd0285b:37445 in memory (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T04:25:48.916+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:48 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T04:25:48.991+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T04:25:48.995+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T04:25:49.004+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:49 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T04:25:49.005+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T04:25:49.006+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T04:25:49.015+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T04:25:49.795+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:49 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-31T04:25:49.795+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:49 INFO DAGScheduler: Got job 8 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:25:49.798+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:49 INFO DAGScheduler: Final stage: ResultStage 8 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:25:49.798+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:49 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:25:49.798+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:49 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:25:49.804+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:49 INFO DAGScheduler: Submitting ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:25:50.012+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:50 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 209.9 KiB, free 434.0 MiB)
[2023-01-31T04:25:50.017+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:50 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 74.3 KiB, free 433.9 MiB)
[2023-01-31T04:25:50.020+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:50 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 81d5fcd0285b:37445 (size: 74.3 KiB, free: 434.3 MiB)
[2023-01-31T04:25:50.022+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:50 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:25:50.024+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (ParallelCollectionRDD[118] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:25:50.026+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:50 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2023-01-31T04:25:50.036+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:50 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7484 bytes) taskResourceAssignments Map()
[2023-01-31T04:25:50.037+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:50 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2023-01-31T04:25:50.166+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T04:25:50.167+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T04:25:50.167+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:50 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T04:25:50.167+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T04:25:50.168+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T04:25:50.173+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T04:25:50.222+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:50 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T04:25:50.226+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:50 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T04:25:50.285+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:50 INFO ParquetOutputFormat: Parquet block size to 134217728
[2023-01-31T04:25:50.286+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:50 INFO ParquetOutputFormat: Validation is off
[2023-01-31T04:25:50.287+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:50 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-01-31T04:25:50.287+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:50 INFO ParquetOutputFormat: Parquet properties are:
[2023-01-31T04:25:50.287+0000] {spark_submit.py:495} INFO - Parquet page size to 1048576
[2023-01-31T04:25:50.287+0000] {spark_submit.py:495} INFO - Parquet dictionary page size to 1048576
[2023-01-31T04:25:50.288+0000] {spark_submit.py:495} INFO - Dictionary is true
[2023-01-31T04:25:50.288+0000] {spark_submit.py:495} INFO - Writer version is: PARQUET_1_0
[2023-01-31T04:25:50.288+0000] {spark_submit.py:495} INFO - Page size checking is: estimated
[2023-01-31T04:25:50.288+0000] {spark_submit.py:495} INFO - Min row count for page size check is: 100
[2023-01-31T04:25:50.289+0000] {spark_submit.py:495} INFO - Max row count for page size check is: 10000
[2023-01-31T04:25:50.289+0000] {spark_submit.py:495} INFO - Truncate length for column indexes is: 64
[2023-01-31T04:25:50.289+0000] {spark_submit.py:495} INFO - Truncate length for statistics min/max  is: 2147483647
[2023-01-31T04:25:50.289+0000] {spark_submit.py:495} INFO - Bloom filter enabled: false
[2023-01-31T04:25:50.290+0000] {spark_submit.py:495} INFO - Max Bloom filter size for a column is 1048576
[2023-01-31T04:25:50.290+0000] {spark_submit.py:495} INFO - Bloom filter expected number of distinct values are: null
[2023-01-31T04:25:50.290+0000] {spark_submit.py:495} INFO - Page row count limit to 20000
[2023-01-31T04:25:50.290+0000] {spark_submit.py:495} INFO - Writing page checksums is: on
[2023-01-31T04:25:50.360+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:50 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-01-31T04:25:50.361+0000] {spark_submit.py:495} INFO - {
[2023-01-31T04:25:50.361+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-31T04:25:50.361+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-31T04:25:50.362+0000] {spark_submit.py:495} INFO - "name" : "measured_at",
[2023-01-31T04:25:50.362+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T04:25:50.362+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T04:25:50.362+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:25:50.362+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:25:50.363+0000] {spark_submit.py:495} INFO - "name" : "created_at",
[2023-01-31T04:25:50.363+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T04:25:50.363+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T04:25:50.363+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:25:50.364+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:25:50.366+0000] {spark_submit.py:495} INFO - "name" : "Biomass - Actual Aggregated",
[2023-01-31T04:25:50.366+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:25:50.366+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:25:50.366+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:25:50.367+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:25:50.367+0000] {spark_submit.py:495} INFO - "name" : "Fossil Brown coal/Lignite - Actual Aggregated",
[2023-01-31T04:25:50.367+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:25:50.368+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:25:50.368+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:25:50.368+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:25:50.369+0000] {spark_submit.py:495} INFO - "name" : "Fossil Gas - Actual Aggregated",
[2023-01-31T04:25:50.369+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:25:50.369+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:25:50.371+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:25:50.372+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:25:50.372+0000] {spark_submit.py:495} INFO - "name" : "Fossil Hard coal - Actual Aggregated",
[2023-01-31T04:25:50.372+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:25:50.372+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:25:50.373+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:25:50.373+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:25:50.373+0000] {spark_submit.py:495} INFO - "name" : "Fossil Oil - Actual Aggregated",
[2023-01-31T04:25:50.373+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:25:50.376+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:25:50.377+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:25:50.377+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:25:50.377+0000] {spark_submit.py:495} INFO - "name" : "Geothermal - Actual Aggregated",
[2023-01-31T04:25:50.378+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:25:50.378+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:25:50.378+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:25:50.378+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:25:50.379+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Aggregated",
[2023-01-31T04:25:50.379+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:25:50.379+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:25:50.379+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:25:50.380+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:25:50.380+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Consumption",
[2023-01-31T04:25:50.381+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:25:50.389+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:25:50.390+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:25:50.390+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:25:50.391+0000] {spark_submit.py:495} INFO - "name" : "Hydro Run-of-river and poundage - Actual Aggregated",
[2023-01-31T04:25:50.392+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:25:50.394+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:25:50.396+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:25:50.397+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:25:50.397+0000] {spark_submit.py:495} INFO - "name" : "Hydro Water Reservoir - Actual Aggregated",
[2023-01-31T04:25:50.398+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:25:50.399+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:25:50.399+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:25:50.400+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:25:50.400+0000] {spark_submit.py:495} INFO - "name" : "Nuclear - Actual Aggregated",
[2023-01-31T04:25:50.401+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:25:50.404+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:25:50.405+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:25:50.405+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:25:50.406+0000] {spark_submit.py:495} INFO - "name" : "Other - Actual Aggregated",
[2023-01-31T04:25:50.406+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:25:50.406+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:25:50.407+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:25:50.407+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:25:50.407+0000] {spark_submit.py:495} INFO - "name" : "Other renewable - Actual Aggregated",
[2023-01-31T04:25:50.407+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:25:50.408+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:25:50.408+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:25:50.408+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:25:50.409+0000] {spark_submit.py:495} INFO - "name" : "Solar - Actual Aggregated",
[2023-01-31T04:25:50.409+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:25:50.409+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:25:50.410+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:25:50.410+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:25:50.412+0000] {spark_submit.py:495} INFO - "name" : "Waste - Actual Aggregated",
[2023-01-31T04:25:50.418+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:25:50.418+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:25:50.418+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:25:50.419+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:25:50.419+0000] {spark_submit.py:495} INFO - "name" : "Wind Offshore - Actual Aggregated",
[2023-01-31T04:25:50.419+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:25:50.419+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:25:50.420+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:25:50.420+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:25:50.420+0000] {spark_submit.py:495} INFO - "name" : "Wind Onshore - Actual Aggregated",
[2023-01-31T04:25:50.420+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:25:50.421+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:25:50.421+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:25:50.421+0000] {spark_submit.py:495} INFO - } ]
[2023-01-31T04:25:50.421+0000] {spark_submit.py:495} INFO - }
[2023-01-31T04:25:50.421+0000] {spark_submit.py:495} INFO - and corresponding Parquet message type:
[2023-01-31T04:25:50.422+0000] {spark_submit.py:495} INFO - message spark_schema {
[2023-01-31T04:25:50.422+0000] {spark_submit.py:495} INFO - required int96 measured_at;
[2023-01-31T04:25:50.422+0000] {spark_submit.py:495} INFO - required int96 created_at;
[2023-01-31T04:25:50.422+0000] {spark_submit.py:495} INFO - optional double Biomass - Actual Aggregated;
[2023-01-31T04:25:50.423+0000] {spark_submit.py:495} INFO - optional double Fossil Brown coal/Lignite - Actual Aggregated;
[2023-01-31T04:25:50.423+0000] {spark_submit.py:495} INFO - optional double Fossil Gas - Actual Aggregated;
[2023-01-31T04:25:50.423+0000] {spark_submit.py:495} INFO - optional double Fossil Hard coal - Actual Aggregated;
[2023-01-31T04:25:50.423+0000] {spark_submit.py:495} INFO - optional double Fossil Oil - Actual Aggregated;
[2023-01-31T04:25:50.423+0000] {spark_submit.py:495} INFO - optional double Geothermal - Actual Aggregated;
[2023-01-31T04:25:50.424+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Aggregated;
[2023-01-31T04:25:50.425+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Consumption;
[2023-01-31T04:25:50.425+0000] {spark_submit.py:495} INFO - optional double Hydro Run-of-river and poundage - Actual Aggregated;
[2023-01-31T04:25:50.432+0000] {spark_submit.py:495} INFO - optional double Hydro Water Reservoir - Actual Aggregated;
[2023-01-31T04:25:50.433+0000] {spark_submit.py:495} INFO - optional double Nuclear - Actual Aggregated;
[2023-01-31T04:25:50.433+0000] {spark_submit.py:495} INFO - optional double Other - Actual Aggregated;
[2023-01-31T04:25:50.433+0000] {spark_submit.py:495} INFO - optional double Other renewable - Actual Aggregated;
[2023-01-31T04:25:50.434+0000] {spark_submit.py:495} INFO - optional double Solar - Actual Aggregated;
[2023-01-31T04:25:50.434+0000] {spark_submit.py:495} INFO - optional double Waste - Actual Aggregated;
[2023-01-31T04:25:50.434+0000] {spark_submit.py:495} INFO - optional double Wind Offshore - Actual Aggregated;
[2023-01-31T04:25:50.434+0000] {spark_submit.py:495} INFO - optional double Wind Onshore - Actual Aggregated;
[2023-01-31T04:25:50.434+0000] {spark_submit.py:495} INFO - }
[2023-01-31T04:25:50.435+0000] {spark_submit.py:495} INFO - 
[2023-01-31T04:25:50.435+0000] {spark_submit.py:495} INFO - 
[2023-01-31T04:25:50.852+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:50 INFO CodecPool: Got brand-new compressor [.snappy]
[2023-01-31T04:25:54.678+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:54 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675139089259-c50956ad-d503-4efb-95d3-854ac8fb229b/_temporary/0/_temporary/' directory.
[2023-01-31T04:25:54.679+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:54 INFO FileOutputCommitter: Saved output of task 'attempt_202301310425495929483664118798165_0008_m_000000_8' to gs://entsoe_temp_1009/.spark-bigquery-local-1675139089259-c50956ad-d503-4efb-95d3-854ac8fb229b/_temporary/0/task_202301310425495929483664118798165_0008_m_000000
[2023-01-31T04:25:54.684+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:54 INFO SparkHadoopMapRedUtil: attempt_202301310425495929483664118798165_0008_m_000000_8: Committed. Elapsed time: 841 ms.
[2023-01-31T04:25:54.745+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:54 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2588 bytes result sent to driver
[2023-01-31T04:25:54.759+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:54 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 4731 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:25:54.760+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:54 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2023-01-31T04:25:54.764+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:54 INFO DAGScheduler: ResultStage 8 (save at BigQueryWriteHelper.java:105) finished in 4.955 s
[2023-01-31T04:25:54.765+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:54 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:25:54.765+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2023-01-31T04:25:54.765+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:54 INFO DAGScheduler: Job 8 finished: save at BigQueryWriteHelper.java:105, took 4.967371 s
[2023-01-31T04:25:54.792+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:54 INFO FileFormatWriter: Start to commit write Job ad3039f5-5971-4f77-8701-f907273eec92.
[2023-01-31T04:25:55.512+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:55 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675139089259-c50956ad-d503-4efb-95d3-854ac8fb229b/_temporary/0/task_202301310425495929483664118798165_0008_m_000000/' directory.
[2023-01-31T04:25:55.984+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:55 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675139089259-c50956ad-d503-4efb-95d3-854ac8fb229b/' directory.
[2023-01-31T04:25:56.249+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:56 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 81d5fcd0285b:37445 in memory (size: 74.3 KiB, free: 434.4 MiB)
[2023-01-31T04:25:57.028+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:57 INFO FileFormatWriter: Write Job ad3039f5-5971-4f77-8701-f907273eec92 committed. Elapsed time: 2234 ms.
[2023-01-31T04:25:57.048+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:57 INFO FileFormatWriter: Finished processing stats for write job ad3039f5-5971-4f77-8701-f907273eec92.
[2023-01-31T04:25:58.455+0000] {spark_submit.py:495} INFO - 23/01/31 04:25:58 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=entsoe_analytics_1009, projectId=rafzul-analytics-1009, tableId=TEST_total_generation_staging}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=null, ignoreUnknownValue=null, sourceUris=[gs://entsoe_temp_1009/.spark-bigquery-local-1675139089259-c50956ad-d503-4efb-95d3-854ac8fb229b/part-00000-d80522fe-a861-4320-bd00-5d4a31b54474-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=true, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=rafzul-analytics-1009, job=e302f2ab-ea48-4420-a738-4953582cd5d8, location=US}
[2023-01-31T04:26:01.949+0000] {spark_submit.py:495} INFO - 23/01/31 04:26:01 INFO BigQueryClient: Done loading to rafzul-analytics-1009.entsoe_analytics_1009.TEST_total_generation_staging. jobId: JobId{project=rafzul-analytics-1009, job=e302f2ab-ea48-4420-a738-4953582cd5d8, location=US}
[2023-01-31T04:26:02.383+0000] {spark_submit.py:495} INFO - MANTEP!!!!
[2023-01-31T04:26:02.504+0000] {spark_submit.py:495} INFO - 23/01/31 04:26:02 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-31T04:26:02.514+0000] {spark_submit.py:495} INFO - 23/01/31 04:26:02 INFO SparkUI: Stopped Spark web UI at http://81d5fcd0285b:4041
[2023-01-31T04:26:02.526+0000] {spark_submit.py:495} INFO - 23/01/31 04:26:02 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-31T04:26:02.543+0000] {spark_submit.py:495} INFO - 23/01/31 04:26:02 INFO MemoryStore: MemoryStore cleared
[2023-01-31T04:26:02.544+0000] {spark_submit.py:495} INFO - 23/01/31 04:26:02 INFO BlockManager: BlockManager stopped
[2023-01-31T04:26:02.548+0000] {spark_submit.py:495} INFO - 23/01/31 04:26:02 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-31T04:26:02.552+0000] {spark_submit.py:495} INFO - 23/01/31 04:26:02 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-31T04:26:02.558+0000] {spark_submit.py:495} INFO - 23/01/31 04:26:02 INFO SparkContext: Successfully stopped SparkContext
[2023-01-31T04:26:02.559+0000] {spark_submit.py:495} INFO - 23/01/31 04:26:02 INFO ShutdownHookManager: Shutdown hook called
[2023-01-31T04:26:02.560+0000] {spark_submit.py:495} INFO - 23/01/31 04:26:02 INFO ShutdownHookManager: Deleting directory /tmp/spark-0ac15da1-368f-45a6-987e-2b523089f0f2
[2023-01-31T04:26:02.564+0000] {spark_submit.py:495} INFO - 23/01/31 04:26:02 INFO ShutdownHookManager: Deleting directory /tmp/spark-0ac15da1-368f-45a6-987e-2b523089f0f2/pyspark-657240ae-9929-4e6c-a103-542a657d4f8d
[2023-01-31T04:26:02.569+0000] {spark_submit.py:495} INFO - 23/01/31 04:26:02 INFO ShutdownHookManager: Deleting directory /tmp/spark-a5d21c86-97b0-42db-98e2-5eda45d201b9
[2023-01-31T04:26:02.658+0000] {taskinstance.py:1322} INFO - Marking task as SUCCESS. dag_id=entsoe-energydata, task_id=stage_total_generation, execution_date=20210101T000000, start_date=20230131T042415, end_date=20230131T042602
[2023-01-31T04:26:02.704+0000] {local_task_job.py:159} INFO - Task exited with return code 0
[2023-01-31T04:26:02.723+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T04:30:50.560+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-31T04:30:50.623+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-31T04:30:50.624+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T04:30:50.625+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T04:30:50.625+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T04:30:50.706+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 00:00:00+00:00
[2023-01-31T04:30:50.733+0000] {standard_task_runner.py:55} INFO - Started process 2637 to run task
[2023-01-31T04:30:50.766+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata', 'stage_total_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '852', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmpge9anw5i']
[2023-01-31T04:30:50.777+0000] {standard_task_runner.py:83} INFO - Job 852: Subtask stage_total_generation
[2023-01-31T04:30:51.102+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 81d5fcd0285b
[2023-01-31T04:30:51.469+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-31T04:30:51.510+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T04:30:51.516+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-31T04:31:24.715+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:24 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T04:31:25.898+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T04:31:27.994+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:27 INFO ResourceUtils: ==============================================================
[2023-01-31T04:31:28.016+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:28 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T04:31:28.026+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:28 INFO ResourceUtils: ==============================================================
[2023-01-31T04:31:28.038+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:28 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T04:31:28.423+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:28 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T04:31:28.460+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:28 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T04:31:28.467+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:28 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T04:31:28.735+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:28 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T04:31:28.755+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:28 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T04:31:28.757+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:28 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T04:31:28.767+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:28 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T04:31:28.775+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:28 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T04:31:31.268+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:31 INFO Utils: Successfully started service 'sparkDriver' on port 38031.
[2023-01-31T04:31:31.933+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:31 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T04:31:32.494+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:32 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T04:31:32.901+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:32 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T04:31:32.902+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:32 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T04:31:32.940+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:32 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T04:31:33.230+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:33 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d2320895-25ce-4182-9b47-4dafcf3b1ad4
[2023-01-31T04:31:33.382+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:33 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T04:31:33.708+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:33 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T04:31:35.849+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:35 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-01-31T04:31:36.003+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:36 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://81d5fcd0285b:38031/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675139484663
[2023-01-31T04:31:36.007+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:36 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://81d5fcd0285b:38031/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675139484663
[2023-01-31T04:31:36.569+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:36 INFO Executor: Starting executor ID driver on host 81d5fcd0285b
[2023-01-31T04:31:36.614+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:36 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T04:31:36.704+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:36 INFO Executor: Fetching spark://81d5fcd0285b:38031/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675139484663
[2023-01-31T04:31:37.057+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:37 INFO TransportClientFactory: Successfully created connection to 81d5fcd0285b/172.19.0.9:38031 after 183 ms (0 ms spent in bootstraps)
[2023-01-31T04:31:37.092+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:37 INFO Utils: Fetching spark://81d5fcd0285b:38031/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-2683b6fd-0438-4bd8-b192-8fd7f930ef2f/userFiles-0bb3b8af-cee7-48de-874b-a55995fda7e4/fetchFileTemp12821868941314136022.tmp
[2023-01-31T04:31:39.210+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:39 INFO Executor: Adding file:/tmp/spark-2683b6fd-0438-4bd8-b192-8fd7f930ef2f/userFiles-0bb3b8af-cee7-48de-874b-a55995fda7e4/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T04:31:39.210+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:39 INFO Executor: Fetching spark://81d5fcd0285b:38031/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675139484663
[2023-01-31T04:31:39.215+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:39 INFO Utils: Fetching spark://81d5fcd0285b:38031/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-2683b6fd-0438-4bd8-b192-8fd7f930ef2f/userFiles-0bb3b8af-cee7-48de-874b-a55995fda7e4/fetchFileTemp10302203284237276932.tmp
[2023-01-31T04:31:40.189+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:40 INFO Executor: Adding file:/tmp/spark-2683b6fd-0438-4bd8-b192-8fd7f930ef2f/userFiles-0bb3b8af-cee7-48de-874b-a55995fda7e4/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T04:31:40.280+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:40 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34123.
[2023-01-31T04:31:40.281+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:40 INFO NettyBlockTransferService: Server created on 81d5fcd0285b:34123
[2023-01-31T04:31:40.282+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:40 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T04:31:40.452+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:40 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 34123, None)
[2023-01-31T04:31:40.459+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:40 INFO BlockManagerMasterEndpoint: Registering block manager 81d5fcd0285b:34123 with 434.4 MiB RAM, BlockManagerId(driver, 81d5fcd0285b, 34123, None)
[2023-01-31T04:31:40.497+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:40 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 34123, None)
[2023-01-31T04:31:40.500+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:40 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 81d5fcd0285b, 34123, None)
[2023-01-31T04:31:44.244+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:44 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T04:31:44.259+0000] {spark_submit.py:495} INFO - 23/01/31 04:31:44 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T04:32:03.508+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:03 INFO InMemoryFileIndex: It took 394 ms to list leaf files for 1 paths.
[2023-01-31T04:32:06.292+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:06 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T04:32:07.140+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:07 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T04:32:07.256+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:07 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 81d5fcd0285b:34123 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T04:32:07.396+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:07 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T04:32:13.320+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:13 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T04:32:13.451+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:13 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T04:32:13.730+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:13 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T04:32:13.984+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:13 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T04:32:14.032+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:13 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T04:32:14.042+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:14 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:32:14.065+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:14 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:32:14.150+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:14 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T04:32:15.297+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:15 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T04:32:15.371+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:15 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T04:32:15.380+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:15 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 81d5fcd0285b:34123 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T04:32:15.392+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:15 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:32:15.837+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:32:15.837+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:15 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T04:32:16.091+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:16 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T04:32:16.775+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:16 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T04:32:19.653+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:19 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010000__202101010100.json:0+9369
[2023-01-31T04:32:23.105+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:23 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3437 bytes result sent to driver
[2023-01-31T04:32:23.401+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:23 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 7397 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:32:23.423+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:23 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T04:32:23.524+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:23 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 9.140 s
[2023-01-31T04:32:23.582+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:23 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:32:23.592+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T04:32:23.633+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:23 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 9.894590 s
[2023-01-31T04:32:26.419+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:26 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 81d5fcd0285b:34123 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T04:32:26.478+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:26 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 81d5fcd0285b:34123 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T04:32:51.215+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:51 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T04:32:51.259+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:51 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T04:32:51.289+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:51 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T04:32:55.010+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:55 INFO CodeGenerator: Code generated in 2698.894046 ms
[2023-01-31T04:32:55.156+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:55 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T04:32:55.558+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:55 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T04:32:55.601+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:55 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 81d5fcd0285b:34123 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T04:32:55.637+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:55 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T04:32:56.006+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:56 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T04:32:56.484+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:56 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T04:32:56.491+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:56 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T04:32:56.492+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:56 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T04:32:56.493+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:56 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:32:56.493+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:56 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:32:56.508+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:56 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T04:32:56.532+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:56 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T04:32:56.549+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:56 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T04:32:56.554+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:56 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 81d5fcd0285b:34123 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T04:32:56.575+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:56 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:32:56.628+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:32:56.634+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:56 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T04:32:56.684+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:56 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T04:32:56.685+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:56 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T04:32:57.334+0000] {spark_submit.py:495} INFO - 23/01/31 04:32:57 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T04:33:00.574+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:00 INFO CodeGenerator: Code generated in 3080.951123 ms
[2023-01-31T04:33:02.098+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:02 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1628 bytes result sent to driver
[2023-01-31T04:33:02.118+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:02 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 5473 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:33:02.118+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:02 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 5.600 s
[2023-01-31T04:33:02.118+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:02 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:33:02.124+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:02 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T04:33:02.133+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T04:33:02.135+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:02 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 5.643506 s
[2023-01-31T04:33:02.415+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:02 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 81d5fcd0285b:34123 in memory (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T04:33:03.055+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:03 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T04:33:03.055+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:03 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T04:33:03.085+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:03 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T04:33:03.449+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:03 INFO CodeGenerator: Code generated in 128.685055 ms
[2023-01-31T04:33:03.491+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:03 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T04:33:03.683+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:03 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T04:33:03.699+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:03 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 81d5fcd0285b:34123 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T04:33:03.706+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:03 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T04:33:03.747+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:03 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T04:33:04.293+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:04 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T04:33:04.299+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:04 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T04:33:04.316+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:04 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T04:33:04.318+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:04 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:33:04.321+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:04 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:33:04.365+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:04 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T04:33:04.452+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:04 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T04:33:04.466+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:04 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T04:33:04.468+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:04 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 81d5fcd0285b:34123 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T04:33:04.472+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:04 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:33:04.481+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:33:04.482+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:04 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T04:33:04.491+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:04 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T04:33:04.493+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:04 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T04:33:05.441+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:05 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T04:33:06.634+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:06 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1585 bytes result sent to driver
[2023-01-31T04:33:06.682+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:06 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 2196 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:33:06.684+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:06 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 2.289 s
[2023-01-31T04:33:06.684+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:06 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:33:06.685+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:06 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T04:33:06.686+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T04:33:06.689+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:06 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 2.395734 s
[2023-01-31T04:33:07.116+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:07 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 81d5fcd0285b:34123 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T04:33:08.343+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:08 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T04:33:08.389+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:08 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T04:33:08.391+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:08 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T04:33:10.191+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:10 INFO CodeGenerator: Code generated in 1172.08348 ms
[2023-01-31T04:33:10.306+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:10 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T04:33:10.987+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:10 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T04:33:10.999+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:10 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 81d5fcd0285b:34123 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T04:33:11.004+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:11 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T04:33:11.041+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:11 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T04:33:11.774+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:11 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T04:33:11.775+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:11 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T04:33:11.776+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:11 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T04:33:11.776+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:11 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:33:11.776+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:11 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:33:11.781+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:11 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T04:33:11.835+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:11 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.7 MiB)
[2023-01-31T04:33:11.883+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:11 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.7 MiB)
[2023-01-31T04:33:11.889+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:11 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 81d5fcd0285b:34123 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T04:33:11.900+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:11 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:33:11.949+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:33:11.950+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:11 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T04:33:11.952+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:11 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T04:33:11.964+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:11 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T04:33:12.229+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:12 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T04:33:13.193+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:13 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1755 bytes result sent to driver
[2023-01-31T04:33:13.358+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:13 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 1410 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:33:13.435+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:13 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T04:33:13.511+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:13 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 1.702 s
[2023-01-31T04:33:13.512+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:13 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:33:13.512+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T04:33:13.549+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:13 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 1.756500 s
[2023-01-31T04:33:14.112+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:14 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 81d5fcd0285b:34123 in memory (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T04:33:14.420+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:14 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T04:33:14.420+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:14 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T04:33:14.423+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:14 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T04:33:14.456+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:14 INFO CodeGenerator: Code generated in 22.593026 ms
[2023-01-31T04:33:14.465+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:14 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.5 MiB)
[2023-01-31T04:33:14.479+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:14 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.5 MiB)
[2023-01-31T04:33:14.480+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:14 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 81d5fcd0285b:34123 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T04:33:14.483+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:14 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T04:33:14.487+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:14 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T04:33:14.553+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:14 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T04:33:14.665+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:14 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T04:33:14.665+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:14 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T04:33:14.666+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:14 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:33:14.666+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:14 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:33:14.712+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:14 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T04:33:14.788+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:14 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.5 MiB)
[2023-01-31T04:33:14.812+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:14 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.5 MiB)
[2023-01-31T04:33:14.813+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:14 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 81d5fcd0285b:34123 (size: 7.6 KiB, free: 434.3 MiB)
[2023-01-31T04:33:14.813+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:14 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:33:14.813+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:33:14.813+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:14 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T04:33:14.813+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:14 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T04:33:14.839+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:14 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T04:33:14.892+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:14 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T04:33:15.230+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:15 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1586 bytes result sent to driver
[2023-01-31T04:33:15.234+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:15 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 422 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:33:15.235+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:15 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T04:33:15.241+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:15 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 0.537 s
[2023-01-31T04:33:15.241+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:15 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:33:15.241+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T04:33:15.241+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:15 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 0.683339 s
[2023-01-31T04:33:18.998+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:18 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 81d5fcd0285b:34123 in memory (size: 7.6 KiB, free: 434.3 MiB)
[2023-01-31T04:33:20.571+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:20 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T04:33:20.573+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:20 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T04:33:20.580+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:20 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T04:33:21.021+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:21 INFO CodeGenerator: Code generated in 332.06401 ms
[2023-01-31T04:33:21.089+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:21 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.3 MiB)
[2023-01-31T04:33:21.413+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:21 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T04:33:21.442+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:21 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 81d5fcd0285b:34123 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T04:33:21.454+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:21 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T04:33:21.645+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:21 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T04:33:23.521+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:23 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T04:33:23.548+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:23 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) with 1 output partitions
[2023-01-31T04:33:23.584+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:23 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201)
[2023-01-31T04:33:23.584+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:23 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:33:23.587+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:23 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:33:23.591+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:23 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201), which has no missing parents
[2023-01-31T04:33:23.615+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:23 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.2 MiB)
[2023-01-31T04:33:23.647+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:23 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.2 MiB)
[2023-01-31T04:33:23.665+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:23 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 81d5fcd0285b:34123 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T04:33:23.676+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:23 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:33:23.709+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:33:23.710+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:23 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T04:33:23.887+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:23 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T04:33:23.901+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:23 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T04:33:24.020+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:24 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T04:33:24.995+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:24 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2558 bytes result sent to driver
[2023-01-31T04:33:25.014+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:25 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 1282 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:33:25.027+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:25 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) finished in 1.453 s
[2023-01-31T04:33:25.028+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:25 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:33:25.028+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:25 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T04:33:25.045+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T04:33:25.047+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:25 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201, took 1.515029 s
[2023-01-31T04:33:25.700+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:25 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T04:33:25.704+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:25 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T04:33:25.705+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:25 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T04:33:27.134+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:27 INFO CodeGenerator: Code generated in 896.516519 ms
[2023-01-31T04:33:27.159+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:27 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 433.0 MiB)
[2023-01-31T04:33:27.210+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:27 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.0 MiB)
[2023-01-31T04:33:27.235+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:27 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 81d5fcd0285b:34123 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T04:33:27.237+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:27 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T04:33:27.241+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:27 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T04:33:27.279+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:27 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T04:33:27.281+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:27 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T04:33:27.282+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:27 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T04:33:27.282+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:27 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:33:27.282+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:27 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:33:27.289+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:27 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T04:33:27.298+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:27 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 432.9 MiB)
[2023-01-31T04:33:27.298+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:27 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.9 MiB)
[2023-01-31T04:33:27.299+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:27 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 81d5fcd0285b:34123 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T04:33:27.301+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:27 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:33:27.302+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:33:27.305+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:27 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T04:33:27.315+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:27 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T04:33:27.317+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:27 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T04:33:27.332+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:27 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T04:33:27.486+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:27 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1784 bytes result sent to driver
[2023-01-31T04:33:27.496+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:27 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 182 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:33:27.501+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:27 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 0.216 s
[2023-01-31T04:33:27.502+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:27 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:33:27.513+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:27 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T04:33:27.514+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T04:33:27.515+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:27 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 0.234833 s
[2023-01-31T04:33:28.137+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:28 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 81d5fcd0285b:34123 in memory (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T04:33:28.462+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:28 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T04:33:28.507+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:28 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T04:33:28.512+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:28 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T04:33:28.526+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:28 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 81d5fcd0285b:34123 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T04:33:29.549+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:29 INFO CodeGenerator: Code generated in 401.985125 ms
[2023-01-31T04:33:29.575+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:29 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.8 MiB)
[2023-01-31T04:33:29.849+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:29 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.8 MiB)
[2023-01-31T04:33:29.862+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:29 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 81d5fcd0285b:34123 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T04:33:29.875+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:29 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T04:33:29.986+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:29 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T04:33:30.556+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:30 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T04:33:30.567+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:30 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T04:33:30.589+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:30 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T04:33:30.590+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:30 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:33:30.592+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:30 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:33:30.607+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:30 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T04:33:30.658+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:30 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.8 MiB)
[2023-01-31T04:33:30.681+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:30 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.7 MiB)
[2023-01-31T04:33:30.692+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:30 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 81d5fcd0285b:34123 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T04:33:30.697+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:30 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:33:30.709+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:33:30.710+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:30 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T04:33:30.715+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:30 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T04:33:30.716+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:30 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T04:33:30.866+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:30 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T04:33:31.425+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:31 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1729 bytes result sent to driver
[2023-01-31T04:33:31.428+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:31 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 720 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:33:31.428+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:31 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T04:33:31.445+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:31 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 0.828 s
[2023-01-31T04:33:31.446+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:31 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:33:31.446+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T04:33:31.468+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:31 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 0.906476 s
[2023-01-31T04:33:32.790+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:32 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 81d5fcd0285b:34123 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T04:33:32.882+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:32 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 81d5fcd0285b:34123 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T04:33:33.017+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:33 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 81d5fcd0285b:34123 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T04:33:33.143+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:33 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 81d5fcd0285b:34123 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T04:33:38.611+0000] {spark_submit.py:495} INFO - 23/01/31 04:33:38 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 81d5fcd0285b:34123 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T04:34:18.781+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:18 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T04:34:19.232+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T04:34:19.233+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T04:34:19.233+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:19 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T04:34:19.233+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T04:34:19.233+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T04:34:19.237+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T04:34:24.651+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:24 INFO CodeGenerator: Code generated in 280.876775 ms
[2023-01-31T04:34:25.405+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:25 INFO DAGScheduler: Registering RDD 118 (save at BigQueryWriteHelper.java:105) as input to shuffle 0
[2023-01-31T04:34:25.533+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:25 INFO DAGScheduler: Got map stage job 8 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:34:25.543+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:25 INFO DAGScheduler: Final stage: ShuffleMapStage 8 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:34:25.548+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:25 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:34:25.558+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:25 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:34:25.607+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:25 INFO DAGScheduler: Submitting ShuffleMapStage 8 (MapPartitionsRDD[118] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:34:25.627+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:25 INFO CodeGenerator: Code generated in 180.008082 ms
[2023-01-31T04:34:26.155+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:26 INFO CodeGenerator: Code generated in 183.007274 ms
[2023-01-31T04:34:26.185+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:26 INFO CodeGenerator: Code generated in 10.371285 ms
[2023-01-31T04:34:26.216+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:26 INFO CodeGenerator: Code generated in 20.054608 ms
[2023-01-31T04:34:26.239+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:26 INFO CodeGenerator: Code generated in 9.426579 ms
[2023-01-31T04:34:26.266+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:26 INFO CodeGenerator: Code generated in 9.203875 ms
[2023-01-31T04:34:26.276+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:26 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 11.1 KiB, free 433.7 MiB)
[2023-01-31T04:34:26.284+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:26 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 5.7 KiB, free 433.7 MiB)
[2023-01-31T04:34:26.299+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:26 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 81d5fcd0285b:34123 (size: 5.7 KiB, free: 434.3 MiB)
[2023-01-31T04:34:26.299+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:26 INFO CodeGenerator: Code generated in 11.743493 ms
[2023-01-31T04:34:26.305+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:26 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:34:26.457+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:26 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[118] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:34:26.457+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:26 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2023-01-31T04:34:26.470+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:26 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7660 bytes) taskResourceAssignments Map()
[2023-01-31T04:34:26.473+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:26 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2023-01-31T04:34:26.487+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:26 INFO DAGScheduler: Registering RDD 120 (save at BigQueryWriteHelper.java:105) as input to shuffle 1
[2023-01-31T04:34:26.488+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:26 INFO DAGScheduler: Got map stage job 9 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:34:26.489+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:26 INFO DAGScheduler: Final stage: ShuffleMapStage 9 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:34:26.493+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:26 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:34:26.494+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:26 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:34:26.542+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:26 INFO CodeGenerator: Code generated in 74.8022 ms
[2023-01-31T04:34:26.542+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:26 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[120] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:34:27.167+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:27 INFO CodeGenerator: Code generated in 353.726926 ms
[2023-01-31T04:34:27.272+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:27 INFO CodeGenerator: Code generated in 9.666496 ms
[2023-01-31T04:34:27.400+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:27 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 15.6 KiB, free 433.7 MiB)
[2023-01-31T04:34:27.403+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:27 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.7 MiB)
[2023-01-31T04:34:27.403+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:27 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 81d5fcd0285b:34123 (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:34:27.406+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:27 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:34:27.408+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:27 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[120] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:34:27.409+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:27 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
[2023-01-31T04:34:27.446+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:27 INFO DAGScheduler: Registering RDD 122 (save at BigQueryWriteHelper.java:105) as input to shuffle 2
[2023-01-31T04:34:27.449+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:27 INFO DAGScheduler: Got map stage job 10 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:34:27.449+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:27 INFO DAGScheduler: Final stage: ShuffleMapStage 10 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:34:27.449+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:27 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:34:27.449+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:27 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:34:27.494+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:27 INFO DAGScheduler: Submitting ShuffleMapStage 10 (MapPartitionsRDD[122] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:34:27.693+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:27 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 15.7 KiB, free 433.7 MiB)
[2023-01-31T04:34:27.828+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:27 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.6 MiB)
[2023-01-31T04:34:27.830+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:27 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 81d5fcd0285b:34123 (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:34:27.834+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:27 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:34:27.887+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:27 INFO CodeGenerator: Code generated in 193.175375 ms
[2023-01-31T04:34:27.918+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:27 INFO CodeGenerator: Code generated in 651.804473 ms
[2023-01-31T04:34:27.925+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:27 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[122] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:34:27.930+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:27 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0
[2023-01-31T04:34:27.974+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:27 INFO DAGScheduler: Registering RDD 124 (save at BigQueryWriteHelper.java:105) as input to shuffle 3
[2023-01-31T04:34:27.975+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:27 INFO DAGScheduler: Got map stage job 11 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:34:27.980+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:27 INFO DAGScheduler: Final stage: ShuffleMapStage 11 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:34:27.991+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:27 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:34:27.991+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:27 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:34:28.001+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:28 INFO DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[124] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:34:28.071+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:28 INFO CodeGenerator: Code generated in 146.217319 ms
[2023-01-31T04:34:28.117+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:28 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 15.7 KiB, free 433.6 MiB)
[2023-01-31T04:34:28.219+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:28 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.6 MiB)
[2023-01-31T04:34:28.224+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:28 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 81d5fcd0285b:34123 (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:34:28.236+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:28 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:34:28.296+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:28 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[124] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:34:28.305+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:28 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
[2023-01-31T04:34:28.306+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:28 INFO DAGScheduler: Registering RDD 126 (save at BigQueryWriteHelper.java:105) as input to shuffle 4
[2023-01-31T04:34:28.307+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:28 INFO DAGScheduler: Got map stage job 12 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:34:28.369+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:28 INFO DAGScheduler: Final stage: ShuffleMapStage 12 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:34:28.369+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:28 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:34:28.409+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:28 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:34:28.464+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:28 INFO DAGScheduler: Submitting ShuffleMapStage 12 (MapPartitionsRDD[126] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:34:28.582+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:28 INFO CodeGenerator: Code generated in 330.686871 ms
[2023-01-31T04:34:28.618+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:28 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 15.7 KiB, free 433.6 MiB)
[2023-01-31T04:34:28.805+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:28 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.6 MiB)
[2023-01-31T04:34:28.839+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:28 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 81d5fcd0285b:34123 (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:34:28.845+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:28 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:34:28.879+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:28 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[126] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:34:28.896+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:28 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
[2023-01-31T04:34:28.997+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:28 INFO DAGScheduler: Registering RDD 128 (save at BigQueryWriteHelper.java:105) as input to shuffle 5
[2023-01-31T04:34:28.997+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:28 INFO DAGScheduler: Got map stage job 13 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:34:28.997+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:28 INFO DAGScheduler: Final stage: ShuffleMapStage 13 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:34:29.024+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:29 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:34:29.026+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:29 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:34:29.032+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:29 INFO DAGScheduler: Submitting ShuffleMapStage 13 (MapPartitionsRDD[128] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:34:29.107+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:29 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 15.7 KiB, free 433.6 MiB)
[2023-01-31T04:34:29.108+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:29 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.6 MiB)
[2023-01-31T04:34:29.109+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:29 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 81d5fcd0285b:34123 (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:34:29.122+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:29 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:34:29.123+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:29 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 13 (MapPartitionsRDD[128] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:34:29.123+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:29 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0
[2023-01-31T04:34:29.123+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:29 INFO CodeGenerator: Code generated in 168.482421 ms
[2023-01-31T04:34:29.150+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:29 INFO DAGScheduler: Registering RDD 130 (save at BigQueryWriteHelper.java:105) as input to shuffle 6
[2023-01-31T04:34:29.151+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:29 INFO DAGScheduler: Got map stage job 14 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:34:29.151+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:29 INFO DAGScheduler: Final stage: ShuffleMapStage 14 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:34:29.152+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:29 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:34:29.152+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:29 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:34:29.214+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:29 INFO DAGScheduler: Submitting ShuffleMapStage 14 (MapPartitionsRDD[130] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:34:29.400+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:29 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 15.7 KiB, free 433.6 MiB)
[2023-01-31T04:34:29.453+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:29 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.6 MiB)
[2023-01-31T04:34:29.454+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:29 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 81d5fcd0285b:34123 (size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T04:34:29.455+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:29 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:34:29.471+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:29 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[130] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:34:29.471+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:29 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0
[2023-01-31T04:34:29.473+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:29 INFO DAGScheduler: Registering RDD 132 (save at BigQueryWriteHelper.java:105) as input to shuffle 7
[2023-01-31T04:34:29.475+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:29 INFO DAGScheduler: Got map stage job 15 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:34:29.477+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:29 INFO DAGScheduler: Final stage: ShuffleMapStage 15 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:34:29.481+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:29 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:34:29.502+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:29 INFO CodeGenerator: Code generated in 239.77637 ms
[2023-01-31T04:34:29.531+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:29 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:34:29.549+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:29 INFO DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[132] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:34:29.626+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:29 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 15.7 KiB, free 433.5 MiB)
[2023-01-31T04:34:29.651+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:29 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.5 MiB)
[2023-01-31T04:34:29.657+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:29 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 81d5fcd0285b:34123 (size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T04:34:29.661+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:29 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:34:29.661+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:29 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[132] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:34:29.662+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:29 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
[2023-01-31T04:34:29.696+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:29 INFO DAGScheduler: Registering RDD 134 (save at BigQueryWriteHelper.java:105) as input to shuffle 8
[2023-01-31T04:34:29.707+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:29 INFO DAGScheduler: Got map stage job 16 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:34:29.714+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:29 INFO DAGScheduler: Final stage: ShuffleMapStage 16 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:34:29.716+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:29 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:34:29.722+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:29 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:34:29.804+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:29 INFO DAGScheduler: Submitting ShuffleMapStage 16 (MapPartitionsRDD[134] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:34:29.810+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:29 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2087 bytes result sent to driver
[2023-01-31T04:34:29.815+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:29 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:34:29.817+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:29 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)
[2023-01-31T04:34:29.817+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:29 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 3357 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:34:29.817+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:29 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2023-01-31T04:34:29.891+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:29 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 15.7 KiB, free 433.5 MiB)
[2023-01-31T04:34:30.342+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:30 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.5 MiB)
[2023-01-31T04:34:30.377+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:30 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 81d5fcd0285b:34123 (size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T04:34:30.378+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:30 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:34:30.437+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:30 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 16 (MapPartitionsRDD[134] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:34:30.438+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:30 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0
[2023-01-31T04:34:30.440+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:30 INFO CodeGenerator: Code generated in 788.501232 ms
[2023-01-31T04:34:30.443+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:30 INFO DAGScheduler: Registering RDD 136 (save at BigQueryWriteHelper.java:105) as input to shuffle 9
[2023-01-31T04:34:30.445+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:30 INFO DAGScheduler: Got map stage job 17 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:34:30.446+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:30 INFO DAGScheduler: Final stage: ShuffleMapStage 17 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:34:30.451+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:30 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:34:30.458+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:30 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:34:30.605+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:30 INFO DAGScheduler: Submitting ShuffleMapStage 17 (MapPartitionsRDD[136] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:34:30.741+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:30 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 15.7 KiB, free 433.5 MiB)
[2023-01-31T04:34:30.746+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:30 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.5 MiB)
[2023-01-31T04:34:30.752+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:30 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 81d5fcd0285b:34123 (size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T04:34:30.753+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:30 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:34:30.848+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:30 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 17 (MapPartitionsRDD[136] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:34:30.849+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:30 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0
[2023-01-31T04:34:30.866+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:30 INFO DAGScheduler: Registering RDD 138 (save at BigQueryWriteHelper.java:105) as input to shuffle 10
[2023-01-31T04:34:30.880+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:30 INFO DAGScheduler: Got map stage job 18 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:34:30.881+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:30 INFO DAGScheduler: Final stage: ShuffleMapStage 18 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:34:30.887+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:30 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:34:30.925+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:30 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:34:30.954+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:30 INFO DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[138] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:34:31.070+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:31 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 15.7 KiB, free 433.5 MiB)
[2023-01-31T04:34:31.122+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:31 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.5 MiB)
[2023-01-31T04:34:31.125+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:31 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 81d5fcd0285b:34123 (size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T04:34:31.129+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:31 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:34:31.130+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:31 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[138] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:34:31.131+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:31 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0
[2023-01-31T04:34:31.160+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:31 INFO DAGScheduler: Registering RDD 140 (save at BigQueryWriteHelper.java:105) as input to shuffle 11
[2023-01-31T04:34:31.177+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:31 INFO DAGScheduler: Got map stage job 19 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:34:31.181+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:31 INFO DAGScheduler: Final stage: ShuffleMapStage 19 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:34:31.192+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:31 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:34:31.195+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:31 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:34:31.233+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:31 INFO DAGScheduler: Submitting ShuffleMapStage 19 (MapPartitionsRDD[140] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:34:31.284+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:31 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 15.7 KiB, free 433.4 MiB)
[2023-01-31T04:34:31.285+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:31 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.4 MiB)
[2023-01-31T04:34:31.302+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:31 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 81d5fcd0285b:34123 (size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T04:34:31.325+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:31 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:34:31.326+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:31 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 19 (MapPartitionsRDD[140] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:34:31.327+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:31 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0
[2023-01-31T04:34:31.328+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:31 INFO DAGScheduler: Registering RDD 142 (save at BigQueryWriteHelper.java:105) as input to shuffle 12
[2023-01-31T04:34:31.363+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:31 INFO DAGScheduler: Got map stage job 20 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:34:31.363+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:31 INFO DAGScheduler: Final stage: ShuffleMapStage 20 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:34:31.369+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:31 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:34:31.371+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:31 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:34:31.406+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:31 INFO DAGScheduler: Submitting ShuffleMapStage 20 (MapPartitionsRDD[142] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:34:31.529+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:31 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 15.7 KiB, free 433.4 MiB)
[2023-01-31T04:34:31.540+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:31 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.4 MiB)
[2023-01-31T04:34:31.541+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:31 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 81d5fcd0285b:34123 (size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T04:34:31.545+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:31 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:34:31.568+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:31 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[142] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:34:31.571+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:31 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks resource profile 0
[2023-01-31T04:34:31.609+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:31 INFO DAGScheduler: Registering RDD 144 (save at BigQueryWriteHelper.java:105) as input to shuffle 13
[2023-01-31T04:34:31.617+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:31 INFO DAGScheduler: Got map stage job 21 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:34:31.620+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:31 INFO DAGScheduler: Final stage: ShuffleMapStage 21 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:34:31.652+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:31 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:34:31.653+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:31 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:34:31.655+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:31 INFO DAGScheduler: Submitting ShuffleMapStage 21 (MapPartitionsRDD[144] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:34:31.918+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:31 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 15.7 KiB, free 433.4 MiB)
[2023-01-31T04:34:31.918+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:31 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.4 MiB)
[2023-01-31T04:34:31.919+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:31 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 81d5fcd0285b:34123 (size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T04:34:31.920+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:31 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:34:31.921+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:31 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 21 (MapPartitionsRDD[144] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:34:31.921+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:31 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0
[2023-01-31T04:34:31.935+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:31 INFO DAGScheduler: Registering RDD 146 (save at BigQueryWriteHelper.java:105) as input to shuffle 14
[2023-01-31T04:34:31.940+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:31 INFO DAGScheduler: Got map stage job 22 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:34:31.943+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:31 INFO DAGScheduler: Final stage: ShuffleMapStage 22 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:34:31.944+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:31 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:34:31.970+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:31 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:34:32.008+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:32 INFO DAGScheduler: Submitting ShuffleMapStage 22 (MapPartitionsRDD[146] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:34:32.342+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:32 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 81d5fcd0285b:34123 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T04:34:32.368+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:32 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 15.7 KiB, free 433.6 MiB)
[2023-01-31T04:34:32.370+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:32 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.6 MiB)
[2023-01-31T04:34:32.370+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:32 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 81d5fcd0285b:34123 (size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T04:34:32.402+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:32 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:34:32.409+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:32 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 22 (MapPartitionsRDD[146] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:34:32.409+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:32 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0
[2023-01-31T04:34:32.444+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:32 INFO DAGScheduler: Registering RDD 148 (save at BigQueryWriteHelper.java:105) as input to shuffle 15
[2023-01-31T04:34:32.444+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:32 INFO DAGScheduler: Got map stage job 23 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:34:32.445+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:32 INFO DAGScheduler: Final stage: ShuffleMapStage 23 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:34:32.445+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:32 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:34:32.449+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:32 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:34:32.488+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:32 INFO DAGScheduler: Submitting ShuffleMapStage 23 (MapPartitionsRDD[148] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:34:32.493+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:32 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 81d5fcd0285b:34123 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T04:34:32.531+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:32 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 15.7 KiB, free 433.8 MiB)
[2023-01-31T04:34:32.534+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:32 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.8 MiB)
[2023-01-31T04:34:32.535+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:32 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 81d5fcd0285b:34123 (size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T04:34:32.536+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:32 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:34:32.564+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:32 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 23 (MapPartitionsRDD[148] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:34:32.582+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:32 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks resource profile 0
[2023-01-31T04:34:32.599+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:32 INFO DAGScheduler: ShuffleMapStage 8 (save at BigQueryWriteHelper.java:105) finished in 6.884 s
[2023-01-31T04:34:32.632+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:32 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:34:32.634+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:32 INFO DAGScheduler: running: HashSet(ShuffleMapStage 9, ShuffleMapStage 10, ShuffleMapStage 11, ShuffleMapStage 12, ShuffleMapStage 13, ShuffleMapStage 14, ShuffleMapStage 15, ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 18, ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23)
[2023-01-31T04:34:32.662+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:32 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:34:32.663+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:32 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:34:32.977+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:32 INFO DAGScheduler: Registering RDD 150 (save at BigQueryWriteHelper.java:105) as input to shuffle 16
[2023-01-31T04:34:32.977+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:32 INFO DAGScheduler: Got map stage job 24 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:34:32.977+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:32 INFO DAGScheduler: Final stage: ShuffleMapStage 24 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:34:32.978+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:32 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:34:32.978+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:32 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:34:32.983+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:32 INFO DAGScheduler: Submitting ShuffleMapStage 24 (MapPartitionsRDD[150] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:34:33.035+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:33 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 15.7 KiB, free 433.8 MiB)
[2023-01-31T04:34:33.209+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:33 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.8 MiB)
[2023-01-31T04:34:33.214+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:33 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 81d5fcd0285b:34123 (size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T04:34:33.276+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:33 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:34:33.277+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:33 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 24 (MapPartitionsRDD[150] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:34:33.277+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:33 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks resource profile 0
[2023-01-31T04:34:33.491+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:33 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 81d5fcd0285b:34123 in memory (size: 5.7 KiB, free: 434.2 MiB)
[2023-01-31T04:34:38.507+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:38 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2023-01-31T04:34:42.347+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:42 INFO CodeGenerator: Code generated in 905.485674 ms
[2023-01-31T04:34:43.866+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:43 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2023-01-31T04:34:43.871+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:43 INFO DAGScheduler: Got job 25 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
[2023-01-31T04:34:43.872+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:43 INFO DAGScheduler: Final stage: ResultStage 26 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2023-01-31T04:34:43.873+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:43 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 25)
[2023-01-31T04:34:43.877+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:43 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:34:43.901+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:43 INFO DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[154] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2023-01-31T04:34:44.452+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:44 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 21.6 KiB, free 433.8 MiB)
[2023-01-31T04:34:44.454+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:44 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 433.8 MiB)
[2023-01-31T04:34:44.456+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:44 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 81d5fcd0285b:34123 (size: 10.2 KiB, free: 434.2 MiB)
[2023-01-31T04:34:44.457+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:44 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:34:44.459+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[154] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:34:44.460+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:44 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks resource profile 0
[2023-01-31T04:34:50.595+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:50 INFO CodeGenerator: Code generated in 148.611836 ms
[2023-01-31T04:34:51.506+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:51 INFO PythonRunner: Times: total = 20415, boot = 19324, init = 1087, finish = 4
[2023-01-31T04:34:51.711+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:51 INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 2521 bytes result sent to driver
[2023-01-31T04:34:51.716+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:51 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:34:51.718+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:51 INFO Executor: Running task 0.0 in stage 10.0 (TID 10)
[2023-01-31T04:34:51.733+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:51 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 21918 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:34:51.733+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:51 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
[2023-01-31T04:34:51.734+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:51 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 39145
[2023-01-31T04:34:51.741+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:51 INFO DAGScheduler: ShuffleMapStage 9 (save at BigQueryWriteHelper.java:105) finished in 25.198 s
[2023-01-31T04:34:51.742+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:51 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:34:51.744+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:51 INFO DAGScheduler: running: HashSet(ShuffleMapStage 10, ShuffleMapStage 11, ShuffleMapStage 12, ShuffleMapStage 13, ShuffleMapStage 14, ShuffleMapStage 15, ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 18, ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T04:34:51.760+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:51 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:34:51.760+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:51 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:34:54.918+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:54 INFO PythonRunner: Times: total = 2453, boot = -159, init = 2611, finish = 1
[2023-01-31T04:34:55.005+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:55 INFO Executor: Finished task 0.0 in stage 10.0 (TID 10). 2478 bytes result sent to driver
[2023-01-31T04:34:55.045+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:55 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 11) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:34:55.076+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:55 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 3359 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:34:55.078+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:55 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool
[2023-01-31T04:34:55.087+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:55 INFO DAGScheduler: ShuffleMapStage 10 (save at BigQueryWriteHelper.java:105) finished in 27.593 s
[2023-01-31T04:34:55.089+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:55 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:34:55.090+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:55 INFO DAGScheduler: running: HashSet(ShuffleMapStage 11, ShuffleMapStage 12, ShuffleMapStage 13, ShuffleMapStage 14, ShuffleMapStage 15, ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 18, ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T04:34:55.091+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:55 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:34:55.091+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:55 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:34:55.091+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:55 INFO Executor: Running task 0.0 in stage 11.0 (TID 11)
[2023-01-31T04:34:56.765+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:56 INFO PythonRunner: Times: total = 1580, boot = -836, init = 2416, finish = 0
[2023-01-31T04:34:56.820+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:56 INFO Executor: Finished task 0.0 in stage 11.0 (TID 11). 2521 bytes result sent to driver
[2023-01-31T04:34:56.898+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:56 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 12) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:34:56.905+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:56 INFO Executor: Running task 0.0 in stage 12.0 (TID 12)
[2023-01-31T04:34:56.925+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:56 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 11) in 1911 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:34:56.925+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:56 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool
[2023-01-31T04:34:56.928+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:56 INFO DAGScheduler: ShuffleMapStage 11 (save at BigQueryWriteHelper.java:105) finished in 28.900 s
[2023-01-31T04:34:56.929+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:56 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:34:56.929+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:56 INFO DAGScheduler: running: HashSet(ShuffleMapStage 12, ShuffleMapStage 13, ShuffleMapStage 14, ShuffleMapStage 15, ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 18, ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T04:34:56.929+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:56 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:34:56.929+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:56 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:34:58.377+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:58 INFO PythonRunner: Times: total = 1389, boot = -81, init = 1470, finish = 0
[2023-01-31T04:34:58.409+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:58 INFO Executor: Finished task 0.0 in stage 12.0 (TID 12). 2478 bytes result sent to driver
[2023-01-31T04:34:58.417+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:58 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 13) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:34:58.418+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:58 INFO Executor: Running task 0.0 in stage 13.0 (TID 13)
[2023-01-31T04:34:58.425+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:58 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 12) in 1525 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:34:58.426+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:58 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool
[2023-01-31T04:34:58.427+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:58 INFO DAGScheduler: ShuffleMapStage 12 (save at BigQueryWriteHelper.java:105) finished in 29.955 s
[2023-01-31T04:34:58.427+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:58 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:34:58.428+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:58 INFO DAGScheduler: running: HashSet(ShuffleMapStage 13, ShuffleMapStage 14, ShuffleMapStage 15, ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 18, ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T04:34:58.428+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:58 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:34:58.428+0000] {spark_submit.py:495} INFO - 23/01/31 04:34:58 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:35:01.358+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:01 INFO PythonRunner: Times: total = 2429, boot = -265, init = 2693, finish = 1
[2023-01-31T04:35:01.563+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:01 INFO Executor: Finished task 0.0 in stage 13.0 (TID 13). 2478 bytes result sent to driver
[2023-01-31T04:35:01.567+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:01 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 14) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:35:01.574+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:01 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 13) in 3154 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:35:01.574+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:01 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool
[2023-01-31T04:35:01.578+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:01 INFO DAGScheduler: ShuffleMapStage 13 (save at BigQueryWriteHelper.java:105) finished in 32.541 s
[2023-01-31T04:35:01.578+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:01 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:35:01.579+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:01 INFO DAGScheduler: running: HashSet(ShuffleMapStage 14, ShuffleMapStage 15, ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 18, ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T04:35:01.581+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:01 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:35:01.581+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:01 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:35:01.606+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:01 INFO Executor: Running task 0.0 in stage 14.0 (TID 14)
[2023-01-31T04:35:02.846+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:02 INFO PythonRunner: Times: total = 1144, boot = -458, init = 1601, finish = 1
[2023-01-31T04:35:02.850+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:02 INFO Executor: Finished task 0.0 in stage 14.0 (TID 14). 2521 bytes result sent to driver
[2023-01-31T04:35:02.878+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:02 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 15) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:35:02.906+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:02 INFO Executor: Running task 0.0 in stage 15.0 (TID 15)
[2023-01-31T04:35:02.914+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:02 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 14) in 1345 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:35:02.918+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:02 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool
[2023-01-31T04:35:02.919+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:02 INFO DAGScheduler: ShuffleMapStage 14 (save at BigQueryWriteHelper.java:105) finished in 33.700 s
[2023-01-31T04:35:02.919+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:02 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:35:02.919+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:02 INFO DAGScheduler: running: HashSet(ShuffleMapStage 15, ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 18, ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T04:35:02.919+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:02 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:35:02.919+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:02 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:35:04.751+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:04 INFO PythonRunner: Times: total = 1625, boot = -59, init = 1684, finish = 0
[2023-01-31T04:35:04.953+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:04 INFO Executor: Finished task 0.0 in stage 15.0 (TID 15). 2478 bytes result sent to driver
[2023-01-31T04:35:04.977+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:04 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 16) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:35:04.979+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:04 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 15) in 2105 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:35:04.980+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:04 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool
[2023-01-31T04:35:04.995+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:04 INFO DAGScheduler: ShuffleMapStage 15 (save at BigQueryWriteHelper.java:105) finished in 35.422 s
[2023-01-31T04:35:05.003+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:04 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:35:05.004+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:04 INFO DAGScheduler: running: HashSet(ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 18, ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T04:35:05.004+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:04 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:35:05.004+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:04 INFO Executor: Running task 0.0 in stage 16.0 (TID 16)
[2023-01-31T04:35:05.004+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:04 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:35:07.742+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:07 INFO PythonRunner: Times: total = 2344, boot = -389, init = 2733, finish = 0
[2023-01-31T04:35:07.822+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:07 INFO Executor: Finished task 0.0 in stage 16.0 (TID 16). 2478 bytes result sent to driver
[2023-01-31T04:35:07.823+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:07 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 17) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:35:07.832+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:07 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 16) in 2863 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:35:07.832+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:07 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool
[2023-01-31T04:35:07.833+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:07 INFO DAGScheduler: ShuffleMapStage 16 (save at BigQueryWriteHelper.java:105) finished in 38.007 s
[2023-01-31T04:35:07.833+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:07 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:35:07.833+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:07 INFO DAGScheduler: running: HashSet(ShuffleMapStage 17, ShuffleMapStage 18, ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T04:35:07.833+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:07 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:35:07.834+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:07 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:35:07.889+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:07 INFO Executor: Running task 0.0 in stage 17.0 (TID 17)
[2023-01-31T04:35:08.462+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:08 INFO PythonRunner: Times: total = 499, boot = -189, init = 688, finish = 0
[2023-01-31T04:35:08.604+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:08 INFO Executor: Finished task 0.0 in stage 17.0 (TID 17). 2478 bytes result sent to driver
[2023-01-31T04:35:08.629+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:08 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 18) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:35:08.630+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:08 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 17) in 794 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:35:08.657+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:08 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool
[2023-01-31T04:35:08.657+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:08 INFO DAGScheduler: ShuffleMapStage 17 (save at BigQueryWriteHelper.java:105) finished in 38.050 s
[2023-01-31T04:35:08.657+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:08 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:35:08.657+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:08 INFO DAGScheduler: running: HashSet(ShuffleMapStage 18, ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T04:35:08.658+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:08 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:35:08.658+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:08 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:35:08.968+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:08 INFO Executor: Running task 0.0 in stage 18.0 (TID 18)
[2023-01-31T04:35:11.379+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:11 INFO PythonRunner: Times: total = 2230, boot = -526, init = 2755, finish = 1
[2023-01-31T04:35:11.472+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:11 INFO Executor: Finished task 0.0 in stage 18.0 (TID 18). 2478 bytes result sent to driver
[2023-01-31T04:35:11.481+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:11 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 19) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:35:11.503+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:11 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 18) in 2887 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:35:11.513+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:11 INFO Executor: Running task 0.0 in stage 19.0 (TID 19)
[2023-01-31T04:35:11.517+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:11 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool
[2023-01-31T04:35:11.529+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:11 INFO DAGScheduler: ShuffleMapStage 18 (save at BigQueryWriteHelper.java:105) finished in 40.535 s
[2023-01-31T04:35:11.532+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:11 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:35:11.550+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:11 INFO DAGScheduler: running: HashSet(ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T04:35:11.556+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:11 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:35:11.556+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:11 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:35:13.759+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:13 INFO PythonRunner: Times: total = 2138, boot = -31, init = 2168, finish = 1
[2023-01-31T04:35:13.845+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:13 INFO Executor: Finished task 0.0 in stage 19.0 (TID 19). 2478 bytes result sent to driver
[2023-01-31T04:35:13.862+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:13 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 20) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:35:13.867+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:13 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 19) in 2390 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:35:13.867+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:13 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool
[2023-01-31T04:35:13.870+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:13 INFO DAGScheduler: ShuffleMapStage 19 (save at BigQueryWriteHelper.java:105) finished in 42.646 s
[2023-01-31T04:35:13.870+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:13 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:35:13.870+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:13 INFO DAGScheduler: running: HashSet(ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T04:35:13.870+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:13 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:35:13.871+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:13 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:35:13.919+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:13 INFO Executor: Running task 0.0 in stage 20.0 (TID 20)
[2023-01-31T04:35:14.316+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:14 INFO PythonRunner: Times: total = 355, boot = -57, init = 412, finish = 0
[2023-01-31T04:35:14.335+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:14 INFO Executor: Finished task 0.0 in stage 20.0 (TID 20). 2478 bytes result sent to driver
[2023-01-31T04:35:14.340+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:14 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 21) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:35:14.341+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:14 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 20) in 478 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:35:14.341+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:14 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool
[2023-01-31T04:35:14.341+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:14 INFO DAGScheduler: ShuffleMapStage 20 (save at BigQueryWriteHelper.java:105) finished in 42.937 s
[2023-01-31T04:35:14.341+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:14 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:35:14.357+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:14 INFO DAGScheduler: running: HashSet(ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T04:35:14.358+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:14 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:35:14.358+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:14 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:35:14.373+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:14 INFO Executor: Running task 0.0 in stage 21.0 (TID 21)
[2023-01-31T04:35:14.790+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:14 INFO PythonRunner: Times: total = 258, boot = -157, init = 415, finish = 0
[2023-01-31T04:35:14.842+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:14 INFO Executor: Finished task 0.0 in stage 21.0 (TID 21). 2478 bytes result sent to driver
[2023-01-31T04:35:14.846+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:14 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 22) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:35:14.847+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:14 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 21) in 508 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:35:14.847+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:14 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool
[2023-01-31T04:35:14.847+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:14 INFO DAGScheduler: ShuffleMapStage 21 (save at BigQueryWriteHelper.java:105) finished in 43.190 s
[2023-01-31T04:35:14.847+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:14 INFO Executor: Running task 0.0 in stage 22.0 (TID 22)
[2023-01-31T04:35:14.848+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:14 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:35:14.848+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:14 INFO DAGScheduler: running: HashSet(ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T04:35:14.848+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:14 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:35:14.848+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:14 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:35:17.469+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:17 INFO PythonRunner: Times: total = 2210, boot = -216, init = 2425, finish = 1
[2023-01-31T04:35:17.567+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:17 INFO Executor: Finished task 0.0 in stage 22.0 (TID 22). 2521 bytes result sent to driver
[2023-01-31T04:35:17.575+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:17 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 23) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:35:17.580+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:17 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 22) in 2736 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:35:17.580+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:17 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool
[2023-01-31T04:35:17.585+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:17 INFO Executor: Running task 0.0 in stage 23.0 (TID 23)
[2023-01-31T04:35:17.586+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:17 INFO DAGScheduler: ShuffleMapStage 22 (save at BigQueryWriteHelper.java:105) finished in 45.563 s
[2023-01-31T04:35:17.587+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:17 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:35:17.588+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:17 INFO DAGScheduler: running: HashSet(ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T04:35:17.588+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:17 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:35:17.589+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:17 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:35:20.384+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:20 INFO PythonRunner: Times: total = 2641, boot = -358, init = 2999, finish = 0
[2023-01-31T04:35:20.473+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:20 INFO Executor: Finished task 0.0 in stage 23.0 (TID 23). 2478 bytes result sent to driver
[2023-01-31T04:35:20.475+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:20 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 24) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:35:20.476+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:20 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 23) in 2903 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:35:20.476+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:20 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool
[2023-01-31T04:35:20.479+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:20 INFO DAGScheduler: ShuffleMapStage 23 (save at BigQueryWriteHelper.java:105) finished in 47.989 s
[2023-01-31T04:35:20.480+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:20 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:35:20.480+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:20 INFO DAGScheduler: running: HashSet(ShuffleMapStage 24, ResultStage 26)
[2023-01-31T04:35:20.480+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:20 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:35:20.480+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:20 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:35:20.494+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:20 INFO Executor: Running task 0.0 in stage 24.0 (TID 24)
[2023-01-31T04:35:21.330+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:21 INFO PythonRunner: Times: total = 656, boot = -34, init = 690, finish = 0
[2023-01-31T04:35:21.365+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:21 INFO Executor: Finished task 0.0 in stage 24.0 (TID 24). 2478 bytes result sent to driver
[2023-01-31T04:35:21.389+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:21 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 25) (81d5fcd0285b, executor driver, partition 0, NODE_LOCAL, 7399 bytes) taskResourceAssignments Map()
[2023-01-31T04:35:21.394+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:21 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 24) in 919 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:35:21.394+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:21 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool
[2023-01-31T04:35:21.401+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:21 INFO Executor: Running task 0.0 in stage 26.0 (TID 25)
[2023-01-31T04:35:21.417+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:21 INFO DAGScheduler: ShuffleMapStage 24 (save at BigQueryWriteHelper.java:105) finished in 48.424 s
[2023-01-31T04:35:21.418+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:21 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:35:21.419+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:21 INFO DAGScheduler: running: HashSet(ResultStage 26)
[2023-01-31T04:35:21.420+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:21 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:35:21.420+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:21 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:35:23.267+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:23 INFO ShuffleBlockFetcherIterator: Getting 1 (97.0 B) non-empty blocks including 1 (97.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:35:23.355+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 681 ms
[2023-01-31T04:35:24.818+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:24 INFO CodeGenerator: Code generated in 579.929281 ms
[2023-01-31T04:35:25.512+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:25 INFO CodeGenerator: Code generated in 214.007345 ms
[2023-01-31T04:35:26.016+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:26 INFO CodeGenerator: Code generated in 28.602875 ms
[2023-01-31T04:35:26.051+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:26 INFO CodeGenerator: Code generated in 29.909587 ms
[2023-01-31T04:35:27.216+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:27 INFO CodeGenerator: Code generated in 157.350252 ms
[2023-01-31T04:35:27.450+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:27 INFO CodeGenerator: Code generated in 55.402756 ms
[2023-01-31T04:35:27.492+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:27 INFO CodeGenerator: Code generated in 23.381873 ms
[2023-01-31T04:35:27.600+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:27 INFO Executor: Finished task 0.0 in stage 26.0 (TID 25). 3217 bytes result sent to driver
[2023-01-31T04:35:27.671+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:27 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 25) in 6264 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:35:27.695+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:27 INFO DAGScheduler: ResultStage 26 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 43.426 s
[2023-01-31T04:35:27.697+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:27 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool
[2023-01-31T04:35:27.718+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:27 INFO DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:35:27.757+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 26: Stage finished
[2023-01-31T04:35:27.818+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:27 INFO DAGScheduler: Job 25 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 43.871665 s
[2023-01-31T04:35:28.702+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:28 INFO CodeGenerator: Code generated in 352.795427 ms
[2023-01-31T04:35:28.853+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:28 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 1024.1 KiB, free 432.8 MiB)
[2023-01-31T04:35:28.948+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:28 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 278.0 B, free 432.8 MiB)
[2023-01-31T04:35:28.961+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:28 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 81d5fcd0285b:34123 (size: 278.0 B, free: 434.2 MiB)
[2023-01-31T04:35:28.962+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:28 INFO SparkContext: Created broadcast 34 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2023-01-31T04:35:31.958+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:31 INFO CodeGenerator: Code generated in 66.218709 ms
[2023-01-31T04:35:32.019+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:32 INFO DAGScheduler: Registering RDD 156 (save at BigQueryWriteHelper.java:105) as input to shuffle 17
[2023-01-31T04:35:32.019+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:32 INFO DAGScheduler: Got map stage job 26 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:35:32.019+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:32 INFO DAGScheduler: Final stage: ShuffleMapStage 27 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:35:32.020+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:32 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:35:32.020+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:32 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:35:32.037+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:32 INFO DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[156] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:35:32.171+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:32 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 18.1 KiB, free 432.7 MiB)
[2023-01-31T04:35:32.173+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:32 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 9.2 KiB, free 432.7 MiB)
[2023-01-31T04:35:32.175+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:32 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 81d5fcd0285b:34123 (size: 9.2 KiB, free: 434.2 MiB)
[2023-01-31T04:35:32.188+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:32 INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:35:32.188+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:32 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[156] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:35:32.189+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:32 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks resource profile 0
[2023-01-31T04:35:32.190+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:32 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 26) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:35:32.199+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:32 INFO Executor: Running task 0.0 in stage 27.0 (TID 26)
[2023-01-31T04:35:32.475+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:32 INFO CodeGenerator: Code generated in 131.101254 ms
[2023-01-31T04:35:32.548+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:32 INFO PythonRunner: Times: total = 313, boot = -10807, init = 11120, finish = 0
[2023-01-31T04:35:32.587+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:32 INFO Executor: Finished task 0.0 in stage 27.0 (TID 26). 2577 bytes result sent to driver
[2023-01-31T04:35:32.589+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:32 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 26) in 401 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:35:32.589+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:32 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool
[2023-01-31T04:35:32.614+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:32 INFO DAGScheduler: ShuffleMapStage 27 (save at BigQueryWriteHelper.java:105) finished in 0.582 s
[2023-01-31T04:35:32.614+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:32 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:35:32.614+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:32 INFO DAGScheduler: running: HashSet()
[2023-01-31T04:35:32.615+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:32 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:35:32.615+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:32 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:35:32.965+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:32 INFO BlockManagerInfo: Removed broadcast_35_piece0 on 81d5fcd0285b:34123 in memory (size: 9.2 KiB, free: 434.2 MiB)
[2023-01-31T04:35:34.456+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:34 INFO ShufflePartitionsUtil: For shuffle(17, 1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2023-01-31T04:35:36.580+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:36 INFO CodeGenerator: Code generated in 641.086582 ms
[2023-01-31T04:35:36.693+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:36 INFO CodeGenerator: Code generated in 93.040304 ms
[2023-01-31T04:35:37.471+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:37 INFO CodeGenerator: Code generated in 276.458127 ms
[2023-01-31T04:35:38.416+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:38 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2023-01-31T04:35:38.451+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:38 INFO DAGScheduler: Got job 27 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
[2023-01-31T04:35:38.452+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:38 INFO DAGScheduler: Final stage: ResultStage 30 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2023-01-31T04:35:38.453+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:38 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 28, ShuffleMapStage 29)
[2023-01-31T04:35:38.453+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:38 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:35:38.474+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:38 INFO DAGScheduler: Submitting ResultStage 30 (MapPartitionsRDD[163] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2023-01-31T04:35:38.672+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:38 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 48.6 KiB, free 432.7 MiB)
[2023-01-31T04:35:38.694+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:38 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 22.6 KiB, free 432.7 MiB)
[2023-01-31T04:35:38.700+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:38 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on 81d5fcd0285b:34123 (size: 22.6 KiB, free: 434.2 MiB)
[2023-01-31T04:35:38.714+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:38 INFO SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:35:38.780+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 30 (MapPartitionsRDD[163] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:35:38.781+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:38 INFO TaskSchedulerImpl: Adding task set 30.0 with 1 tasks resource profile 0
[2023-01-31T04:35:38.791+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:38 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 27) (81d5fcd0285b, executor driver, partition 0, NODE_LOCAL, 7897 bytes) taskResourceAssignments Map()
[2023-01-31T04:35:38.791+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:38 INFO Executor: Running task 0.0 in stage 30.0 (TID 27)
[2023-01-31T04:35:39.733+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:39 INFO ShuffleBlockFetcherIterator: Getting 1 (288.0 B) non-empty blocks including 1 (288.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:35:39.757+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 135 ms
[2023-01-31T04:35:40.457+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:40 INFO CodeGenerator: Code generated in 520.844891 ms
[2023-01-31T04:35:41.340+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:41 INFO CodeGenerator: Code generated in 423.790319 ms
[2023-01-31T04:35:41.590+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:41 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:35:41.711+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms
[2023-01-31T04:35:42.144+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:42 INFO CodeGenerator: Code generated in 32.70654 ms
[2023-01-31T04:35:42.197+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:42 INFO CodeGenerator: Code generated in 36.770416 ms
[2023-01-31T04:35:42.965+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:42 INFO Executor: Finished task 0.0 in stage 30.0 (TID 27). 6257 bytes result sent to driver
[2023-01-31T04:35:42.981+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:42 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 27) in 4196 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:35:42.981+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:42 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool
[2023-01-31T04:35:42.998+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:42 INFO DAGScheduler: ResultStage 30 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 4.417 s
[2023-01-31T04:35:43.009+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:43 INFO DAGScheduler: Job 27 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:35:43.010+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 30: Stage finished
[2023-01-31T04:35:43.031+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:43 INFO DAGScheduler: Job 27 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 4.605021 s
[2023-01-31T04:35:43.499+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:43 INFO CodeGenerator: Code generated in 141.309162 ms
[2023-01-31T04:35:43.512+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:43 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 1024.1 KiB, free 431.7 MiB)
[2023-01-31T04:35:43.526+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:43 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 285.0 B, free 431.7 MiB)
[2023-01-31T04:35:43.526+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:43 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on 81d5fcd0285b:34123 (size: 285.0 B, free: 434.2 MiB)
[2023-01-31T04:35:43.529+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:43 INFO SparkContext: Created broadcast 37 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2023-01-31T04:35:44.132+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:44 INFO ShufflePartitionsUtil: For shuffle(2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2023-01-31T04:35:45.370+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:45 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 81d5fcd0285b:34123 in memory (size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T04:35:45.446+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:45 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 81d5fcd0285b:34123 in memory (size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T04:35:45.557+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:45 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 81d5fcd0285b:34123 in memory (size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T04:35:45.745+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:45 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 81d5fcd0285b:34123 in memory (size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T04:35:46.054+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:46 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 81d5fcd0285b:34123 in memory (size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T04:35:46.259+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:46 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 81d5fcd0285b:34123 in memory (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:35:46.363+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:46 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 81d5fcd0285b:34123 in memory (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:35:46.505+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:46 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 81d5fcd0285b:34123 in memory (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:35:46.573+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:46 INFO BlockManagerInfo: Removed broadcast_31_piece0 on 81d5fcd0285b:34123 in memory (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:35:46.719+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:46 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 81d5fcd0285b:34123 in memory (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:35:46.813+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:46 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 81d5fcd0285b:34123 in memory (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:35:46.858+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:46 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 81d5fcd0285b:34123 in memory (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:35:46.997+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:46 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 81d5fcd0285b:34123 in memory (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:35:47.040+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:47 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 81d5fcd0285b:34123 in memory (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:35:47.448+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:47 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 81d5fcd0285b:34123 in memory (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:35:48.253+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:48 INFO CodeGenerator: Code generated in 930.790147 ms
[2023-01-31T04:35:49.526+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:49 INFO CodeGenerator: Code generated in 867.85807 ms
[2023-01-31T04:35:49.793+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:49 INFO BlockManagerInfo: Removed broadcast_36_piece0 on 81d5fcd0285b:34123 in memory (size: 22.6 KiB, free: 434.3 MiB)
[2023-01-31T04:35:50.145+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:50 INFO CodeGenerator: Code generated in 370.718844 ms
[2023-01-31T04:35:50.192+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:50 INFO CodeGenerator: Code generated in 30.791632 ms
[2023-01-31T04:35:50.264+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:50 INFO CodeGenerator: Code generated in 64.334838 ms
[2023-01-31T04:35:50.540+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:50 INFO CodeGenerator: Code generated in 246.663014 ms
[2023-01-31T04:35:50.810+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:50 INFO CodeGenerator: Code generated in 227.914009 ms
[2023-01-31T04:35:51.202+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:51 INFO CodeGenerator: Code generated in 324.124548 ms
[2023-01-31T04:35:51.714+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:51 INFO CodeGenerator: Code generated in 472.804184 ms
[2023-01-31T04:35:52.642+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:52 INFO CodeGenerator: Code generated in 774.83359 ms
[2023-01-31T04:35:53.131+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:53 INFO CodeGenerator: Code generated in 399.392564 ms
[2023-01-31T04:35:53.556+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:53 INFO CodeGenerator: Code generated in 361.78957 ms
[2023-01-31T04:35:54.347+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:54 INFO CodeGenerator: Code generated in 490.40402 ms
[2023-01-31T04:35:54.576+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:54 INFO CodeGenerator: Code generated in 129.716858 ms
[2023-01-31T04:35:55.057+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:55 INFO CodeGenerator: Code generated in 394.260585 ms
[2023-01-31T04:35:56.320+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:56 INFO CodeGenerator: Code generated in 420.718504 ms
[2023-01-31T04:35:57.243+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:57 INFO BlockManagerInfo: Removed broadcast_33_piece0 on 81d5fcd0285b:34123 in memory (size: 10.2 KiB, free: 434.4 MiB)
[2023-01-31T04:35:57.264+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:57 INFO BlockManagerInfo: Removed broadcast_32_piece0 on 81d5fcd0285b:34123 in memory (size: 8.1 KiB, free: 434.4 MiB)
[2023-01-31T04:35:57.306+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:57 INFO CodeGenerator: Code generated in 633.657655 ms
[2023-01-31T04:35:58.856+0000] {spark_submit.py:495} INFO - 23/01/31 04:35:58 INFO CodeGenerator: Code generated in 283.572234 ms
[2023-01-31T04:36:00.363+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:00 INFO CodeGenerator: Code generated in 1053.865701 ms
[2023-01-31T04:36:02.068+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:02 INFO CodeGenerator: Code generated in 1129.384828 ms
[2023-01-31T04:36:03.794+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:03 INFO CodeGenerator: Code generated in 490.637056 ms
[2023-01-31T04:36:05.778+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:05 INFO CodeGenerator: Code generated in 480.05503 ms
[2023-01-31T04:36:06.732+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:06 INFO CodeGenerator: Code generated in 58.239773 ms
[2023-01-31T04:36:07.318+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:07 INFO CodeGenerator: Code generated in 320.520346 ms
[2023-01-31T04:36:08.345+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:08 INFO CodeGenerator: Code generated in 578.55317 ms
[2023-01-31T04:36:09.836+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:09 INFO CodeGenerator: Code generated in 948.646471 ms
[2023-01-31T04:36:10.295+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:10 INFO CodeGenerator: Code generated in 259.049508 ms
[2023-01-31T04:36:11.533+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:11 INFO CodeGenerator: Code generated in 406.051484 ms
[2023-01-31T04:36:11.823+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:11 INFO CodeGenerator: Code generated in 27.546011 ms
[2023-01-31T04:36:12.135+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:12 INFO CodeGenerator: Code generated in 55.357465 ms
[2023-01-31T04:36:14.515+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:14 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-31T04:36:14.519+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:14 INFO DAGScheduler: Got job 28 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:36:14.519+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:14 INFO DAGScheduler: Final stage: ResultStage 46 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:36:14.520+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:14 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 32, ShuffleMapStage 33, ShuffleMapStage 34, ShuffleMapStage 35, ShuffleMapStage 36, ShuffleMapStage 37, ShuffleMapStage 38, ShuffleMapStage 39, ShuffleMapStage 40, ShuffleMapStage 41, ShuffleMapStage 42, ShuffleMapStage 43, ShuffleMapStage 44, ShuffleMapStage 45, ShuffleMapStage 31)
[2023-01-31T04:36:14.520+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:14 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:36:14.541+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:14 INFO DAGScheduler: Submitting ResultStage 46 (MapPartitionsRDD[224] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:36:14.666+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:14 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 276.3 KiB, free 431.9 MiB)
[2023-01-31T04:36:14.905+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:14 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 84.6 KiB, free 431.8 MiB)
[2023-01-31T04:36:14.914+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:14 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on 81d5fcd0285b:34123 (size: 84.6 KiB, free: 434.3 MiB)
[2023-01-31T04:36:14.919+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:14 INFO SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:36:14.929+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 46 (MapPartitionsRDD[224] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:36:14.930+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:14 INFO TaskSchedulerImpl: Adding task set 46.0 with 1 tasks resource profile 0
[2023-01-31T04:36:14.936+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:14 INFO TaskSetManager: Starting task 0.0 in stage 46.0 (TID 28) (81d5fcd0285b, executor driver, partition 0, NODE_LOCAL, 8781 bytes) taskResourceAssignments Map()
[2023-01-31T04:36:14.936+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:14 INFO Executor: Running task 0.0 in stage 46.0 (TID 28)
[2023-01-31T04:36:16.351+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:16 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:36:16.354+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 60 ms
[2023-01-31T04:36:16.479+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:16 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:36:16.480+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
[2023-01-31T04:36:16.728+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:16 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:36:16.739+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
[2023-01-31T04:36:16.805+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:16 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:36:16.811+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
[2023-01-31T04:36:16.967+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:16 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:36:16.990+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 56 ms
[2023-01-31T04:36:17.615+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:17 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:36:17.617+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 44 ms
[2023-01-31T04:36:18.031+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:18 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:36:18.033+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 77 ms
[2023-01-31T04:36:20.504+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:20 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:36:20.505+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
[2023-01-31T04:36:20.771+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:20 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:36:20.772+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2023-01-31T04:36:20.985+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:20 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:36:20.986+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2023-01-31T04:36:21.212+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:21 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:36:21.216+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
[2023-01-31T04:36:21.373+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:21 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:36:21.374+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2023-01-31T04:36:21.633+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:21 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:36:21.634+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2023-01-31T04:36:21.809+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:21 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:36:21.813+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 59 ms
[2023-01-31T04:36:22.203+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:22 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:36:22.205+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2023-01-31T04:36:22.539+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:22 INFO CodeGenerator: Code generated in 187.290857 ms
[2023-01-31T04:36:30.171+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:30 INFO Executor: Finished task 0.0 in stage 46.0 (TID 28). 25165 bytes result sent to driver
[2023-01-31T04:36:30.530+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:30 INFO TaskSetManager: Finished task 0.0 in stage 46.0 (TID 28) in 15586 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:36:30.552+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:30 INFO TaskSchedulerImpl: Removed TaskSet 46.0, whose tasks have all completed, from pool
[2023-01-31T04:36:30.619+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:30 INFO DAGScheduler: ResultStage 46 (save at BigQueryWriteHelper.java:105) finished in 16.016 s
[2023-01-31T04:36:30.673+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:30 INFO DAGScheduler: Job 28 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:36:30.675+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 46: Stage finished
[2023-01-31T04:36:30.685+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:30 INFO DAGScheduler: Job 28 finished: save at BigQueryWriteHelper.java:105, took 16.162516 s
[2023-01-31T04:36:34.425+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:34 INFO DAGScheduler: Registering RDD 225 (save at BigQueryWriteHelper.java:105) as input to shuffle 18
[2023-01-31T04:36:34.430+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:34 INFO DAGScheduler: Got map stage job 29 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:36:34.431+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:34 INFO DAGScheduler: Final stage: ShuffleMapStage 62 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:36:34.431+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:34 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 47, ShuffleMapStage 48, ShuffleMapStage 49, ShuffleMapStage 50, ShuffleMapStage 51, ShuffleMapStage 52, ShuffleMapStage 53, ShuffleMapStage 54, ShuffleMapStage 55, ShuffleMapStage 56, ShuffleMapStage 57, ShuffleMapStage 58, ShuffleMapStage 59, ShuffleMapStage 60, ShuffleMapStage 61)
[2023-01-31T04:36:34.431+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:34 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:36:34.466+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:34 INFO DAGScheduler: Submitting ShuffleMapStage 62 (MapPartitionsRDD[225] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:36:35.197+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:35 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 276.2 KiB, free 431.5 MiB)
[2023-01-31T04:36:35.527+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:35 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 84.6 KiB, free 431.5 MiB)
[2023-01-31T04:36:35.562+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:35 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on 81d5fcd0285b:34123 (size: 84.6 KiB, free: 434.2 MiB)
[2023-01-31T04:36:35.594+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:35 INFO SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:36:35.594+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:35 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 62 (MapPartitionsRDD[225] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:36:35.594+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:35 INFO TaskSchedulerImpl: Adding task set 62.0 with 1 tasks resource profile 0
[2023-01-31T04:36:35.649+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:35 INFO TaskSetManager: Starting task 0.0 in stage 62.0 (TID 29) (81d5fcd0285b, executor driver, partition 0, NODE_LOCAL, 8770 bytes) taskResourceAssignments Map()
[2023-01-31T04:36:35.677+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:35 INFO Executor: Running task 0.0 in stage 62.0 (TID 29)
[2023-01-31T04:36:36.371+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:36 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:36:36.379+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 18 ms
[2023-01-31T04:36:36.486+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:36 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:36:36.587+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 132 ms
[2023-01-31T04:36:37.149+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:37 INFO BlockManagerInfo: Removed broadcast_38_piece0 on 81d5fcd0285b:34123 in memory (size: 84.6 KiB, free: 434.3 MiB)
[2023-01-31T04:36:37.162+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:37 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:36:37.163+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
[2023-01-31T04:36:37.239+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:37 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:36:37.327+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 87 ms
[2023-01-31T04:36:37.603+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:37 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:36:37.604+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
[2023-01-31T04:36:37.664+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:37 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:36:37.665+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2023-01-31T04:36:37.848+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:37 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:36:37.853+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
[2023-01-31T04:36:37.864+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:37 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:36:37.869+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
[2023-01-31T04:36:37.881+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:37 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:36:37.892+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
[2023-01-31T04:36:37.906+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:37 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:36:37.913+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
[2023-01-31T04:36:37.933+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:37 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:36:37.939+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
[2023-01-31T04:36:38.049+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:38 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:36:38.145+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 90 ms
[2023-01-31T04:36:38.208+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:38 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:36:38.318+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 17 ms
[2023-01-31T04:36:38.378+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:38 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:36:38.379+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 37 ms
[2023-01-31T04:36:38.410+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:38 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:36:38.454+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 44 ms
[2023-01-31T04:36:39.185+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:39 INFO Executor: Finished task 0.0 in stage 62.0 (TID 29). 24984 bytes result sent to driver
[2023-01-31T04:36:39.347+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:39 INFO TaskSetManager: Finished task 0.0 in stage 62.0 (TID 29) in 3687 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:36:39.348+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:39 INFO TaskSchedulerImpl: Removed TaskSet 62.0, whose tasks have all completed, from pool
[2023-01-31T04:36:39.353+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:39 INFO DAGScheduler: ShuffleMapStage 62 (save at BigQueryWriteHelper.java:105) finished in 4.564 s
[2023-01-31T04:36:39.365+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:39 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:36:39.366+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:39 INFO DAGScheduler: running: HashSet()
[2023-01-31T04:36:39.366+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:39 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:36:39.366+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:39 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:36:39.443+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:39 INFO ShufflePartitionsUtil: For shuffle(18), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2023-01-31T04:36:40.101+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:40 INFO BlockManagerInfo: Removed broadcast_39_piece0 on 81d5fcd0285b:34123 in memory (size: 84.6 KiB, free: 434.4 MiB)
[2023-01-31T04:36:40.415+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:40 INFO CodeGenerator: Code generated in 642.623819 ms
[2023-01-31T04:36:41.849+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:41 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-31T04:36:41.968+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:41 INFO DAGScheduler: Got job 30 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:36:41.969+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:41 INFO DAGScheduler: Final stage: ResultStage 79 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:36:41.971+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:41 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 78)
[2023-01-31T04:36:41.972+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:41 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:36:42.129+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:42 INFO DAGScheduler: Submitting ResultStage 79 (MapPartitionsRDD[227] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:36:42.955+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:42 INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 354.4 KiB, free 431.8 MiB)
[2023-01-31T04:36:42.992+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:42 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 110.5 KiB, free 431.7 MiB)
[2023-01-31T04:36:42.993+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:42 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on 81d5fcd0285b:34123 (size: 110.5 KiB, free: 434.3 MiB)
[2023-01-31T04:36:43.006+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:42 INFO SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:36:43.008+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 79 (MapPartitionsRDD[227] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:36:43.009+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:43 INFO TaskSchedulerImpl: Adding task set 79.0 with 1 tasks resource profile 0
[2023-01-31T04:36:43.010+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:43 INFO TaskSetManager: Starting task 0.0 in stage 79.0 (TID 30) (81d5fcd0285b, executor driver, partition 0, NODE_LOCAL, 7399 bytes) taskResourceAssignments Map()
[2023-01-31T04:36:43.017+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:43 INFO Executor: Running task 0.0 in stage 79.0 (TID 30)
[2023-01-31T04:36:44.366+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:44 INFO ShuffleBlockFetcherIterator: Getting 1 (568.0 B) non-empty blocks including 1 (568.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:36:44.366+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
[2023-01-31T04:36:44.496+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:44 INFO CodeGenerator: Code generated in 20.627077 ms
[2023-01-31T04:36:44.518+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T04:36:44.526+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T04:36:44.529+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:44 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T04:36:44.530+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T04:36:44.530+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T04:36:44.532+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T04:36:44.649+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:44 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T04:36:44.679+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:44 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T04:36:44.979+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:44 INFO ParquetOutputFormat: Parquet block size to 134217728
[2023-01-31T04:36:45.007+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:45 INFO ParquetOutputFormat: Validation is off
[2023-01-31T04:36:45.009+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:45 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-01-31T04:36:45.010+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:45 INFO ParquetOutputFormat: Parquet properties are:
[2023-01-31T04:36:45.014+0000] {spark_submit.py:495} INFO - Parquet page size to 1048576
[2023-01-31T04:36:45.017+0000] {spark_submit.py:495} INFO - Parquet dictionary page size to 1048576
[2023-01-31T04:36:45.017+0000] {spark_submit.py:495} INFO - Dictionary is true
[2023-01-31T04:36:45.017+0000] {spark_submit.py:495} INFO - Writer version is: PARQUET_1_0
[2023-01-31T04:36:45.017+0000] {spark_submit.py:495} INFO - Page size checking is: estimated
[2023-01-31T04:36:45.018+0000] {spark_submit.py:495} INFO - Min row count for page size check is: 100
[2023-01-31T04:36:45.018+0000] {spark_submit.py:495} INFO - Max row count for page size check is: 10000
[2023-01-31T04:36:45.018+0000] {spark_submit.py:495} INFO - Truncate length for column indexes is: 64
[2023-01-31T04:36:45.018+0000] {spark_submit.py:495} INFO - Truncate length for statistics min/max  is: 2147483647
[2023-01-31T04:36:45.019+0000] {spark_submit.py:495} INFO - Bloom filter enabled: false
[2023-01-31T04:36:45.057+0000] {spark_submit.py:495} INFO - Max Bloom filter size for a column is 1048576
[2023-01-31T04:36:45.057+0000] {spark_submit.py:495} INFO - Bloom filter expected number of distinct values are: null
[2023-01-31T04:36:45.057+0000] {spark_submit.py:495} INFO - Page row count limit to 20000
[2023-01-31T04:36:45.057+0000] {spark_submit.py:495} INFO - Writing page checksums is: on
[2023-01-31T04:36:46.490+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:46 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-01-31T04:36:46.492+0000] {spark_submit.py:495} INFO - {
[2023-01-31T04:36:46.493+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-31T04:36:46.495+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-31T04:36:46.498+0000] {spark_submit.py:495} INFO - "name" : "measured_at",
[2023-01-31T04:36:46.501+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T04:36:46.504+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T04:36:46.506+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:36:46.507+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:36:46.507+0000] {spark_submit.py:495} INFO - "name" : "created_at",
[2023-01-31T04:36:46.507+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T04:36:46.507+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T04:36:46.508+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:36:46.509+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:36:46.510+0000] {spark_submit.py:495} INFO - "name" : "Biomass - Actual Aggregated",
[2023-01-31T04:36:46.512+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:36:46.513+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:36:46.514+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:36:46.516+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:36:46.517+0000] {spark_submit.py:495} INFO - "name" : "Fossil Brown coal/Lignite - Actual Aggregated",
[2023-01-31T04:36:46.518+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:36:46.518+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:36:46.518+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:36:46.519+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:36:46.521+0000] {spark_submit.py:495} INFO - "name" : "Fossil Gas - Actual Aggregated",
[2023-01-31T04:36:46.522+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:36:46.524+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:36:46.527+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:36:46.529+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:36:46.529+0000] {spark_submit.py:495} INFO - "name" : "Fossil Hard coal - Actual Aggregated",
[2023-01-31T04:36:46.529+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:36:46.532+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:36:46.534+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:36:46.537+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:36:46.540+0000] {spark_submit.py:495} INFO - "name" : "Fossil Oil - Actual Aggregated",
[2023-01-31T04:36:46.543+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:36:46.546+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:36:46.547+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:36:46.548+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:36:46.554+0000] {spark_submit.py:495} INFO - "name" : "Geothermal - Actual Aggregated",
[2023-01-31T04:36:46.555+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:36:46.556+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:36:46.563+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:36:46.564+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:36:46.566+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Aggregated",
[2023-01-31T04:36:46.567+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:36:46.569+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:36:46.570+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:36:46.571+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:36:46.572+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Consumption",
[2023-01-31T04:36:46.572+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:36:46.573+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:36:46.579+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:36:46.581+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:36:46.583+0000] {spark_submit.py:495} INFO - "name" : "Hydro Run-of-river and poundage - Actual Aggregated",
[2023-01-31T04:36:46.583+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:36:46.583+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:36:46.583+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:36:46.584+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:36:46.585+0000] {spark_submit.py:495} INFO - "name" : "Hydro Water Reservoir - Actual Aggregated",
[2023-01-31T04:36:46.586+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:36:46.588+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:36:46.601+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:36:46.603+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:36:46.605+0000] {spark_submit.py:495} INFO - "name" : "Nuclear - Actual Aggregated",
[2023-01-31T04:36:46.605+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:36:46.605+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:36:46.608+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:36:46.611+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:36:46.617+0000] {spark_submit.py:495} INFO - "name" : "Other - Actual Aggregated",
[2023-01-31T04:36:46.619+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:36:46.620+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:36:46.622+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:36:46.626+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:36:46.627+0000] {spark_submit.py:495} INFO - "name" : "Other renewable - Actual Aggregated",
[2023-01-31T04:36:46.630+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:36:46.650+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:36:46.685+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:36:46.690+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:36:46.691+0000] {spark_submit.py:495} INFO - "name" : "Solar - Actual Aggregated",
[2023-01-31T04:36:46.691+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:36:46.697+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:36:46.698+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:36:46.700+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:36:46.709+0000] {spark_submit.py:495} INFO - "name" : "Waste - Actual Aggregated",
[2023-01-31T04:36:46.710+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:36:46.712+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:36:46.712+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:36:46.712+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:36:46.713+0000] {spark_submit.py:495} INFO - "name" : "Wind Offshore - Actual Aggregated",
[2023-01-31T04:36:46.729+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:36:46.737+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:36:46.741+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:36:46.743+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:36:46.754+0000] {spark_submit.py:495} INFO - "name" : "Wind Onshore - Actual Aggregated",
[2023-01-31T04:36:46.758+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:36:46.774+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:36:46.816+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:36:46.818+0000] {spark_submit.py:495} INFO - } ]
[2023-01-31T04:36:46.819+0000] {spark_submit.py:495} INFO - }
[2023-01-31T04:36:46.819+0000] {spark_submit.py:495} INFO - and corresponding Parquet message type:
[2023-01-31T04:36:46.819+0000] {spark_submit.py:495} INFO - message spark_schema {
[2023-01-31T04:36:46.820+0000] {spark_submit.py:495} INFO - required int96 measured_at;
[2023-01-31T04:36:46.820+0000] {spark_submit.py:495} INFO - required int96 created_at;
[2023-01-31T04:36:46.915+0000] {spark_submit.py:495} INFO - optional double Biomass - Actual Aggregated;
[2023-01-31T04:36:46.916+0000] {spark_submit.py:495} INFO - optional double Fossil Brown coal/Lignite - Actual Aggregated;
[2023-01-31T04:36:46.918+0000] {spark_submit.py:495} INFO - optional double Fossil Gas - Actual Aggregated;
[2023-01-31T04:36:46.993+0000] {spark_submit.py:495} INFO - optional double Fossil Hard coal - Actual Aggregated;
[2023-01-31T04:36:46.994+0000] {spark_submit.py:495} INFO - optional double Fossil Oil - Actual Aggregated;
[2023-01-31T04:36:46.995+0000] {spark_submit.py:495} INFO - optional double Geothermal - Actual Aggregated;
[2023-01-31T04:36:46.996+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Aggregated;
[2023-01-31T04:36:46.997+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Consumption;
[2023-01-31T04:36:46.998+0000] {spark_submit.py:495} INFO - optional double Hydro Run-of-river and poundage - Actual Aggregated;
[2023-01-31T04:36:46.999+0000] {spark_submit.py:495} INFO - optional double Hydro Water Reservoir - Actual Aggregated;
[2023-01-31T04:36:46.999+0000] {spark_submit.py:495} INFO - optional double Nuclear - Actual Aggregated;
[2023-01-31T04:36:46.999+0000] {spark_submit.py:495} INFO - optional double Other - Actual Aggregated;
[2023-01-31T04:36:46.999+0000] {spark_submit.py:495} INFO - optional double Other renewable - Actual Aggregated;
[2023-01-31T04:36:47.000+0000] {spark_submit.py:495} INFO - optional double Solar - Actual Aggregated;
[2023-01-31T04:36:47.001+0000] {spark_submit.py:495} INFO - optional double Waste - Actual Aggregated;
[2023-01-31T04:36:47.002+0000] {spark_submit.py:495} INFO - optional double Wind Offshore - Actual Aggregated;
[2023-01-31T04:36:47.003+0000] {spark_submit.py:495} INFO - optional double Wind Onshore - Actual Aggregated;
[2023-01-31T04:36:47.004+0000] {spark_submit.py:495} INFO - }
[2023-01-31T04:36:47.005+0000] {spark_submit.py:495} INFO - 
[2023-01-31T04:36:47.006+0000] {spark_submit.py:495} INFO - 
[2023-01-31T04:36:50.553+0000] {spark_submit.py:495} INFO - 23/01/31 04:36:50 INFO CodecPool: Got brand-new compressor [.snappy]
[2023-01-31T04:37:07.480+0000] {spark_submit.py:495} INFO - 23/01/31 04:37:07 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675139496254-cf3e1794-e7de-4426-9537-0cee1afe45b0/_temporary/0/_temporary/' directory.
[2023-01-31T04:37:07.486+0000] {spark_submit.py:495} INFO - 23/01/31 04:37:07 INFO FileOutputCommitter: Saved output of task 'attempt_202301310436407402067589987596276_0079_m_000000_30' to gs://entsoe_temp_1009/.spark-bigquery-local-1675139496254-cf3e1794-e7de-4426-9537-0cee1afe45b0/_temporary/0/task_202301310436407402067589987596276_0079_m_000000
[2023-01-31T04:37:07.498+0000] {spark_submit.py:495} INFO - 23/01/31 04:37:07 INFO SparkHadoopMapRedUtil: attempt_202301310436407402067589987596276_0079_m_000000_30: Committed. Elapsed time: 3705 ms.
[2023-01-31T04:37:07.985+0000] {spark_submit.py:495} INFO - 23/01/31 04:37:07 INFO Executor: Finished task 0.0 in stage 79.0 (TID 30). 26967 bytes result sent to driver
[2023-01-31T04:37:08.003+0000] {spark_submit.py:495} INFO - 23/01/31 04:37:08 INFO TaskSetManager: Finished task 0.0 in stage 79.0 (TID 30) in 25000 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:37:08.013+0000] {spark_submit.py:495} INFO - 23/01/31 04:37:08 INFO DAGScheduler: ResultStage 79 (save at BigQueryWriteHelper.java:105) finished in 25.880 s
[2023-01-31T04:37:08.024+0000] {spark_submit.py:495} INFO - 23/01/31 04:37:08 INFO TaskSchedulerImpl: Removed TaskSet 79.0, whose tasks have all completed, from pool
[2023-01-31T04:37:08.027+0000] {spark_submit.py:495} INFO - 23/01/31 04:37:08 INFO DAGScheduler: Job 30 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:37:08.027+0000] {spark_submit.py:495} INFO - 23/01/31 04:37:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 79: Stage finished
[2023-01-31T04:37:08.043+0000] {spark_submit.py:495} INFO - 23/01/31 04:37:08 INFO DAGScheduler: Job 30 finished: save at BigQueryWriteHelper.java:105, took 26.192166 s
[2023-01-31T04:37:08.109+0000] {spark_submit.py:495} INFO - 23/01/31 04:37:08 INFO FileFormatWriter: Start to commit write Job f0592367-583d-4510-b97d-18ccf41ebba2.
[2023-01-31T04:37:09.754+0000] {spark_submit.py:495} INFO - 23/01/31 04:37:09 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675139496254-cf3e1794-e7de-4426-9537-0cee1afe45b0/_temporary/0/task_202301310436407402067589987596276_0079_m_000000/' directory.
[2023-01-31T04:37:10.604+0000] {spark_submit.py:495} INFO - 23/01/31 04:37:10 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675139496254-cf3e1794-e7de-4426-9537-0cee1afe45b0/' directory.
[2023-01-31T04:37:12.317+0000] {spark_submit.py:495} INFO - 23/01/31 04:37:12 INFO BlockManagerInfo: Removed broadcast_40_piece0 on 81d5fcd0285b:34123 in memory (size: 110.5 KiB, free: 434.4 MiB)
[2023-01-31T04:37:12.361+0000] {spark_submit.py:495} INFO - 23/01/31 04:37:12 INFO FileFormatWriter: Write Job f0592367-583d-4510-b97d-18ccf41ebba2 committed. Elapsed time: 4199 ms.
[2023-01-31T04:37:12.386+0000] {spark_submit.py:495} INFO - 23/01/31 04:37:12 INFO FileFormatWriter: Finished processing stats for write job f0592367-583d-4510-b97d-18ccf41ebba2.
[2023-01-31T04:37:16.485+0000] {spark_submit.py:495} INFO - 23/01/31 04:37:16 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=entsoe_analytics_1009, projectId=rafzul-analytics-1009, tableId=TEST_total_generation_staging}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=null, ignoreUnknownValue=null, sourceUris=[gs://entsoe_temp_1009/.spark-bigquery-local-1675139496254-cf3e1794-e7de-4426-9537-0cee1afe45b0/part-00000-15be36eb-5aff-475b-8804-e6cc2b223a85-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=true, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=rafzul-analytics-1009, job=505afeab-1266-4443-a78a-e0c14a16b6e5, location=US}
[2023-01-31T04:37:22.859+0000] {spark_submit.py:495} INFO - 23/01/31 04:37:22 INFO BigQueryClient: Done loading to rafzul-analytics-1009.entsoe_analytics_1009.TEST_total_generation_staging. jobId: JobId{project=rafzul-analytics-1009, job=505afeab-1266-4443-a78a-e0c14a16b6e5, location=US}
[2023-01-31T04:37:23.475+0000] {spark_submit.py:495} INFO - MANTEP!!!!
[2023-01-31T04:37:23.833+0000] {spark_submit.py:495} INFO - 23/01/31 04:37:23 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-31T04:37:23.903+0000] {spark_submit.py:495} INFO - 23/01/31 04:37:23 INFO SparkUI: Stopped Spark web UI at http://81d5fcd0285b:4040
[2023-01-31T04:37:23.946+0000] {spark_submit.py:495} INFO - 23/01/31 04:37:23 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-31T04:37:24.057+0000] {spark_submit.py:495} INFO - 23/01/31 04:37:24 INFO MemoryStore: MemoryStore cleared
[2023-01-31T04:37:24.057+0000] {spark_submit.py:495} INFO - 23/01/31 04:37:24 INFO BlockManager: BlockManager stopped
[2023-01-31T04:37:24.069+0000] {spark_submit.py:495} INFO - 23/01/31 04:37:24 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-31T04:37:24.080+0000] {spark_submit.py:495} INFO - 23/01/31 04:37:24 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-31T04:37:24.114+0000] {spark_submit.py:495} INFO - 23/01/31 04:37:24 INFO SparkContext: Successfully stopped SparkContext
[2023-01-31T04:37:24.115+0000] {spark_submit.py:495} INFO - 23/01/31 04:37:24 INFO ShutdownHookManager: Shutdown hook called
[2023-01-31T04:37:24.120+0000] {spark_submit.py:495} INFO - 23/01/31 04:37:24 INFO ShutdownHookManager: Deleting directory /tmp/spark-a9eb6d9b-654a-46a5-8127-653a5163146e
[2023-01-31T04:37:24.140+0000] {spark_submit.py:495} INFO - 23/01/31 04:37:24 INFO ShutdownHookManager: Deleting directory /tmp/spark-2683b6fd-0438-4bd8-b192-8fd7f930ef2f/pyspark-de9a7abf-1893-4b25-b5da-e6ea6990170c
[2023-01-31T04:37:24.151+0000] {spark_submit.py:495} INFO - 23/01/31 04:37:24 INFO ShutdownHookManager: Deleting directory /tmp/spark-2683b6fd-0438-4bd8-b192-8fd7f930ef2f
[2023-01-31T04:37:24.731+0000] {taskinstance.py:1322} INFO - Marking task as SUCCESS. dag_id=entsoe-energydata, task_id=stage_total_generation, execution_date=20210101T000000, start_date=20230131T043050, end_date=20230131T043724
[2023-01-31T04:37:24.899+0000] {local_task_job.py:159} INFO - Task exited with return code 0
[2023-01-31T04:37:24.975+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-01-31T04:48:16.769+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-31T04:48:16.808+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T00:00:00+00:00 [queued]>
[2023-01-31T04:48:16.809+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T04:48:16.809+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 4
[2023-01-31T04:48:16.810+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-31T04:48:16.855+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): stage_total_generation> on 2021-01-01 00:00:00+00:00
[2023-01-31T04:48:16.873+0000] {standard_task_runner.py:55} INFO - Started process 7293 to run task
[2023-01-31T04:48:16.891+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'entsoe-energydata', 'stage_total_generation', 'scheduled__2021-01-01T00:00:00+00:00', '--job-id', '866', '--raw', '--subdir', 'DAGS_FOLDER/ingestion_energydata_dag.py', '--cfg-path', '/tmp/tmp45kji8w3']
[2023-01-31T04:48:16.908+0000] {standard_task_runner.py:83} INFO - Job 866: Subtask stage_total_generation
[2023-01-31T04:48:17.375+0000] {task_command.py:389} INFO - Running <TaskInstance: entsoe-energydata.stage_total_generation scheduled__2021-01-01T00:00:00+00:00 [running]> on host 81d5fcd0285b
[2023-01-31T04:48:17.754+0000] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=rafzul
AIRFLOW_CTX_DAG_ID=entsoe-energydata
AIRFLOW_CTX_TASK_ID=stage_total_generation
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-01-01T00:00:00+00:00
[2023-01-31T04:48:17.817+0000] {base.py:73} INFO - Using connection ID 'spark_local' for task execution.
[2023-01-31T04:48:17.821+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark /opt/***/plugins/scripts/transform_raw_staging.py total_generation 202101010000 202101010100 DE_TENNET
[2023-01-31T04:48:52.931+0000] {spark_submit.py:495} INFO - 23/01/31 04:48:52 INFO SparkContext: Running Spark version 3.3.1
[2023-01-31T04:48:53.547+0000] {spark_submit.py:495} INFO - 23/01/31 04:48:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-31T04:48:54.798+0000] {spark_submit.py:495} INFO - 23/01/31 04:48:54 INFO ResourceUtils: ==============================================================
[2023-01-31T04:48:54.806+0000] {spark_submit.py:495} INFO - 23/01/31 04:48:54 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-31T04:48:54.813+0000] {spark_submit.py:495} INFO - 23/01/31 04:48:54 INFO ResourceUtils: ==============================================================
[2023-01-31T04:48:54.820+0000] {spark_submit.py:495} INFO - 23/01/31 04:48:54 INFO SparkContext: Submitted application: gcp_playground
[2023-01-31T04:48:55.249+0000] {spark_submit.py:495} INFO - 23/01/31 04:48:55 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-31T04:48:55.348+0000] {spark_submit.py:495} INFO - 23/01/31 04:48:55 INFO ResourceProfile: Limiting resource is cpu
[2023-01-31T04:48:55.362+0000] {spark_submit.py:495} INFO - 23/01/31 04:48:55 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-31T04:48:55.996+0000] {spark_submit.py:495} INFO - 23/01/31 04:48:55 INFO SecurityManager: Changing view acls to: ***
[2023-01-31T04:48:56.014+0000] {spark_submit.py:495} INFO - 23/01/31 04:48:56 INFO SecurityManager: Changing modify acls to: ***
[2023-01-31T04:48:56.018+0000] {spark_submit.py:495} INFO - 23/01/31 04:48:56 INFO SecurityManager: Changing view acls groups to:
[2023-01-31T04:48:56.029+0000] {spark_submit.py:495} INFO - 23/01/31 04:48:56 INFO SecurityManager: Changing modify acls groups to:
[2023-01-31T04:48:56.037+0000] {spark_submit.py:495} INFO - 23/01/31 04:48:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2023-01-31T04:48:58.520+0000] {spark_submit.py:495} INFO - 23/01/31 04:48:58 INFO Utils: Successfully started service 'sparkDriver' on port 46143.
[2023-01-31T04:48:58.777+0000] {spark_submit.py:495} INFO - 23/01/31 04:48:58 INFO SparkEnv: Registering MapOutputTracker
[2023-01-31T04:48:59.012+0000] {spark_submit.py:495} INFO - 23/01/31 04:48:59 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-31T04:48:59.102+0000] {spark_submit.py:495} INFO - 23/01/31 04:48:59 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-31T04:48:59.106+0000] {spark_submit.py:495} INFO - 23/01/31 04:48:59 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-31T04:48:59.118+0000] {spark_submit.py:495} INFO - 23/01/31 04:48:59 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-31T04:48:59.193+0000] {spark_submit.py:495} INFO - 23/01/31 04:48:59 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-976faccf-dd01-4f80-ab28-06dbe7eba833
[2023-01-31T04:48:59.256+0000] {spark_submit.py:495} INFO - 23/01/31 04:48:59 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-31T04:48:59.399+0000] {spark_submit.py:495} INFO - 23/01/31 04:48:59 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-31T04:49:02.164+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:02 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-31T04:49:02.262+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:02 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2023-01-31T04:49:02.706+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:02 INFO SparkContext: Added JAR /opt/spark/jars/gcs-connector-hadoop3-latest.jar at spark://81d5fcd0285b:46143/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675140532880
[2023-01-31T04:49:02.707+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:02 INFO SparkContext: Added JAR /opt/spark/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar at spark://81d5fcd0285b:46143/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675140532880
[2023-01-31T04:49:03.394+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:03 INFO Executor: Starting executor ID driver on host 81d5fcd0285b
[2023-01-31T04:49:03.483+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:03 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-31T04:49:03.659+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:03 INFO Executor: Fetching spark://81d5fcd0285b:46143/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar with timestamp 1675140532880
[2023-01-31T04:49:04.162+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:04 INFO TransportClientFactory: Successfully created connection to 81d5fcd0285b/172.19.0.9:46143 after 393 ms (0 ms spent in bootstraps)
[2023-01-31T04:49:04.243+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:04 INFO Utils: Fetching spark://81d5fcd0285b:46143/jars/spark-bigquery-with-dependencies_2.13-0.27.1.jar to /tmp/spark-7a081703-ac1c-4588-97fa-7fb5f6aa8b5d/userFiles-0a5dfc4b-83cd-4b5e-8335-cc923bef4f5b/fetchFileTemp3858527322851799472.tmp
[2023-01-31T04:49:05.476+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:05 INFO Executor: Adding file:/tmp/spark-7a081703-ac1c-4588-97fa-7fb5f6aa8b5d/userFiles-0a5dfc4b-83cd-4b5e-8335-cc923bef4f5b/spark-bigquery-with-dependencies_2.13-0.27.1.jar to class loader
[2023-01-31T04:49:05.477+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:05 INFO Executor: Fetching spark://81d5fcd0285b:46143/jars/gcs-connector-hadoop3-latest.jar with timestamp 1675140532880
[2023-01-31T04:49:05.504+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:05 INFO Utils: Fetching spark://81d5fcd0285b:46143/jars/gcs-connector-hadoop3-latest.jar to /tmp/spark-7a081703-ac1c-4588-97fa-7fb5f6aa8b5d/userFiles-0a5dfc4b-83cd-4b5e-8335-cc923bef4f5b/fetchFileTemp5310300405464836452.tmp
[2023-01-31T04:49:07.959+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:07 INFO Executor: Adding file:/tmp/spark-7a081703-ac1c-4588-97fa-7fb5f6aa8b5d/userFiles-0a5dfc4b-83cd-4b5e-8335-cc923bef4f5b/gcs-connector-hadoop3-latest.jar to class loader
[2023-01-31T04:49:08.040+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:08 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 32823.
[2023-01-31T04:49:08.041+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:08 INFO NettyBlockTransferService: Server created on 81d5fcd0285b:32823
[2023-01-31T04:49:08.057+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:08 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-31T04:49:08.136+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:08 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 81d5fcd0285b, 32823, None)
[2023-01-31T04:49:08.184+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:08 INFO BlockManagerMasterEndpoint: Registering block manager 81d5fcd0285b:32823 with 434.4 MiB RAM, BlockManagerId(driver, 81d5fcd0285b, 32823, None)
[2023-01-31T04:49:08.217+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:08 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 81d5fcd0285b, 32823, None)
[2023-01-31T04:49:08.226+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:08 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 81d5fcd0285b, 32823, None)
[2023-01-31T04:49:15.349+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:15 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-01-31T04:49:15.422+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:15 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-01-31T04:49:36.639+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:36 INFO InMemoryFileIndex: It took 1045 ms to list leaf files for 1 paths.
[2023-01-31T04:49:38.762+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:38 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.9 KiB, free 434.2 MiB)
[2023-01-31T04:49:39.722+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:39 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)
[2023-01-31T04:49:39.765+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:39 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 81d5fcd0285b:32823 (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T04:49:39.795+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:39 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0
[2023-01-31T04:49:45.124+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:45 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T04:49:45.254+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:45 INFO FileInputFormat: Total input files to process : 1
[2023-01-31T04:49:45.471+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:45 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2023-01-31T04:49:45.699+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:45 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-01-31T04:49:45.714+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:45 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2023-01-31T04:49:45.716+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:45 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:49:45.813+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:45 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:49:45.922+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:45 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-01-31T04:49:46.415+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:46 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.6 KiB, free 434.2 MiB)
[2023-01-31T04:49:46.449+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:46 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.2 MiB)
[2023-01-31T04:49:46.452+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:46 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 81d5fcd0285b:32823 (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T04:49:46.454+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:46 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:49:46.590+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:49:46.597+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:46 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-01-31T04:49:47.366+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:47 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7578 bytes) taskResourceAssignments Map()
[2023-01-31T04:49:47.507+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:47 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-01-31T04:49:49.388+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:49 INFO BinaryFileRDD: Input split: Paths:/total_generation__DE_TENNET__202101010000__202101010100.json:0+9369
[2023-01-31T04:49:52.260+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:52 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3480 bytes result sent to driver
[2023-01-31T04:49:52.323+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:52 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 5144 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:49:52.335+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:52 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-01-31T04:49:52.392+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:52 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 6.264 s
[2023-01-31T04:49:52.457+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:52 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:49:52.459+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-01-31T04:49:52.538+0000] {spark_submit.py:495} INFO - 23/01/31 04:49:52 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 7.050515 s
[2023-01-31T04:50:07.718+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:07 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 81d5fcd0285b:32823 in memory (size: 34.6 KiB, free: 434.4 MiB)
[2023-01-31T04:50:07.809+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:07 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 81d5fcd0285b:32823 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2023-01-31T04:50:22.278+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:22 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T04:50:22.285+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:22 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T04:50:22.289+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:22 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T04:50:24.203+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:24 INFO CodeGenerator: Code generated in 920.217514 ms
[2023-01-31T04:50:24.218+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:24 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 201.3 KiB, free 434.2 MiB)
[2023-01-31T04:50:24.257+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:24 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
[2023-01-31T04:50:24.259+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:24 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 81d5fcd0285b:32823 (size: 34.5 KiB, free: 434.4 MiB)
[2023-01-31T04:50:24.262+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:24 INFO SparkContext: Created broadcast 2 from collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T04:50:24.304+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:24 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T04:50:24.523+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:24 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:40
[2023-01-31T04:50:24.527+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:24 INFO DAGScheduler: Got job 1 (collect at /opt/***/plugins/helpers/parsers.py:40) with 1 output partitions
[2023-01-31T04:50:24.527+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:24 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40)
[2023-01-31T04:50:24.528+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:24 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:50:24.528+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:24 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:50:24.538+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:24 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40), which has no missing parents
[2023-01-31T04:50:24.552+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:24 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.5 KiB, free 434.2 MiB)
[2023-01-31T04:50:24.561+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:24 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
[2023-01-31T04:50:24.564+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:24 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 81d5fcd0285b:32823 (size: 7.7 KiB, free: 434.4 MiB)
[2023-01-31T04:50:24.565+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:24 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:50:24.568+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at /opt/***/plugins/helpers/parsers.py:40) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:50:24.569+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:24 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-01-31T04:50:24.589+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:24 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T04:50:24.592+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:24 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-01-31T04:50:25.700+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:25 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T04:50:26.708+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:26 INFO CodeGenerator: Code generated in 767.502075 ms
[2023-01-31T04:50:27.392+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:27 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1628 bytes result sent to driver
[2023-01-31T04:50:27.435+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:27 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2862 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:50:27.441+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:27 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-01-31T04:50:27.478+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:27 INFO DAGScheduler: ResultStage 1 (collect at /opt/***/plugins/helpers/parsers.py:40) finished in 2.908 s
[2023-01-31T04:50:27.496+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:27 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:50:27.505+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-01-31T04:50:27.508+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:27 INFO DAGScheduler: Job 1 finished: collect at /opt/***/plugins/helpers/parsers.py:40, took 2.982930 s
[2023-01-31T04:50:28.307+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:28 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T04:50:28.309+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:28 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T04:50:28.316+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:28 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T04:50:28.698+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:28 INFO CodeGenerator: Code generated in 184.528431 ms
[2023-01-31T04:50:28.713+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:28 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.3 KiB, free 434.0 MiB)
[2023-01-31T04:50:28.755+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:28 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
[2023-01-31T04:50:28.756+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:28 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 81d5fcd0285b:32823 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T04:50:28.757+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:28 INFO SparkContext: Created broadcast 4 from collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T04:50:28.760+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:28 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T04:50:28.922+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:28 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:41
[2023-01-31T04:50:28.925+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:28 INFO DAGScheduler: Got job 2 (collect at /opt/***/plugins/helpers/parsers.py:41) with 1 output partitions
[2023-01-31T04:50:28.929+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:28 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41)
[2023-01-31T04:50:28.929+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:28 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:50:28.937+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:28 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:50:28.942+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:28 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41), which has no missing parents
[2023-01-31T04:50:29.010+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:29 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.5 KiB, free 433.9 MiB)
[2023-01-31T04:50:29.076+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:29 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 433.9 MiB)
[2023-01-31T04:50:29.089+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:29 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 81d5fcd0285b:32823 (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T04:50:29.097+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:29 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:50:29.107+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at collect at /opt/***/plugins/helpers/parsers.py:41) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:50:29.108+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:29 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-01-31T04:50:29.113+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:29 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T04:50:29.117+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:29 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-01-31T04:50:29.242+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:29 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T04:50:29.267+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:29 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 81d5fcd0285b:32823 in memory (size: 7.7 KiB, free: 434.3 MiB)
[2023-01-31T04:50:29.568+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:29 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1628 bytes result sent to driver
[2023-01-31T04:50:29.594+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:29 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 478 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:50:29.595+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:29 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-01-31T04:50:29.599+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:29 INFO DAGScheduler: ResultStage 2 (collect at /opt/***/plugins/helpers/parsers.py:41) finished in 0.638 s
[2023-01-31T04:50:29.599+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:29 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:50:29.600+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-01-31T04:50:29.601+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:29 INFO DAGScheduler: Job 2 finished: collect at /opt/***/plugins/helpers/parsers.py:41, took 0.678312 s
[2023-01-31T04:50:30.375+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:30 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T04:50:30.383+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:30 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.resolution)
[2023-01-31T04:50:30.387+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:30 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T04:50:30.959+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:30 INFO CodeGenerator: Code generated in 367.230267 ms
[2023-01-31T04:50:30.994+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:30 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 201.3 KiB, free 433.7 MiB)
[2023-01-31T04:50:31.111+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:31 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)
[2023-01-31T04:50:31.117+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:31 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 81d5fcd0285b:32823 (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T04:50:31.125+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:31 INFO SparkContext: Created broadcast 6 from collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T04:50:31.157+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:31 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T04:50:31.412+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:31 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:53
[2023-01-31T04:50:31.414+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:31 INFO DAGScheduler: Got job 3 (collect at /opt/***/plugins/helpers/parsers.py:53) with 1 output partitions
[2023-01-31T04:50:31.415+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:31 INFO DAGScheduler: Final stage: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53)
[2023-01-31T04:50:31.415+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:31 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:50:31.415+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:31 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:50:31.434+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:31 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53), which has no missing parents
[2023-01-31T04:50:31.470+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:31 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 36.5 KiB, free 433.7 MiB)
[2023-01-31T04:50:31.494+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:31 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.6 MiB)
[2023-01-31T04:50:31.499+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:31 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 81d5fcd0285b:32823 (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T04:50:31.510+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:31 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:50:31.524+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at collect at /opt/***/plugins/helpers/parsers.py:53) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:50:31.529+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:31 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-01-31T04:50:31.545+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:31 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T04:50:31.548+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:31 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2023-01-31T04:50:31.635+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:31 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T04:50:31.982+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:31 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1712 bytes result sent to driver
[2023-01-31T04:50:31.994+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:31 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 451 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:50:31.998+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:31 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-01-31T04:50:32.004+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:32 INFO DAGScheduler: ResultStage 3 (collect at /opt/***/plugins/helpers/parsers.py:53) finished in 0.564 s
[2023-01-31T04:50:32.005+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:32 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:50:32.005+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2023-01-31T04:50:32.005+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:32 INFO DAGScheduler: Job 3 finished: collect at /opt/***/plugins/helpers/parsers.py:53, took 0.588441 s
[2023-01-31T04:50:32.268+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:32 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T04:50:32.268+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:32 INFO FileSourceStrategy: Post-Scan Filters:
[2023-01-31T04:50:32.280+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:32 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T04:50:32.443+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:32 INFO CodeGenerator: Code generated in 104.265108 ms
[2023-01-31T04:50:32.501+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:32 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.3 KiB, free 433.4 MiB)
[2023-01-31T04:50:32.595+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:32 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)
[2023-01-31T04:50:32.598+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:32 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 81d5fcd0285b:32823 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T04:50:32.603+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:32 INFO SparkContext: Created broadcast 8 from collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T04:50:32.637+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:32 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T04:50:32.750+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:32 INFO SparkContext: Starting job: collect at /opt/***/plugins/helpers/parsers.py:57
[2023-01-31T04:50:32.754+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:32 INFO DAGScheduler: Got job 4 (collect at /opt/***/plugins/helpers/parsers.py:57) with 1 output partitions
[2023-01-31T04:50:32.755+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:32 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57)
[2023-01-31T04:50:32.755+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:32 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:50:32.755+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:32 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:50:32.788+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:32 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57), which has no missing parents
[2023-01-31T04:50:32.796+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:32 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 15.3 KiB, free 433.4 MiB)
[2023-01-31T04:50:32.800+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:32 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 433.4 MiB)
[2023-01-31T04:50:32.803+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:32 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 81d5fcd0285b:32823 (size: 7.6 KiB, free: 434.2 MiB)
[2023-01-31T04:50:32.805+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:32 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:50:32.807+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at collect at /opt/***/plugins/helpers/parsers.py:57) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:50:32.809+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:32 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2023-01-31T04:50:32.827+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:32 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T04:50:32.828+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:32 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2023-01-31T04:50:32.898+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:32 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T04:50:33.070+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:33 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1586 bytes result sent to driver
[2023-01-31T04:50:33.073+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:33 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 248 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:50:33.073+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:33 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2023-01-31T04:50:33.075+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:33 INFO DAGScheduler: ResultStage 4 (collect at /opt/***/plugins/helpers/parsers.py:57) finished in 0.310 s
[2023-01-31T04:50:33.075+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:33 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:50:33.078+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2023-01-31T04:50:33.079+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:33 INFO DAGScheduler: Job 4 finished: collect at /opt/***/plugins/helpers/parsers.py:57, took 0.324340 s
[2023-01-31T04:50:34.015+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:34 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 81d5fcd0285b:32823 in memory (size: 7.7 KiB, free: 434.2 MiB)
[2023-01-31T04:50:34.086+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:34 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 81d5fcd0285b:32823 in memory (size: 12.7 KiB, free: 434.3 MiB)
[2023-01-31T04:50:34.151+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:34 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 81d5fcd0285b:32823 in memory (size: 7.6 KiB, free: 434.3 MiB)
[2023-01-31T04:50:34.189+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:34 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T04:50:34.196+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:34 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.Period.Point)
[2023-01-31T04:50:34.200+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:34 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T04:50:35.052+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:35 INFO CodeGenerator: Code generated in 564.317116 ms
[2023-01-31T04:50:35.067+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:35 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 201.3 KiB, free 433.3 MiB)
[2023-01-31T04:50:35.134+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:35 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)
[2023-01-31T04:50:35.139+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:35 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 81d5fcd0285b:32823 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T04:50:35.143+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:35 INFO SparkContext: Created broadcast 10 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T04:50:35.162+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:35 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T04:50:35.317+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:35 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201
[2023-01-31T04:50:35.321+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:35 INFO DAGScheduler: Got job 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) with 1 output partitions
[2023-01-31T04:50:35.321+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:35 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201)
[2023-01-31T04:50:35.322+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:35 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:50:35.322+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:35 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:50:35.328+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:35 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201), which has no missing parents
[2023-01-31T04:50:35.363+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:35 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.3 KiB, free 433.2 MiB)
[2023-01-31T04:50:35.383+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:35 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 433.2 MiB)
[2023-01-31T04:50:35.384+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:35 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 81d5fcd0285b:32823 (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T04:50:35.387+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:35 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:50:35.395+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:50:35.396+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:35 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-01-31T04:50:35.407+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:35 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T04:50:35.408+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:35 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2023-01-31T04:50:35.448+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:35 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T04:50:35.724+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:35 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2558 bytes result sent to driver
[2023-01-31T04:50:35.735+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:35 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 328 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:50:35.739+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:35 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-01-31T04:50:35.746+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:35 INFO DAGScheduler: ResultStage 5 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:201) finished in 0.412 s
[2023-01-31T04:50:35.746+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:35 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:50:35.746+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-01-31T04:50:35.746+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:35 INFO DAGScheduler: Job 5 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:201, took 0.425776 s
[2023-01-31T04:50:36.212+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:36 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T04:50:36.215+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:36 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.MktPSRType.psrType)
[2023-01-31T04:50:36.218+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:36 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T04:50:37.034+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:37 INFO CodeGenerator: Code generated in 677.310714 ms
[2023-01-31T04:50:37.052+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:37 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 201.3 KiB, free 433.0 MiB)
[2023-01-31T04:50:37.082+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:37 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.0 MiB)
[2023-01-31T04:50:37.085+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:37 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 81d5fcd0285b:32823 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T04:50:37.085+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:37 INFO SparkContext: Created broadcast 12 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T04:50:37.088+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:37 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T04:50:37.134+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:37 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202
[2023-01-31T04:50:37.136+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:37 INFO DAGScheduler: Got job 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) with 1 output partitions
[2023-01-31T04:50:37.137+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:37 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202)
[2023-01-31T04:50:37.137+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:37 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:50:37.137+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:37 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:50:37.143+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:37 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202), which has no missing parents
[2023-01-31T04:50:37.153+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:37 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 36.5 KiB, free 432.9 MiB)
[2023-01-31T04:50:37.161+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:37 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.9 MiB)
[2023-01-31T04:50:37.164+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:37 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 81d5fcd0285b:32823 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T04:50:37.168+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:37 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:50:37.171+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[26] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:50:37.172+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:37 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2023-01-31T04:50:37.176+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:37 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T04:50:37.176+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:37 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2023-01-31T04:50:37.206+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:37 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T04:50:37.369+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:37 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1827 bytes result sent to driver
[2023-01-31T04:50:37.380+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:37 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 197 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:50:37.381+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:37 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2023-01-31T04:50:37.381+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:37 INFO DAGScheduler: ResultStage 6 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:202) finished in 0.230 s
[2023-01-31T04:50:37.381+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:37 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:50:37.381+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2023-01-31T04:50:37.382+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:37 INFO DAGScheduler: Job 6 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:202, took 0.244439 s
[2023-01-31T04:50:37.771+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:37 INFO FileSourceStrategy: Pushed Filters:
[2023-01-31T04:50:37.783+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:37 INFO FileSourceStrategy: Post-Scan Filters: (size(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text, true) > 0),isnotnull(cast(GL_MarketDocument#0 as struct<xmlns:string,TimeSeries:array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain_mRID:struct<text:string,codingScheme:string>,quantity_Measure_Unit_name:string>>,createdDateTime:string,mRID:string,process_processType:string,receiver_MarketParticipant_mRID:struct<text:string,codingScheme:string>,receiver_MarketParticipant_marketRole_type:string,revisionNumber:string,sender_MarketParticipant_mRID:struct<text:string,codingScheme:string>,sender_MarketParticipant_marketRole_type:string,time_Period_timeInterval:struct<end:string,start:string>,type:string>).TimeSeries.inBiddingZone_Domain_mRID.text)
[2023-01-31T04:50:37.783+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:37 INFO FileSourceStrategy: Output Data Schema: struct<GL_MarketDocument: struct<@xmlns: string, TimeSeries: array<struct<MktPSRType:struct<psrType:string>,Period:struct<Point:array<struct<position:string,quantity:string>>,resolution:string,timeInterval:struct<end:string,start:string>>,businessType:string,curveType:string,inBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,mRID:string,objectAggregation:string,outBiddingZone_Domain.mRID:struct<#text:string,@codingScheme:string>,quantity_Measure_Unit.name:string>>, createdDateTime: string, mRID: string, process.processType: string ... 10 more fields>>
[2023-01-31T04:50:38.294+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:38 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 81d5fcd0285b:32823 in memory (size: 13.4 KiB, free: 434.2 MiB)
[2023-01-31T04:50:38.362+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:38 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 81d5fcd0285b:32823 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T04:50:38.393+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:38 INFO CodeGenerator: Code generated in 374.479669 ms
[2023-01-31T04:50:38.423+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:38 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.3 KiB, free 432.8 MiB)
[2023-01-31T04:50:38.479+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:38 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 432.8 MiB)
[2023-01-31T04:50:38.480+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:38 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 81d5fcd0285b:32823 (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T04:50:38.481+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:38 INFO SparkContext: Created broadcast 14 from collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T04:50:38.488+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203673 bytes, open cost is considered as scanning 4194304 bytes.
[2023-01-31T04:50:38.583+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:38 INFO SparkContext: Starting job: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203
[2023-01-31T04:50:38.584+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:38 INFO DAGScheduler: Got job 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) with 1 output partitions
[2023-01-31T04:50:38.584+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:38 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203)
[2023-01-31T04:50:38.584+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:38 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:50:38.585+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:38 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:50:38.590+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:38 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203), which has no missing parents
[2023-01-31T04:50:38.598+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:38 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 36.5 KiB, free 432.8 MiB)
[2023-01-31T04:50:38.603+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:38 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 432.7 MiB)
[2023-01-31T04:50:38.607+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:38 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 81d5fcd0285b:32823 (size: 12.6 KiB, free: 434.2 MiB)
[2023-01-31T04:50:38.610+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:38 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:50:38.612+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:50:38.612+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:38 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2023-01-31T04:50:38.618+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:38 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7904 bytes) taskResourceAssignments Map()
[2023-01-31T04:50:38.618+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:38 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2023-01-31T04:50:38.671+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:38 INFO FileScanRDD: Reading File path: gs://entsoe_analytics_1009/total_generation__DE_TENNET__202101010000__202101010100.json, range: 0-9369, partition values: [empty row]
[2023-01-31T04:50:38.835+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:38 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1729 bytes result sent to driver
[2023-01-31T04:50:38.838+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:38 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 220 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:50:38.843+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:38 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2023-01-31T04:50:38.844+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:38 INFO DAGScheduler: ResultStage 7 (collect at /opt/***/plugins/scripts/transform_raw_staging.py:203) finished in 0.248 s
[2023-01-31T04:50:38.844+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:38 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:50:38.844+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2023-01-31T04:50:38.844+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:38 INFO DAGScheduler: Job 7 finished: collect at /opt/***/plugins/scripts/transform_raw_staging.py:203, took 0.258099 s
[2023-01-31T04:50:43.025+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:43 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 81d5fcd0285b:32823 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T04:50:43.164+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:43 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 81d5fcd0285b:32823 in memory (size: 34.5 KiB, free: 434.2 MiB)
[2023-01-31T04:50:43.246+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:43 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 81d5fcd0285b:32823 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T04:50:43.384+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:43 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 81d5fcd0285b:32823 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T04:50:43.554+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:43 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 81d5fcd0285b:32823 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T04:50:43.667+0000] {spark_submit.py:495} INFO - 23/01/31 04:50:43 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 81d5fcd0285b:32823 in memory (size: 12.6 KiB, free: 434.3 MiB)
[2023-01-31T04:51:03.997+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:03 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T04:51:04.039+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T04:51:04.039+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T04:51:04.044+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:04 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T04:51:04.045+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T04:51:04.046+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T04:51:04.049+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T04:51:05.309+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:05 INFO CodeGenerator: Code generated in 29.522718 ms
[2023-01-31T04:51:05.410+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:05 INFO DAGScheduler: Registering RDD 118 (save at BigQueryWriteHelper.java:105) as input to shuffle 0
[2023-01-31T04:51:05.433+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:05 INFO DAGScheduler: Got map stage job 8 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:51:05.438+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:05 INFO DAGScheduler: Final stage: ShuffleMapStage 8 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:51:05.439+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:05 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:51:05.442+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:05 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:51:05.452+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:05 INFO DAGScheduler: Submitting ShuffleMapStage 8 (MapPartitionsRDD[118] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:51:05.538+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:05 INFO CodeGenerator: Code generated in 111.299793 ms
[2023-01-31T04:51:05.704+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:05 INFO CodeGenerator: Code generated in 101.419203 ms
[2023-01-31T04:51:05.824+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:05 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 11.1 KiB, free 433.9 MiB)
[2023-01-31T04:51:05.898+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:05 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 5.7 KiB, free 433.9 MiB)
[2023-01-31T04:51:05.905+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:05 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 81d5fcd0285b:32823 (size: 5.7 KiB, free: 434.3 MiB)
[2023-01-31T04:51:05.943+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:05 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:51:06.007+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:05 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[118] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:51:06.014+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:06 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2023-01-31T04:51:06.071+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:06 INFO DAGScheduler: Registering RDD 120 (save at BigQueryWriteHelper.java:105) as input to shuffle 1
[2023-01-31T04:51:06.074+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:06 INFO DAGScheduler: Got map stage job 9 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:51:06.076+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:06 INFO DAGScheduler: Final stage: ShuffleMapStage 9 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:51:06.078+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:06 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:51:06.086+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:06 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:51:06.127+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:06 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[120] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:51:06.155+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:06 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7660 bytes) taskResourceAssignments Map()
[2023-01-31T04:51:06.165+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:06 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2023-01-31T04:51:06.307+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:06 INFO CodeGenerator: Code generated in 256.332383 ms
[2023-01-31T04:51:06.639+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:06 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 15.6 KiB, free 433.9 MiB)
[2023-01-31T04:51:06.681+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:06 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.9 MiB)
[2023-01-31T04:51:06.723+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:06 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 81d5fcd0285b:32823 (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:51:06.737+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:06 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:51:06.739+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:06 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[120] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:51:06.741+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:06 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
[2023-01-31T04:51:06.830+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:06 INFO CodeGenerator: Code generated in 304.119397 ms
[2023-01-31T04:51:06.834+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:06 INFO DAGScheduler: Registering RDD 122 (save at BigQueryWriteHelper.java:105) as input to shuffle 2
[2023-01-31T04:51:06.841+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:06 INFO DAGScheduler: Got map stage job 10 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:51:06.842+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:06 INFO DAGScheduler: Final stage: ShuffleMapStage 10 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:51:06.843+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:06 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:51:06.881+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:06 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:51:06.918+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:06 INFO DAGScheduler: Submitting ShuffleMapStage 10 (MapPartitionsRDD[122] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:51:06.919+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:06 INFO CodeGenerator: Code generated in 176.64261 ms
[2023-01-31T04:51:07.015+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:07 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 15.7 KiB, free 433.9 MiB)
[2023-01-31T04:51:07.052+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:07 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.9 MiB)
[2023-01-31T04:51:07.055+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:07 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 81d5fcd0285b:32823 (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:51:07.065+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:07 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:51:07.080+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:07 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[122] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:51:07.081+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:07 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0
[2023-01-31T04:51:07.104+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:07 INFO DAGScheduler: Registering RDD 124 (save at BigQueryWriteHelper.java:105) as input to shuffle 3
[2023-01-31T04:51:07.108+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:07 INFO DAGScheduler: Got map stage job 11 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:51:07.114+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:07 INFO DAGScheduler: Final stage: ShuffleMapStage 11 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:51:07.116+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:07 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:51:07.118+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:07 INFO CodeGenerator: Code generated in 169.893765 ms
[2023-01-31T04:51:07.129+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:07 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:51:07.146+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:07 INFO DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[124] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:51:07.440+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:07 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 15.7 KiB, free 433.9 MiB)
[2023-01-31T04:51:07.541+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:07 INFO CodeGenerator: Code generated in 99.878496 ms
[2023-01-31T04:51:07.599+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:07 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.9 MiB)
[2023-01-31T04:51:07.656+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:07 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 81d5fcd0285b:32823 (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:51:07.703+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:07 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:51:07.719+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:07 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[124] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:51:07.721+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:07 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
[2023-01-31T04:51:07.746+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:07 INFO DAGScheduler: Registering RDD 126 (save at BigQueryWriteHelper.java:105) as input to shuffle 4
[2023-01-31T04:51:07.748+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:07 INFO DAGScheduler: Got map stage job 12 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:51:07.753+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:07 INFO DAGScheduler: Final stage: ShuffleMapStage 12 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:51:07.754+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:07 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:51:07.756+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:07 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:51:07.788+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:07 INFO DAGScheduler: Submitting ShuffleMapStage 12 (MapPartitionsRDD[126] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:51:07.875+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:07 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 15.7 KiB, free 433.8 MiB)
[2023-01-31T04:51:07.954+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:07 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.8 MiB)
[2023-01-31T04:51:07.957+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:07 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 81d5fcd0285b:32823 (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:51:07.970+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:07 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:51:07.995+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:07 INFO CodeGenerator: Code generated in 142.212804 ms
[2023-01-31T04:51:07.998+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:07 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[126] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:51:08.007+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:07 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
[2023-01-31T04:51:08.051+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:08 INFO DAGScheduler: Registering RDD 128 (save at BigQueryWriteHelper.java:105) as input to shuffle 5
[2023-01-31T04:51:08.077+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:08 INFO DAGScheduler: Got map stage job 13 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:51:08.097+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:08 INFO DAGScheduler: Final stage: ShuffleMapStage 13 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:51:08.097+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:08 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:51:08.098+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:08 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:51:08.120+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:08 INFO DAGScheduler: Submitting ShuffleMapStage 13 (MapPartitionsRDD[128] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:51:08.275+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:08 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2087 bytes result sent to driver
[2023-01-31T04:51:08.280+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:08 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:51:08.281+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:08 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)
[2023-01-31T04:51:08.282+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:08 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 2250 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:51:08.282+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:08 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2023-01-31T04:51:08.306+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:08 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 15.7 KiB, free 433.8 MiB)
[2023-01-31T04:51:08.391+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:08 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.8 MiB)
[2023-01-31T04:51:08.392+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:08 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 81d5fcd0285b:32823 (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:51:08.394+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:08 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:51:08.396+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:08 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 13 (MapPartitionsRDD[128] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:51:08.397+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:08 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0
[2023-01-31T04:51:08.399+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:08 INFO DAGScheduler: Registering RDD 130 (save at BigQueryWriteHelper.java:105) as input to shuffle 6
[2023-01-31T04:51:08.400+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:08 INFO DAGScheduler: Got map stage job 14 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:51:08.402+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:08 INFO DAGScheduler: Final stage: ShuffleMapStage 14 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:51:08.402+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:08 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:51:08.404+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:08 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:51:08.430+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:08 INFO DAGScheduler: Submitting ShuffleMapStage 14 (MapPartitionsRDD[130] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:51:08.504+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:08 INFO CodeGenerator: Code generated in 224.847533 ms
[2023-01-31T04:51:08.636+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:08 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 15.7 KiB, free 433.8 MiB)
[2023-01-31T04:51:08.721+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:08 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.8 MiB)
[2023-01-31T04:51:08.724+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:08 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 81d5fcd0285b:32823 (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:51:08.730+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:08 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:51:08.740+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:08 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[130] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:51:08.746+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:08 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0
[2023-01-31T04:51:08.761+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:08 INFO DAGScheduler: Registering RDD 132 (save at BigQueryWriteHelper.java:105) as input to shuffle 7
[2023-01-31T04:51:08.771+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:08 INFO DAGScheduler: Got map stage job 15 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:51:08.773+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:08 INFO DAGScheduler: Final stage: ShuffleMapStage 15 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:51:08.776+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:08 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:51:08.776+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:08 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:51:08.789+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:08 INFO DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[132] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:51:08.866+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:08 INFO CodeGenerator: Code generated in 201.645576 ms
[2023-01-31T04:51:08.933+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:08 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 15.7 KiB, free 433.8 MiB)
[2023-01-31T04:51:08.970+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:08 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.8 MiB)
[2023-01-31T04:51:08.979+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:08 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 81d5fcd0285b:32823 (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:51:08.983+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:08 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:51:08.989+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:08 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[132] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:51:08.993+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:08 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
[2023-01-31T04:51:09.011+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO DAGScheduler: ShuffleMapStage 8 (save at BigQueryWriteHelper.java:105) finished in 3.543 s
[2023-01-31T04:51:09.015+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:51:09.022+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO DAGScheduler: running: HashSet(ShuffleMapStage 9, ShuffleMapStage 10, ShuffleMapStage 11, ShuffleMapStage 12, ShuffleMapStage 13, ShuffleMapStage 14, ShuffleMapStage 15)
[2023-01-31T04:51:09.032+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:51:09.038+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:51:09.056+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO CodeGenerator: Code generated in 65.758628 ms
[2023-01-31T04:51:09.092+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO DAGScheduler: Registering RDD 134 (save at BigQueryWriteHelper.java:105) as input to shuffle 8
[2023-01-31T04:51:09.093+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO DAGScheduler: Got map stage job 16 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:51:09.094+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO DAGScheduler: Final stage: ShuffleMapStage 16 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:51:09.094+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:51:09.095+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:51:09.099+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO DAGScheduler: Submitting ShuffleMapStage 16 (MapPartitionsRDD[134] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:51:09.191+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 15.7 KiB, free 433.7 MiB)
[2023-01-31T04:51:09.371+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.7 MiB)
[2023-01-31T04:51:09.402+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 81d5fcd0285b:32823 (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:51:09.403+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:51:09.403+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 16 (MapPartitionsRDD[134] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:51:09.403+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0
[2023-01-31T04:51:09.407+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO DAGScheduler: Registering RDD 136 (save at BigQueryWriteHelper.java:105) as input to shuffle 9
[2023-01-31T04:51:09.407+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO DAGScheduler: Got map stage job 17 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:51:09.407+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO DAGScheduler: Final stage: ShuffleMapStage 17 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:51:09.408+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:51:09.409+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:51:09.431+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO CodeGenerator: Code generated in 266.01209 ms
[2023-01-31T04:51:09.436+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO DAGScheduler: Submitting ShuffleMapStage 17 (MapPartitionsRDD[136] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:51:09.466+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 81d5fcd0285b:32823 in memory (size: 5.7 KiB, free: 434.3 MiB)
[2023-01-31T04:51:09.516+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 15.7 KiB, free 433.7 MiB)
[2023-01-31T04:51:09.547+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.7 MiB)
[2023-01-31T04:51:09.553+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 81d5fcd0285b:32823 (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:51:09.557+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:51:09.616+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO CodeGenerator: Code generated in 74.519342 ms
[2023-01-31T04:51:09.674+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 17 (MapPartitionsRDD[136] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:51:09.674+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0
[2023-01-31T04:51:09.682+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO DAGScheduler: Registering RDD 138 (save at BigQueryWriteHelper.java:105) as input to shuffle 10
[2023-01-31T04:51:09.686+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO DAGScheduler: Got map stage job 18 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:51:09.699+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO DAGScheduler: Final stage: ShuffleMapStage 18 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:51:09.700+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:51:09.703+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:51:09.745+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[138] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:51:09.769+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 15.7 KiB, free 433.7 MiB)
[2023-01-31T04:51:09.776+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.7 MiB)
[2023-01-31T04:51:09.780+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 81d5fcd0285b:32823 (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:51:09.782+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:51:09.791+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[138] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:51:09.797+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0
[2023-01-31T04:51:09.804+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO DAGScheduler: Registering RDD 140 (save at BigQueryWriteHelper.java:105) as input to shuffle 11
[2023-01-31T04:51:09.805+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO DAGScheduler: Got map stage job 19 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:51:09.807+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO DAGScheduler: Final stage: ShuffleMapStage 19 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:51:09.809+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:51:09.812+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:51:09.852+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO DAGScheduler: Submitting ShuffleMapStage 19 (MapPartitionsRDD[140] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:51:09.879+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 15.7 KiB, free 433.7 MiB)
[2023-01-31T04:51:09.900+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO CodeGenerator: Code generated in 109.216507 ms
[2023-01-31T04:51:09.933+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.7 MiB)
[2023-01-31T04:51:09.935+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 81d5fcd0285b:32823 (size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T04:51:09.939+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:51:09.939+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 19 (MapPartitionsRDD[140] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:51:09.941+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0
[2023-01-31T04:51:09.963+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO DAGScheduler: Registering RDD 142 (save at BigQueryWriteHelper.java:105) as input to shuffle 12
[2023-01-31T04:51:09.964+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO DAGScheduler: Got map stage job 20 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:51:09.965+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO DAGScheduler: Final stage: ShuffleMapStage 20 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:51:09.966+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:51:09.967+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:09 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:51:10.068+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:10 INFO DAGScheduler: Submitting ShuffleMapStage 20 (MapPartitionsRDD[142] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:51:10.135+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:10 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 15.7 KiB, free 433.7 MiB)
[2023-01-31T04:51:10.138+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:10 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.7 MiB)
[2023-01-31T04:51:10.162+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:10 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 81d5fcd0285b:32823 (size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T04:51:10.164+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:10 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:51:10.177+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:10 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[142] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:51:10.178+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:10 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks resource profile 0
[2023-01-31T04:51:10.197+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:10 INFO DAGScheduler: Registering RDD 144 (save at BigQueryWriteHelper.java:105) as input to shuffle 13
[2023-01-31T04:51:10.205+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:10 INFO DAGScheduler: Got map stage job 21 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:51:10.206+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:10 INFO DAGScheduler: Final stage: ShuffleMapStage 21 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:51:10.209+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:10 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:51:10.210+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:10 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:51:10.229+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:10 INFO DAGScheduler: Submitting ShuffleMapStage 21 (MapPartitionsRDD[144] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:51:10.239+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:10 INFO CodeGenerator: Code generated in 148.679655 ms
[2023-01-31T04:51:10.267+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:10 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 15.7 KiB, free 433.6 MiB)
[2023-01-31T04:51:10.272+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:10 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.6 MiB)
[2023-01-31T04:51:10.298+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:10 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 81d5fcd0285b:32823 (size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T04:51:10.301+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:10 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:51:10.303+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:10 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 21 (MapPartitionsRDD[144] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:51:10.304+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:10 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0
[2023-01-31T04:51:10.343+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:10 INFO DAGScheduler: Registering RDD 146 (save at BigQueryWriteHelper.java:105) as input to shuffle 14
[2023-01-31T04:51:10.344+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:10 INFO DAGScheduler: Got map stage job 22 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:51:10.344+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:10 INFO DAGScheduler: Final stage: ShuffleMapStage 22 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:51:10.344+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:10 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:51:10.345+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:10 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:51:10.363+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:10 INFO DAGScheduler: Submitting ShuffleMapStage 22 (MapPartitionsRDD[146] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:51:10.384+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:10 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 15.7 KiB, free 433.6 MiB)
[2023-01-31T04:51:10.547+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:10 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.6 MiB)
[2023-01-31T04:51:10.551+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:10 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 81d5fcd0285b:32823 (size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T04:51:10.561+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:10 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:51:10.567+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:10 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 22 (MapPartitionsRDD[146] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:51:10.567+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:10 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0
[2023-01-31T04:51:10.569+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:10 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 81d5fcd0285b:32823 in memory (size: 34.5 KiB, free: 434.3 MiB)
[2023-01-31T04:51:10.603+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:10 INFO CodeGenerator: Code generated in 59.804036 ms
[2023-01-31T04:51:10.674+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:10 INFO DAGScheduler: Registering RDD 148 (save at BigQueryWriteHelper.java:105) as input to shuffle 15
[2023-01-31T04:51:10.675+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:10 INFO DAGScheduler: Got map stage job 23 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:51:10.675+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:10 INFO DAGScheduler: Final stage: ShuffleMapStage 23 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:51:10.675+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:10 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:51:10.676+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:10 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:51:10.709+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:10 INFO DAGScheduler: Submitting ShuffleMapStage 23 (MapPartitionsRDD[148] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:51:10.735+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:10 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 15.7 KiB, free 433.8 MiB)
[2023-01-31T04:51:10.747+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:10 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.8 MiB)
[2023-01-31T04:51:10.751+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:10 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 81d5fcd0285b:32823 (size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T04:51:10.754+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:10 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:51:10.761+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:10 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 23 (MapPartitionsRDD[148] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:51:10.763+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:10 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks resource profile 0
[2023-01-31T04:51:10.859+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:10 INFO CodeGenerator: Code generated in 128.740116 ms
[2023-01-31T04:51:10.995+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:10 INFO DAGScheduler: Registering RDD 150 (save at BigQueryWriteHelper.java:105) as input to shuffle 16
[2023-01-31T04:51:10.998+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:10 INFO DAGScheduler: Got map stage job 24 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:51:11.000+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:10 INFO DAGScheduler: Final stage: ShuffleMapStage 24 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:51:11.001+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:10 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:51:11.002+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:10 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:51:11.012+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:11 INFO DAGScheduler: Submitting ShuffleMapStage 24 (MapPartitionsRDD[150] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:51:11.039+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:11 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 15.7 KiB, free 433.8 MiB)
[2023-01-31T04:51:11.050+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:11 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 433.8 MiB)
[2023-01-31T04:51:11.054+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:11 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 81d5fcd0285b:32823 (size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T04:51:11.061+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:11 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:51:11.064+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:11 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 24 (MapPartitionsRDD[150] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:51:11.067+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:11 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks resource profile 0
[2023-01-31T04:51:12.533+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:12 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2023-01-31T04:51:13.313+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:13 INFO CodeGenerator: Code generated in 98.49403 ms
[2023-01-31T04:51:14.110+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:14 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2023-01-31T04:51:14.113+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:14 INFO DAGScheduler: Got job 25 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
[2023-01-31T04:51:14.115+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:14 INFO DAGScheduler: Final stage: ResultStage 26 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2023-01-31T04:51:14.116+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:14 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 25)
[2023-01-31T04:51:14.116+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:14 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:51:14.125+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:14 INFO DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[154] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2023-01-31T04:51:14.253+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:14 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 21.6 KiB, free 433.8 MiB)
[2023-01-31T04:51:14.272+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:14 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 433.8 MiB)
[2023-01-31T04:51:14.275+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:14 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 81d5fcd0285b:32823 (size: 10.2 KiB, free: 434.2 MiB)
[2023-01-31T04:51:14.282+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:14 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:51:14.286+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[154] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:51:14.289+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:14 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks resource profile 0
[2023-01-31T04:51:18.998+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:18 INFO CodeGenerator: Code generated in 496.510754 ms
[2023-01-31T04:51:19.475+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:19 INFO PythonRunner: Times: total = 10402, boot = 9160, init = 1242, finish = 0
[2023-01-31T04:51:19.551+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:19 INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 2521 bytes result sent to driver
[2023-01-31T04:51:19.552+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:19 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:51:19.566+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:19 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 11288 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:51:19.567+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:19 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
[2023-01-31T04:51:19.572+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:19 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 50309
[2023-01-31T04:51:19.578+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:19 INFO Executor: Running task 0.0 in stage 10.0 (TID 10)
[2023-01-31T04:51:19.586+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:19 INFO DAGScheduler: ShuffleMapStage 9 (save at BigQueryWriteHelper.java:105) finished in 13.447 s
[2023-01-31T04:51:19.604+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:19 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:51:19.605+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:19 INFO DAGScheduler: running: HashSet(ShuffleMapStage 10, ShuffleMapStage 11, ShuffleMapStage 12, ShuffleMapStage 13, ShuffleMapStage 14, ShuffleMapStage 15, ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 18, ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T04:51:19.605+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:19 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:51:19.605+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:19 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:51:21.272+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:21 INFO PythonRunner: Times: total = 1296, boot = -344, init = 1632, finish = 8
[2023-01-31T04:51:21.391+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:21 INFO Executor: Finished task 0.0 in stage 10.0 (TID 10). 2478 bytes result sent to driver
[2023-01-31T04:51:21.412+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:21 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 11) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:51:21.416+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:21 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 1866 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:51:21.417+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:21 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool
[2023-01-31T04:51:21.418+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:21 INFO DAGScheduler: ShuffleMapStage 10 (save at BigQueryWriteHelper.java:105) finished in 14.491 s
[2023-01-31T04:51:21.418+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:21 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:51:21.419+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:21 INFO DAGScheduler: running: HashSet(ShuffleMapStage 11, ShuffleMapStage 12, ShuffleMapStage 13, ShuffleMapStage 14, ShuffleMapStage 15, ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 18, ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T04:51:21.419+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:21 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:51:21.420+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:21 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:51:21.490+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:21 INFO Executor: Running task 0.0 in stage 11.0 (TID 11)
[2023-01-31T04:51:22.108+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:22 INFO PythonRunner: Times: total = 484, boot = -240, init = 724, finish = 0
[2023-01-31T04:51:22.151+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:22 INFO Executor: Finished task 0.0 in stage 11.0 (TID 11). 2478 bytes result sent to driver
[2023-01-31T04:51:22.157+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:22 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 12) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:51:22.160+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:22 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 11) in 748 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:51:22.161+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:22 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool
[2023-01-31T04:51:22.162+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:22 INFO DAGScheduler: ShuffleMapStage 11 (save at BigQueryWriteHelper.java:105) finished in 15.011 s
[2023-01-31T04:51:22.163+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:22 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:51:22.164+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:22 INFO DAGScheduler: running: HashSet(ShuffleMapStage 12, ShuffleMapStage 13, ShuffleMapStage 14, ShuffleMapStage 15, ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 18, ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T04:51:22.164+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:22 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:51:22.169+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:22 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:51:22.173+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:22 INFO Executor: Running task 0.0 in stage 12.0 (TID 12)
[2023-01-31T04:51:23.512+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:23 INFO PythonRunner: Times: total = 1186, boot = -67, init = 1253, finish = 0
[2023-01-31T04:51:23.598+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:23 INFO Executor: Finished task 0.0 in stage 12.0 (TID 12). 2521 bytes result sent to driver
[2023-01-31T04:51:23.606+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:23 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 13) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:51:23.611+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:23 INFO Executor: Running task 0.0 in stage 13.0 (TID 13)
[2023-01-31T04:51:23.617+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:23 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 12) in 1456 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:51:23.618+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:23 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool
[2023-01-31T04:51:23.618+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:23 INFO DAGScheduler: ShuffleMapStage 12 (save at BigQueryWriteHelper.java:105) finished in 15.826 s
[2023-01-31T04:51:23.619+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:23 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:51:23.623+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:23 INFO DAGScheduler: running: HashSet(ShuffleMapStage 13, ShuffleMapStage 14, ShuffleMapStage 15, ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 18, ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T04:51:23.630+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:23 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:51:23.639+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:23 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:51:25.646+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:25 INFO PythonRunner: Times: total = 1881, boot = -223, init = 2104, finish = 0
[2023-01-31T04:51:25.770+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:25 INFO Executor: Finished task 0.0 in stage 13.0 (TID 13). 2478 bytes result sent to driver
[2023-01-31T04:51:25.779+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:25 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 14) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:51:25.786+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:25 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 13) in 2178 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:51:25.787+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:25 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool
[2023-01-31T04:51:25.787+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:25 INFO DAGScheduler: ShuffleMapStage 13 (save at BigQueryWriteHelper.java:105) finished in 17.661 s
[2023-01-31T04:51:25.787+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:25 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:51:25.788+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:25 INFO DAGScheduler: running: HashSet(ShuffleMapStage 14, ShuffleMapStage 15, ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 18, ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T04:51:25.788+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:25 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:51:25.788+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:25 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:51:25.814+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:25 INFO Executor: Running task 0.0 in stage 14.0 (TID 14)
[2023-01-31T04:51:27.199+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO PythonRunner: Times: total = 1278, boot = -198, init = 1476, finish = 0
[2023-01-31T04:51:27.251+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO Executor: Finished task 0.0 in stage 14.0 (TID 14). 2478 bytes result sent to driver
[2023-01-31T04:51:27.260+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 15) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:51:27.261+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO Executor: Running task 0.0 in stage 15.0 (TID 15)
[2023-01-31T04:51:27.262+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 14) in 1488 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:51:27.262+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool
[2023-01-31T04:51:27.264+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO DAGScheduler: ShuffleMapStage 14 (save at BigQueryWriteHelper.java:105) finished in 18.837 s
[2023-01-31T04:51:27.268+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:51:27.269+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO DAGScheduler: running: HashSet(ShuffleMapStage 15, ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 18, ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T04:51:27.279+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:51:27.282+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:27 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:51:28.011+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO PythonRunner: Times: total = 610, boot = -75, init = 685, finish = 0
[2023-01-31T04:51:28.087+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO Executor: Finished task 0.0 in stage 15.0 (TID 15). 2478 bytes result sent to driver
[2023-01-31T04:51:28.093+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 16) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:51:28.095+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 15) in 838 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:51:28.095+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool
[2023-01-31T04:51:28.097+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO Executor: Running task 0.0 in stage 16.0 (TID 16)
[2023-01-31T04:51:28.097+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO DAGScheduler: ShuffleMapStage 15 (save at BigQueryWriteHelper.java:105) finished in 19.305 s
[2023-01-31T04:51:28.117+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:51:28.117+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO DAGScheduler: running: HashSet(ShuffleMapStage 16, ShuffleMapStage 17, ShuffleMapStage 18, ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T04:51:28.118+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:51:28.118+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:28 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:51:29.180+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:29 INFO PythonRunner: Times: total = 967, boot = -153, init = 1120, finish = 0
[2023-01-31T04:51:29.245+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:29 INFO Executor: Finished task 0.0 in stage 16.0 (TID 16). 2521 bytes result sent to driver
[2023-01-31T04:51:29.253+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:29 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 17) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:51:29.255+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:29 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 16) in 1162 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:51:29.255+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:29 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool
[2023-01-31T04:51:29.256+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:29 INFO Executor: Running task 0.0 in stage 17.0 (TID 17)
[2023-01-31T04:51:29.256+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:29 INFO DAGScheduler: ShuffleMapStage 16 (save at BigQueryWriteHelper.java:105) finished in 20.155 s
[2023-01-31T04:51:29.257+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:29 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:51:29.258+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:29 INFO DAGScheduler: running: HashSet(ShuffleMapStage 17, ShuffleMapStage 18, ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T04:51:29.263+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:29 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:51:29.267+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:29 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:51:30.481+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:30 INFO PythonRunner: Times: total = 1105, boot = -71, init = 1174, finish = 2
[2023-01-31T04:51:30.557+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:30 INFO Executor: Finished task 0.0 in stage 17.0 (TID 17). 2478 bytes result sent to driver
[2023-01-31T04:51:30.569+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:30 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 18) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:51:30.578+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:30 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 17) in 1327 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:51:30.579+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:30 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool
[2023-01-31T04:51:30.615+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:30 INFO DAGScheduler: ShuffleMapStage 17 (save at BigQueryWriteHelper.java:105) finished in 21.149 s
[2023-01-31T04:51:30.623+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:30 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:51:30.638+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:30 INFO DAGScheduler: running: HashSet(ShuffleMapStage 18, ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T04:51:30.638+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:30 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:51:30.654+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:30 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:51:30.684+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:30 INFO Executor: Running task 0.0 in stage 18.0 (TID 18)
[2023-01-31T04:51:31.647+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:31 INFO PythonRunner: Times: total = 922, boot = -142, init = 1064, finish = 0
[2023-01-31T04:51:31.684+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:31 INFO Executor: Finished task 0.0 in stage 18.0 (TID 18). 2478 bytes result sent to driver
[2023-01-31T04:51:31.703+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:31 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 19) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:51:31.709+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:31 INFO Executor: Running task 0.0 in stage 19.0 (TID 19)
[2023-01-31T04:51:31.711+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:31 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 18) in 1148 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:51:31.712+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:31 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool
[2023-01-31T04:51:31.719+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:31 INFO DAGScheduler: ShuffleMapStage 18 (save at BigQueryWriteHelper.java:105) finished in 21.967 s
[2023-01-31T04:51:31.726+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:31 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:51:31.726+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:31 INFO DAGScheduler: running: HashSet(ShuffleMapStage 19, ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T04:51:31.738+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:31 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:51:31.741+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:31 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:51:33.197+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:33 INFO PythonRunner: Times: total = 1376, boot = -25, init = 1401, finish = 0
[2023-01-31T04:51:33.233+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:33 INFO Executor: Finished task 0.0 in stage 19.0 (TID 19). 2478 bytes result sent to driver
[2023-01-31T04:51:33.246+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:33 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 20) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:51:33.247+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:33 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 19) in 1545 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:51:33.248+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:33 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool
[2023-01-31T04:51:33.249+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:33 INFO Executor: Running task 0.0 in stage 20.0 (TID 20)
[2023-01-31T04:51:33.260+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:33 INFO DAGScheduler: ShuffleMapStage 19 (save at BigQueryWriteHelper.java:105) finished in 23.410 s
[2023-01-31T04:51:33.261+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:33 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:51:33.262+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:33 INFO DAGScheduler: running: HashSet(ShuffleMapStage 20, ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T04:51:33.262+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:33 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:51:33.269+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:33 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:51:33.889+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:33 INFO PythonRunner: Times: total = 547, boot = 26, init = 521, finish = 0
[2023-01-31T04:51:33.909+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:33 INFO Executor: Finished task 0.0 in stage 20.0 (TID 20). 2478 bytes result sent to driver
[2023-01-31T04:51:33.917+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:33 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 21) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:51:33.918+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:33 INFO Executor: Running task 0.0 in stage 21.0 (TID 21)
[2023-01-31T04:51:33.918+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:33 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 20) in 674 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:51:33.919+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:33 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool
[2023-01-31T04:51:33.928+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:33 INFO DAGScheduler: ShuffleMapStage 20 (save at BigQueryWriteHelper.java:105) finished in 23.853 s
[2023-01-31T04:51:33.929+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:33 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:51:33.929+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:33 INFO DAGScheduler: running: HashSet(ShuffleMapStage 21, ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T04:51:33.930+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:33 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:51:33.930+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:33 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:51:35.065+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:35 INFO PythonRunner: Times: total = 1096, boot = -31, init = 1126, finish = 1
[2023-01-31T04:51:35.107+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:35 INFO Executor: Finished task 0.0 in stage 21.0 (TID 21). 2478 bytes result sent to driver
[2023-01-31T04:51:35.109+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:35 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 22) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:51:35.110+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:35 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 21) in 1199 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:51:35.111+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:35 INFO Executor: Running task 0.0 in stage 22.0 (TID 22)
[2023-01-31T04:51:35.111+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:35 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool
[2023-01-31T04:51:35.113+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:35 INFO DAGScheduler: ShuffleMapStage 21 (save at BigQueryWriteHelper.java:105) finished in 24.889 s
[2023-01-31T04:51:35.113+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:35 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:51:35.116+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:35 INFO DAGScheduler: running: HashSet(ShuffleMapStage 22, ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T04:51:35.120+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:35 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:51:35.121+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:35 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:51:36.111+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:36 INFO PythonRunner: Times: total = 826, boot = -53, init = 879, finish = 0
[2023-01-31T04:51:36.133+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:36 INFO Executor: Finished task 0.0 in stage 22.0 (TID 22). 2478 bytes result sent to driver
[2023-01-31T04:51:36.151+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:36 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 23) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:51:36.154+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:36 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 22) in 1045 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:51:36.154+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:36 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool
[2023-01-31T04:51:36.159+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:36 INFO Executor: Running task 0.0 in stage 23.0 (TID 23)
[2023-01-31T04:51:36.180+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:36 INFO DAGScheduler: ShuffleMapStage 22 (save at BigQueryWriteHelper.java:105) finished in 25.818 s
[2023-01-31T04:51:36.287+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:36 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:51:36.287+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:36 INFO DAGScheduler: running: HashSet(ShuffleMapStage 23, ShuffleMapStage 24, ResultStage 26)
[2023-01-31T04:51:36.288+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:36 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:51:36.288+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:36 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:51:38.180+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:38 INFO PythonRunner: Times: total = 1927, boot = -195, init = 2122, finish = 0
[2023-01-31T04:51:38.209+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:38 INFO Executor: Finished task 0.0 in stage 23.0 (TID 23). 2478 bytes result sent to driver
[2023-01-31T04:51:38.220+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:38 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 24) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:51:38.228+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:38 INFO Executor: Running task 0.0 in stage 24.0 (TID 24)
[2023-01-31T04:51:38.229+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:38 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 23) in 2082 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:51:38.230+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:38 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool
[2023-01-31T04:51:38.234+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:38 INFO DAGScheduler: ShuffleMapStage 23 (save at BigQueryWriteHelper.java:105) finished in 27.519 s
[2023-01-31T04:51:38.237+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:38 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:51:38.239+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:38 INFO DAGScheduler: running: HashSet(ShuffleMapStage 24, ResultStage 26)
[2023-01-31T04:51:38.241+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:38 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:51:38.249+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:38 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:51:38.879+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:38 INFO PythonRunner: Times: total = 543, boot = -31, init = 573, finish = 1
[2023-01-31T04:51:38.931+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:38 INFO Executor: Finished task 0.0 in stage 24.0 (TID 24). 2521 bytes result sent to driver
[2023-01-31T04:51:38.954+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:38 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 25) (81d5fcd0285b, executor driver, partition 0, NODE_LOCAL, 7399 bytes) taskResourceAssignments Map()
[2023-01-31T04:51:38.957+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:38 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 24) in 744 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:51:38.958+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:38 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool
[2023-01-31T04:51:38.961+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:38 INFO Executor: Running task 0.0 in stage 26.0 (TID 25)
[2023-01-31T04:51:38.965+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:38 INFO DAGScheduler: ShuffleMapStage 24 (save at BigQueryWriteHelper.java:105) finished in 27.956 s
[2023-01-31T04:51:38.966+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:38 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:51:38.967+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:38 INFO DAGScheduler: running: HashSet(ResultStage 26)
[2023-01-31T04:51:38.967+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:38 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:51:38.968+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:38 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:51:39.927+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:39 INFO ShuffleBlockFetcherIterator: Getting 1 (97.0 B) non-empty blocks including 1 (97.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:51:39.953+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 159 ms
[2023-01-31T04:51:40.355+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:40 INFO CodeGenerator: Code generated in 297.428983 ms
[2023-01-31T04:51:40.675+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:40 INFO CodeGenerator: Code generated in 217.857833 ms
[2023-01-31T04:51:40.913+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:40 INFO CodeGenerator: Code generated in 78.26611 ms
[2023-01-31T04:51:41.100+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:41 INFO CodeGenerator: Code generated in 160.026058 ms
[2023-01-31T04:51:41.136+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:41 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 81d5fcd0285b:32823 in memory (size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T04:51:41.263+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:41 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 81d5fcd0285b:32823 in memory (size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T04:51:41.315+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:41 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 81d5fcd0285b:32823 in memory (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:51:41.353+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:41 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 81d5fcd0285b:32823 in memory (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:51:42.337+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:42 INFO CodeGenerator: Code generated in 224.751982 ms
[2023-01-31T04:51:42.574+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:42 INFO CodeGenerator: Code generated in 132.880068 ms
[2023-01-31T04:51:42.755+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:42 INFO CodeGenerator: Code generated in 172.862699 ms
[2023-01-31T04:51:42.906+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:42 INFO Executor: Finished task 0.0 in stage 26.0 (TID 25). 3217 bytes result sent to driver
[2023-01-31T04:51:42.915+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:42 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 25) in 3977 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:51:42.915+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:42 INFO DAGScheduler: ResultStage 26 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 28.697 s
[2023-01-31T04:51:42.916+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:42 INFO DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:51:42.916+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:42 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool
[2023-01-31T04:51:42.959+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 26: Stage finished
[2023-01-31T04:51:42.960+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:42 INFO DAGScheduler: Job 25 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 28.806826 s
[2023-01-31T04:51:43.147+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:43 INFO CodeGenerator: Code generated in 87.909569 ms
[2023-01-31T04:51:43.330+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:43 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 1024.1 KiB, free 432.9 MiB)
[2023-01-31T04:51:43.395+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:43 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 278.0 B, free 432.9 MiB)
[2023-01-31T04:51:43.398+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:43 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 81d5fcd0285b:32823 (size: 278.0 B, free: 434.3 MiB)
[2023-01-31T04:51:43.423+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:43 INFO SparkContext: Created broadcast 34 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2023-01-31T04:51:44.074+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:44 INFO CodeGenerator: Code generated in 55.443145 ms
[2023-01-31T04:51:44.100+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:44 INFO DAGScheduler: Registering RDD 156 (save at BigQueryWriteHelper.java:105) as input to shuffle 17
[2023-01-31T04:51:44.100+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:44 INFO DAGScheduler: Got map stage job 26 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:51:44.101+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:44 INFO DAGScheduler: Final stage: ShuffleMapStage 27 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:51:44.101+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:44 INFO DAGScheduler: Parents of final stage: List()
[2023-01-31T04:51:44.102+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:44 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:51:44.118+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:44 INFO DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[156] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:51:44.227+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:44 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 18.1 KiB, free 432.8 MiB)
[2023-01-31T04:51:44.246+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:44 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 9.2 KiB, free 432.8 MiB)
[2023-01-31T04:51:44.249+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:44 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 81d5fcd0285b:32823 (size: 9.2 KiB, free: 434.3 MiB)
[2023-01-31T04:51:44.253+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:44 INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:51:44.263+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:44 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[156] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:51:44.271+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:44 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks resource profile 0
[2023-01-31T04:51:44.277+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:44 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 26) (81d5fcd0285b, executor driver, partition 0, PROCESS_LOCAL, 7510 bytes) taskResourceAssignments Map()
[2023-01-31T04:51:44.277+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:44 INFO Executor: Running task 0.0 in stage 27.0 (TID 26)
[2023-01-31T04:51:44.454+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:44 INFO CodeGenerator: Code generated in 82.91036 ms
[2023-01-31T04:51:45.486+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:45 INFO PythonRunner: Times: total = 1101, boot = -5381, init = 6482, finish = 0
[2023-01-31T04:51:45.566+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:45 INFO Executor: Finished task 0.0 in stage 27.0 (TID 26). 2577 bytes result sent to driver
[2023-01-31T04:51:45.572+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:45 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 26) in 1293 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:51:45.573+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:45 INFO DAGScheduler: ShuffleMapStage 27 (save at BigQueryWriteHelper.java:105) finished in 1.447 s
[2023-01-31T04:51:45.573+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:45 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:51:45.573+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:45 INFO DAGScheduler: running: HashSet()
[2023-01-31T04:51:45.574+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:45 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:51:45.574+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:45 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:51:45.589+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:45 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool
[2023-01-31T04:51:45.970+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:45 INFO ShufflePartitionsUtil: For shuffle(17, 1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2023-01-31T04:51:46.454+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:46 INFO CodeGenerator: Code generated in 169.833936 ms
[2023-01-31T04:51:46.533+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:46 INFO CodeGenerator: Code generated in 58.968187 ms
[2023-01-31T04:51:46.751+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:46 INFO CodeGenerator: Code generated in 101.709112 ms
[2023-01-31T04:51:46.954+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:46 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2023-01-31T04:51:46.962+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:46 INFO DAGScheduler: Got job 27 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
[2023-01-31T04:51:46.964+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:46 INFO DAGScheduler: Final stage: ResultStage 30 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2023-01-31T04:51:46.964+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:46 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 28, ShuffleMapStage 29)
[2023-01-31T04:51:46.967+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:46 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:51:46.980+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:46 INFO DAGScheduler: Submitting ResultStage 30 (MapPartitionsRDD[163] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2023-01-31T04:51:47.025+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:47 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 48.6 KiB, free 432.8 MiB)
[2023-01-31T04:51:47.104+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:47 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 22.6 KiB, free 432.8 MiB)
[2023-01-31T04:51:47.105+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:47 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on 81d5fcd0285b:32823 (size: 22.6 KiB, free: 434.2 MiB)
[2023-01-31T04:51:47.107+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:47 INFO SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:51:47.133+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 30 (MapPartitionsRDD[163] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:51:47.133+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:47 INFO TaskSchedulerImpl: Adding task set 30.0 with 1 tasks resource profile 0
[2023-01-31T04:51:47.134+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:47 INFO BlockManagerInfo: Removed broadcast_35_piece0 on 81d5fcd0285b:32823 in memory (size: 9.2 KiB, free: 434.2 MiB)
[2023-01-31T04:51:47.143+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:47 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 27) (81d5fcd0285b, executor driver, partition 0, NODE_LOCAL, 7897 bytes) taskResourceAssignments Map()
[2023-01-31T04:51:47.152+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:47 INFO Executor: Running task 0.0 in stage 30.0 (TID 27)
[2023-01-31T04:51:47.447+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:47 INFO ShuffleBlockFetcherIterator: Getting 1 (288.0 B) non-empty blocks including 1 (288.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:51:47.450+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
[2023-01-31T04:51:47.661+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:47 INFO CodeGenerator: Code generated in 190.775817 ms
[2023-01-31T04:51:47.663+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:47 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 81d5fcd0285b:32823 in memory (size: 8.1 KiB, free: 434.2 MiB)
[2023-01-31T04:51:47.746+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:47 INFO CodeGenerator: Code generated in 59.378026 ms
[2023-01-31T04:51:47.768+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:47 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:51:47.770+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
[2023-01-31T04:51:47.941+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:47 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 81d5fcd0285b:32823 in memory (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:51:48.011+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:48 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 81d5fcd0285b:32823 in memory (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:51:48.111+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:48 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 81d5fcd0285b:32823 in memory (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:51:48.189+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:48 INFO CodeGenerator: Code generated in 292.761048 ms
[2023-01-31T04:51:48.223+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:48 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 81d5fcd0285b:32823 in memory (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:51:48.264+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:48 INFO CodeGenerator: Code generated in 35.49138 ms
[2023-01-31T04:51:48.334+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:48 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 81d5fcd0285b:32823 in memory (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:51:48.405+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:48 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 81d5fcd0285b:32823 in memory (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:51:48.849+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:48 INFO Executor: Finished task 0.0 in stage 30.0 (TID 27). 6257 bytes result sent to driver
[2023-01-31T04:51:48.853+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:48 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 27) in 1721 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:51:48.857+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:48 INFO DAGScheduler: ResultStage 30 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 1.863 s
[2023-01-31T04:51:48.857+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:48 INFO DAGScheduler: Job 27 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:51:48.861+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:48 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool
[2023-01-31T04:51:48.862+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 30: Stage finished
[2023-01-31T04:51:48.863+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:48 INFO DAGScheduler: Job 27 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 1.906781 s
[2023-01-31T04:51:49.050+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:49 INFO BlockManagerInfo: Removed broadcast_33_piece0 on 81d5fcd0285b:32823 in memory (size: 10.2 KiB, free: 434.3 MiB)
[2023-01-31T04:51:49.088+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:49 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 81d5fcd0285b:32823 in memory (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:51:49.181+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:49 INFO BlockManagerInfo: Removed broadcast_32_piece0 on 81d5fcd0285b:32823 in memory (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:51:49.197+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:49 INFO CodeGenerator: Code generated in 144.515077 ms
[2023-01-31T04:51:49.203+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:49 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 1024.1 KiB, free 432.0 MiB)
[2023-01-31T04:51:49.226+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:49 INFO BlockManagerInfo: Removed broadcast_31_piece0 on 81d5fcd0285b:32823 in memory (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:51:49.231+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:49 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 285.0 B, free 432.1 MiB)
[2023-01-31T04:51:49.232+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:49 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on 81d5fcd0285b:32823 (size: 285.0 B, free: 434.3 MiB)
[2023-01-31T04:51:49.240+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:49 INFO SparkContext: Created broadcast 37 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2023-01-31T04:51:49.322+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:49 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 81d5fcd0285b:32823 in memory (size: 8.1 KiB, free: 434.3 MiB)
[2023-01-31T04:51:49.379+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:49 INFO BlockManagerInfo: Removed broadcast_36_piece0 on 81d5fcd0285b:32823 in memory (size: 22.6 KiB, free: 434.4 MiB)
[2023-01-31T04:51:49.523+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:49 INFO ShufflePartitionsUtil: For shuffle(2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2023-01-31T04:51:50.563+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:50 INFO CodeGenerator: Code generated in 192.151656 ms
[2023-01-31T04:51:50.762+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:50 INFO CodeGenerator: Code generated in 146.94443 ms
[2023-01-31T04:51:50.864+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:50 INFO CodeGenerator: Code generated in 68.860472 ms
[2023-01-31T04:51:50.922+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:50 INFO CodeGenerator: Code generated in 47.289204 ms
[2023-01-31T04:51:50.987+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:50 INFO CodeGenerator: Code generated in 55.258195 ms
[2023-01-31T04:51:51.167+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:51 INFO CodeGenerator: Code generated in 170.832103 ms
[2023-01-31T04:51:51.369+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:51 INFO CodeGenerator: Code generated in 152.855468 ms
[2023-01-31T04:51:51.548+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:51 INFO CodeGenerator: Code generated in 159.486507 ms
[2023-01-31T04:51:51.758+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:51 INFO CodeGenerator: Code generated in 182.662448 ms
[2023-01-31T04:51:51.881+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:51 INFO CodeGenerator: Code generated in 103.393054 ms
[2023-01-31T04:51:52.074+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:52 INFO CodeGenerator: Code generated in 171.032921 ms
[2023-01-31T04:51:52.442+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:52 INFO CodeGenerator: Code generated in 328.833188 ms
[2023-01-31T04:51:52.643+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:52 INFO CodeGenerator: Code generated in 167.671201 ms
[2023-01-31T04:51:52.905+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:52 INFO CodeGenerator: Code generated in 228.99 ms
[2023-01-31T04:51:53.219+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:53 INFO CodeGenerator: Code generated in 218.016914 ms
[2023-01-31T04:51:53.571+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:53 INFO CodeGenerator: Code generated in 123.024048 ms
[2023-01-31T04:51:54.379+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:54 INFO CodeGenerator: Code generated in 299.636103 ms
[2023-01-31T04:51:54.888+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:54 INFO CodeGenerator: Code generated in 103.616923 ms
[2023-01-31T04:51:55.639+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:55 INFO CodeGenerator: Code generated in 308.909064 ms
[2023-01-31T04:51:56.071+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:56 INFO CodeGenerator: Code generated in 70.743994 ms
[2023-01-31T04:51:56.197+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:56 INFO CodeGenerator: Code generated in 38.802531 ms
[2023-01-31T04:51:56.399+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:56 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 81d5fcd0285b:32823 in memory (size: 8.1 KiB, free: 434.4 MiB)
[2023-01-31T04:51:56.581+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:56 INFO CodeGenerator: Code generated in 108.595644 ms
[2023-01-31T04:51:56.979+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:56 INFO CodeGenerator: Code generated in 90.865525 ms
[2023-01-31T04:51:57.136+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:57 INFO CodeGenerator: Code generated in 61.380384 ms
[2023-01-31T04:51:57.345+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:57 INFO CodeGenerator: Code generated in 62.737805 ms
[2023-01-31T04:51:57.548+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:57 INFO CodeGenerator: Code generated in 72.985446 ms
[2023-01-31T04:51:57.787+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:57 INFO CodeGenerator: Code generated in 82.239112 ms
[2023-01-31T04:51:58.044+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:58 INFO CodeGenerator: Code generated in 92.212975 ms
[2023-01-31T04:51:58.377+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:58 INFO CodeGenerator: Code generated in 161.782641 ms
[2023-01-31T04:51:59.688+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:59 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-31T04:51:59.718+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:59 INFO DAGScheduler: Got job 28 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:51:59.719+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:59 INFO DAGScheduler: Final stage: ResultStage 46 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:51:59.721+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:59 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 32, ShuffleMapStage 33, ShuffleMapStage 34, ShuffleMapStage 35, ShuffleMapStage 36, ShuffleMapStage 37, ShuffleMapStage 38, ShuffleMapStage 39, ShuffleMapStage 40, ShuffleMapStage 41, ShuffleMapStage 42, ShuffleMapStage 43, ShuffleMapStage 44, ShuffleMapStage 45, ShuffleMapStage 31)
[2023-01-31T04:51:59.722+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:59 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:51:59.725+0000] {spark_submit.py:495} INFO - 23/01/31 04:51:59 INFO DAGScheduler: Submitting ResultStage 46 (MapPartitionsRDD[224] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:52:00.029+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:00 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 276.5 KiB, free 431.9 MiB)
[2023-01-31T04:52:00.102+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:00 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 84.7 KiB, free 431.8 MiB)
[2023-01-31T04:52:00.119+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:00 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on 81d5fcd0285b:32823 (size: 84.7 KiB, free: 434.3 MiB)
[2023-01-31T04:52:00.132+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:00 INFO SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:52:00.153+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 46 (MapPartitionsRDD[224] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:52:00.168+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:00 INFO TaskSchedulerImpl: Adding task set 46.0 with 1 tasks resource profile 0
[2023-01-31T04:52:00.179+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:00 INFO TaskSetManager: Starting task 0.0 in stage 46.0 (TID 28) (81d5fcd0285b, executor driver, partition 0, NODE_LOCAL, 8781 bytes) taskResourceAssignments Map()
[2023-01-31T04:52:00.180+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:00 INFO Executor: Running task 0.0 in stage 46.0 (TID 28)
[2023-01-31T04:52:01.464+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:01 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:01.465+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
[2023-01-31T04:52:01.558+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:01 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:01.561+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 14 ms
[2023-01-31T04:52:01.647+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:01 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:01.649+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
[2023-01-31T04:52:01.699+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:01 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:01.701+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
[2023-01-31T04:52:01.747+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:01 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:01.750+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
[2023-01-31T04:52:01.792+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:01 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:01.795+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
[2023-01-31T04:52:01.825+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:01 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:01.827+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
[2023-01-31T04:52:01.855+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:01 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:01.857+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
[2023-01-31T04:52:01.875+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:01 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:01.877+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
[2023-01-31T04:52:01.908+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:01 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:01.910+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
[2023-01-31T04:52:01.946+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:01 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:01.948+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
[2023-01-31T04:52:02.007+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:02 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:02.010+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
[2023-01-31T04:52:02.036+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:02 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:02.037+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
[2023-01-31T04:52:02.065+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:02 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:02.066+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
[2023-01-31T04:52:02.085+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:02 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:02.092+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
[2023-01-31T04:52:02.197+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:02 INFO CodeGenerator: Code generated in 43.522221 ms
[2023-01-31T04:52:03.338+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:03 INFO Executor: Finished task 0.0 in stage 46.0 (TID 28). 25165 bytes result sent to driver
[2023-01-31T04:52:03.356+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:03 INFO TaskSetManager: Finished task 0.0 in stage 46.0 (TID 28) in 3181 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:52:03.360+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:03 INFO TaskSchedulerImpl: Removed TaskSet 46.0, whose tasks have all completed, from pool
[2023-01-31T04:52:03.362+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:03 INFO DAGScheduler: ResultStage 46 (save at BigQueryWriteHelper.java:105) finished in 3.628 s
[2023-01-31T04:52:03.363+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:03 INFO DAGScheduler: Job 28 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:52:03.364+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 46: Stage finished
[2023-01-31T04:52:03.364+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:03 INFO DAGScheduler: Job 28 finished: save at BigQueryWriteHelper.java:105, took 3.672296 s
[2023-01-31T04:52:03.653+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:03 INFO DAGScheduler: Registering RDD 225 (save at BigQueryWriteHelper.java:105) as input to shuffle 18
[2023-01-31T04:52:03.654+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:03 INFO DAGScheduler: Got map stage job 29 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:52:03.697+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:03 INFO DAGScheduler: Final stage: ShuffleMapStage 62 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:52:03.698+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:03 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 47, ShuffleMapStage 48, ShuffleMapStage 49, ShuffleMapStage 50, ShuffleMapStage 51, ShuffleMapStage 52, ShuffleMapStage 53, ShuffleMapStage 54, ShuffleMapStage 55, ShuffleMapStage 56, ShuffleMapStage 57, ShuffleMapStage 58, ShuffleMapStage 59, ShuffleMapStage 60, ShuffleMapStage 61)
[2023-01-31T04:52:03.701+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:03 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:52:03.713+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:03 INFO DAGScheduler: Submitting ShuffleMapStage 62 (MapPartitionsRDD[225] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:52:04.099+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:04 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 276.5 KiB, free 431.5 MiB)
[2023-01-31T04:52:04.295+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:04 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 84.8 KiB, free 431.5 MiB)
[2023-01-31T04:52:04.297+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:04 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on 81d5fcd0285b:32823 (size: 84.8 KiB, free: 434.2 MiB)
[2023-01-31T04:52:04.299+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:04 INFO SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:52:04.300+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:04 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 62 (MapPartitionsRDD[225] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:52:04.301+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:04 INFO TaskSchedulerImpl: Adding task set 62.0 with 1 tasks resource profile 0
[2023-01-31T04:52:04.305+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:04 INFO TaskSetManager: Starting task 0.0 in stage 62.0 (TID 29) (81d5fcd0285b, executor driver, partition 0, NODE_LOCAL, 8770 bytes) taskResourceAssignments Map()
[2023-01-31T04:52:04.307+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:04 INFO Executor: Running task 0.0 in stage 62.0 (TID 29)
[2023-01-31T04:52:04.929+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:04 INFO BlockManagerInfo: Removed broadcast_38_piece0 on 81d5fcd0285b:32823 in memory (size: 84.7 KiB, free: 434.3 MiB)
[2023-01-31T04:52:05.371+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:05 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:05.372+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
[2023-01-31T04:52:05.402+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:05 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:05.406+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
[2023-01-31T04:52:05.429+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:05 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:05.432+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
[2023-01-31T04:52:05.445+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:05 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:05.446+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
[2023-01-31T04:52:05.468+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:05 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:05.470+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
[2023-01-31T04:52:05.492+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:05 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:05.495+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
[2023-01-31T04:52:05.513+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:05 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:05.514+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
[2023-01-31T04:52:05.524+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:05 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:05.524+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2023-01-31T04:52:05.534+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:05 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:05.537+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
[2023-01-31T04:52:05.558+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:05 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:05.559+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
[2023-01-31T04:52:05.585+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:05 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:05.589+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
[2023-01-31T04:52:05.607+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:05 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:05.609+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
[2023-01-31T04:52:05.940+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:05 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:05.943+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 16 ms
[2023-01-31T04:52:06.075+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:06 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:06.076+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 38 ms
[2023-01-31T04:52:06.144+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:06 INFO ShuffleBlockFetcherIterator: Getting 1 (264.0 B) non-empty blocks including 1 (264.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:06.149+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 17 ms
[2023-01-31T04:52:06.738+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:06 INFO Executor: Finished task 0.0 in stage 62.0 (TID 29). 24984 bytes result sent to driver
[2023-01-31T04:52:06.791+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:06 INFO TaskSetManager: Finished task 0.0 in stage 62.0 (TID 29) in 2487 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:52:06.821+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:06 INFO TaskSchedulerImpl: Removed TaskSet 62.0, whose tasks have all completed, from pool
[2023-01-31T04:52:06.824+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:06 INFO DAGScheduler: ShuffleMapStage 62 (save at BigQueryWriteHelper.java:105) finished in 2.991 s
[2023-01-31T04:52:06.825+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:06 INFO DAGScheduler: looking for newly runnable stages
[2023-01-31T04:52:06.826+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:06 INFO DAGScheduler: running: HashSet()
[2023-01-31T04:52:06.826+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:06 INFO DAGScheduler: waiting: HashSet()
[2023-01-31T04:52:06.827+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:06 INFO DAGScheduler: failed: HashSet()
[2023-01-31T04:52:06.847+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:06 INFO ShufflePartitionsUtil: For shuffle(18), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2023-01-31T04:52:07.145+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:07 INFO CodeGenerator: Code generated in 204.904858 ms
[2023-01-31T04:52:07.152+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:07 INFO BlockManagerInfo: Removed broadcast_39_piece0 on 81d5fcd0285b:32823 in memory (size: 84.8 KiB, free: 434.4 MiB)
[2023-01-31T04:52:07.983+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:07 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105
[2023-01-31T04:52:08.003+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:08 INFO DAGScheduler: Got job 30 (save at BigQueryWriteHelper.java:105) with 1 output partitions
[2023-01-31T04:52:08.004+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:08 INFO DAGScheduler: Final stage: ResultStage 79 (save at BigQueryWriteHelper.java:105)
[2023-01-31T04:52:08.005+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:08 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 78)
[2023-01-31T04:52:08.005+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:08 INFO DAGScheduler: Missing parents: List()
[2023-01-31T04:52:08.015+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:08 INFO DAGScheduler: Submitting ResultStage 79 (MapPartitionsRDD[227] at save at BigQueryWriteHelper.java:105), which has no missing parents
[2023-01-31T04:52:08.419+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:08 INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 349.2 KiB, free 431.8 MiB)
[2023-01-31T04:52:08.432+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:08 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 109.6 KiB, free 431.7 MiB)
[2023-01-31T04:52:08.442+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:08 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on 81d5fcd0285b:32823 (size: 109.6 KiB, free: 434.3 MiB)
[2023-01-31T04:52:08.445+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:08 INFO SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:1513
[2023-01-31T04:52:08.446+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 79 (MapPartitionsRDD[227] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))
[2023-01-31T04:52:08.447+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:08 INFO TaskSchedulerImpl: Adding task set 79.0 with 1 tasks resource profile 0
[2023-01-31T04:52:08.453+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:08 INFO TaskSetManager: Starting task 0.0 in stage 79.0 (TID 30) (81d5fcd0285b, executor driver, partition 0, NODE_LOCAL, 7399 bytes) taskResourceAssignments Map()
[2023-01-31T04:52:08.454+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:08 INFO Executor: Running task 0.0 in stage 79.0 (TID 30)
[2023-01-31T04:52:08.917+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:08 INFO ShuffleBlockFetcherIterator: Getting 1 (582.0 B) non-empty blocks including 1 (582.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-01-31T04:52:08.918+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:08 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
[2023-01-31T04:52:08.972+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T04:52:08.973+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T04:52:08.982+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:08 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T04:52:08.984+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-01-31T04:52:08.985+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-01-31T04:52:09.105+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2023-01-31T04:52:09.131+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:09 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T04:52:09.153+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:09 INFO CodecConfig: Compression: SNAPPY
[2023-01-31T04:52:09.382+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:09 INFO ParquetOutputFormat: Parquet block size to 134217728
[2023-01-31T04:52:09.383+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:09 INFO ParquetOutputFormat: Validation is off
[2023-01-31T04:52:09.383+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:09 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-01-31T04:52:09.383+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:09 INFO ParquetOutputFormat: Parquet properties are:
[2023-01-31T04:52:09.384+0000] {spark_submit.py:495} INFO - Parquet page size to 1048576
[2023-01-31T04:52:09.384+0000] {spark_submit.py:495} INFO - Parquet dictionary page size to 1048576
[2023-01-31T04:52:09.384+0000] {spark_submit.py:495} INFO - Dictionary is true
[2023-01-31T04:52:09.386+0000] {spark_submit.py:495} INFO - Writer version is: PARQUET_1_0
[2023-01-31T04:52:09.387+0000] {spark_submit.py:495} INFO - Page size checking is: estimated
[2023-01-31T04:52:09.387+0000] {spark_submit.py:495} INFO - Min row count for page size check is: 100
[2023-01-31T04:52:09.387+0000] {spark_submit.py:495} INFO - Max row count for page size check is: 10000
[2023-01-31T04:52:09.387+0000] {spark_submit.py:495} INFO - Truncate length for column indexes is: 64
[2023-01-31T04:52:09.388+0000] {spark_submit.py:495} INFO - Truncate length for statistics min/max  is: 2147483647
[2023-01-31T04:52:09.388+0000] {spark_submit.py:495} INFO - Bloom filter enabled: false
[2023-01-31T04:52:09.388+0000] {spark_submit.py:495} INFO - Max Bloom filter size for a column is 1048576
[2023-01-31T04:52:09.389+0000] {spark_submit.py:495} INFO - Bloom filter expected number of distinct values are: null
[2023-01-31T04:52:09.414+0000] {spark_submit.py:495} INFO - Page row count limit to 20000
[2023-01-31T04:52:09.414+0000] {spark_submit.py:495} INFO - Writing page checksums is: on
[2023-01-31T04:52:10.103+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:10 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2023-01-31T04:52:10.103+0000] {spark_submit.py:495} INFO - {
[2023-01-31T04:52:10.103+0000] {spark_submit.py:495} INFO - "type" : "struct",
[2023-01-31T04:52:10.104+0000] {spark_submit.py:495} INFO - "fields" : [ {
[2023-01-31T04:52:10.104+0000] {spark_submit.py:495} INFO - "name" : "measured_at",
[2023-01-31T04:52:10.104+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T04:52:10.105+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T04:52:10.105+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:52:10.105+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:52:10.106+0000] {spark_submit.py:495} INFO - "name" : "created_at",
[2023-01-31T04:52:10.106+0000] {spark_submit.py:495} INFO - "type" : "timestamp",
[2023-01-31T04:52:10.106+0000] {spark_submit.py:495} INFO - "nullable" : false,
[2023-01-31T04:52:10.106+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:52:10.107+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:52:10.107+0000] {spark_submit.py:495} INFO - "name" : "Biomass - Actual Aggregated",
[2023-01-31T04:52:10.107+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:52:10.113+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:52:10.113+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:52:10.113+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:52:10.114+0000] {spark_submit.py:495} INFO - "name" : "Fossil Brown coal/Lignite - Actual Aggregated",
[2023-01-31T04:52:10.114+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:52:10.114+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:52:10.115+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:52:10.115+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:52:10.115+0000] {spark_submit.py:495} INFO - "name" : "Fossil Gas - Actual Aggregated",
[2023-01-31T04:52:10.115+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:52:10.116+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:52:10.116+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:52:10.132+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:52:10.133+0000] {spark_submit.py:495} INFO - "name" : "Fossil Hard coal - Actual Aggregated",
[2023-01-31T04:52:10.133+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:52:10.134+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:52:10.134+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:52:10.134+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:52:10.134+0000] {spark_submit.py:495} INFO - "name" : "Fossil Oil - Actual Aggregated",
[2023-01-31T04:52:10.135+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:52:10.135+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:52:10.135+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:52:10.136+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:52:10.136+0000] {spark_submit.py:495} INFO - "name" : "Geothermal - Actual Aggregated",
[2023-01-31T04:52:10.136+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:52:10.157+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:52:10.157+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:52:10.157+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:52:10.158+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Aggregated",
[2023-01-31T04:52:10.158+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:52:10.158+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:52:10.159+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:52:10.159+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:52:10.159+0000] {spark_submit.py:495} INFO - "name" : "Hydro Pumped Storage - Actual Consumption",
[2023-01-31T04:52:10.159+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:52:10.160+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:52:10.160+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:52:10.176+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:52:10.177+0000] {spark_submit.py:495} INFO - "name" : "Hydro Run-of-river and poundage - Actual Aggregated",
[2023-01-31T04:52:10.177+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:52:10.178+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:52:10.178+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:52:10.178+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:52:10.178+0000] {spark_submit.py:495} INFO - "name" : "Hydro Water Reservoir - Actual Aggregated",
[2023-01-31T04:52:10.179+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:52:10.179+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:52:10.179+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:52:10.179+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:52:10.180+0000] {spark_submit.py:495} INFO - "name" : "Nuclear - Actual Aggregated",
[2023-01-31T04:52:10.180+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:52:10.180+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:52:10.191+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:52:10.191+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:52:10.192+0000] {spark_submit.py:495} INFO - "name" : "Other - Actual Aggregated",
[2023-01-31T04:52:10.192+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:52:10.192+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:52:10.193+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:52:10.193+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:52:10.193+0000] {spark_submit.py:495} INFO - "name" : "Other renewable - Actual Aggregated",
[2023-01-31T04:52:10.193+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:52:10.194+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:52:10.203+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:52:10.204+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:52:10.204+0000] {spark_submit.py:495} INFO - "name" : "Solar - Actual Aggregated",
[2023-01-31T04:52:10.208+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:52:10.208+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:52:10.213+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:52:10.216+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:52:10.217+0000] {spark_submit.py:495} INFO - "name" : "Waste - Actual Aggregated",
[2023-01-31T04:52:10.218+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:52:10.221+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:52:10.224+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:52:10.226+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:52:10.228+0000] {spark_submit.py:495} INFO - "name" : "Wind Offshore - Actual Aggregated",
[2023-01-31T04:52:10.231+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:52:10.232+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:52:10.234+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:52:10.237+0000] {spark_submit.py:495} INFO - }, {
[2023-01-31T04:52:10.240+0000] {spark_submit.py:495} INFO - "name" : "Wind Onshore - Actual Aggregated",
[2023-01-31T04:52:10.241+0000] {spark_submit.py:495} INFO - "type" : "double",
[2023-01-31T04:52:10.244+0000] {spark_submit.py:495} INFO - "nullable" : true,
[2023-01-31T04:52:10.246+0000] {spark_submit.py:495} INFO - "metadata" : { }
[2023-01-31T04:52:10.248+0000] {spark_submit.py:495} INFO - } ]
[2023-01-31T04:52:10.250+0000] {spark_submit.py:495} INFO - }
[2023-01-31T04:52:10.252+0000] {spark_submit.py:495} INFO - and corresponding Parquet message type:
[2023-01-31T04:52:10.254+0000] {spark_submit.py:495} INFO - message spark_schema {
[2023-01-31T04:52:10.255+0000] {spark_submit.py:495} INFO - required int96 measured_at;
[2023-01-31T04:52:10.259+0000] {spark_submit.py:495} INFO - required int96 created_at;
[2023-01-31T04:52:10.260+0000] {spark_submit.py:495} INFO - optional double Biomass - Actual Aggregated;
[2023-01-31T04:52:10.262+0000] {spark_submit.py:495} INFO - optional double Fossil Brown coal/Lignite - Actual Aggregated;
[2023-01-31T04:52:10.264+0000] {spark_submit.py:495} INFO - optional double Fossil Gas - Actual Aggregated;
[2023-01-31T04:52:10.265+0000] {spark_submit.py:495} INFO - optional double Fossil Hard coal - Actual Aggregated;
[2023-01-31T04:52:10.268+0000] {spark_submit.py:495} INFO - optional double Fossil Oil - Actual Aggregated;
[2023-01-31T04:52:10.269+0000] {spark_submit.py:495} INFO - optional double Geothermal - Actual Aggregated;
[2023-01-31T04:52:10.271+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Aggregated;
[2023-01-31T04:52:10.274+0000] {spark_submit.py:495} INFO - optional double Hydro Pumped Storage - Actual Consumption;
[2023-01-31T04:52:10.277+0000] {spark_submit.py:495} INFO - optional double Hydro Run-of-river and poundage - Actual Aggregated;
[2023-01-31T04:52:10.278+0000] {spark_submit.py:495} INFO - optional double Hydro Water Reservoir - Actual Aggregated;
[2023-01-31T04:52:10.281+0000] {spark_submit.py:495} INFO - optional double Nuclear - Actual Aggregated;
[2023-01-31T04:52:10.283+0000] {spark_submit.py:495} INFO - optional double Other - Actual Aggregated;
[2023-01-31T04:52:10.286+0000] {spark_submit.py:495} INFO - optional double Other renewable - Actual Aggregated;
[2023-01-31T04:52:10.288+0000] {spark_submit.py:495} INFO - optional double Solar - Actual Aggregated;
[2023-01-31T04:52:10.290+0000] {spark_submit.py:495} INFO - optional double Waste - Actual Aggregated;
[2023-01-31T04:52:10.292+0000] {spark_submit.py:495} INFO - optional double Wind Offshore - Actual Aggregated;
[2023-01-31T04:52:10.293+0000] {spark_submit.py:495} INFO - optional double Wind Onshore - Actual Aggregated;
[2023-01-31T04:52:10.293+0000] {spark_submit.py:495} INFO - }
[2023-01-31T04:52:10.294+0000] {spark_submit.py:495} INFO - 
[2023-01-31T04:52:10.294+0000] {spark_submit.py:495} INFO - 
[2023-01-31T04:52:11.360+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:11 INFO CodecPool: Got brand-new compressor [.snappy]
[2023-01-31T04:52:23.059+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:23 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675140543041-2f0e8f48-230a-4a3a-9a22-f3ecf288a02c/_temporary/0/_temporary/' directory.
[2023-01-31T04:52:23.061+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:23 INFO FileOutputCommitter: Saved output of task 'attempt_202301310452075291186680503327455_0079_m_000000_30' to gs://entsoe_temp_1009/.spark-bigquery-local-1675140543041-2f0e8f48-230a-4a3a-9a22-f3ecf288a02c/_temporary/0/task_202301310452075291186680503327455_0079_m_000000
[2023-01-31T04:52:23.065+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:23 INFO SparkHadoopMapRedUtil: attempt_202301310452075291186680503327455_0079_m_000000_30: Committed. Elapsed time: 1990 ms.
[2023-01-31T04:52:23.200+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:23 INFO Executor: Finished task 0.0 in stage 79.0 (TID 30). 26967 bytes result sent to driver
[2023-01-31T04:52:23.271+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:23 INFO TaskSetManager: Finished task 0.0 in stage 79.0 (TID 30) in 14821 ms on 81d5fcd0285b (executor driver) (1/1)
[2023-01-31T04:52:23.275+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:23 INFO DAGScheduler: ResultStage 79 (save at BigQueryWriteHelper.java:105) finished in 15.243 s
[2023-01-31T04:52:23.276+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:23 INFO DAGScheduler: Job 30 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-01-31T04:52:23.282+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:23 INFO TaskSchedulerImpl: Removed TaskSet 79.0, whose tasks have all completed, from pool
[2023-01-31T04:52:23.297+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 79: Stage finished
[2023-01-31T04:52:23.297+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:23 INFO DAGScheduler: Job 30 finished: save at BigQueryWriteHelper.java:105, took 15.312014 s
[2023-01-31T04:52:23.308+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:23 INFO FileFormatWriter: Start to commit write Job 7d0db4e2-3a16-4d53-82af-34141e29ef38.
[2023-01-31T04:52:24.908+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:24 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675140543041-2f0e8f48-230a-4a3a-9a22-f3ecf288a02c/_temporary/0/task_202301310452075291186680503327455_0079_m_000000/' directory.
[2023-01-31T04:52:25.669+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:25 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://entsoe_temp_1009/.spark-bigquery-local-1675140543041-2f0e8f48-230a-4a3a-9a22-f3ecf288a02c/' directory.
[2023-01-31T04:52:26.465+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:26 INFO FileFormatWriter: Write Job 7d0db4e2-3a16-4d53-82af-34141e29ef38 committed. Elapsed time: 3139 ms.
[2023-01-31T04:52:26.493+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:26 INFO FileFormatWriter: Finished processing stats for write job 7d0db4e2-3a16-4d53-82af-34141e29ef38.
[2023-01-31T04:52:31.077+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:31 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=entsoe_analytics_1009, projectId=rafzul-analytics-1009, tableId=TEST_total_generation_staging}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_APPEND, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=null, ignoreUnknownValue=null, sourceUris=[gs://entsoe_temp_1009/.spark-bigquery-local-1675140543041-2f0e8f48-230a-4a3a-9a22-f3ecf288a02c/part-00000-bd7a1fbb-5aa8-4c58-9b35-56c6ea649897-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=true, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=rafzul-analytics-1009, job=a660f8a0-800e-4256-8d93-2af60c4da0b8, location=US}
[2023-01-31T04:52:35.201+0000] {spark_submit.py:495} INFO - 23/01/31 04:52:35 INFO BigQueryClient: Done loading to rafzul-analytics-1009.entsoe_analytics_1009.TEST_total_generation_staging. jobId: JobId{project=rafzul-analytics-1009, job=a660f8a0-800e-4256-8d93-2af60c4da0b8, location=US}
[2023-01-31T04:52:44.703+0000] {spark_submit.py:495} INFO - MANTEP!!!!
[2023-01-31T04:53:00.813+0000] {spark_submit.py:495} INFO - 23/01/31 04:53:00 INFO SparkContext: Invoking stop() from shutdown hook
[2023-01-31T04:53:02.347+0000] {spark_submit.py:495} INFO - 23/01/31 04:53:02 INFO SparkUI: Stopped Spark web UI at http://81d5fcd0285b:4041
[2023-01-31T04:53:02.737+0000] {spark_submit.py:495} INFO - 23/01/31 04:53:02 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-01-31T04:53:05.219+0000] {spark_submit.py:495} INFO - 23/01/31 04:53:05 INFO MemoryStore: MemoryStore cleared
[2023-01-31T04:53:05.224+0000] {spark_submit.py:495} INFO - 23/01/31 04:53:05 INFO BlockManager: BlockManager stopped
[2023-01-31T04:53:05.432+0000] {spark_submit.py:495} INFO - 23/01/31 04:53:05 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-01-31T04:53:05.522+0000] {spark_submit.py:495} INFO - 23/01/31 04:53:05 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-01-31T04:53:05.703+0000] {spark_submit.py:495} INFO - 23/01/31 04:53:05 INFO SparkContext: Successfully stopped SparkContext
[2023-01-31T04:53:05.704+0000] {spark_submit.py:495} INFO - 23/01/31 04:53:05 INFO ShutdownHookManager: Shutdown hook called
[2023-01-31T04:53:05.705+0000] {spark_submit.py:495} INFO - 23/01/31 04:53:05 INFO ShutdownHookManager: Deleting directory /tmp/spark-99a2f3e1-81fc-43b6-bbd5-929e389d5b3b
[2023-01-31T04:53:05.800+0000] {spark_submit.py:495} INFO - 23/01/31 04:53:05 INFO ShutdownHookManager: Deleting directory /tmp/spark-7a081703-ac1c-4588-97fa-7fb5f6aa8b5d
[2023-01-31T04:53:06.053+0000] {spark_submit.py:495} INFO - 23/01/31 04:53:06 INFO ShutdownHookManager: Deleting directory /tmp/spark-7a081703-ac1c-4588-97fa-7fb5f6aa8b5d/pyspark-020a93dc-e7b7-45a3-b746-7bb6b41f37db
[2023-01-31T04:53:07.924+0000] {taskinstance.py:1322} INFO - Marking task as SUCCESS. dag_id=entsoe-energydata, task_id=stage_total_generation, execution_date=20210101T000000, start_date=20230131T044816, end_date=20230131T045307
[2023-01-31T04:53:08.350+0000] {local_task_job.py:159} INFO - Task exited with return code 0
[2023-01-31T04:53:08.561+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
